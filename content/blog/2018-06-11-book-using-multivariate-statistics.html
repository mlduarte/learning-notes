---
title: 'Book: Using Multivariate Statistics'
author: Marie
date: 2018-06-11T13:39:46+02:00
slug: book-using-multivariate-statistics
banner: "img/banners/tabachnick.png"
categories:
  - Study-Notes
tags:
  - Book
  - Study-Notes
---



<div id="principal-components-and-factor-analysis" class="section level1">
<h1>Principal Components and Factor Analysis</h1>
<div id="general-purpose" class="section level2">
<h2>General Purpose</h2>
<ul>
<li>To find subsets of variables (factors/components/latent variables) that are independent, where variables in a subset are correlated</li>
<li>To summarise patterns of correlations among variables</li>
<li>To decrease number of variables to a smaller number of factors/components, by using linear combinations of observed variables (more parsimonious and because factors/components are uncorrelated, more reliable)</li>
<li>To understand underlying processes</li>
<li>To describe (and maybe understand) relationship between variables</li>
<li>To test theory about underlying processes</li>
</ul>
</div>
<div id="steps" class="section level2">
<h2>Steps:</h2>
<ul>
<li>Hypothesise factors underlying domain of interest</li>
<li>Select and measure variables</li>
<li>Prepare correlation matrix</li>
<li>Extract factors/components</li>
<li>Determine number of factors/components</li>
<li>Rotate to increase interpretability</li>
<li>Interpret results</li>
<li>Should make sense</li>
<li>Easier if variables within one factor do not correlate with variables in another factor</li>
<li>Verify factor structure by establishing validity, for example, confirm that factor scores change with experimental conditions as predicted</li>
</ul>
<p>For factor analysis</p>
<ul>
<li>Hypothesise factors underlying domain of interest</li>
<li>Select 5-6 variables thought to be a pure measure of the factor (marker variables); consider variable complexity (i.e the number of factors for which a variable may correlate)</li>
<li>Choose a sample across variables/factors with spread of scores</li>
<li>Be wary to pool results of several samples; samples differing, for example, in SES may also have different factors. Only pool if different samples produce the same factors.</li>
</ul>
</div>
<div id="types-of-factor-analysis" class="section level2">
<h2>Types of Factor Analysis</h2>
<table>
<colgroup>
<col width="56%" />
<col width="43%" />
</colgroup>
<thead>
<tr class="header">
<th>Exploratory</th>
<th>Confirmatory</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Early stages of research</td>
<td>Advanced stages of research</td>
</tr>
<tr class="even">
<td>To summarise data by grouping together correlated variables</td>
<td>To test theory about latent processes</td>
</tr>
<tr class="odd">
<td>Variables may not have been chosen with underlying processes in mind</td>
<td>Variables chosen to reveal underlying processes</td>
</tr>
<tr class="even">
<td>Factor Analysis / PCA</td>
<td>Usually via Structural Equation Modelling (SEM)</td>
</tr>
</tbody>
</table>
</div>
<div id="research-questions" class="section level2">
<h2>Research Questions</h2>
<ul>
<li>How many factors</li>
<li>What do the factors mean</li>
<li>How much variability is accounted for by each/all factors</li>
<li>How well does the factor solution fit that expected</li>
<li>Had factors been measured directly, what scored would subjects have received?</li>
</ul>
</div>
<div id="limitations" class="section level2">
<h2>Limitations</h2>
<ul>
<li>No ready criteria for verification</li>
<li>Infinite rotations; final choice is based on subjective assessment of interpretability</li>
<li>Often used as a last resort to make order from chaos (i.e. suffers from a poor reputation)</li>
<li>Exploratory</li>
<li>Subjective</li>
<li>Sensitive to outliers, missing data, poorly distributed samples, small samples</li>
</ul>
<div id="outliers-among-cases" class="section level3">
<h3>Outliers (among cases)</h3>
<ul>
<li>Univariate and multivariate (combination of variables) outliers will have greater influence on factor solution</li>
<li>TODO chapter 4 and 13.7.4.14 to detect and reduce influence</li>
</ul>
</div>
<div id="outliers-among-variables-fa-only" class="section level3">
<h3>Outliers among variables (FA only)</h3>
<p>If there is a factor with <span class="math inline">\(\leq\)</span> 2 variables and significant variable accounted for:</p>
<ul>
<li>Ignore, or treat with caution</li>
<li>The factor should be researched further with structural equation modelling (SEM)</li>
</ul>
<p>Factors with few variables and small variance accounted for are unreliable.</p>
</div>
<div id="missing-data" class="section level3">
<h3>Missing data</h3>
<p>Estimate if</p>
<ul>
<li>Missing distribution of values not random</li>
<li>Sample size will be too small if delete cases;</li>
</ul>
<p>Otherwise, delete cases</p>
</div>
<div id="poorly-distributed-variables-non-normality" class="section level3">
<h3>Poorly distributed variables (Non-Normality)</h3>
<ul>
<li>If PCA/FA used descriptively, i.e. to summarise relationships, normality assumptions not in force, but solution is enhanced with normality</li>
<li>If statistical inference used to determine number of factors, single variable and multivariate normality (linear combinations of variables are normally distributed) is assumed</li>
<li>If variable has substantial skewness and kurtosis, consider variable transformation</li>
<li>Multivariate normality tests are sensitive</li>
<li>Note that some SEMS permit PCA/FA with non-normal variables</li>
</ul>
</div>
<div id="non-linearity" class="section level3">
<h3>Non-Linearity</h3>
<ul>
<li>Correlations measures linear relationships therefore analysis is degraded by non-linear relationship (as not captured)</li>
<li>Assessed visually using scatterplots</li>
</ul>
</div>
<div id="small-samples" class="section level3">
<h3>Small samples</h3>
<p>Sample size required will depend on population correlations and the number of factors</p>
<table>
<thead>
<tr class="header">
<th>Characteristics</th>
<th>Recommended Sample Size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Communalities &gt; 0.6</td>
<td>&lt; 100</td>
</tr>
<tr class="even">
<td>Communalities ~ 0.5, loadings &gt; 0.8</td>
<td>100-200</td>
</tr>
<tr class="odd">
<td>Low communality; small # factors,3-4 indicators for each factor</td>
<td>&gt; 300</td>
</tr>
<tr class="even">
<td>Low communality, large # weakly determined factors</td>
<td>&gt; 500</td>
</tr>
</tbody>
</table>
</div>
<div id="multicollinearity-and-singularity-fa-only" class="section level3">
<h3>Multicollinearity and Singularity (FA only)</h3>
<p>Although in FA/PCA interest is in finding correlated variables, if correlation is too high, matrix inversion becomes unstable (multicollinearity) or impossible (singularity). If determinant of correlation matrix (<span class="math inline">\(R\)</span>) is close to zero, then multicollinearity or singularity may be present</p>
<p>To detect, look at Squared Multiple Correlation (<strong>SMC</strong>), if SMC is high (&gt; 0.9 multicollinearity, 1=singularity) then delete that variable</p>
</div>
<div id="lack-of-factorability-within-correlation-matrix-r" class="section level3">
<h3>Lack of Factorability within Correlation Matrix (R)</h3>
<p>FA assumes relationships between variables; ; a factorable matrix should have several sizeable correlations. Note however that high bivariate correlations are not a guarantee of factors; they may merely be two similar variables and not reflect an underlying process simultaneously affecting them.</p>
<p>If factors are present, then</p>
<ul>
<li>Look at correlation matrix (<strong>R</strong>): Should expect many sizeable correlations. if no correlation within the correlation matrix (<strong>R</strong>) with exceeds 0.3 then there is probably nothing to factor analyse.<br />
</li>
<li>the anti-image correlations matrix (pairwise correlations adjusted for the effects of all other variables) will have mostly small values among the off-diagonal elements.</li>
<li>Kaiser’s measure of sampling adequacy, <span class="math inline">\(\sum(\text{squared correlations})/(\sum(\text{squared correlations}) + \sum(\text{squared partial correlations}))\)</span> &gt; 0.6</li>
</ul>
</div>
</div>
<div id="fundamental-equations-for-factor-analysis" class="section level2">
<h2>Fundamental Equations for Factor Analysis</h2>
<p>Matrices</p>
<table style="width:100%;">
<colgroup>
<col width="8%" />
<col width="19%" />
<col width="50%" />
<col width="12%" />
<col width="9%" />
</colgroup>
<thead>
<tr class="header">
<th>Label</th>
<th>Matrix Name</th>
<th>Description</th>
<th>Orthogonal</th>
<th>Oblique</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(R\)</span></td>
<td>Correlation</td>
<td>(Observed) Between-variable correlation</td>
<td><i class="fa fa-check-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="even">
<td><span class="math inline">\(Z\)</span></td>
<td>Variable</td>
<td>Standardized observed variable values(scores)</td>
<td><i class="fa fa-check-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(F\)</span></td>
<td>Factor-score</td>
<td>Standardized factor scores</td>
<td><i class="fa fa-check-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A\)</span></td>
<td>Factor loading</td>
<td>Correlations between factor &amp; variable</td>
<td><i class="fa fa-check-square-o"></i></td>
<td><i class="fa fa-square-o"></i></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(A\)</span></td>
<td>Pattern</td>
<td>Contribution of factor to variance</td>
<td><i class="fa fa-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="even">
<td><span class="math inline">\(C\)</span></td>
<td>Structure</td>
<td>Correlations between variables and correlated factors</td>
<td><i class="fa fa-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(B\)</span></td>
<td>Factor-Score Coefficients</td>
<td>Coefficients to generate factor scores from variables</td>
<td><i class="fa fa-check-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\Phi\)</span></td>
<td>Factor correlation</td>
<td>Correlations among factors</td>
<td><i class="fa fa-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(L\)</span></td>
<td>Eigenvalue</td>
<td>Diagonal matrix of eigenvalues, one per factor</td>
<td><i class="fa fa-check-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="even">
<td><span class="math inline">\(V\)</span></td>
<td>Eigenvector</td>
<td>Eigenvectors, one per eigenvalue</td>
<td><i class="fa fa-check-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
</tbody>
</table>
<p><br><br> Equations:</p>
<ul>
<li><strong>Reproduced Correlation Matrix (<span class="math inline">\(\bar{R}\)</span>)</strong>: Factor correlation, <span class="math inline">\(\bar{R} = A A&#39;\)</span></li>
<li><strong>Residual Correlation Matrix (<span class="math inline">\(R_\text{res}\)</span>)</strong>: Observed - Reproduced (small for good factor analysis), <span class="math inline">\(R_\text{res} = R - \bar{R}\)</span></li>
<li><strong>Factor Score Coefficients Matrix (<span class="math inline">\(B\)</span>)</strong>: Coefficients from linear combinations of variables, <span class="math inline">\(B = R^{-1} A\)</span>.</li>
<li><strong>Factor Scores</strong>: <span class="math inline">\(F = ZB\)</span></li>
<li><strong>Predicted Scores</strong>: <span class="math inline">\(Z = FA&#39;\)</span></li>
</ul>
<div id="fundamental-equations-for-factor-analysis-extraction" class="section level3">
<h3>Fundamental Equations for Factor Analysis: Extraction</h3>
<p>The correlation matrix (<span class="math inline">\(R\)</span>), which by nature is symmetric, can be diagonalised by the matrix <span class="math inline">\(V\)</span> (whose columns are the eigenvectors), such that <span class="math display">\[L = V&#39;RV\]</span> where <span class="math inline">\(L\)</span> is a diagonal matrix with values eigenvalues in the diagonal</p>
<p><strong>Example</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skiers</th>
<th align="right">cost</th>
<th align="right">lift</th>
<th align="right">depth</th>
<th align="right">powder</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">S1</td>
<td align="right">32</td>
<td align="right">64</td>
<td align="right">65</td>
<td align="right">67</td>
</tr>
<tr class="even">
<td align="left">S2</td>
<td align="right">61</td>
<td align="right">37</td>
<td align="right">62</td>
<td align="right">65</td>
</tr>
<tr class="odd">
<td align="left">S3</td>
<td align="right">59</td>
<td align="right">40</td>
<td align="right">45</td>
<td align="right">43</td>
</tr>
<tr class="even">
<td align="left">S4</td>
<td align="right">36</td>
<td align="right">62</td>
<td align="right">34</td>
<td align="right">35</td>
</tr>
<tr class="odd">
<td align="left">S5</td>
<td align="right">62</td>
<td align="right">46</td>
<td align="right">43</td>
<td align="right">40</td>
</tr>
<tr class="even">
<td align="left"><br><br></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>Correlation Matrix</p>
<pre class="r"><code>cor.ski &lt;- cor(dplyr::select(dat.ski, -skiers)) 
cor.ski</code></pre>
<pre><code>##               cost        lift       depth      powder
## cost    1.00000000 -0.95299048 -0.05527555 -0.12999882
## lift   -0.95299048  1.00000000 -0.09110654 -0.03624823
## depth  -0.05527555 -0.09110654  1.00000000  0.99017435
## powder -0.12999882 -0.03624823  0.99017435  1.00000000</code></pre>
<p>Strong correlations between:</p>
<ul>
<li>importance placed on <em>cost</em> of ski ticket and speed of ski <em>lift</em></li>
<li>importance placed on snow <em>depth</em> and moisture (<em>powder</em>)</li>
</ul>
<p>Eigenvalues and Eigenvectors:</p>
<pre class="r"><code>eig.ski &lt;- eigen(cor.ski)
eig.ski</code></pre>
<pre><code>## eigen() decomposition
## $values
## [1] 2.016305104 1.941513814 0.037812306 0.004368776
## 
## $vectors
##            [,1]       [,2]       [,3]       [,4]
## [1,]  0.3524130  0.6143298  0.6624913  0.2439451
## [2,] -0.2511248 -0.6637642  0.6758934  0.1988000
## [3,] -0.6273987  0.3222291  0.2754625 -0.6531919
## [4,] -0.6473888  0.2796147 -0.1685044  0.6887014</code></pre>
<p>Note that <span class="math inline">\(L = V&#39;RV&#39;\)</span></p>
<pre class="r"><code>zapsmall(t(eig.ski$vectors) %*% cor.ski %*% eig.ski$vectors)</code></pre>
<pre><code>##          [,1]     [,2]      [,3]      [,4]
## [1,] 2.016305 0.000000 0.0000000 0.0000000
## [2,] 0.000000 1.941514 0.0000000 0.0000000
## [3,] 0.000000 0.000000 0.0378123 0.0000000
## [4,] 0.000000 0.000000 0.0000000 0.0043688</code></pre>
<pre class="r"><code>diag(eig.ski$values)</code></pre>
<pre><code>##          [,1]     [,2]       [,3]        [,4]
## [1,] 2.016305 0.000000 0.00000000 0.000000000
## [2,] 0.000000 1.941514 0.00000000 0.000000000
## [3,] 0.000000 0.000000 0.03781231 0.000000000
## [4,] 0.000000 0.000000 0.00000000 0.004368776</code></pre>
<p>and that pre-and post-multiplying the correlation matrix by eigenvectors does not change it so much as repackage it: <span class="math display">\[V&#39;V = I\]</span></p>
<pre class="r"><code>zapsmall(crossprod(eig.ski$vectors))</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]    1    0    0    0
## [2,]    0    1    0    0
## [3,]    0    0    1    0
## [4,]    0    0    0    1</code></pre>
<p>Rearranging the equation <span class="math inline">\(L=V&#39;RV\)</span>,</p>
<p><span class="math display">\[\begin{align}
  R &amp;= VLV&#39;\\
    &amp;= V\sqrt{L}\sqrt{L} V&#39;\\
    &amp;=(V\sqrt{L})(\sqrt{L}V&#39;) \\
    &amp;= A A&#39;
    \end{align}\]</span> where <span class="math inline">\(A=V\sqrt{L}\)</span> is referred to as the factor loading matrix; the correlation matrix is a product of the factor loading matrix <span class="math inline">\(A\)</span> and its transpose.</p>
<p>In this case, the (unrotated) factor loading matrix is:</p>
<pre class="r"><code>eig.ski$vectors %*% sqrt(diag(eig.ski$values))</code></pre>
<pre><code>##            [,1]       [,2]        [,3]        [,4]
## [1,]  0.5004147  0.8559962  0.12882400  0.01612397
## [2,] -0.3565888 -0.9248772  0.13143009  0.01314003
## [3,] -0.8908852  0.4489882  0.05356475 -0.04317384
## [4,] -0.9192705  0.3896101 -0.03276633  0.04552090</code></pre>
<p>The correlation matrix can therefore be considered a product of two matrices - each a combination of eigenvectors and the square root of eigenvalues</p>
<ul>
<li>Because there are 4 variables, there are 4 eigenvalues</li>
<li>Each eigenvalue corresponds to a different potential factor; usually only factors with large eigenvalues are retrained</li>
<li>In good FA, a few factors will almost duplicate correlation matrix</li>
<li>In this example, only first 2 factors, with eigenvalues &gt; 1 are large enough</li>
</ul>
<p>Starting with the correlation matrix <span class="math inline">\(R\)</span> and assuming <span class="math inline">\(k\)</span> factors are used, the steps of factor analysis (principal factor solution method) are as follows:</p>
<ol style="list-style-type: decimal">
<li>Get eigenvalues <span class="math inline">\(L\)</span> and eigenvectors <span class="math inline">\(V\)</span> of correlation matrix <span class="math inline">\(r\)</span></li>
<li>Calculate <span class="math inline">\(C = \sum \text{diag}(R)\)</span></li>
<li>Calculate the loadings, <span class="math inline">\(A = V[,1:k]\sqrt{L[1:k]}\)</span>, (eq 13.6 of text)</li>
<li>Set <span class="math inline">\(R^* = AA&#39;\)</span> (eq 13.5 of text, R=AA’)</li>
<li>Set <span class="math inline">\(C^* = \sum \text{diag} (R^*)\)</span></li>
<li>Update <span class="math inline">\(\text{diag}(R) = \text{diag}(R^*)\)</span> Repeat above steps until max iterations reached, or until <span class="math inline">\(e = |C-C^*)|\)</span> is smaller than some threshold</li>
</ol>
<pre class="r"><code># This is taken from the function fac and is replicated using 
#fit &lt;- fa(cor.ski, nfactors=2, fm=&quot;pa&quot;)
#fit # print results

# Initialisation
r &lt;- cor(dplyr::select(dat.ski, -skiers)) 
n &lt;- dim(r)[2]
r.mat &lt;- r
colnames(r.mat) &lt;- rownames(r.mat) &lt;- colnames(r)
nfactors &lt;- 2
max.iter &lt;- 50
min.err &lt;- 0.001

orig &lt;- diag(r)
comm &lt;- sum(diag(r.mat))
err &lt;- comm
i &lt;- 1
comm.list &lt;- list()

e.values &lt;- eigen(r)$values

# Loop
while (err &gt; min.err) {
  eigens &lt;- eigen(r.mat, symmetric = TRUE)
  if (nfactors &gt; 1) {
    loadings &lt;- eigens$vectors[, 1:nfactors] %*% 
      diag(sqrt(eigens$values[1:nfactors]))
  }
  else {
    loadings &lt;- eigens$vectors[, 1] * sqrt(eigens$values[1]) #A in book
  }
  model &lt;- loadings %*% t(loadings) # eqn 13.5: R = AA&#39; (if r is n x n this will always be an n x n matrix, because n x k %*% k x n gives n x n matrix)
  new &lt;- diag(model)
  comm1 &lt;- sum(new)
  diag(r.mat) &lt;- new                # update diagonals of correlation matrix
  err &lt;- abs(comm - comm1)
  if (is.na(err)) {
    warning(&quot;imaginary eigen value condition encountered in fa\n Try again with SMC=FALSE \n exiting fa&quot;)
    break
  }
  comm &lt;- comm1
  comm.list[[i]] &lt;- comm1
  i &lt;- i + 1
  if (i &gt; max.iter) {
    if (warnings) {
      message(&quot;maximum iteration exceeded&quot;)
    }
    err &lt;- 0
  }
}

# Clean-up
eigenv &lt;- eigens$vectors
eigens &lt;- eigens$values
if (nfactors &gt; 1) {
  sign.tot &lt;- vector(mode = &quot;numeric&quot;, length = nfactors)
  sign.tot &lt;- sign(colSums(loadings))
  sign.tot[sign.tot == 0] &lt;- 1
  loadings &lt;- loadings %*% diag(sign.tot)
} else {
  if (sum(loadings) &lt; 0) {
    loadings &lt;- -as.matrix(loadings)
  }
  else {
    loadings &lt;- as.matrix(loadings)
  }
  colnames(loadings) &lt;- &quot;MR1&quot;
}
colnames(loadings) &lt;- paste(&quot;PA&quot;, 1:nfactors, sep = &quot;&quot;)
rownames(loadings) &lt;- rownames(r)
loadings[loadings == 0] &lt;- 10^-15
model &lt;- loadings %*% t(loadings)
f.loadings &lt;- loadings</code></pre>
<p>After iterating through the Factor Analysis 8 times we get the following eigenvectors:</p>
<pre><code>##       V1     V2
## 1  0.284  0.649
## 2 -0.179 -0.686
## 3 -0.657  0.253
## 4 -0.675  0.209</code></pre>
<p>and eigenvalues</p>
<pre><code>## [1] 2.005 1.911</code></pre>
<p>Note that the property, <span class="math inline">\(VV&#39; = I\)</span>, is maintained.</p>
<pre class="r"><code>crossprod(eigenv[, 1:nfactors])</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1</code></pre>
<p>The factor loading matrix, <span class="math inline">\(A = V\sqrt{L}\)</span>, is</p>
<pre><code>##           PA1    PA2
## cost   -0.403  0.898
## lift    0.254 -0.948
## depth   0.930  0.350
## powder  0.956  0.289</code></pre>
<p>The factor loading matrix lists the correlations between factors and variables:</p>
<ul>
<li>Factor 1 reflects snow conditions</li>
<li>Factor 2 reflects resort conditions; people who score high on this factor place a lot of importance on the cost of a ski ticket without concern for the lift speeds whereas those that score low on this factor value lift speeds more than ski ticket costs.</li>
</ul>
</div>
</div>
<div id="orthogonal-rotation" class="section level2">
<h2>Orthogonal Rotation</h2>
<p>When factors are uncorrelated; produces a loading matrix (correlations between factors and variables)</p>
<p>Rotation used to</p>
<ul>
<li>Maximise high correlations between factors and variables</li>
<li>Minimise low correlations between factors and variables</li>
</ul>
<p><span class="math display">\[\begin{align}
  \text{Rotated Loading Matrix} &amp;= (\text{Unrotated Loading Matrix})(\text{Transformation Matrix})\\
    A_\text{Rotated} &amp;= A_\text{Unrotated}\Lambda
    \end{align}\]</span></p>
<p>The transformation matrix is a matrix of sines and cosines of an angle <span class="math inline">\(\psi\)</span>. In this example an angle of <span class="math inline">\(\psi = 19^\circ\)</span> is used, with <span class="math display">\[\Lambda = \begin{bmatrix}\cos \psi &amp; -\sin \psi\\ \sin \psi &amp; \cos \psi\end{bmatrix}\]</span></p>
<pre class="r"><code>rotated &lt;- stats::varimax(loadings)
loadings_rotated &lt;- rotated$loadings
rot.mat &lt;- rotated$rotmat
rot.mat  # transformation matrix</code></pre>
<pre><code>##           [,1]       [,2]
## [1,] 0.9418515 -0.3360294
## [2,] 0.3360294  0.9418515</code></pre>
<p>The communalities, variance and covariance are shown in the following table:</p>
<pre class="r"><code>fit &lt;- fa(cor.ski, nfactors=2, fm=&quot;pa&quot;, rotate=&quot;varimax&quot;)
loadings_sol &lt;- unclass(fit$loadings)
ssls &lt;- apply(loadings_sol, 2, function(x) sum(x^2)) # row-total
comms &lt;- apply(loadings_sol, 1, function(x) sum(x^2)) # column-total
prop_var &lt;- ssls/nrow(loadings_sol)
prop_cov &lt;- ssls/sum(comms)

res &lt;- rbind(loadings_sol, ssls, prop_var, prop_cov)
res &lt;- data.frame(var = rownames(res), res) %&gt;% mutate(communalities = ifelse(var %in% c(&#39;cost&#39;, &#39;lift&#39;, &#39;depth&#39;, &#39;powder&#39;), PA1^2 + PA2^2, PA1 + PA2))
res</code></pre>
<pre><code>##        var         PA1         PA2 communalities
## 1     cost -0.07757836 -0.98203830     0.9704176
## 2     lift -0.07933109  0.97641250     0.9596748
## 3    depth  0.99387040 -0.01742487     0.9880820
## 4   powder  0.99708024  0.04875513     0.9965461
## 5     ssls  1.99425920  1.92046127     3.9147205
## 6 prop_var  0.49856480  0.48011532     0.9786801
## 7 prop_cov  0.50942570  0.49057430     1.0000000</code></pre>
<p>The <strong>reproduced correlation matrix</strong> is <span class="math inline">\(\bar{R} = AA&#39;\)</span>:</p>
<pre class="r"><code>round(loadings_sol %*% t(loadings_sol),3)</code></pre>
<pre><code>##          cost   lift  depth powder
## cost    0.970 -0.953 -0.060 -0.125
## lift   -0.953  0.960 -0.096 -0.031
## depth  -0.060 -0.096  0.988  0.990
## powder -0.125 -0.031  0.990  0.997</code></pre>
<p>The <strong>residual correlation matrix</strong> is <span class="math inline">\(R_\text{res} = R - \bar{R}\)</span></p>
<pre class="r"><code>rres &lt;- cor.ski - loadings_sol %*% t(loadings_sol)
diag(rres) &lt;- 0 # could otherwise replace with communalities
round(rres,3)</code></pre>
<pre><code>##          cost   lift depth powder
## cost    0.000  0.000 0.005 -0.005
## lift    0.000  0.000 0.005 -0.005
## depth   0.005  0.005 0.000  0.000
## powder -0.005 -0.005 0.000  0.000</code></pre>
<p>The FA is good if numbers in residual correlation matrix are small.</p>
<p>Factor score coefficients (for estimating factor scores) are found by multiplying the inverse correlation matrix and the factor loading matrix, <span class="math inline">\(B = R^{-1}A\)</span></p>
<pre class="r"><code>B &lt;- solve(r) %*% loadings_sol
B</code></pre>
<pre><code>##               PA1         PA2
## cost   0.13783453 -0.59197611
## lift   0.09353632  0.41350483
## depth  0.08326838  0.03113046
## powder 0.93593888 -0.04403683</code></pre>
<p>Factor scores are a product of standardized variable scores and factor score coefficients, <span class="math inline">\(F = ZB\)</span>:</p>
<pre class="r"><code>F &lt;- as.matrix(select(dat.ski, -skiers) %&gt;% mutate_all(scale)) %*% B
F</code></pre>
<pre><code>##             PA1        PA2
## [1,]  1.1012894  1.1795542
## [2,]  1.0263476 -0.8816516
## [3,] -0.4589879 -0.6769470
## [4,] -1.0816206  0.9740182
## [5,] -0.5870286 -0.5949738</code></pre>
<p>Here, for example, the first subject scores strongly on both snow (strong importance on both depth and powder) and resort factor (strong importance on lift over cost).</p>
<p>Predicting standardized scores on variables from scores on factors is also possible, using a product of scores on factors weighted by factor loadings, <span class="math inline">\(Z = FA&#39;\)</span>:</p>
<pre class="r"><code>Z &lt;- F %*% t(loadings_sol)
round(Z, 3)</code></pre>
<pre><code>##        cost   lift  depth powder
## [1,] -1.244  1.064  1.074  1.156
## [2,]  0.786 -0.942  1.035  0.980
## [3,]  0.700 -0.625 -0.444 -0.491
## [4,] -0.873  1.037 -1.092 -1.031
## [5,]  0.630 -0.534 -0.573 -0.614</code></pre>
<p>In algebraic form, the formula is: <span class="math display">\[\begin{align}
  z_\text{COST} &amp;= a_{11}F_1 + a_{12}F_2 \\
  z_\text{LFIT} &amp;= a_{21}F_1 + a_{22}F_2 \\
  z_\text{DEPTH} &amp;= a_{31}F_1 + a_{312}F_2 \\
  z_\text{POWDER} &amp;= a_{41}F_1 + a_{42}F_2
  \end{align}
  \]</span></p>
<p>The assumption is that each subject has the same latent structure but different scores on the factors themselves.</p>
<p>Tying it all together, the following is the computer output from the <code>psych::fa</code></p>
<pre class="r"><code>invisible(fit &lt;- fa(cor.ski, nfactors=2, fm=&quot;pa&quot;, rotate=&quot;varimax&quot;))
fit</code></pre>
<pre><code>## Factor Analysis using method =  pa
## Call: fa(r = cor.ski, nfactors = 2, rotate = &quot;varimax&quot;, fm = &quot;pa&quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##          PA1   PA2   h2     u2 com
## cost   -0.08 -0.98 0.97 0.0296   1
## lift   -0.08  0.98 0.96 0.0403   1
## depth   0.99 -0.02 0.99 0.0119   1
## powder  1.00  0.05 1.00 0.0035   1
## 
##                        PA1  PA2
## SS loadings           1.99 1.92
## Proportion Var        0.50 0.48
## Cumulative Var        0.50 0.98
## Proportion Explained  0.51 0.49
## Cumulative Proportion 0.51 1.00
## 
## Mean item complexity =  1
## Test of the hypothesis that 2 factors are sufficient.
## 
## The degrees of freedom for the null model are  6  and the objective function was  7.34
## The degrees of freedom for the model are -1  and the objective function was  0.43 
## 
## The root mean square of the residuals (RMSR) is  0 
## The df corrected root mean square of the residuals is  NA 
## 
## Fit based upon off diagonal values = 1
## Measures of factor score adequacy             
##                                                   PA1  PA2
## Correlation of (regression) scores with factors     1 0.99
## Multiple R square of scores with factors            1 0.98
## Minimum correlation of possible factor scores       1 0.96</code></pre>
<ul>
<li><strong>Factor Loadings (PA1, PA2)</strong>: The correlations between factors and variables.</li>
<li><strong>Communality (h2)</strong>: The proportion of variance in a variable that is explained for by the factors, equal to the sum of the squared loadings (SSL). E.g., 97.0% of the variance in COST is accounted for by the two factors.</li>
<li><strong>Uniqueness (u2)</strong>: Approximately, <span class="math inline">\(R = FF&#39; + U^2\)</span>. Because unique and error variances are omitted, a linear combination of factors approximates but does not duplicate the observed correlation matrix and scores on observed variables.<br />
</li>
<li><strong>Complexity (com)</strong>: Hoffman’s index of complexity, <span class="math inline">\(\frac{(\sum a_I^2)^2}{\sum a_i^4}\)</span></li>
<li><strong>SS Loadings</strong>: Sum of squared loadings</li>
<li><strong>Proportion Var</strong>: Proportion of variance accounted for by each factor; 49.9% of the variance in the variables is accounted for by the first factor and 49.9% of the variance in the variables is accounted for by the second factor.<br />
</li>
<li><strong>Cumulative Var</strong>: Cumulative proportion of variance explained (across factors). Because rotation is orthogonal, the two factors together account for 97.9% of the variance in the variables</li>
<li><strong>Proportion Explained</strong>: The proportion of variance in the solution accounted for by a factor; the two factors account for 50.9% and 49.1% of the variance in the solution, respectively.</li>
</ul>
</div>
<div id="oblique-rotation" class="section level2">
<h2>Oblique Rotation</h2>
<p>When factors are correlated; the loading matrix (correlations between variables and correlated factors) is decomposed into the following:</p>
<ul>
<li>Pattern matrix (<span class="math inline">\(A\)</span>); when squared, contribution of each factor to variance of variable, without segments o variance that come from overlap between correlated factors</li>
<li>Structure matrix (<span class="math inline">\(C\)</span>); correlations between variables and factors.<br />
<span class="math display">\[C = A\Phi\]</span></li>
</ul>
<p>The factor correlations matrix <span class="math inline">\(\Phi\)</span> is also produced.</p>
<p>To determine the structure matrix, the following steps are required: 1. Determine factor score coefficients, <span class="math inline">\(B = R^{-1}A\)</span> 2. Determine factor scores, which are a product of the standardized variable scores and factor score coefficients, <span class="math inline">\(F = ZB\)</span> 3. Determine correlations among factors</p>
<pre class="r"><code>invisible(fit &lt;- fa(dat.ski[,-1], nfactors=2, fm=&quot;pa&quot;, rotate=&quot;oblimin&quot;, scores=&quot;Thurstone&quot;))

# Pattern matrix
oblimin_A &lt;- unclass(fit$loadings)

# Factor-Score Coefficients
oblimin_B &lt;- solve(r) %*% oblimin_A %*% fit$Phi
oblimin_B</code></pre>
<pre><code>##               PA1         PA2
## cost   0.12847275  0.59206210
## lift   0.10005202 -0.41344632
## depth  0.08374941 -0.03107844
## powder 0.93512712  0.04462147</code></pre>
<pre class="r"><code>fit$weights </code></pre>
<pre><code>##               PA1         PA2
## cost   0.12847275  0.59206210
## lift   0.10005202 -0.41344632
## depth  0.08374941 -0.03107844
## powder 0.93512712  0.04462147</code></pre>
<pre class="r"><code># Factor Scores
Z &lt;- as.matrix(select(dat.ski, -skiers) %&gt;% mutate_all(scale)) 
oblimin_F &lt;- Z %*% oblimin_B 
oblimin_F</code></pre>
<pre><code>##             PA1        PA2
## [1,]  1.1197720 -1.1788660
## [2,]  1.0123025  0.8822925
## [3,] -0.4696166  0.6766602
## [4,] -1.0661105 -0.9746937
## [5,] -0.5963473  0.5946070</code></pre>
<pre class="r"><code># Factor Correlation; cross product of standardized factor scores / (number of cases - 1)
oblimin_Phi &lt;- (1/(nrow(oblimin_F)-1)) * t(oblimin_F) %*% oblimin_F
diag(oblimin_Phi) &lt;- 1
oblimin_Phi</code></pre>
<pre><code>##             PA1         PA2
## PA1  1.00000000 -0.01503656
## PA2 -0.01503656  1.00000000</code></pre>
<pre class="r"><code>fit$Phi</code></pre>
<pre><code>##             PA1         PA2
## PA1  1.00000000 -0.01516085
## PA2 -0.01516085  1.00000000</code></pre>
<pre class="r"><code># Structure
oblimin_C &lt;- oblimin_A %*% fit$Phi
oblimin_C</code></pre>
<pre><code>##                PA1         PA2
## cost   -0.09307059  0.98198964
## lift   -0.06390811 -0.97646186
## depth   0.99347151  0.01804570
## powder  0.99772563 -0.04813229</code></pre>
<pre class="r"><code>unclass(fit$Structure)</code></pre>
<pre><code>##                PA1         PA2
## cost   -0.09307059  0.98198964
## lift   -0.06390811 -0.97646186
## depth   0.99347151  0.01804570
## powder  0.99772563 -0.04813229</code></pre>
<p>From Phi, see that correlation between first factor and second factor is quite low. In this case, one would use orthogonal rotation.</p>
</div>
<div id="factor-extraction-techniques" class="section level2">
<h2>Factor Extraction Techniques:</h2>
<ul>
<li>Principal components; to determine components with maximum variance. Firs PC is the linear combination of observed variables that maximally separated subjects by maximising the variance of their component scores. Subsequent components are linear combinations of observed variables that extracts maximum variability from residual correlations that are orthogonal to previous extracted components. Choose solution to reduce large number of variables to smaller number of components.</li>
<li>Principal factors: Estimates communality in the positive diagonal of the observed correlation matrix through an iterative procedure with the squared multiple correlations (SMCs) of each variables with all other variables used as the starting values in the iteration.<br />
</li>
<li>Maximum likelihood factoring: Factor loadings calculated by maximising the probability of sampling the observed correlation matrix from a population</li>
<li>Image factoring: distributes among factors the variance of a n observed variable that is reflected by the other variables; variable image scores are produced by multiple regression with variable as independent variable and other variables as dependent variables. A covariance matrix is calculated from the image (predicted) scores</li>
<li>Unweighted least squares factoring: Aims to minimises squared differences between the observed and reproduced correlation matrices. Only considers=s off-diagonal differences.</li>
<li>Generalized (weighted) least squares factoring: Also aims to minimize (off-diagonal) squared differences between observed and reproduced correlation matrices, but applies weights to variables. Variables with substantial shared variance with other variables are weighted more heavily (i.e. considered more important).
<ul>
<li>Alpha factoring: Used when there is interest in discovering common factors with repeated samples of variables taken from a population of variables.</li>
</ul></li>
</ul>
<div id="comparison-of-pca-to-fa" class="section level3">
<h3>Comparison of PCA to FA</h3>
<p>In both, the variance that is analysed is the sum of the values in the positive diagonal.</p>
<table>
<tr>
<th>
Factor Analysis
</th>
<th>
Principal Components Analysis
</th>
</tr>
<tr>
<td>
Only the variance that each observed variable shares with other observed variables is available for analysis (error and unique variance excluded); shared variance is estimated by communalities that are inserted in the positive diagonal of the correlation matrix. The solution concentrates on variables with high communality values. The sum of the communalities (sum of the SSLS) is the variance that is distributed among factors.
</td>
<td>
Ones are in the diagonal; each variables contributes a unit of variance by contributing a 1 to the positive diagonal of the correlation matrix. All the variance is distributed to components, including error and unique variance for each observed variables.
</td>
</tr>
<tr>
<td>
Because unique and error variances are omitted, a linear combination of factors approximates but does not duplicate the observed correlation matrix and scores on observed variables.
</td>
<td>
If all components are retained, PCA duplicates exactly the observed correlation matrix.
</td>
</tr>
<tr>
<td>
Analyses covariance (communalities)
</td>
<td>
Analyses variance
</td>
</tr>
<tr>
<td>
Goal: Reproduce correlation matrix with few orthogonal factors
</td>
<td>
Goal: Reproduce correlation matrix with few orthogonal factors
</td>
</tr>
<tr>
<td>
Non-unique solution
</td>
<td>
Unique solution
</td>
</tr>
<tr>
<td>
Factors are thought to cause variables, <em>what are the underlying processes that could have produced correlations among these variables?</em>; <em>are the correlations among variables consistent with a hypothesised factor structure</em>?
</td>
<td>
Variables cause the component; no underlying theory about which variables should be associated with which factors
</td>
</tr>
<tr>
<td>
Best if want to understand underlying structure
</td>
<td>
Best when after an empirical summary of the data set
</td>
</tr>
<tr>
<td>
<img src="/graphs/fa.png" />
</td>
<td>
<img src="/graphs/pca.png" />
</td>
</tr>
</table>
</div>
</div>
<div id="rotation-techniques" class="section level2">
<h2>Rotation Techniques</h2>
<p>Rotation is used to improve interpretability of solution, not the quality of the mathematical fit between the observed and reproduced correlation matrices (which is the same before and after rotation). If underlying processes are correlated, oblique rotation is used, otherwise orthogonal rotation is used (with the advantage of ease in reporting, describing and interpreting results)</p>
<p>Orthogonal Rotation Techniques:</p>
<ul>
<li>Varimax: maximised variance of loadings on each factor</li>
<li>Quartimax: Maximise variance of loadings on each variable</li>
<li>Equimax: Compromise between quartimax and varimax; goal is to simplify both variables and factors</li>
</ul>
<p>Oblique Rotation Techniques; allow for range of correlation between factors by altering <span class="math inline">\(\delta\)</span> (-4=orthogonal, 0=fairly highly correlation, 1=highly correlated).</p>
<ul>
<li><p>Oblimin (<span class="math inline">\(\delta=0\)</span>): simplify factors by minimising cross-products of loadings</p></li>
<li><p>Equamax</p></li>
</ul>
</div>
</div>
<div id="time-series-analysis" class="section level1">
<h1>Time-Series Analysis</h1>
<div id="general-purpose-and-description" class="section level2">
<h2>General Purpose and Description</h2>
<ul>
<li>Observations over <span class="math inline">\(\geq\)</span> 50 time periods</li>
<li>Observations may be aggregates (e.g. monthly or daily traffic tickets)</li>
<li>Goals:
<ul>
<li>Identify patterns in obsevations correlated with themselves, but offset in time</li>
<li>Test impact of intervention(s)</li>
<li>Forecast</li>
</ul></li>
<li>Method: Decompose score into:
<ul>
<li>shocks (random process)</li>
<li>Trend
<ul>
<li>Linear: increasing or decreasing mean</li>
<li>Quadratic: Mean increases or decreases</li>
</ul></li>
<li>Effect of earlier scores</li>
<li>Effect of earlier shocks</li>
<li>Seaonality/periodocity/cyclic</li>
</ul></li>
<li>Models:
<ul>
<li>ARIMA(<span class="math inline">\(p, d, q\)</span>): Auto-regressive integrated moving average:
<ul>
<li><span class="math inline">\(p\)</span>: <strong>Auto-regressive</strong>; effect of earlier scores. A model with two auto-regressive terms (<span class="math inline">\(p = 2\)</span>) depends on two previous observations</li>
<li><span class="math inline">\(d\)</span>: <strong>Trend</strong>; the terms needed to make a nonstationary time series stationary. With two trend terms ($d = 2), teh model has to be differenced twice to make it stationary. The first difference removes linear trendes and the second quadratic, etc.</li>
<li><span class="math inline">\(q\)</span>: <strong>Moving average</strong> (of earlier shocks); with two moving average terms, an observation depends on two preceding random shocks</li>
</ul></li>
</ul></li>
<li>Steps in analysis:
<ol style="list-style-type: decimal">
<li>Identification; of autocorrelation functions (ACFs) and partial autocorrelation functions (PACFs)
<ul>
<li><strong>Autocorrelation</strong>: self-scorrelation at different lags</li>
<li><strong>Partial Autocorrelation</strong>: self correlation at different lags with intermediate correlations parialled out</li>
</ul></li>
<li>Estimation</li>
<li>Diagnosis</li>
</ol></li>
</ul>
<p>Recommended References:</p>
<ul>
<li><p><a href="https://www.wiley.com/en-au/Time+Series+Analysis%3A+Forecasting+and+Control%2C+5th+Edition-p-9781118675021">Time Series Analysis: Forecasting and Control, 5th Edition George E. P. Box, Gwilym M. Jenkins, Gregory C. Reinsel, Greta M. Ljung</a></p></li>
<li><p>Hershberger,S. L., Molenaar, P.C. M., &amp; Corneal, S.E. (1996). A hierarchy of univariate andmultivariatestructural time series models. In G. A. Marcoulides &amp; R. E. Shumacker (Eds.), Advanced structural equation modeling: Issues and Techniques (pp. 159–194).Mahwah, NJ: Lawrence Erlbaum Associates, Inc</p></li>
</ul>
</div>
<div id="kinds-of-research-questions" class="section level2">
<h2>Kinds of Research Questions</h2>
<p>Determine:</p>
<ol style="list-style-type: decimal">
<li>Patterns in series
<ol style="list-style-type: lower-alpha">
<li>Autocorrelation: Pattern may be of interest to intself or in prepration for forecasing or testing the effects of interventions. Types of questions:
<ul>
<li>Are there linear or quadratic trends in the data?</li>
<li>How quickly to random shocks linger?</li>
</ul></li>
<li>Seasonal cycles and trends<br />
</li>
</ol></li>
<li>Predicted values in future (forecasting)</li>
<li>Effect of intervention</li>
<li>Relationships between time series (comparing time series). AKA multivariate time eries, cross-correlation functions, transfer function models, models with input series, dynamic regression.</li>
<li>Relationship to covariates (dv)</li>
<li>Variability due to the chosen model (effect size and power)</li>
</ol>
</div>
<div id="assumptions-of-time-series-analysis" class="section level2">
<h2>Assumptions of Time-Series Analysis</h2>
<div id="theoretical-issues" class="section level3">
<h3>Theoretical Issues</h3>
<p>To determine causality require true experient, i.e.</p>
<ul>
<li>Random assignment to ttreatment conditions</li>
<li>Control of extraneious variables</li>
<li>Manipulation of intervention(s)</li>
</ul>
</div>
<div id="practical-issues-around-assumptions" class="section level3">
<h3>Practical Issues Around Assumptions</h3>
<ul>
<li><p>Normality of Distributions of Residuals, <span class="math inline">\(e ~ N(0, \sigma^2)\)</span>. Examine normalised plot of residuals before evaluating an intervantion. Transform DV if residuals are nonnormal (e.g square root, log, inverse)</p></li>
<li><p>Homogeneity of Variance and Zero Mean of Residuals: Examine plots of standardized residuals against predicted values. Transform DV if width of plot varies over time (e.g. use log)</p></li>
<li><p>Independence of Residuals. Obviously inherent within time series due to autocorrelation, but after identification of autocorrelation and differencing there should be no remaining autocreelations or partial autocorellations at various lags in the ACFs and PACFs (if so, examine ACFs and PACFs and adjust model accordingly)</p></li>
<li><p>Absence of Outliers. Outliers sometimes show up in original plot of DV against time, but often more noticeable after initial modelling. Can greatly affect results and must be dealt with (by deletion and then possibly imputing). Examine time series plot before and after adjusting for autocorelation and seasonality to identify outlier patterns.</p></li>
</ul>
</div>
</div>
<div id="fundamental-equations-for-time-series-arima-models" class="section level2">
<h2>Fundamental Equations for Time Series: ARIMA models</h2>
<div id="identification-of-arima-p-d-q-models" class="section level3">
<h3>Identification of ARIMA (<span class="math inline">\(p, d, q\)</span>) Models</h3>
<ul>
<li>Requires finding values for <span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span> and <span class="math inline">\(q\)</span></li>
<li>If a value is 0, then term is not required</li>
<li>First step is to identify the trend (<span class="math inline">\(d\)</span>); if it is not stationary, ie. if <span class="math inline">\(d \neq 0\)</span>, then the series needs to be made stationary before identifying <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>. A stationary process has both constant mean and constant variance of the time period of the study</li>
</ul>
<div id="trend-components-d-making-the-process-stationary" class="section level4">
<h4>Trend Components, <span class="math inline">\(d:\)</span> Making the Process Stationary</h4>
<ul>
<li>First step: Plot time series
<ul>
<li>Is mean shifting? If so, apply differening one or more times, e.g. calculate <span class="math inline">\(t_2 - t_1\)</span></li>
<li>Is dispersion increasing or decreasing over time? If so, apply a logarithmic transformation</li>
</ul></li>
</ul>
<pre class="r"><code>df &lt;- data.frame(week = 1:20, quality = c(19, 21, 17, 19, 20, 21, 27, 28, 20, 24, 31, 20, 29, 21, 28, 28, 29, 31, 23, 34)) 
df$q1 &lt;- c(NA, df$quality[-1] - df$quality[-nrow(df)])
df$q2 &lt;- c(NA, df$q1[-c(1)] - df$q1[-nrow(df)])
df$logq &lt;- log(df$quality)
df$logq_1 = c(NA, df$logq[-1] - df$logq[-nrow(df)])

tab &lt;- df %&gt;%
    mutate_at(vars(logq, logq_1), round, 2) %&gt;%
    mutate_all(as.character) %&gt;%
    bind_rows(c(week = &quot;&quot;, quality = &quot;&quot;, q1 = paste0(&quot;mean=&quot;, round(mean(df$q1, na.rm = TRUE),2)), q2 = &quot;&quot;, logq = &quot;&quot;, logq_1 = &quot;&quot;) )
kable(tab, digits = 2)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">week</th>
<th align="left">quality</th>
<th align="left">q1</th>
<th align="left">q2</th>
<th align="left">logq</th>
<th align="left">logq_1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">19</td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="left">2.94</td>
<td align="left">NA</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">21</td>
<td align="left">2</td>
<td align="left">NA</td>
<td align="left">3.04</td>
<td align="left">0.1</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">17</td>
<td align="left">-4</td>
<td align="left">-6</td>
<td align="left">2.83</td>
<td align="left">-0.21</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">19</td>
<td align="left">2</td>
<td align="left">6</td>
<td align="left">2.94</td>
<td align="left">0.11</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">20</td>
<td align="left">1</td>
<td align="left">-1</td>
<td align="left">3</td>
<td align="left">0.05</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left">21</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">3.04</td>
<td align="left">0.05</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="left">27</td>
<td align="left">6</td>
<td align="left">5</td>
<td align="left">3.3</td>
<td align="left">0.25</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="left">28</td>
<td align="left">1</td>
<td align="left">-5</td>
<td align="left">3.33</td>
<td align="left">0.04</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="left">20</td>
<td align="left">-8</td>
<td align="left">-9</td>
<td align="left">3</td>
<td align="left">-0.34</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="left">24</td>
<td align="left">4</td>
<td align="left">12</td>
<td align="left">3.18</td>
<td align="left">0.18</td>
</tr>
<tr class="odd">
<td align="left">11</td>
<td align="left">31</td>
<td align="left">7</td>
<td align="left">3</td>
<td align="left">3.43</td>
<td align="left">0.26</td>
</tr>
<tr class="even">
<td align="left">12</td>
<td align="left">20</td>
<td align="left">-11</td>
<td align="left">-18</td>
<td align="left">3</td>
<td align="left">-0.44</td>
</tr>
<tr class="odd">
<td align="left">13</td>
<td align="left">29</td>
<td align="left">9</td>
<td align="left">20</td>
<td align="left">3.37</td>
<td align="left">0.37</td>
</tr>
<tr class="even">
<td align="left">14</td>
<td align="left">21</td>
<td align="left">-8</td>
<td align="left">-17</td>
<td align="left">3.04</td>
<td align="left">-0.32</td>
</tr>
<tr class="odd">
<td align="left">15</td>
<td align="left">28</td>
<td align="left">7</td>
<td align="left">15</td>
<td align="left">3.33</td>
<td align="left">0.29</td>
</tr>
<tr class="even">
<td align="left">16</td>
<td align="left">28</td>
<td align="left">0</td>
<td align="left">-7</td>
<td align="left">3.33</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">17</td>
<td align="left">29</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">3.37</td>
<td align="left">0.04</td>
</tr>
<tr class="even">
<td align="left">18</td>
<td align="left">31</td>
<td align="left">2</td>
<td align="left">1</td>
<td align="left">3.43</td>
<td align="left">0.07</td>
</tr>
<tr class="odd">
<td align="left">19</td>
<td align="left">23</td>
<td align="left">-8</td>
<td align="left">-10</td>
<td align="left">3.14</td>
<td align="left">-0.3</td>
</tr>
<tr class="even">
<td align="left">20</td>
<td align="left">34</td>
<td align="left">11</td>
<td align="left">19</td>
<td align="left">3.53</td>
<td align="left">0.39</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left">mean=0.79</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<pre class="r"><code>ggplot(df, aes(week, quality)) + 
    geom_line() + 
    geom_point() + 
    geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="/blog/2018-06-11-book-using-multivariate-statistics_files/figure-html/ts_plot-1.png" width="672" /></p>
<p>Models:</p>
<ol style="list-style-type: decimal">
<li>Observation = f(random shock) only: <span class="math inline">\(Y_t = a_t\)</span> where <span class="math inline">\(a_t\)</span> is random shock with constant mean and variance. This means that observations also have constant mean and variance.</li>
<li>Observation = f(previous observation + random shock): <span class="math inline">\(Y_t = \theta_0(Y_{t-1}) + a_t\)</span>. Here, the slope is defined by the mean of the first difference, 0.79. The following plot shows that differerencing has removed the trend.</li>
</ol>
<pre class="r"><code>ggplot(df, aes(week, q1)) + 
    geom_line() + 
    geom_point() + 
    geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="/blog/2018-06-11-book-using-multivariate-statistics_files/figure-html/ts_plot_diff1-1.png" width="672" /></p>
<p>However, after taking the log, the variability is the same and so untransformed difference is used in future analyses</p>
<pre class="r"><code>ggplot(df, aes(week, logq_1)) + 
    geom_line() + 
    geom_point() + 
    geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="/blog/2018-06-11-book-using-multivariate-statistics_files/figure-html/ts_plot_diff1log-1.png" width="672" /></p>
</div>
<div id="auto-regressive-components-p" class="section level4">
<h4>Auto-Regressive Components (<span class="math inline">\(p\)</span>)</h4>
<p>Memory of process for preceding observations, equal to 0 if no relationship between adjacent observations. Example of <span class="math inline">\(p = 2\)</span> model is ARIMA (2, 0, 0): <span class="math display">\[Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + a_t\]</span> where <span class="math inline">\(\phi_l\)</span> is the correlation coefficient of the observations with those at lag <span class="math inline">\(l\)</span>.</p>
</div>
<div id="moving-average-components-q" class="section level4">
<h4>Moving Average Components (<span class="math inline">\(q\)</span>)</h4>
<p>Memory of process for preceding random shocks. For example, when <span class="math inline">\(q = 2\)</span> there is a relationship between the current score and the random shock at lag 2 and the model is ARIMA (0, 0, 2): <span class="math display">\[Y_t = a_t -\theta_1 a_{t-1} - \theta_2 a_{t-2}\]</span></p>
</div>
<div id="mixed-models" class="section level4">
<h4>Mixed Models</h4>
<p>When the series has both auto-regressive and moving average components so that both types of correlations are required, e.g. ARIMA(1, 0, 1):<span class="math display">\[Y_t = \phi_1 Y_{t-1} - \theta_1 a_{t-1} + a_t\]</span></p>
</div>
<div id="acfs-and-pacfs" class="section level4">
<h4>ACFs and PACFs</h4>
<p>Lag 1: Autocorrelation between <span class="math inline">\(Y_{t-1}\)</span> and <span class="math inline">\(Y_t\)</span> Lag 2: Autocorrelation and partial autocorrelation between <span class="math inline">\(Y_{t-2}\)</span> and <span class="math inline">\(Y_t\)</span></p>
<p>Autocorelation: <span class="math display">\[r_k = \frac{\frac{1}{N-k}(Y_t - \bar{Y})(Y_{t-k}-\bar{Y})}{\frac{1}{N-1}(Y_t - \bar{Y})^2}\]</span> where <span class="math inline">\(N\)</span> is the number of observations, <span class="math inline">\(k\)</span> is the lag, <span class="math inline">\(\bar{Y}\)</span> is the mean of the series and the denominatior is the variance</p>
<p>Standard error of an autocorrelation is based on the squared autocorrelation at previous lags; <span class="math display">\[\text{SE}_{r_k} = \begin{cases} 0 &amp; k = 1\\
                        \sqrt{\frac{1 + 2 \sum^{k - 1}_{l = 0} r_l^2}{N}}  &amp; k &gt; 1
                    \end{cases}\]</span></p>
<p>Partial autocorrelation: More complex; based on recurive technique, however the following have been shown: <span class="math display">\[\begin{align} \text{PACF}(1) &amp; = \text{ACF}(1) \\
                  \text{PACF}(2) &amp; = \frac{\text{ACF}(2) - [\text{ACF}(1)]^2}{1 - [\text{ACF}(1)]^2} \\
                   \text{PACF}(3) &amp; = \frac{-2\text{ACF}(1)\text{ACF}(2) - [\text{ACF}(1)]^2\text{ACF}(3)}{1 + 2[\text{ACF}(1)]^2 \text{ACF}(2) - [\text{ACF}(2)]^2 - 2 [\text{ACF}(1)]^2} \\
                   \end{align} \]</span></p>
<p>Standard error of a partial autocorrelation: <span class="math display">\[\text{SE}_{pr} = \frac{1}{\sqrt{N}}\]</span></p>
<p>Differenced scores are used when differencing has been used to remove trend.</p>
<p>The standard errors are used to assess full and partial autocorrelation significance.</p>
<pre class="r"><code>df %&gt;% 
    select(week, q1) %&gt;%
    arrange(desc(week)) %&gt;%
    mutate(q_tm1 = lead(q1),
        ybar = mean(q1, na.rm = TRUE),
           num = (q1 - ybar)*(q_tm1 - ybar),
            den = (q1 - ybar)^2) %&gt;%
    summarise(r1 = (1/(n() - 1) * sum(num, na.rm = TRUE)) / ( 1/(n() - 1) * sum(den, na.rm = TRUE)) ,
              se_r1 = sqrt(1 / n()))</code></pre>
<pre><code>##          r1     se_r1
## 1 -0.614752 0.2236068</code></pre>
<p>The ACF and PACF plots are used to identify whether the process is an autoregressive (<span class="math inline">\(p\)</span>) or moving average (<span class="math inline">\(q\)</span>) process, noting that hybrid processes are quire rare.</p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Autoregressive (AR) Process (<span class="math inline">\(p\)</span>)</th>
<th>Moving Average (MA) Process (<span class="math inline">\(q\)</span>)</th>
<th>Mixed ARMA model</th>
<th>Nonstationary (<span class="math inline">\(d\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>* Noise / shock has a lasting effect</td>
<td>* Noise / shock quickly vanishes with time</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>* ACF slowly declines, PACF spikes at lag <span class="math inline">\(p\)</span></td>
<td>* ACF spikes at <span class="math inline">\(q\)</span> lags, PACF slowly declines</td>
<td>* slowly declining ACF and PACF</td>
<td>* Single PACF peak; ACF has spikes or damped sign wave</td>
</tr>
<tr class="odd">
<td>* <span class="math inline">\(p\)</span> = Number of spikes on PACF</td>
<td>* <span class="math inline">\(q\)</span> = Number of spikes on ACF</td>
<td>* <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> not evident; start with 1</td>
<td></td>
</tr>
</tbody>
</table>
<pre class="r"><code>ts &lt;- ts(df$quality)
ts_diff &lt;- diff(ts)
acf(ts_diff, lag.max = 16)</code></pre>
<p><img src="/blog/2018-06-11-book-using-multivariate-statistics_files/figure-html/ts_diff_pacf-1.png" width="672" /></p>
<pre class="r"><code>pacf(ts_diff)</code></pre>
<p><img src="/blog/2018-06-11-book-using-multivariate-statistics_files/figure-html/ts_diff_pacf-2.png" width="672" /></p>
<p>Note that lag 0 is shown; whereas SAS/SPSS start with lag 1. Here is there is spike in the ACF at lag 1 and the PACF trends towards 0, so this is probably an ARIMA(0, 0, 1) model, although because it has been differenced it is ARIMA(0, 1, 1). The series has both trend and memory for the preceding random shock. If this time series is about the quality of computes in a manufacturing process then we would say that teh quality is generally increasing, however the quality is one week is influenced by random events in the manufacturing process.</p>
</div>
</div>
<div id="estimating-model-parameters" class="section level3">
<h3>Estimating Model Parameters</h3>
</div>
<div id="diagnose-a-model" class="section level3">
<h3>Diagnose a Model</h3>
</div>
<div id="computer-analysis-of-small-sample-time-series-example" class="section level3">
<h3>Computer Analysis of Small-Sample Time-Series Example</h3>
</div>
</div>
<div id="types-of-time-series-analyses" class="section level2">
<h2>Types of Time-Series ANAlyses</h2>
<div id="models-with-seasonal-components" class="section level3">
<h3>Models With Seasonal Components</h3>
</div>
<div id="models-with-interventions" class="section level3">
<h3>Models With Interventions</h3>
<div id="abrupt-permanent-effects" class="section level4">
<h4>Abrupt, Permanent Effects</h4>
</div>
<div id="abrupt-temporary-effects" class="section level4">
<h4>Abrupt, Temporary Effects</h4>
</div>
<div id="gradual-permanent-effects" class="section level4">
<h4>Gradual, Permanent Effects</h4>
</div>
<div id="models-with-multiple-interventions" class="section level4">
<h4>Models With Multiple Interventions</h4>
</div>
</div>
<div id="adding-continuous-variables" class="section level3">
<h3>Adding Continuous Variables</h3>
</div>
</div>
<div id="some-important-issues" class="section level2">
<h2>Some Important Issues</h2>
<div id="patterns-of-acfs-and-pacfs" class="section level3">
<h3>Patterns of ACFs and PACFs</h3>
</div>
<div id="effect-size" class="section level3">
<h3>Effect Size</h3>
</div>
<div id="forecasting" class="section level3">
<h3>Forecasting</h3>
</div>
<div id="statistical-methods-for-comparing-two-models" class="section level3">
<h3>Statistical Methods for Comparing Two Models</h3>
</div>
</div>
<div id="complete-example-of-a-time-series-analysis" class="section level2">
<h2>Complete Example of a Time-Series Analysis</h2>
<div id="evaluation-of-assumptions" class="section level3">
<h3>Evaluation of Assumptions</h3>
<div id="normality-of-sampling-distributions" class="section level4">
<h4>Normality of Sampling Distributions</h4>
</div>
<div id="homogeneity-of-variance" class="section level4">
<h4>Homogeneity of Variance</h4>
</div>
<div id="outliers" class="section level4">
<h4>Outliers</h4>
</div>
</div>
<div id="baseline-model-identification-and-estimation" class="section level3">
<h3>Baseline Model Identification and Estimation</h3>
</div>
<div id="baseline-model-diagnosis" class="section level3">
<h3>Baseline Model Diagnosis</h3>
</div>
<div id="intervention-analysis" class="section level3">
<h3>Intervention Analysis</h3>
<div id="model-diagnosis" class="section level4">
<h4>Model Diagnosis</h4>
</div>
<div id="model-interpretation" class="section level4">
<h4>Model Interpretation</h4>
</div>
</div>
</div>
</div>
