---
title: 'Book: Using Multivariate Statistics'
author: Marie
date: '2018-06-11'
slug: book-using-multivariate-statistics
banner: "img/banners/tabachnick.png"
categories:
  - Study-Notes
tags:
  - Book
  - Study-Notes
---



<div id="principal-components-and-factor-analysis" class="section level1">
<h1>Principal Components and Factor Analysis</h1>
<div id="general-purpose" class="section level2">
<h2>General Purpose</h2>
<ul>
<li>To find subsets of variables (factors/components/latent variables) that are independent, where variables in a subset are correlated</li>
<li>To summarise patterns of correlations among variables</li>
<li>To decrease number of variables to a smaller number of factors/components, by using linear combinations of observed variables (more parsimonious and because factors/components are uncorrelated, more reliable)</li>
<li>To understand underlying processes</li>
<li>To describe (and maybe understand) relationship between variables</li>
<li>To test theory about underlying processes</li>
</ul>
</div>
<div id="steps" class="section level2">
<h2>Steps:</h2>
<ul>
<li>Hypothesise factors underlying domain of interest</li>
<li>Select and measure variables</li>
<li>Prepare correlation matrix</li>
<li>Extract factors/components</li>
<li>Determine number of factors/components</li>
<li>Rotate to increase interpretability</li>
<li>Interpret results</li>
<li>Should make sense</li>
<li>Easier if variables within one factor do not correlate with variables in another factor</li>
<li>Verify factor structure by establishing validity, for example, confirm that factor scores change with experimental conditions as predicted</li>
</ul>
<p>For factor analysis</p>
<ul>
<li>Hypothesise factors underlying domain of interest</li>
<li>Select 5-6 variables thought to be a pure measure of the factor (marker variables); consider variable complexity (i.e the number of factors for which a variable may correlate)</li>
<li>Choose a sample across variables/factors with spread of scores</li>
<li>Be wary to pool results of several samples; samples differing, for example, in SES may also have different factors. Only pool if different samples produce the same factors.</li>
</ul>
</div>
<div id="types-of-factor-analysis" class="section level2">
<h2>Types of Factor Analysis</h2>
<table>
<colgroup>
<col width="56%" />
<col width="43%" />
</colgroup>
<thead>
<tr class="header">
<th>Exploratory</th>
<th>Confirmatory</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Early stages of research</td>
<td>Advanced stages of research</td>
</tr>
<tr class="even">
<td>To summarise data by grouping together correlated variables</td>
<td>To test theory about latent processes</td>
</tr>
<tr class="odd">
<td>Variables may not have been chosen with underlying processes in mind</td>
<td>Variables chosen to reveal underlying processes</td>
</tr>
<tr class="even">
<td>Factor Analysis / PCA</td>
<td>Usually via Structural Equation Modelling (SEM)</td>
</tr>
</tbody>
</table>
</div>
<div id="research-questions" class="section level2">
<h2>Research Questions</h2>
<ul>
<li>How many factors</li>
<li>What do the factors mean</li>
<li>How much variability is accounted for by each/all factors</li>
<li>How well does the factor solution fit that expected</li>
<li>Had factors been measured directly, what scored would subjects have received?</li>
</ul>
</div>
<div id="limitations" class="section level2">
<h2>Limitations</h2>
<ul>
<li>No ready criteria for verification</li>
<li>Infinite rotations; final choice is based on subjective assessment of interpretability</li>
<li>Often used as a last resort to make order from chaos (i.e. suffers from a poor reputation)</li>
<li>Exploratory</li>
<li>Subjective</li>
<li>Sensitive to outliers, missing data, poorly distributed samples, small samples</li>
</ul>
<div id="outliers-among-cases" class="section level3">
<h3>Outliers (among cases)</h3>
<ul>
<li>Univariate and multivariate (combination of variables) outliers will have greater influence on factor solution</li>
<li>TODO chapter 4 and 13.7.4.14 to detect and reduce influence</li>
</ul>
</div>
<div id="outliers-among-variables-fa-only" class="section level3">
<h3>Outliers among variables (FA only)</h3>
<p>If there is a factor with <span class="math inline">\(\leq\)</span> 2 variables and significant variable accounted for:</p>
<ul>
<li>Ignore, or treat with caution</li>
<li>The factor should be researched further with structural equation modelling (SEM)</li>
</ul>
<p>Factors with few variables and small variance accounted for are unreliable.</p>
</div>
<div id="missing-data" class="section level3">
<h3>Missing data</h3>
<p>Estimate if</p>
<ul>
<li>Missing distribution of values not random</li>
<li>Sample size will be too small if delete cases;</li>
</ul>
<p>Otherwise, delete cases</p>
</div>
<div id="poorly-distributed-variables-non-normality" class="section level3">
<h3>Poorly distributed variables (Non-Normality)</h3>
<ul>
<li>If PCA/FA used descriptively, i.e. to summarise relationships, normality assumptions not in force, but solution is enhanced with normality</li>
<li>If statistical inference used to determine number of factors, single variable and multivariate normality (linear combinations of variables are normally distributed) is assumed</li>
<li>If variable has substantial skewness and kurtosis, consider variable transformation</li>
<li>Multivariate normality tests are sensitive</li>
<li>Note that some SEMS permit PCA/FA with non-normal variables</li>
</ul>
</div>
<div id="non-linearity" class="section level3">
<h3>Non-Linearity</h3>
<ul>
<li>Correlations measures linear relationships therefore analysis is degraded by non-linear relationship (as not captured)</li>
<li>Assessed visually using scatterplots</li>
</ul>
</div>
<div id="small-samples" class="section level3">
<h3>Small samples</h3>
<p>Sample size required will depend on population correlations and the number of factors</p>
<table>
<thead>
<tr class="header">
<th>Characteristics</th>
<th>Recommended Sample Size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Communalities &gt; 0.6</td>
<td>&lt; 100</td>
</tr>
<tr class="even">
<td>Communalities ~ 0.5, loadings &gt; 0.8</td>
<td>100-200</td>
</tr>
<tr class="odd">
<td>Low communality; small # factors,3-4 indicators for each factor</td>
<td>&gt; 300</td>
</tr>
<tr class="even">
<td>Low communality, large # weakly determined factors</td>
<td>&gt; 500</td>
</tr>
</tbody>
</table>
</div>
<div id="multicollinearity-and-singularity-fa-only" class="section level3">
<h3>Multicollinearity and Singularity (FA only)</h3>
<p>Although in FA/PCA interest is in finding correlated variables, if correlation is too high, matrix inversion becomes unstable (multicollinearity) or impossible (singularity). If determinant of correlation matrix (<span class="math inline">\(R\)</span>) is close to zero, then multicollinearity or singularity may be present</p>
<p>To detect, look at Squared Multiple Correlation (<strong>SMC</strong>), if SMC is high (&gt; 0.9 multicollinearity, 1=singularity) then delete that variable</p>
</div>
<div id="lack-of-factorability-within-correlation-matrix-r" class="section level3">
<h3>Lack of Factorability within Correlation Matrix (R)</h3>
<p>FA assumes relationships between variables; ; a factorable matrix should have several sizeable correlations. Note however that high bivariate correlations are not a guarantee of factors; they may merely be two similar variables and not reflect an underlying process simultaneously affecting them.</p>
<p>If factors are present, then</p>
<ul>
<li>Look at correlation matrix (<strong>R</strong>): Should expect many sizeable correlations. if no correlation within the correlation matrix (<strong>R</strong>) with exceeds 0.3 then there is probably nothing to factor analyse.<br />
</li>
<li>the anti-image correlations matrix (pairwise correlations adjusted for the effects of all other variables) will have mostly small values among the off-diagonal elements.</li>
<li>Kaiser’s measure of sampling adequacy, <span class="math inline">\(\sum(\text{squared correlations})/(\sum(\text{squared correlations}) + \sum(\text{squared partial correlations}))\)</span> &gt; 0.6</li>
</ul>
</div>
</div>
<div id="fundamental-equations-for-factor-analysis" class="section level2">
<h2>Fundamental Equations for Factor Analysis</h2>
<p>Matrices</p>
<table style="width:100%;">
<colgroup>
<col width="8%" />
<col width="19%" />
<col width="50%" />
<col width="12%" />
<col width="9%" />
</colgroup>
<thead>
<tr class="header">
<th>Label</th>
<th>Matrix Name</th>
<th>Description</th>
<th>Orthogonal</th>
<th>Oblique</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(R\)</span></td>
<td>Correlation</td>
<td>(Observed) Between-variable correlation</td>
<td><i class="fa fa-check-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="even">
<td><span class="math inline">\(Z\)</span></td>
<td>Variable</td>
<td>Standardized observed variable values(scores)</td>
<td><i class="fa fa-check-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(F\)</span></td>
<td>Factor-score</td>
<td>Standardized factor scores</td>
<td><i class="fa fa-check-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A\)</span></td>
<td>Factor loading</td>
<td>Correlations between factor &amp; variable</td>
<td><i class="fa fa-check-square-o"></i></td>
<td><i class="fa fa-square-o"></i></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(A\)</span></td>
<td>Pattern</td>
<td>Contribution of factor to variance</td>
<td><i class="fa fa-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="even">
<td><span class="math inline">\(C\)</span></td>
<td>Structure</td>
<td>Correlations between variables and correlated factors</td>
<td><i class="fa fa-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(B\)</span></td>
<td>Factor-Score Coefficients</td>
<td>Coefficients to generate factor scores from variables</td>
<td><i class="fa fa-check-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\Phi\)</span></td>
<td>Factor correlation</td>
<td>Correlations among factors</td>
<td><i class="fa fa-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(L\)</span></td>
<td>Eigenvalue</td>
<td>Diagonal matrix of eigenvalues, one per factor</td>
<td><i class="fa fa-check-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
<tr class="even">
<td><span class="math inline">\(V\)</span></td>
<td>Eigenvector</td>
<td>Eigenvectors, one per eigenvalue</td>
<td><i class="fa fa-check-square-o"></i></td>
<td><i class="fa fa-check-square-o"></i></td>
</tr>
</tbody>
</table>
<p><br><br> Equations:</p>
<ul>
<li><strong>Reproduced Correlation Matrix (<span class="math inline">\(\bar{R}\)</span>)</strong>: Factor correlation, <span class="math inline">\(\bar{R} = A A&#39;\)</span></li>
<li><strong>Residual Correlation Matrix (<span class="math inline">\(R_\text{res}\)</span>)</strong>: Observed - Reproduced (small for good factor analysis), <span class="math inline">\(R_\text{res} = R - \bar{R}\)</span></li>
<li><strong>Factor Score Coefficients Matrix (<span class="math inline">\(B\)</span>)</strong>: Coefficients from linear combinations of variables, <span class="math inline">\(B = R^{-1} A\)</span>.</li>
<li><strong>Factor Scores</strong>: <span class="math inline">\(F = ZB\)</span></li>
<li><strong>Predicted Scores</strong>: <span class="math inline">\(Z = FA&#39;\)</span></li>
</ul>
<div id="fundamental-equations-for-factor-analysis-extraction" class="section level3">
<h3>Fundamental Equations for Factor Analysis: Extraction</h3>
<p>The correlation matrix (<span class="math inline">\(R\)</span>), which by nature is symmetric, can be diagonalised by the matrix <span class="math inline">\(V\)</span> (whose columns are the eigenvectors), such that <span class="math display">\[L = V&#39;RV\]</span> where <span class="math inline">\(L\)</span> is a diagonal matrix with values eigenvalues in the diagonal</p>
<p><strong>Example</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skiers</th>
<th align="right">cost</th>
<th align="right">lift</th>
<th align="right">depth</th>
<th align="right">powder</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">S1</td>
<td align="right">32</td>
<td align="right">64</td>
<td align="right">65</td>
<td align="right">67</td>
</tr>
<tr class="even">
<td align="left">S2</td>
<td align="right">61</td>
<td align="right">37</td>
<td align="right">62</td>
<td align="right">65</td>
</tr>
<tr class="odd">
<td align="left">S3</td>
<td align="right">59</td>
<td align="right">40</td>
<td align="right">45</td>
<td align="right">43</td>
</tr>
<tr class="even">
<td align="left">S4</td>
<td align="right">36</td>
<td align="right">62</td>
<td align="right">34</td>
<td align="right">35</td>
</tr>
<tr class="odd">
<td align="left">S5</td>
<td align="right">62</td>
<td align="right">46</td>
<td align="right">43</td>
<td align="right">40</td>
</tr>
<tr class="even">
<td align="left"><br><br></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>Correlation Matrix</p>
<pre class="r"><code>cor.ski &lt;- cor(dplyr::select(dat.ski, -skiers)) 
cor.ski</code></pre>
<pre><code>##               cost        lift       depth      powder
## cost    1.00000000 -0.95299048 -0.05527555 -0.12999882
## lift   -0.95299048  1.00000000 -0.09110654 -0.03624823
## depth  -0.05527555 -0.09110654  1.00000000  0.99017435
## powder -0.12999882 -0.03624823  0.99017435  1.00000000</code></pre>
<p>Strong correlations between:</p>
<ul>
<li>importance placed on <em>cost</em> of ski ticket and speed of ski <em>lift</em></li>
<li>importance placed on snow <em>depth</em> and mositure (<em>powder</em>)</li>
</ul>
<p>Eigenvalues and Eigenvectors:</p>
<pre class="r"><code>eig.ski &lt;- eigen(cor.ski)
eig.ski</code></pre>
<pre><code>## eigen() decomposition
## $values
## [1] 2.016305104 1.941513814 0.037812306 0.004368776
## 
## $vectors
##            [,1]       [,2]       [,3]       [,4]
## [1,]  0.3524130  0.6143298  0.6624913  0.2439451
## [2,] -0.2511248 -0.6637642  0.6758934  0.1988000
## [3,] -0.6273987  0.3222291  0.2754625 -0.6531919
## [4,] -0.6473888  0.2796147 -0.1685044  0.6887014</code></pre>
<p>Note that <span class="math inline">\(L = V&#39;RV&#39;\)</span></p>
<pre class="r"><code>zapsmall(t(eig.ski$vectors) %*% cor.ski %*% eig.ski$vectors)</code></pre>
<pre><code>##          [,1]     [,2]      [,3]      [,4]
## [1,] 2.016305 0.000000 0.0000000 0.0000000
## [2,] 0.000000 1.941514 0.0000000 0.0000000
## [3,] 0.000000 0.000000 0.0378123 0.0000000
## [4,] 0.000000 0.000000 0.0000000 0.0043688</code></pre>
<pre class="r"><code>diag(eig.ski$values)</code></pre>
<pre><code>##          [,1]     [,2]       [,3]        [,4]
## [1,] 2.016305 0.000000 0.00000000 0.000000000
## [2,] 0.000000 1.941514 0.00000000 0.000000000
## [3,] 0.000000 0.000000 0.03781231 0.000000000
## [4,] 0.000000 0.000000 0.00000000 0.004368776</code></pre>
<p>and that pre-and post-multplying the corelation matrix by eigenvectors does not change it so much as repackage it: <span class="math display">\[V&#39;V = I\]</span></p>
<pre class="r"><code>zapsmall(crossprod(eig.ski$vectors))</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]    1    0    0    0
## [2,]    0    1    0    0
## [3,]    0    0    1    0
## [4,]    0    0    0    1</code></pre>
<p>Rearranging the equation <span class="math inline">\(L=V&#39;RV\)</span>,</p>
<p><span class="math display">\[\begin{align}
  R &amp;= VLV&#39;\\
    &amp;= V\sqrt{L}\sqrt{L} V&#39;\\
    &amp;=(V\sqrt{L})(\sqrt{L}V&#39;) \\
    &amp;= A A&#39;
    \end{align}\]</span> where <span class="math inline">\(A=V\sqrt{L}\)</span> is referred to as the factor loading matrix; the correlation matrix is a product of hte factor laoding matrix <span class="math inline">\(A\)</span> and its transpose.</p>
<p>In this case, the (unrotated) factor loading matrix is:</p>
<pre class="r"><code>eig.ski$vectors %*% sqrt(diag(eig.ski$values))</code></pre>
<pre><code>##            [,1]       [,2]        [,3]        [,4]
## [1,]  0.5004147  0.8559962  0.12882400  0.01612397
## [2,] -0.3565888 -0.9248772  0.13143009  0.01314003
## [3,] -0.8908852  0.4489882  0.05356475 -0.04317384
## [4,] -0.9192705  0.3896101 -0.03276633  0.04552090</code></pre>
<p>The correlation matrix can herefore be considered a product of two matrices - each a combination of eigenvectors and the square root of eigenvalues</p>
<ul>
<li>Because there are 4 variables, there are 4 eigenvalues</li>
<li>Each eigenvalue corresponds to a different potential factor; usually only factors with large eigenvalues are retrained</li>
<li>In good FA, a few factors will almost duplicate correlation matrix</li>
<li>In this example, only first 2 factors, with eigenvalues &gt; 1 are large enough</li>
</ul>
<p>Starting with the correlation matrix <span class="math inline">\(R\)</span> and assuming <span class="math inline">\(k\)</span> factors are used, the steps of factor analysis (principal factor solution method) are as follows:</p>
<ol style="list-style-type: decimal">
<li>Get eigenvalues <span class="math inline">\(L\)</span> and eigenvectors <span class="math inline">\(V\)</span> of correlation matrix <span class="math inline">\(r\)</span></li>
<li>Calculate <span class="math inline">\(C = \sum \text{diag}(R)\)</span></li>
<li>Calculate the loadings, <span class="math inline">\(A = V[,1:k]\sqrt{L[1:k]}\)</span>, (eq 13.6 of text)</li>
<li>Set <span class="math inline">\(R^* = AA&#39;\)</span> (eq 13.5 of text, R=AA’)</li>
<li>Set <span class="math inline">\(C^* = \sum \text{diag} (R^*)\)</span></li>
<li>Update <span class="math inline">\(\text{diag}(R) = \text{diag}(R^*)\)</span> Repeat above steps until max iterations reached, or until <span class="math inline">\(e = |C-C^*)|\)</span> is smaller than some threshold</li>
</ol>
<pre class="r"><code># This is taken from the function fac and is replicated using 
#fit &lt;- fa(cor.ski, nfactors=2, fm=&quot;pa&quot;)
#fit # print results

# Initialisation
r &lt;- cor(dplyr::select(dat.ski, -skiers)) 
n &lt;- dim(r)[2]
r.mat &lt;- r
colnames(r.mat) &lt;- rownames(r.mat) &lt;- colnames(r)
nfactors &lt;- 2
max.iter &lt;- 50
min.err &lt;- 0.001

orig &lt;- diag(r)
comm &lt;- sum(diag(r.mat))
err &lt;- comm
i &lt;- 1
comm.list &lt;- list()

e.values &lt;- eigen(r)$values

# Loop
while (err &gt; min.err) {
  eigens &lt;- eigen(r.mat, symmetric = TRUE)
  if (nfactors &gt; 1) {
    loadings &lt;- eigens$vectors[, 1:nfactors] %*% 
      diag(sqrt(eigens$values[1:nfactors]))
  }
  else {
    loadings &lt;- eigens$vectors[, 1] * sqrt(eigens$values[1]) #A in book
  }
  model &lt;- loadings %*% t(loadings) # eqn 13.5: R = AA&#39; (if r is n x n this will always be an n x n matrix, because n x k %*% k x n gives n x n matrix)
  new &lt;- diag(model)
  comm1 &lt;- sum(new)
  diag(r.mat) &lt;- new                # update diagonals of correlation matrix
  err &lt;- abs(comm - comm1)
  if (is.na(err)) {
    warning(&quot;imaginary eigen value condition encountered in fa\n Try again with SMC=FALSE \n exiting fa&quot;)
    break
  }
  comm &lt;- comm1
  comm.list[[i]] &lt;- comm1
  i &lt;- i + 1
  if (i &gt; max.iter) {
    if (warnings) {
      message(&quot;maximum iteration exceeded&quot;)
    }
    err &lt;- 0
  }
}

# Clean-up
eigenv &lt;- eigens$vectors
eigens &lt;- eigens$values
if (nfactors &gt; 1) {
  sign.tot &lt;- vector(mode = &quot;numeric&quot;, length = nfactors)
  sign.tot &lt;- sign(colSums(loadings))
  sign.tot[sign.tot == 0] &lt;- 1
  loadings &lt;- loadings %*% diag(sign.tot)
} else {
  if (sum(loadings) &lt; 0) {
    loadings &lt;- -as.matrix(loadings)
  }
  else {
    loadings &lt;- as.matrix(loadings)
  }
  colnames(loadings) &lt;- &quot;MR1&quot;
}
colnames(loadings) &lt;- paste(&quot;PA&quot;, 1:nfactors, sep = &quot;&quot;)
rownames(loadings) &lt;- rownames(r)
loadings[loadings == 0] &lt;- 10^-15
model &lt;- loadings %*% t(loadings)
f.loadings &lt;- loadings</code></pre>
<p>After iterating through the Factor Analysis 8 times we get the following eigenvectors:</p>
<pre><code>##       V1     V2
## 1  0.284  0.649
## 2 -0.179 -0.686
## 3 -0.657  0.253
## 4 -0.675  0.209</code></pre>
<p>and eigenvalues</p>
<pre><code>## [1] 2.005 1.911</code></pre>
<p>Note that the property, <span class="math inline">\(VV&#39; = I\)</span>, is maintained.</p>
<pre class="r"><code>crossprod(eigenv[, 1:nfactors])</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1</code></pre>
<p>The factor loading matrix, <span class="math inline">\(A = V\sqrt{L}\)</span>, is</p>
<pre><code>##           PA1    PA2
## cost   -0.403  0.898
## lift    0.254 -0.948
## depth   0.930  0.350
## powder  0.956  0.289</code></pre>
<p>The factor loading matrix lists the correlations between factors and variables:</p>
<ul>
<li>Factor 1 reflects snow conditions</li>
<li>Factor 2 reflects resort conditions; people who score high on this factor place a lot of importance on the cost of a ski ticket without concern for the lift speeds whereas those that score low on this factor value lift speeds more than ski ticket costs.</li>
</ul>
</div>
</div>
<div id="orthogonal-rotation" class="section level2">
<h2>Orthogonal Rotation</h2>
<p>When factors are uncorrelated; produces a loading matrix (correlations between factors and variables)</p>
<p>Rotation used to</p>
<ul>
<li>Maximise high correlations between factors and variables</li>
<li>Minimise low correlations between factors and variables</li>
</ul>
<p><span class="math display">\[\begin{align}
  \text{Rotated Loading Matrix} &amp;= (\text{Unrotated Loading Matrix})(\text{Transformation Matrix})\\
    A_\text{Rotated} &amp;= A_\text{Unrotated}\Lambda
    \end{align}\]</span></p>
<p>The transformation matrix is a matrix of sines and cosines of an angle <span class="math inline">\(\psi\)</span>. In this example an angle of <span class="math inline">\(\psi = 19^\circ\)</span> is used, with <span class="math display">\[\Lambda = \begin{bmatrix}\cos \psi &amp; -\sin \psi\\ \sin \psi &amp; \cos \psi\end{bmatrix}\]</span></p>
<pre class="r"><code>rotated &lt;- stats::varimax(loadings)
loadings_rotated &lt;- rotated$loadings
rot.mat &lt;- rotated$rotmat
rot.mat  # transformation matrix</code></pre>
<pre><code>##           [,1]       [,2]
## [1,] 0.9418515 -0.3360294
## [2,] 0.3360294  0.9418515</code></pre>
<p>The communalities, variance and covariance are shown in the following table:</p>
<pre class="r"><code>fit &lt;- fa(cor.ski, nfactors=2, fm=&quot;pa&quot;, rotate=&quot;varimax&quot;)
loadings_sol &lt;- unclass(fit$loadings)
ssls &lt;- apply(loadings_sol, 2, function(x) sum(x^2)) # row-total
comms &lt;- apply(loadings_sol, 1, function(x) sum(x^2)) # column-total
prop_var &lt;- ssls/nrow(loadings_sol)
prop_cov &lt;- ssls/sum(comms)

res &lt;- rbind(loadings_sol, ssls, prop_var, prop_cov)
res &lt;- data.frame(var = rownames(res), res) %&gt;% mutate(communalities = ifelse(var %in% c(&#39;cost&#39;, &#39;lift&#39;, &#39;depth&#39;, &#39;powder&#39;), PA1^2 + PA2^2, PA1 + PA2))
res</code></pre>
<pre><code>##        var         PA1         PA2 communalities
## 1     cost -0.07757836 -0.98203830     0.9704176
## 2     lift -0.07933109  0.97641250     0.9596748
## 3    depth  0.99387040 -0.01742487     0.9880820
## 4   powder  0.99708024  0.04875513     0.9965461
## 5     ssls  1.99425920  1.92046127     3.9147205
## 6 prop_var  0.49856480  0.48011532     0.9786801
## 7 prop_cov  0.50942570  0.49057430     1.0000000</code></pre>
<p>The <strong>reproduced correlation matrix</strong> is <span class="math inline">\(\bar{R} = AA&#39;\)</span>:</p>
<pre class="r"><code>round(loadings_sol %*% t(loadings_sol),3)</code></pre>
<pre><code>##          cost   lift  depth powder
## cost    0.970 -0.953 -0.060 -0.125
## lift   -0.953  0.960 -0.096 -0.031
## depth  -0.060 -0.096  0.988  0.990
## powder -0.125 -0.031  0.990  0.997</code></pre>
<p>The <strong>residual correlation matrix</strong> is <span class="math inline">\(R_\text{res} = R - \bar{R}\)</span></p>
<pre class="r"><code>rres &lt;- cor.ski - loadings_sol %*% t(loadings_sol)
diag(rres) &lt;- 0 # could otherwise replace with communalities
round(rres,3)</code></pre>
<pre><code>##          cost   lift depth powder
## cost    0.000  0.000 0.005 -0.005
## lift    0.000  0.000 0.005 -0.005
## depth   0.005  0.005 0.000  0.000
## powder -0.005 -0.005 0.000  0.000</code></pre>
<p>The FA is good if numbers in residual correlation matrix are small.</p>
<p>Factor score coefficients (for estimating factor scores) are found by multiplying the inverse correlation matrix and the factor loading matrix, <span class="math inline">\(B = R^{-1}A\)</span></p>
<pre class="r"><code>B &lt;- solve(r) %*% loadings_sol
B</code></pre>
<pre><code>##               PA1         PA2
## cost   0.13783453 -0.59197611
## lift   0.09353632  0.41350483
## depth  0.08326838  0.03113046
## powder 0.93593888 -0.04403683</code></pre>
<p>Factor scores are a product of standardized variable scores and factor score coefficients, <span class="math inline">\(F = ZB\)</span>:</p>
<pre class="r"><code>F &lt;- as.matrix(select(dat.ski, -skiers) %&gt;% mutate_all(scale)) %*% B
F</code></pre>
<pre><code>##             PA1        PA2
## [1,]  1.1012894  1.1795542
## [2,]  1.0263476 -0.8816516
## [3,] -0.4589879 -0.6769470
## [4,] -1.0816206  0.9740182
## [5,] -0.5870286 -0.5949738</code></pre>
<p>Here, for example, the first subject scores strongly on both snow (strong importance on both depth and powder) and resort factor (strong importance on lift over cost).</p>
<p>Predicting standardized scores on variables from scores on factors is also possible, using a product of scores on factors wiethged by factor laodings, <span class="math inline">\(Z = FA&#39;\)</span>:</p>
<pre class="r"><code>Z &lt;- F %*% t(loadings_sol)
round(Z, 3)</code></pre>
<pre><code>##        cost   lift  depth powder
## [1,] -1.244  1.064  1.074  1.156
## [2,]  0.786 -0.942  1.035  0.980
## [3,]  0.700 -0.625 -0.444 -0.491
## [4,] -0.873  1.037 -1.092 -1.031
## [5,]  0.630 -0.534 -0.573 -0.614</code></pre>
<p>In algebraic form, the formula is: <span class="math display">\[\begin{align}
  z_\text{COST} &amp;= a_{11}F_1 + a_{12}F_2 \\
  z_\text{LFIT} &amp;= a_{21}F_1 + a_{22}F_2 \\
  z_\text{DEPTH} &amp;= a_{31}F_1 + a_{312}F_2 \\
  z_\text{POWDER} &amp;= a_{41}F_1 + a_{42}F_2
  \end{align}
  \]</span></p>
<p>The assumption is that each subject has the same latent structure but different scores on the factors themselves.</p>
<p>Tying it all together, the folloiwng is the computer output from the <code>psych::fa</code></p>
<pre class="r"><code>invisible(fit &lt;- fa(cor.ski, nfactors=2, fm=&quot;pa&quot;, rotate=&quot;varimax&quot;))
fit</code></pre>
<pre><code>## Factor Analysis using method =  pa
## Call: fa(r = cor.ski, nfactors = 2, rotate = &quot;varimax&quot;, fm = &quot;pa&quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##          PA1   PA2   h2     u2 com
## cost   -0.08 -0.98 0.97 0.0296   1
## lift   -0.08  0.98 0.96 0.0403   1
## depth   0.99 -0.02 0.99 0.0119   1
## powder  1.00  0.05 1.00 0.0035   1
## 
##                        PA1  PA2
## SS loadings           1.99 1.92
## Proportion Var        0.50 0.48
## Cumulative Var        0.50 0.98
## Proportion Explained  0.51 0.49
## Cumulative Proportion 0.51 1.00
## 
## Mean item complexity =  1
## Test of the hypothesis that 2 factors are sufficient.
## 
## The degrees of freedom for the null model are  6  and the objective function was  7.34
## The degrees of freedom for the model are -1  and the objective function was  0.43 
## 
## The root mean square of the residuals (RMSR) is  0 
## The df corrected root mean square of the residuals is  NA 
## 
## Fit based upon off diagonal values = 1
## Measures of factor score adequacy             
##                                                   PA1  PA2
## Correlation of (regression) scores with factors     1 0.99
## Multiple R square of scores with factors            1 0.98
## Minimum correlation of possible factor scores       1 0.96</code></pre>
<ul>
<li><strong>Factor Loadings (PA1, PA2)</strong>: The correlations between factors and variables.</li>
<li><strong>Communality (h2)</strong>: The proportion of variance in a variable that is explained for by the factors, equal to the sum of the squared loadings (SSL). E.g., 97% of the variance in COST is accounted for by the two factors.</li>
<li><strong>Uniqueness (u2)</strong>: Approximately, <span class="math inline">\(R = FF&#39; + U^2\)</span>. Because unique and eror variances are omitted, a linear combination of factors approximates but does not duplicate the observed correlation matrix and scores on observed variables.<br />
</li>
<li><strong>Complexity (com)</strong>: Hoffman’s index of complexity, <span class="math inline">\(\frac{(\sum a_I^2)^2}{\sum a_i^4}\)</span></li>
<li><strong>SS Loadings</strong>: Sum of squared loadings</li>
<li><strong>Proportion Var</strong>: Proportion of variance accounted for by each factor; 49.9% of the variance in the variables is accounted for by the first factor and 49.9% of the variance in the variables is accounted for by th esecond factor.<br />
</li>
<li><strong>Cumulative Var</strong>: Cumulative proprtion of variance explained (across factors). Because rotation is orthogonal, the two factors together account for 97.9% of the variance in the variables</li>
<li><strong>Proportion Explained</strong>: The proportion of variance in the solution accounted for by a factor; the two factors account for 50.9% and 49.1% of the variance in the solution, respectively.</li>
</ul>
</div>
<div id="oblique-rotation" class="section level2">
<h2>Oblique Rotation</h2>
<p>When factors are correlated; the loading matrix (correlations between variables and correlated factors) is decomposed into the following:</p>
<ul>
<li>Pattern matrix (<span class="math inline">\(A\)</span>); when squared, contribution of each factor to variance of variable, without segments o variance that come from overlap betweeen correlated factors</li>
<li>Structure matrix (<span class="math inline">\(C\)</span>); correlations between variables and factors.<br />
<span class="math display">\[C = A\Phi\]</span></li>
</ul>
<p>The factor correlations matrix <span class="math inline">\(\Phi\)</span> is also produced.</p>
<p>To determine the structure matrix, the following steps are required: 1. Determine factor score coefficients, <span class="math inline">\(B = R^{-1}A\)</span> 2. Determine factor scores, which are a product of the standardized variable scores and factor score coefficients, <span class="math inline">\(F = ZB\)</span> 3. Determine correlations among factors</p>
<pre class="r"><code>invisible(fit &lt;- fa(dat.ski[,-1], nfactors=2, fm=&quot;pa&quot;, rotate=&quot;oblimin&quot;, scores=&quot;Thurstone&quot;))

# Pattern matrix
oblimin_A &lt;- unclass(fit$loadings)

# Factor-Score Coefficients
oblimin_B &lt;- solve(r) %*% oblimin_A %*% fit$Phi
oblimin_B</code></pre>
<pre><code>##               PA1         PA2
## cost   0.12847275  0.59206210
## lift   0.10005202 -0.41344632
## depth  0.08374941 -0.03107844
## powder 0.93512712  0.04462147</code></pre>
<pre class="r"><code>fit$weights </code></pre>
<pre><code>##               PA1         PA2
## cost   0.12847275  0.59206210
## lift   0.10005202 -0.41344632
## depth  0.08374941 -0.03107844
## powder 0.93512712  0.04462147</code></pre>
<pre class="r"><code># Factor Scores
Z &lt;- as.matrix(select(dat.ski, -skiers) %&gt;% mutate_all(scale)) 
oblimin_F &lt;- Z %*% oblimin_B 
oblimin_F</code></pre>
<pre><code>##             PA1        PA2
## [1,]  1.1197720 -1.1788660
## [2,]  1.0123025  0.8822925
## [3,] -0.4696166  0.6766602
## [4,] -1.0661105 -0.9746937
## [5,] -0.5963473  0.5946070</code></pre>
<pre class="r"><code># Factor Correlation; cross product of standardized factor scores / (number of cases - 1)
oblimin_Phi &lt;- (1/(nrow(oblimin_F)-1)) * t(oblimin_F) %*% oblimin_F
diag(oblimin_Phi) &lt;- 1
oblimin_Phi</code></pre>
<pre><code>##             PA1         PA2
## PA1  1.00000000 -0.01503656
## PA2 -0.01503656  1.00000000</code></pre>
<pre class="r"><code>fit$Phi</code></pre>
<pre><code>##             PA1         PA2
## PA1  1.00000000 -0.01516085
## PA2 -0.01516085  1.00000000</code></pre>
<pre class="r"><code># Structure
oblimin_C &lt;- oblimin_A %*% fit$Phi
oblimin_C</code></pre>
<pre><code>##                PA1         PA2
## cost   -0.09307059  0.98198964
## lift   -0.06390811 -0.97646186
## depth   0.99347151  0.01804570
## powder  0.99772563 -0.04813229</code></pre>
<pre class="r"><code>unclass(fit$Structure)</code></pre>
<pre><code>##                PA1         PA2
## cost   -0.09307059  0.98198964
## lift   -0.06390811 -0.97646186
## depth   0.99347151  0.01804570
## powder  0.99772563 -0.04813229</code></pre>
<p>From Phi, see that correlation between first factor and sezond facotr is quite low. In this case, one would use orthogonal rotation.</p>
</div>
<div id="major-types-of-factor-analyses" class="section level2">
<h2>Major Types of Factor Analyses</h2>
<ul>
<li>Factor Extraction Techniques:</li>
<li>Principal components</li>
<li>Principal factors</li>
<li>Maximum likelihood factoring</li>
<li>Image factoring</li>
<li>Alpha factoring</li>
<li>Generalized (weighted) least squares factoring</li>
</ul>
<div id="comparison-of-pca-to-fa" class="section level3">
<h3>Comparison of PCA to FA</h3>
<pre class="r"><code>library(DiagrammeR)
library(DiagrammeRsvg)
library(magrittr)</code></pre>
<pre><code>## 
## Attaching package: &#39;magrittr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     set_names</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     extract</code></pre>
<pre class="r"><code>library(rsvg)
library(png)

export_svg(DiagrammeR::grViz(&quot;../../static/graphs/fa.dot&quot;, height=200, width=200)) %&gt;%
  charToRaw %&gt;% rsvg %&gt;% png::writePNG(&#39;../../static/graphs/fa.png&#39;)

export_svg(DiagrammeR::grViz(&quot;../../static/graphs/pca.dot&quot;, height=200, width=200)) %&gt;%
  charToRaw %&gt;% rsvg %&gt;% png::writePNG(&#39;../../static/graphs/pca.png&#39;)</code></pre>
<p>In both, the variance that is analysed is the sum of the values in the positive diagonal.</p>
<table>
<tr>
<th>
Factor Analysis
</th>
<th>
Principal Components Analysis
</th>
</tr>
<tr>
<td>
Only the variance that each observed variable shares with other observed variables is available for analysis (error and unique variance excluded); shared variance is estimated by communalities that are inserted in the positive diagonal of the correlation matrix. The solution concentrates on variables with high communality values. The sum of the communalities (sum of the SSLS) is the variance that is distributed among factors.
</td>
<td>
Ones are in the diagonal; each variables contributes a unit of variance by contributing a 1 to the positive diagonal of the correlation matrix. All the variance is distributed to components, including error and unique variance for each observed variables.
</td>
</tr>
<tr>
<td>
Because unique and error variances are omitted, a linear combination of factors approximates but does not duplicate the observed correlation matrix and scores on observed variables.
</td>
<td>
If all components are retained, PCA duplicates exactly the observed correlation matrix.
</td>
</tr>
<tr>
<td>
Analyses covariance (communalities)
</td>
<td>
Analyses variance
</td>
</tr>
<tr>
<td>
Goal: Reproduce correlation matrix with few orthogonal factors
</td>
<td>
Goal: Reproduce correalation matrix with few orthogonal factors
</td>
</tr>
<tr>
<td>
Non-unique solution
</td>
<td>
Unique solution
</td>
</tr>
<tr>
<td>
Factors are thought to cause variables, <em>what are the underlying processes that could have produced correlations among these variables?</em>; <em>are the correlations among variables consistent with a hypothesised factor structure</em>?
</td>
<td>
Variables cause the component; no underlying theory about which variables should be associated with which factors
</td>
</tr>
<tr>
<td>
Best if want to understand underlying structure
</td>
<td>
Best when after an empirical summary of the data set
</td>
</tr>
<tr>
<td>
<img src="/graphs/fa.png" />
</td>
<td>
<img src="/graphs/fa.png" />
</td>
</tr>
</table>
</div>
</div>
</div>
