---
title: Improving Deep Learning Networks
author: ''
date: 2018-12-07T13:39:46+02:00
slug: improving-deep-learning-networks
categories: [Study-Notes]
tags: 
- Coursera
- Study-Notes
banner: img/banners/improvingDNN.png
---



<div id="week1" class="section level1">
<h1>Practical aspects of deep learning</h1>
<div id="setting-up-your-machine-learning-application" class="section level2">
<h2>Setting up your machine learning application</h2>
<div id="traindevtest-sets" class="section level3">
<h3>Train/Dev/Test Sets</h3>
<p>Decisions to make</p>
<ul>
<li>No. of layers</li>
<li>No. of hidden units</li>
<li>Learning rates</li>
<li>Activation functions</li>
</ul>
<p>Impossible to correctly guess. In practice, it is iterative. Idea -&gt; code -&gt; experiment -&gt; Refine Idea -&gt; repeat …</p>
<p>Applied to NLP, computer vision, speech processing and structured data (advertisements, web search, computer security, logistics)</p>
<p>Intuitions from one domain/application area often do not transfer to another</p>
<p>Best choice = f(computer (GPU/CPU), # input features)</p>
<p>Need to be able to go around cycle quickly to be efficient.</p>
<p>Previous Era of Machine Learning (with small data sets): Data = Training (60%) + Hold-Out/Cross-Validation/Development Set/Dev (20%) + Test (20%). Or alternatively 70% training + 30% test</p>
<ul>
<li>Train multiple model</li>
<li>Compare using Dev</li>
<li>Use test for final model</li>
</ul>
<p>In modern big data era (e.g. 1,000,000 training examples), this is no longer best practice. May only need 10,000 examples for dev and test, making a ratio of 98% train, 1% dev, 1% test. With more than 1,000,000 examples, may have dev and test of .4% and .1%, respectively.</p>
<p>More specific guidelines will be given later in course.</p>
<p>Mismatched train/test distributions:</p>
<ul>
<li>E.g to identify cat may have different sources for dev/test and training.</li>
<li>Training set: Cat pictures from webpages (professional)</li>
<li>Dev/Test: Cat pictures from users using your app (blurrier, low-res)</li>
</ul>
<p>Guideline: Ensure dev and test sets come from same distribution. Okay to get training set from different sources</p>
<p>It might be okay not to have a test set. (Goal: unbiased estimate for final)</p>
<p>Otherwise, train on training set. Eval on dev set and try to get to good model, but will no longer have unbiased estimate of performance. Usually people say have train/test but what are really referring to is a train/dev set.</p>
</div>
<div id="biasvariance" class="section level3">
<h3>Bias/Variance</h3>
<p>Bias/Variance - easy to learn. difficult to master. Important Less discussion in deep learning era. Less trade-off in deep learning era.</p>
<p>High bias = underfitting High variance = overfitting</p>
<p>High dimensional problems - difficult to plot decision boundary</p>
<p>Key Metrics:</p>
<ul>
<li>Train set error</li>
<li>Dev set error</li>
</ul>
<p>If train set error &lt; dev set error then <strong>high variance</strong> (overfitting) If train set error <span class="math inline">\(\approx\)</span> dev set error and train set error high then <strong>high bias</strong></p>
<p>Example when considering classification of image as cat:</p>
<table style="width:88%;">
<colgroup>
<col width="15%" />
<col width="18%" />
<col width="18%" />
<col width="18%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th>Data set</th>
<th>Error Eg 1</th>
<th>Error Eg 2</th>
<th>Error Eg 3</th>
<th>Error Eg 4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Train:</td>
<td>1%</td>
<td>15%</td>
<td>15%</td>
<td>0.5%</td>
</tr>
<tr class="even">
<td>Test:</td>
<td>11%</td>
<td>16%</td>
<td>30%</td>
<td>1%</td>
</tr>
<tr class="odd">
<td>Diagnosis:</td>
<td>High variance</td>
<td>High bias</td>
<td>High bias &amp; high variance</td>
<td>Low bias and low variance</td>
</tr>
</tbody>
</table>
<p>Subtlety: Diagnosis depends on optimal error (Bayes error) = <span class="math inline">\(\approx\)</span> 0% (i.e. human can correctly detect cat pictures with almost 0 error). If baseline error around 15%, then the second example would be diagnosed as low bias and low variance.</p>
<p>Assumption: Training and dev sets draw from same distribution, otherwise require a more sophisticated analysis (see later video)</p>
</div>
<div id="basic-recipe-for-machine-learning" class="section level3">
<h3>Basic recipe for machine learning</h3>
<p>Following is a recipe for machine learning to systematically improve machine learning algorithm depending on whether high bias or high variance :-)</p>
<p>After training initial model:</p>
<p>Ask: Does it have high bias (look at training data)?</p>
<div class="figure">
<img src="/img/ImprovingDNN/bias_variance_decisions.png" alt="Deep NN Forward Propagation" />
<p class="caption">Deep NN Forward Propagation</p>
</div>
<p>Pre: Deep learning, it was a <strong>bias-variance tradeoff</strong>. But now there are tools (e.g. ability to use bigger network or more data) to improve both bias and variance. So really don’t need to trade-off anymore.</p>
</div>
</div>
<div id="regularising-your-neural-network" class="section level2">
<h2>Regularising your neural network</h2>
<div id="regularisation" class="section level3">
<h3>Regularisation</h3>
<p>If suspect overfitting (high variance), may not be possible to get more data. Regularisation will help.</p>
<div id="logistic-regression" class="section level4">
<h4>Logistic regression</h4>
<p>Aim <span class="math inline">\(\min\limits_{w, b} J(w,b)\)</span> where <span class="math inline">\(w \in \mathbb{R}^{n_x}, b \in \mathbb{R}\)</span>.</p>
<p><span class="math display">\[J(w,b) = \frac{1}{m} \sum^{m}_{i=1}\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\sum_{j=1}^{n_x}||w^{[j]}||_2^2\]</span></p>
<p><span class="math inline">\(L_2\)</span> regularisation: <span class="math display">\[\frac{\lambda}{2m}\sum_{j=1}^{n_x}||w||_2^2\]</span> <span class="math inline">\(L_1\)</span> regularisation: <span class="math display">\[\frac{\lambda}{2m}\sum_{j=1}^{n_x}|w|_j\]</span></p>
<p><span class="math inline">\(\lambda\)</span> is a hyperparameter.</p>
<ul>
<li>reserved word in Python (use instead <em>lambd</em>)</li>
<li>Needs to be tuned using cross-validation</li>
</ul>
<p>Typically <span class="math inline">\(L_2\)</span> used; <span class="math inline">\(L_1\)</span> results in sparse matrix for <span class="math inline">\(w\)</span> which some say is an advantage but in practice not used often.</p>
<p>Could regularise <span class="math inline">\(b\)</span> but just a single number, whereas <span class="math inline">\(w\)</span> has a lot of parameters.</p>
</div>
<div id="neural-network" class="section level4">
<h4>Neural Network</h4>
<p><span class="math display">\[J(w^{[1]},b^{[1]} \ldots, w^{[L]}, b^{[L]}) = \frac{1}{m} \sum^{m}_{i=1}\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\sum_{l=1}^{L}||w^{[l]}||_F^2\]</span></p>
<p>Matrix norm = Frobenius norm <span class="math display">\[||w^{[l]}||_F^2 = \sum_{i=1}^{n^{[l-1]}}\sum_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^2\]</span></p>
<p>Denoted by <span class="math inline">\(F\)</span> because it is <em>not</em> an <span class="math inline">\(L_2\)</span> norm</p>
<p>Impact on calculations</p>
<p><span class="math display">\[\begin{aligned}
    dw^{[l]} &amp;= \text{(from backprop)} + \frac{\lambda}{m}w^{[l]} \\
    w^{[l]} &amp;:= w^{[l]} - \alpha dw^{[l]} \\
            &amp;:= w^{[l]} - \alpha \left[ \text{(from backprop)} + \frac{\lambda}{m}w^{[l]}\right] \\
            &amp;:= w^{[l]}(1-\frac{\alpha \lambda}{m}) - \alpha\text{(from backprop)}
        \end{aligned}\]</span></p>
<p>L2 regularisation is sometimes called <strong>weight decay</strong> as it is like ordinary gradient descent, where you update <span class="math inline">\(w\)</span> by subtracting alpha times the original gradient you got from backprop, but you now multiply <span class="math inline">\(w\)</span> by <span class="math inline">\((1-\frac{\alpha \lambda}{m})\)</span>, which is a little bit less than 1.</p>
</div>
</div>
<div id="why-regularisation-reduces-overfitting" class="section level3">
<h3>Why regularisation reduces overfitting</h3>
<p>Attempt 1 at Explanation.</p>
<p>With large <span class="math inline">\(\lambda\)</span>, will set weights to zero to zero out a lot hidden units (approaches logistic regression). Although in reality, none are set to zero, but many almost zero. Hopefully intermediate <span class="math inline">\(\lambda\)</span> that gets to “just right”</p>
<p>Attempt 2 at Explanation : If z is quite small, <span class="math inline">\(\tanh(z)\)</span> is almost linear. So by increasing <span class="math inline">\(\lambda\)</span> will decrease <span class="math inline">\(w^{[l]}\)</span>. As <span class="math inline">\(z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}\)</span> then <span class="math inline">\(z^{[l]}\)</span> will be small Every layer will be roughly linear and so network is just linear network and even deep network will be linear.</p>
<p>Implementation Tip: Recall: <span class="math inline">\(J(\centerdot) = \frac{1}{m}\sum\limits_{i=1}^m \mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\sum\limits_{l=1}^{L}||w^{[l]}||^2_F\)</span></p>
<p>Need to plot the new definition of <span class="math inline">\(J\)</span> which includes the regularisation parameter, otherwise will not see <span class="math inline">\(J\)</span> monotonically decreasing.</p>
</div>
<div id="dropout-regularisation" class="section level3">
<h3>Dropout regularisation</h3>
<p>Go through each layer, for each node toss a coin and have .5 probability of removing. Then remove all ingoing and outgoing links from those nodes. Then do back-propagation. On a different training example, you repeat with the toss.</p>
<div id="implementing-dropout-inverted-dropout" class="section level4">
<h4>Implementing dropout (“Inverted dropout”)</h4>
<p>Forward Prop:</p>
<ul>
<li>Illustrate with layer <span class="math inline">\(\ell = 3\)</span></li>
<li>Set vector <span class="math inline">\(d3\)</span> = dropout vector for layer 3 = np.random.rand(a3.shape[0], a3.shape[1]) &lt; keep.prob</li>
<li>Take activations from third layer and set a3 := np.multiply(a3, d3)</li>
<li>Scale <span class="math inline">\(a3\)</span> up by setting a3/= keep.prob. This is so not to reduce the expected value of <span class="math inline">\(a^{[3]}\)</span>. Some earlier algorithms for dropout did not do this step.</li>
</ul>
<p>Noting that on iteration 1 might zero out some hidden units, but on second iteration might zero out different hidden units.</p>
<p>Backward Prop:</p>
<ul>
<li>Shut down the same neurons that were shut down during forward propagation by reapplying the same mask; i.e. set da3 := np.multiply(da3, d3)</li>
<li>Ensure to scale <span class="math inline">\(da3\)</span> up by setting da/= keep_prob.</li>
</ul>
<p>Making predictions at test time:</p>
<p>At test time, don’t use drop out.</p>
<p><span class="math display">\[\begin{aligned}
z^{[0]} &amp;= X\\
z^{[1]} &amp;= w a^{[0]} + b^{[1]}\\
a^{[1]} &amp;= g^{[1]}z^{[1]})\\
z^{[2]} &amp;= w a^{[1]} + b^{[2]}\\
\vdots\\
\hat{y}
\end{aligned}\]</span></p>
</div>
</div>
<div id="understanding-dropout" class="section level3">
<h3>Understanding dropout</h3>
<p>Can’t rely on any one feature, so have to spread out weights, which in turn has an effect of shrinking the weights (similar to L2 regularisation)</p>
<p>Note that can alter keep.prob by layer. e.g. in network with</p>
<ul>
<li>Input layer; <span class="math inline">\(n^{[0]}\)</span> = 3. Usually set keep.prob = 1.0 (i.e.. don’t apply)</li>
<li>Hidden Layer 1; <span class="math inline">\(n^{[1]} = 7\)</span>.</li>
<li>Hidden Layer 2; <span class="math inline">\(n^{[2]} = 7\)</span>. As dimension of weights is <span class="math inline">\(7 \times 7\)</span>, might want lower keep-prob in this layer.</li>
<li>Hidden Layer 3; <span class="math inline">\(n^{[3]} = 3\)</span>.</li>
<li>Hidden Layer 4; <span class="math inline">\(n^{[4]} = 2\)</span>.</li>
<li>Output Layer; <span class="math inline">\(n^{[5]} = 1\)</span></li>
</ul>
<p>Downside of using different keep.probs per layer: More hyperparameters to tune using cross-validation</p>
<p>Alternative: Have some layers where you apply drop out and some where you don’t, therefore have just one hyper-parameter which is the keep.prob for the layers for which you do apply the drop out.</p>
<p>Implementation Notes:</p>
<ul>
<li>First used for computer vision (input many pixels)</li>
<li>Unless algorithm is overfitting, then don’t bother with dropout regularisation. In computer vision, however, often have many pixels and not enough data so often overfit.</li>
</ul>
<p>Downside: Cost function <span class="math inline">\(J\)</span> is less well-defined, then lose debugging team of checking that <span class="math inline">\(J\)</span> is monotonically decreasing with number of iterations. Usually sets keep.prob to 1 and tests that <span class="math inline">\(J\)</span> is monotonically decreasing and that hasn’t added bug when using keep.prob</p>
</div>
<div id="other-regularisation-methods" class="section level3">
<h3>Other regularisation methods</h3>
<p>In addition to</p>
<ul>
<li>L2</li>
<li>Dropout</li>
</ul>
<p>there are other techniques to help with overfitting:</p>
<ul>
<li>Getting more training data (but can be expensive)</li>
<li>Augment training set by, for example,
<ul>
<li>Image detection
<ul>
<li>Flipping training set horizontally (when using computer vision)</li>
<li>Take random crops (rotate, zoom) You wouldn’t flip vertically! These don’t add so much extra info as would a new (independent) picture<br />
</li>
</ul></li>
<li>Optical character recognition can impose random rotations and distortions</li>
</ul></li>
<li>Early stopping. As run gradient descent usually plot training error or cost function <span class="math inline">\(J\)</span> against number of iterations. Should decrease with number of iterations. Also plot dev set error; this usually decreases then increases. Stop at inversion. Andrew Ng sometimes uses but disadvantage is that it takes away from orthogonalisation (i.e. the following two tasks/problems get coupled together; whereas with orthogonality keep as two separate tasks)
<ol style="list-style-type: decimal">
<li>Optimise cost function <span class="math inline">\(J\)</span> via gradient descent, momentum, adam, etc.</li>
<li>Not overfit via regularisation, more data, etc. So early stopping means you are not doing optimisation of cost function very well. Andrew Ng prefers L2 regularisation (although may need to try many values which is more computationally expensive, but on the plus side), Early stopping works similar to L2 regularisation, at earlier iterations <span class="math inline">\(w \approx 0\)</span> whereas for larger iterations, <span class="math inline">\(w\)</span> large.</li>
</ol></li>
</ul>
<p>Note that regularization hurts training set performance! This is because it limits the ability of the network to overfit to the training set. But since it ultimately gives better test accuracy, it is helping your system.</p>
</div>
</div>
<div id="setting-up-your-optimisation-problem" class="section level2">
<h2>Setting up your optimisation problem</h2>
<div id="normalising-inputs" class="section level3">
<h3>Normalising inputs</h3>
<p>Normalising input will speed up training</p>
<ol style="list-style-type: decimal">
<li>Subtract mean to get zero mean; <span class="math display">\[x := x - \mu\]</span> where <span class="math inline">\(\mu = \frac{1}{m}\sum^m_{i = 1}x^{(i)}\)</span></li>
<li>Normalise variance to get variance equal to 1; <span class="math display">\[x := x/\sigma^2\]</span> where <span class="math inline">\(\sigma^2 = \frac{1}{m} \sum^m_{i = 1} {(x^{(i)})}^2\)</span>.</li>
</ol>
<p>Note that do not need to subtract <span class="math inline">\(\mu\)</span> in variance formula here as <span class="math inline">\(\mu\)</span> is already set to zero.</p>
<p>Make sure to use same <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> for training and test set. Data has to go through the same transformation.</p>
<p>Normalisation is done because:</p>
<ul>
<li>With unnormalised, cost function (and therefore contours) is elongated bowl. Will need small learning rate, will oscillate.</li>
<li>With normalisation, get symmetric/spherical, can take much greater steps.</li>
</ul>
<p>Normalisation not so important when features on similar scales; but normalisation won’t hurt so may as well do.</p>
</div>
<div id="vanishing" class="section level3">
<h3>Vanishing/exploding gradients</h3>
<p>Problem with training deep NN is vanishing/exploding gradients - slopes can get very very big or very very small which makes training small.</p>
<p>Making careful choice for random weight initialisation can minimise problem.</p>
<div class="figure">
<img src="/img/ImprovingDNN/ImprovingDNN_explodingGradients.JPG" alt="Deep NN Exploding Gradients" />
<p class="caption">Deep NN Exploding Gradients</p>
</div>
<p>For simplicity:</p>
<ul>
<li>Two hidden units per layer</li>
<li>Assume linear activation function, <span class="math inline">\(g(z) = z\)</span></li>
<li>Ignore <span class="math inline">\(b\)</span>, i.e. set <span class="math inline">\(b = 0\)</span></li>
<li>Therefore <span class="math inline">\(a{[1]} = g(z{[1]}) = z{[1]}\)</span></li>
</ul>
<p>Then</p>
<p><span class="math display">\[\begin{aligned}
    a^{[1]} &amp;= g(z^{[1]}) = g(w^{[1]}X + 0) = w^{[1]}X \\
    a^{[2]} &amp;= g(z^{[2]}) = g(w^{[2]} a^{[1]} + 0) =  w^{[2]} a^{[1]} =  w^{[2]}  w^{[1]}X\\
    a^{[3]} &amp;= g(z^{[3]}) = g(w^{[3]} a^{[2]} + 0) =  w^{[3]} a^{[2]} = w^{[3]}  w^{[2]}  w^{[1]}X\\
     \vdots\\
    \hat{y} &amp;= w^{[l]}w^{[l-1]}w^{[l-2]} \ldots w^{[2]}w^{[1]}X 
\end{aligned}\]</span></p>
<p>Assume <span class="math inline">\(w^{[l]} = \left[\begin{array}{cc} 1.5 &amp; 0 \\  0 &amp; 1.5  \end{array}\right], \forall l\)</span> except last one as it will be of a different dimension</p>
<p>then <span class="math display">\[\hat{y} = w^{[1]}\left[\begin{array}{cc}
1.5 &amp; 0 \\
  0 &amp; 1.5   
 \end{array}\right]^{L-1}X\]</span></p>
<p>So if weights are just larger than identity then activations increase exponentially. Conversely, if weights are just a smaller than identity then activations increase exponentially.</p>
<p>Similar argument can be used to show that gradients also increase/decrease exponentially. With deep learning matrix (i.e. a 152 layer network), then this can cause problems for gradient descent.</p>
</div>
<div id="weight-initialisation-for-deep-networks" class="section level3">
<h3>Weight initialisation for deep networks</h3>
<p>Careful choice of random initialisation will help with problem of vanishing/exploding gradients</p>
<p>Single layer network with <span class="math inline">\(n\)</span> inputs:</p>
<ul>
<li>Assume <span class="math inline">\(b = 0\)</span></li>
<li>$z = w_1x_1 + w_2x_2 + + w_n x_n + 0</li>
<li>The larger <span class="math inline">\(n\)</span> is the smaller you want <span class="math inline">\(w_i\)</span>.</li>
<li>Can set <span class="math inline">\(\text{var}(w_i) = 1/n\)</span>. With ReLU, it is better to have <span class="math inline">\(\text{var}(w_i) = 2/n\)</span></li>
<li><p>With more layers, set <span class="math inline">\(w^{[l]} = \text{np.random.rand(shape) * np.sqrt}(\frac{2}{n^{[l-1]}})\)</span> which draws on the formula for the variance of the Gaussian function. Aim to be closer to one.</p></li>
<li>For tanh activation function, use <span class="math inline">\(\sqrt{\frac{1}{n^{[l-1]}}}\)</span>. Called Xavier initialisation.</li>
<li>Some authors also use <span class="math inline">\(\sqrt{\frac{2}{n^{[l-1]} + n^{[l]}}}\)</span></li>
<li><p>Could also tune variance parameter as a hyperparameter but would be at lower end of what to do (not very important when compared to other parameters you could tune)</p></li>
</ul>
</div>
<div id="numerical-approximation-of-gradients" class="section level3">
<h3>Numerical approximation of gradients</h3>
<ul>
<li>To build up to gradient checking, first look how to approximate gradients</li>
<li>On <span class="math inline">\(x\)</span> axis, start with <span class="math inline">\(\theta\)</span>, e.g. 1
<ul>
<li>Nudge to right <span class="math inline">\(\theta + \epsilon\)</span>, e.g. 1.01</li>
<li>Nudge to left <span class="math inline">\(\theta + \epsilon\)</span>, e.g. 1.01</li>
</ul></li>
<li>Check gradient by using two sides of <span class="math inline">\(epsilon\)</span></li>
<li>Gradient, <span class="math inline">\(g(\theta) \approx \frac{\Delta\text{y}}{\Delta\text{x}} = \frac{f(\theta + \epsilon) - f(\theta - \epsilon)}{2 \epsilon}\)</span></li>
<li>With <span class="math inline">\(f(\theta) = \theta^3\)</span> and <span class="math inline">\(\theta\)</span> = 1 <span class="math display">\[\begin{aligned}
g(\theta) &amp;= f&#39;(\theta) = 3\theta = 3 \\
g(\theta) &amp;\approx \frac{1.01^3 - 0.99^3}{0.02} = 3.0001
\end{aligned}\]</span></li>
</ul>
<p>Note (Calculus), approximation error of derivative is <span class="math inline">\(O(\epsilon^2)\)</span>. If instead used one-sided difference <span class="math inline">\(g(\theta) \approx \frac{f(\theta + \epsilon) - f(\theta)}{\epsilon}\)</span> then approximation error is greater at <span class="math inline">\(O(\epsilon)\)</span>.</p>
</div>
<div id="gradient-checking-grad-check" class="section level3">
<h3>Gradient checking (Grad check)</h3>
<p>Gradient checking will help verify and debug implementation of backpropagation and save LOTS of time.</p>
<p>Steps: 1. Take all parameters <span class="math inline">\(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}, \ldots, W^{[L]}, b^{[L]}\)</span> and reshape into big vector <span class="math inline">\(\theta\)</span>, so that cost function is a function of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(J(\theta)\)</span> 2. Take all derivatives, <span class="math inline">\(dW^{[1]}, db^{[1]}, dW^{[2]}, db^{[2]}, \ldots, dW^{[L]}, db^{[L]}\)</span> and reshape into big vector <span class="math inline">\(d\theta\)</span> which is the same dimension as <span class="math inline">\(\theta\)</span> 3. Is <span class="math inline">\(d\theta\)</span> the gradient, or slope of cost function <span class="math inline">\(J(\theta)\)</span>?</p>
<p>To implement grad check:</p>
<p>for each <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
    d\theta_\text{approx}^{[i]} &amp;= \frac{
        J(\theta_1, \theta_2, \ldots, \theta_{i + \epsilon}, \ldots) - 
        J(\theta_1, \theta_2, \ldots, \theta_{i - \epsilon}, \ldots)}{2\epsilon}\\
        &amp;\approx d\theta^{[i]} ?
\end{aligned}\]</span></p>
<p>Uses Euclidean distances (square root of sum of squares) to calculate difference between two vectors: <span class="math display">\[\frac{||d\theta_\text{approx} - d\theta||_2}{||d\theta_\text{approx}||_2 + ||d\theta||_2}\]</span></p>
<p>With <span class="math inline">\(\epsilon = 10^{-7}\)</span>, if Euclidean distance <span class="math inline">\(\approx\)</span></p>
<ul>
<li><span class="math inline">\(10^{-7}\)</span> then great!<br />
</li>
<li><span class="math inline">\(10^{-5}\)</span> might be okay but take a very careful look.</li>
<li><span class="math inline">\(10^{-3}\)</span> be worried … look at the differences for each value of <span class="math inline">\(i\)</span> to determine where error might be.</li>
</ul>
</div>
<div id="gradient-checking-implementation-notes" class="section level3">
<h3>Gradient checking implementation notes</h3>
<ul>
<li>Don’t use in training in place of <span class="math inline">\(d\theta\)</span>- this is very slow</li>
<li>Use only to debug, once debugged then turn off</li>
<li>If algorithm fails grad check, look at components to try to identify bug, e.g. if <span class="math inline">\(db^{[l]}\)</span> is different, but <span class="math inline">\(dw^{[l]}\)</span> close then bug might be with <span class="math inline">\(db^{[l]}\)</span></li>
<li>Remember regularisation</li>
<li>Doesn’t work with dropout; best to implement grad check without dropout (set keep.prob to 1.0 then turn off), Could otherwise fix pattern and check, but doesn’t recommend.</li>
<li>Run at random initialisation; perhaps again after training. Could be close at <span class="math inline">\(w,b \approx 0\)</span>. Could run grad check at random initialisation, run for a while (train for some number of iterations) then run grad check.</li>
</ul>
</div>
</div>
</div>
<div id="week2" class="section level1">
<h1>Optimization algorithms</h1>
<div id="mini-batch-gradient-descent" class="section level2">
<h2>Mini-batch gradient descent</h2>
<ul>
<li>Vectorisation allows you to efficiently compute on <span class="math inline">\(m\)</span> examples</li>
</ul>
<p><span class="math display">\[\begin{aligned}
X &amp;= x^{(1)} x^{(2)} x^{(3)} \ldots x^{(m)}, X \in \mathbb{R}^{n_x \times m}\\
Y &amp;= y^{(1)} y^{(2)} y^{(3)} \ldots, y  \in \mathbb{R}^{1\times m}
\end{aligned}\]</span></p>
<p>But what if <span class="math inline">\(m\)</span> = 5,000,000?</p>
<p>Will need to train entire training set before take a little step of gradient descent, and then you need to train the entire training step again before taking another step.</p>
<p>Instead, split up training set into mini-batches (baby training set), e.g. with 1000 examples each.</p>
<p>Denote mini-batch <span class="math inline">\(t\)</span> by <span class="math inline">\(X^{\{t\}}, Y^{\{t\}}\)</span>,</p>
<p>where <span class="math display">\[X^{\{1\}}= [x^{(1)} x^{(2)} x^{(3)} \ldots x^{(1000)}], \ \ X^{\{1\}} \in \mathbb{R}^{n_x \times 1000}\]</span>, and <span class="math display">\[Y^{\{1\}}= [y^{(1)} y^{(2)} y^{(3)} \ldots y^{(1000)}], \ \ y^{\{1\}} \in \mathbb{R}^{1 \times 1000}\]</span></p>
<p>So recall:</p>
<ul>
<li>[]: layer index</li>
<li>{}: mini-batch index</li>
<li>(): training index</li>
</ul>
<p>Batch gradient descent refers to gradient descent algorithm used to date.</p>
<p>Mini-batch gradient descent algorithm:</p>
<p>for <span class="math inline">\(t = 1, \ldots, 5000\)</span> (the number of mini-batches)</p>
<ul>
<li><p>Forward prop on <span class="math inline">\(X^{\{t\}}\)</span>, using vectorisation implementation that processes 1000 examples at a time <span class="math display">\[\begin{aligned}
Z^{[1]} &amp;= W^{[1]}X^{\{t\}} + b^{[1]}\\
A^{[1]} &amp;= g^{[1]}(Z^{[1]})\\
\vdots\\
A^{[l]} &amp;= g^{[2]}(Z^{[2]})
\end{aligned}\]</span></p></li>
<li><p>Compute cost function <span class="math display">\[J^{\{t\}} = \frac{1}{1000}  \sum^{1000}_{i=1}\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2 \centerdot 1000}\sum_{l=1}^{L}||w^{[l]}||_F^2\]</span></p></li>
<li><p>Backprop to compute gradients w.r.t. <span class="math inline">\(J^{t}\)</span>, using (<span class="math inline">\(X^{\{t\}}\)</span>, <span class="math inline">\(Y^{\{t\}}\)</span>) <span class="math display">\[\begin{aligned}
w^{[l]} := w^{[l]} - \alpha dw^{[l]} \\
b^{[l]} := b^{[l]} - \alpha db^{[l]} 
\end{aligned}\]</span></p></li>
</ul>
<p><strong>1 epoch</strong>: Going through the 5000 mini-batches (meaning have gone through 5,000,000 training examples); this means you have done 5000 gradient descents. If you had used batch gradient descent you would have only done a single gradient descent with a single pass through the training set.</p>
<p>NB. May want to do more than 1 epoch…</p>
</div>
<div id="understanding-mini-batch-gradient-descent" class="section level2">
<h2>Understanding mini-batch gradient descent</h2>
<ul>
<li>With batch gradient descent, expect cost to go down on every iteration.</li>
<li>With mini-batch gradient descent, cost may not decrease on every iteration. This is because on every iteration, you are using a different training set to plot <span class="math inline">\(J^{\{t\}}\)</span> which is computed using <span class="math inline">\(X^{\{t\}}\)</span>, <span class="math inline">\(Y^{\{t\}}\)</span>. You should expect a downward trend.</li>
</ul>
<div class="figure">
<img src="/img/ImprovingDNN/cost_curves.png" alt="Cost Curves" />
<p class="caption">Cost Curves</p>
</div>
<p>Choosing your mini-batch size:</p>
<ul>
<li><strong>Blue</strong> Batch Gradient Descent: mini-batch size = <span class="math inline">\(m\)</span>
<ul>
<li>Disadvantage: Too long per iteration (with large training set)</li>
</ul></li>
<li><strong>Purple</strong> Stochastic Gradient Descent: mini-batch size = 1
<ul>
<li>Will never converge</li>
<li>Disadvantage: Lose speed-up from vectorisation</li>
</ul></li>
<li><strong>Green</strong> In-between
<ul>
<li>Advantage: Vectorisation + Make progress without needing to wait until process entire training set</li>
</ul></li>
</ul>
<div class="figure">
<img src="/img/ImprovingDNN/batch_contours.png" alt="Batch Contours" />
<p class="caption">Batch Contours</p>
</div>
<p>Guideline:</p>
<ul>
<li>If small training set (<span class="math inline">\(m \leq 2000\)</span>): Use batch gradient descent</li>
<li>Else typical mini-batch size one of {2^6=64, 2^7=128, 2^8=256, 2^9=512}; 2^10}; 1024 is rare. Ensure that mini-batch size fits in CPU/GPU memory, otherwise performance will “fall off the cliff”</li>
<li>Can use a hyperparameter to determine the power of 2 to use.</li>
</ul>
</div>
<div id="exponentially-moving-weighted-averages" class="section level2">
<h2>Exponentially (moving) weighted averages</h2>
<p>Initialise <span class="math inline">\(v_0 = 0\)</span> Set <span class="math display">\[v_t = \beta v_{t-1} + (1-\beta) \theta_t\]</span></p>
<p><span class="math inline">\(v_t\)</span> approximately averages over <span class="math inline">\(\frac{1}{1-\beta}\)</span> values of <span class="math inline">\(\theta\)</span></p>
<p><span class="math display">\[\begin{aligned}
\beta = 0.9 &amp;:\approx 10    \; \theta \text{ values}\\
\beta = 0.98 &amp;:\approx 50   \;\theta \text{ values}\\
\beta = 0.5 &amp;:\approx 2     \;\theta \text{ values}
\end{aligned}\]</span></p>
</div>
<div id="understanding-exponentially-weighted-averages" class="section level2">
<h2>Understanding exponentially weighted averages</h2>
<p>Given; <span class="math display">\[\begin{aligned}
v_{100} &amp;= 0.9 v_{99} + 0.1\theta_{100}\\
v_{99} &amp;= 0.9 v_{98} + 0.1\theta_{99}\\
v_{98} &amp;= 0.9 v_{97} + 0.1\theta_{98}
\end{aligned}\]</span></p>
<p>have that <span class="math display">\[\begin{aligned}
v_{100} &amp;=  0.1\theta_{100} + 0.9 v_{99}\\
        &amp;=  0.1\theta_{100} + 0.9 (0.1\theta_{99} + 0.9 v_{98}) \\
        &amp;=  0.1\theta_{100} + 0.9 \left(0.1\theta_{99} + 0.9 [0.1\theta_{98} + 0.9 v_{97}]\right) \\
        &amp;=  0.1\theta_{100} + 0.1\times0.9 \theta_{99} + 0.1\times 0.9^2 \theta_{98} + 0.1\times 0.9^3 \theta_{97} + \ldots
        \end{aligned}\]</span></p>
<p>Note that all coefficients add up to almost 1, which is why this is an exponentially weighted average (bias correction)</p>
<p>Also, note that <span class="math inline">\(\varepsilon = 1-\beta\)</span> and <span class="math display">\[(1-\varepsilon)^{1/\varepsilon} = \frac{1}{e}\]</span> where <span class="math display">\[\frac{1}{1-\beta}\]</span> is the number of <span class="math inline">\(x\)</span> values over which the average is taken.</p>
<p>For example:</p>
<pre class="r"><code>data.frame(beta = c(0.9, 0.98, 0.5)) %&gt;%
    mutate(epsilon = 1 - beta,
           e_inv = beta^{1/epsilon},
           avg_over = 1/epsilon) %&gt;%
    arrange(beta) %&gt;%
    kable(col.names = c(&quot;$\\beta$&quot;, &quot;$\\epsilon$&quot;, &quot;$1/e$&quot;, &quot;Avg Over&quot;))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right"><span class="math inline">\(\beta\)</span></th>
<th align="right"><span class="math inline">\(\epsilon\)</span></th>
<th align="right"><span class="math inline">\(1/e\)</span></th>
<th align="right">Avg Over</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.50</td>
<td align="right">0.50</td>
<td align="right">0.2500000</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">0.90</td>
<td align="right">0.10</td>
<td align="right">0.3486784</td>
<td align="right">10</td>
</tr>
<tr class="odd">
<td align="right">0.98</td>
<td align="right">0.02</td>
<td align="right">0.3641697</td>
<td align="right">50</td>
</tr>
</tbody>
</table>
<p>In the following image,</p>
<ul>
<li><strong>Yellow</strong>: <span class="math inline">\(\beta = 0.5\)</span></li>
<li><strong>Red</strong>: <span class="math inline">\(\beta = 0.9\)</span></li>
<li><strong>Green</strong>: <span class="math inline">\(\beta = 0.98\)</span></li>
</ul>
<div class="figure">
<img src="/img/ImprovingDNN/exp_weight_avg_betas.png" alt="Betas" />
<p class="caption">Betas</p>
</div>
<p>In general, the number of values over which the exponentially weighted average is averaging over, <em>Avg Over</em>, satisfies the relationship <span class="math inline">\(\beta^\text{Avg Over} \approx \frac{1}{e}\)</span>, i.e. after <em>Avg Over</em> days, the weight decays to less than about a third of the weight of the current day.</p>
<pre class="r"><code># This is showing that, for example, with beta = 0.9, after 10 days, the value of y is 0.34* value of current value of y.
betas &lt;- c(0.9, 0.98, 0.5)
dat &lt;- NULL
for (beta in betas){
    x &lt;- 1:100
    y &lt;- (1-beta)* (beta)^(1-x)
    dat &lt;- rbind(dat, data.frame(beta, x, y))
}
dat %&gt;% 
    group_by(beta) %&gt;%
    mutate(avg_over = 1/(1-beta)) %&gt;%
    mutate(e_inv = 1 - (y - lag(y, avg_over[1]))/y) %&gt;%
    filter(!is.na(e_inv)) %&gt;%
    top_n(n=1, wt = x) %&gt;%
    select(beta, avg_over, e_inv) %&gt;%
    arrange(beta) %&gt;%
    kable(col.names = c(&quot;$\\beta$&quot;, &quot;Avg Over&quot;, &quot;$1/e$&quot;))</code></pre>
<p>With <span class="math inline">\(\beta = 0.9\)</span>, after 10 <span class="math inline">\(\theta\)</span> values, weight of <span class="math inline">\(\theta_{i-11}\)</span> decays to less than a third of the current value.</p>
<p>Implementation:</p>
<ul>
<li>Initialise <span class="math inline">\(v = 0\)</span></li>
<li>Repeat to get next <span class="math inline">\(\theta_t\)</span>: <span class="math display">\[v_\theta := \beta v_\theta + (1-\beta) \theta_t\]</span></li>
</ul>
<p>One line of code!.. not most accurate, could instead average over last 2, 10 or 50 days, but this is more computationally expensive.</p>
</div>
<div id="bias-correction-in-exponentially-weighted-averages" class="section level2">
<h2>Bias correction in exponentially weighted averages</h2>
<p>Bias correction can make computation of averages more accurate; this is correcting bias from initialisation.</p>
<p>For example with <span class="math inline">\(\beta = 0.98\)</span> and <span class="math inline">\(\theta_1 = 40\)</span> <span class="math display">\[\begin{aligned}
    v_t &amp;= \beta v_{t-1} + (1-\beta)\theta_t \\
    v_0 &amp;= 0 \\
    v_1 &amp;= 0.98 v_0 + 0.2 \theta_1 \\
        &amp;= 0 + 8 \\
    v_2 &amp;= 0.98 v_1 + 0.02 \theta_2 \\
        &amp;= 0.98 \times 0.02 \times \theta_1 + 0.02 \theta_2 \\
        &amp;= 0.0196 \theta_1 + 0.02 \theta_2
\end{aligned}\]</span> So both <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> are less than the average <span class="math inline">\(\theta\)</span>.</p>
<p>To correct for bias use <span class="math display">\[\frac{v_t}{1-\beta^t}\]</span> Noting that at <span class="math inline">\(t=2\)</span>, <span class="math inline">\(1-\beta^t = (1-0.98^2) = 0.0396\)</span>, so that <span class="math display">\[\frac{v_t}{1-\beta^t} = \frac{0.0196 \theta_1 + 0.02 \theta_2}{0.0396},\]</span> i.e. the weights in the numerator add up to the denominator and so this is a weighted average.</p>
<p>When <span class="math inline">\(t\)</span> is large enough, the bias correction has no effect.</p>
<p>In machine learning, for exponentially weighted averages, most people don’t worry about bias correction as just wait initial period.</p>
</div>
<div id="gradient-descent-with-momentum" class="section level2">
<h2>Gradient descent with momentum</h2>
<p>Gradient descent with momentum:</p>
<ul>
<li>Is faster</li>
<li>Computes exponentially weighted average of gradients and uses them to update your weights instead.</li>
</ul>
<p>Algorithm: On iteration <span class="math inline">\(t\)</span>:</p>
<ul>
<li>Compute <span class="math inline">\(dW\)</span>, <span class="math inline">\(db\)</span> on current mini-batch</li>
<li><span class="math inline">\(v_{dW} = \beta v_{dW} + (1-\beta) dW\)</span>,</li>
<li><span class="math inline">\(v_{db} = \beta v_{db} + (1-\beta) db\)</span>,</li>
<li><span class="math inline">\(w := w - \alpha v_{dW}\)</span>, <span class="math inline">\(b := b - \alpha v_{db}\)</span></li>
</ul>
<p>So have two hyperparameters: <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>. In practice <span class="math inline">\(\beta = 0.9\)</span> works pretty well. in practice, people don’t use bias correction because after 10 iterations, fixed.</p>
<p><span class="math inline">\(v_{dW}\)</span> is initialised to a vector of zeros with the same dimension as <span class="math inline">\(dW\)</span> and <span class="math inline">\(w\)</span>. Similarly, <span class="math inline">\(v_{db}\)</span> is initialised to a vector of zeros with the same dimension as <span class="math inline">\(db\)</span> and <span class="math inline">\(b\)</span></p>
<p>Note that in the literature they scale <span class="math inline">\(\alpha\)</span> by <span class="math inline">\(1-\beta\)</span>, such that <span class="math inline">\(\alpha^\star = \frac{\alpha}{1-\beta}\)</span> and do following:</p>
<ul>
<li>Compute <span class="math inline">\(dW\)</span>, <span class="math inline">\(db\)</span> on current mini-batch</li>
<li><span class="math inline">\(v_{dW} = \beta v_{dW} + W\)</span>,</li>
<li><span class="math inline">\(v_{db} = \beta v_{db} + db\)</span>,</li>
<li><span class="math inline">\(w := w - \alpha^\star v_{dW}\)</span>, <span class="math inline">\(b := b - \alpha^\star v_{db}\)</span></li>
</ul>
<p>This initial formulation is preferred by Andrew Ng; this is because in the latter formulation, tuning Beta will impact scaling of <span class="math inline">\(v\times dW\)</span> and <span class="math inline">\(v \times db\)</span> and so you may need to return the learning rate alpha.</p>
<p>In the following image,</p>
<ul>
<li><strong>Blue</strong>: Gradient descent</li>
<li><strong>Red</strong>: Gradient descent with momentum</li>
</ul>
<div class="figure">
<img src="/img/ImprovingDNN/momentum.png" alt="Betas" />
<p class="caption">Betas</p>
</div>
</div>
<div id="rmsprop" class="section level2">
<h2>RMSprop</h2>
<p>Root mean square prop can also speed up gradient descent by dampening oscillations in gradient descent.</p>
<p>On iteration <span class="math inline">\(t\)</span>:</p>
<ul>
<li>Compute <span class="math inline">\(dW\)</span>, <span class="math inline">\(db\)</span> on current mini-batch</li>
<li><span class="math inline">\(S_{dW} = \beta_2 S_{dW} + (1-\beta_2) dW^2\)</span> (using element-wise squaring),</li>
<li><span class="math inline">\(S_{db} = \beta_2 S_{db} + (1-\beta_2) db ^2\)</span>,</li>
<li><span class="math inline">\(w := w - \alpha \frac{dW}{\sqrt{s_{dW}}}\)</span>, <span class="math inline">\(b := b - \alpha \frac{db}{\sqrt{s_{db}}}\)</span></li>
</ul>
<p>May therefore be able to use a larger learning rate <span class="math inline">\(\alpha\)</span></p>
<p>In practice, add a small epsilon to denominator to ensure numerical stability and don’t divide by almost zero.</p>
</div>
<div id="adam-optimisation-algorithm" class="section level2">
<h2>Adam optimisation algorithm</h2>
<p>Adam = Momentum + RMSprop</p>
<p>Algorithm:</p>
<ul>
<li><span class="math inline">\(v_{dW} = 0\)</span>, <span class="math inline">\(S_{dW} = 0\)</span>, <span class="math inline">\(v_{db} = 0\)</span>, <span class="math inline">\(S_{db} = 0\)</span></li>
<li>On iteration <span class="math inline">\(t\)</span>:
<ul>
<li>Compute <span class="math inline">\(dW\)</span>, <span class="math inline">\(db\)</span> using current mini-batch</li>
<li><span class="math inline">\(v_{dW} = \beta_1 v_{dW} + (1-\beta_1) dW\)</span>, <span class="math inline">\(v_{db} = \beta_1 v_{db} + (1-\beta_1) db\)</span> (momentum)</li>
<li><span class="math inline">\(S_{dW} = \beta_2 S_{dW} + (1-\beta_2) dW^2\)</span>, <span class="math inline">\(S_{db} = \beta_2 S_{db} + (1-\beta_2) db ^2\)</span> (RMSprop)</li>
<li>Corrections:
<ul>
<li><span class="math inline">\(v_{dw}^\text{corrected} = v_{dw}/(1-\beta_1^t)\)</span></li>
<li><span class="math inline">\(v_{db}^\text{corrected} = v_{db}/(1-\beta_1^t)\)</span></li>
<li><span class="math inline">\(S_{dw}^\text{corrected} = S_{dw}/(1-\beta_2^t)\)</span></li>
<li><span class="math inline">\(S_{db}^\text{corrected} = S_{db}/(1-\beta_2^t)\)</span></li>
</ul></li>
<li>Updates:
<ul>
<li><span class="math inline">\(w := w - \alpha\frac{v_dw^\text{corrected}}{\sqrt{S_{dw}^\text{corrected}} + \varepsilon}\)</span></li>
<li><span class="math inline">\(b := b - \alpha\frac{v_db^\text{corrected}}{\sqrt{S_{db}^\text{corrected}} + \varepsilon}\)</span></li>
</ul></li>
</ul></li>
</ul>
<p>Hyperparameters:</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> - learning rate. needs to be tuned</li>
<li><span class="math inline">\(\beta_1\)</span> - momentum moving average of <span class="math inline">\(dw\)</span> and <span class="math inline">\(db\)</span> (first moment, mean of the derivatives). Typically 0.9.</li>
<li><span class="math inline">\(\beta_2\)</span> - moving weighted average of <span class="math inline">\(dw^2\)</span> and <span class="math inline">\(db^2\)</span> (second moment). Typically 0.999.</li>
<li><span class="math inline">\(\varepsilon\)</span> - Numerical stability adjustment. Doesn’t matter too much; doesn’t affect performance much at all. Typically <span class="math inline">\(10^{-8}\)</span></li>
</ul>
<p>Usually use default values for <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span> and <span class="math inline">\(\varepsilon\)</span></p>
<p>Adam = ADAptive Moment estimation</p>
<p>Note that do use bias correction in ADAM.</p>
</div>
<div id="learning-rate-decay" class="section level2">
<h2>Learning rate decay</h2>
<ul>
<li>Slowly reducing learning rate over time can help speed up learning rate</li>
<li>Recall 1 epoch = 1 pass through the data</li>
<li>Could set <span class="math display">\[\alpha = \frac{1}{1 + \text{decay rate} \times \text{epoch-num}} \alpha_0\]</span></li>
</ul>
<pre class="r"><code>alpha0 = 002
decay_rate = 1

data.frame(epoch = c(1, 2, 3, 4)) %&gt;%
    mutate(alpha = 1 /(1 + decay_rate * epoch) * alpha0) </code></pre>
<pre><code>##   epoch     alpha
## 1     1 1.0000000
## 2     2 0.6666667
## 3     3 0.5000000
## 4     4 0.4000000</code></pre>
<ul>
<li>Alternatively,
<ul>
<li>use exponential decay, set <span class="math inline">\(\alpha = 0.95^\text{epoch.num} \times \alpha_0\)</span></li>
<li>Set <span class="math inline">\(\alpha = \frac{k}{\text{epoch.num}} \alpha_0\)</span></li>
<li>Use step function to decrease alpha over time</li>
<li>Manual decay, watch as it is training a small number of models hour by hour or day by day.</li>
</ul></li>
</ul>
<p>This does help but is lower down Andrew Ng’s list on things to try.</p>
</div>
<div id="the-problem-of-local-optima" class="section level2">
<h2>The problem of local optima</h2>
<ul>
<li>People used to worry about optimisation algorithm getting stuck at local optima</li>
<li>In, e.g. 20000, dimension space would require all 20,000 to be convex / concave which is unlikely.<br />
</li>
<li>So intuition about low-dimension spaces doesn’t transfer to high-dimensional spaces</li>
<li>So, in deep learning, local optima isn’t a problem, but a plateau is a problem as they can really slow down learning</li>
<li>momentum, rmsProp or adam can really help learning algorithm.</li>
<li>Because network is solving optimisation problems over such high dimensional spaces, no-one has great intuition about what these spaces really look like.</li>
</ul>
</div>
</div>
<div id="week3" class="section level1">
<h1>Hyperparameter tuning, Batch Normalization and Programming Frameworks</h1>
<div id="hyperparameter-tuning" class="section level2">
<h2>Hyperparameter Tuning</h2>
<div id="tuning-process" class="section level3">
<h3>Tuning process</h3>
<p>Hyperparameters</p>
<ul>
<li>Most important to tune:
<ul>
<li>Learning rate <span class="math inline">\(\alpha\)</span></li>
</ul></li>
<li>Secondary importance
<ul>
<li>Number of hidden units</li>
<li>Mini-batch size</li>
<li>momentum parameter, <span class="math inline">\(\beta\)</span>, if using momentum (<span class="math inline">\(\sim 0.9\)</span>)</li>
</ul></li>
<li>Of third-most importance
<ul>
<li>decay rate, if using learning rate decay</li>
<li>Number of layers</li>
</ul></li>
<li>Pretty much never tuned:
<ul>
<li>Adam parameters (first and second moments and numerical stability adjustment), <span class="math inline">\(\beta_1\)</span> = 0.9, <span class="math inline">\(\beta_2\)</span> = 0.999 and <span class="math inline">\(\varepsilon\)</span> = <span class="math inline">\(10^{-8}\)</span>, if using adam</li>
</ul></li>
</ul>
<p>How to select set of values to explore:</p>
<ul>
<li>Grid Search
<ul>
<li>Shallow learning (dinosaur era) - a grid was explored, e.g. <span class="math inline">\(2 \times 2\)</span> when looking at 2 hyperparameters.</li>
<li>Works well with small number of hyperparameters</li>
</ul></li>
<li>Random Sampling
<ul>
<li>More efficient deep learning when have many more hyperparameters to tune</li>
<li>Pick points at random from grid (2 parameters), cube, ….</li>
<li>Allows lot more values to be tested</li>
</ul></li>
<li>Coarse to fine
<ul>
<li>May zoom into small region of hyperparameter values to sample more densely in area that seemed to work well</li>
</ul></li>
</ul>
</div>
<div id="using-and-approriate-scale-to-pick-hyperparameters" class="section level3">
<h3>Using and approriate scale to pick hyperparameters</h3>
<ul>
<li>Doesn’t mean you sample uniformly at random</li>
<li>Example, if trying to tune
<ul>
<li>Number of hidden units, <span class="math inline">\(n^{[l]} = 50, \ldots, 100\)</span> - makes sense to sample uniformly at random</li>
<li>Number of layers, <span class="math inline">\(L = 2, ldots, 4\)</span>. May sample uniformly at random, or over grid for values 2, 3, and 4</li>
<li>Learning rate, <span class="math inline">\(\alpha = 0.0001, \ldots, 1\)</span>
<ul>
<li>With uniformly at random, 10.0% of sampled values would be between 0.001 and 0.1 and 90.0% would be between 0.1 and 1</li>
<li>By sampling the log scale, would get
<ul>
<li>25% between 0.0001 and 0.001</li>
<li>25% between 0.001 and 0.01</li>
<li>25% between 0.01 and 0.1</li>
<li>25% between 0.1 and 1</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p>Implementation of random sampling the log scale:</p>
<ul>
<li>If want to sample between <span class="math inline">\(10^a\)</span> and <span class="math inline">\(10^b\)</span></li>
<li>Sample <span class="math inline">\(r\)</span> between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span></li>
<li>Set <span class="math inline">\(\alpha = 10^r\)</span></li>
</ul>
<p>Example: to sample log scale of 0.0001 to 1</p>
<ul>
<li>Find that <span class="math inline">\(a = \log_{10} 0.0001\)</span> = -4 and <span class="math inline">\(b = \log_{10} 1\)</span> = 0</li>
<li>Sample <span class="math inline">\(r\)</span> between -4 and 0</li>
<li>Set <span class="math inline">\(\alpha\)</span> to <span class="math inline">\(10^{r}\)</span></li>
</ul>
<pre class="r"><code>x &lt;- 0.0001
y &lt;- 1

a &lt;- log10(x)
b &lt;- log10(y)
r &lt;- runif(a,b, n = 1000)
alpha &lt;- 10^r

p1 &lt;- ggplot2::ggplot(as.data.frame(alpha)) + 
    geom_histogram(aes(alpha)) + 
    ggtitle(&quot;linear scale&quot;)
p2 &lt;- ggplot2::ggplot(as.data.frame(alpha)) + 
    geom_histogram(aes(alpha)) + 
    scale_x_continuous(trans = &quot;log10&quot;) + 
    ggtitle(&quot;logarithmic scale&quot;)
grid.arrange(p1, p2, nrow = 1)
rm(p1, p2)</code></pre>
<p><img src="/blog/2018-12-07-improving-deep-learning-networks_files/figure-html/sample_log_scale_r-1.png" width="672" /></p>
<pre class="python"><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

x = 0.0001
y = 1
r = np.random.uniform(x, y, size = 1000)
alpha = 10**r

df = pd.DataFrame(alpha, columns = [&#39;alpha&#39;])</code></pre>
<pre class="r"><code>library(ggplot2)
library(gridExtra)
p1 &lt;- ggplot2::ggplot(py$df) + 
    geom_histogram(aes(alpha)) + 
    labs(x = expression(alpha), y = &quot;# Samples&quot;) +
    ggtitle(&quot;linear scale&quot;)
p2 &lt;- ggplot2::ggplot(py$df) + 
    geom_histogram(aes(alpha)) + 
    scale_x_continuous(trans = &quot;log10&quot;) + 
    labs(x = expression(alpha), y = &quot;# Samples&quot;) +
    ggtitle(&quot;logarithmic scale&quot;)
grid.arrange(p1, p2, nrow = 1)
rm(p1, p2)</code></pre>
<p><img src="/blog/2018-12-07-improving-deep-learning-networks_files/figure-html/plot_py_log_sample-1.png" width="672" /></p>
<p>Hyperparameters for exponentially weighted averages</p>
<p>Consider appropriate that <span class="math inline">\(\beta\)</span> takes values <span class="math inline">\(0.9, \ldots, 0.999\)</span>, where 0.9 = last 10 days and 0.999 = last 1000 values. Randomly sampling on linear scale doesn’t make sense. Instead explore range of values if <span class="math inline">\(1-\beta\)</span> which ranges from 0.1 and 0.001 over a logarithmic scale to explore more densely when <span class="math inline">\(1-\beta\)</span> close to 0 (<span class="math inline">\(\beta\)</span> close to 1)</p>
<ul>
<li>When beta goes from 0.9000 to 0.9005 has little impact</li>
<li>When beta goes from 0.9990 to 0.9995 has large impact</li>
</ul>
<pre class="r"><code>x &lt;- 1 - 0.999
y &lt;- 1 - 0.9

a &lt;- log10(x)
b &lt;- log10(y)
r &lt;- runif(a,b, n = 1000)
beta &lt;- 1 - 10^r

ggplot2::ggplot(as.data.frame(beta)) + 
geom_histogram(aes(beta)) + 
labs(x = expression(1-beta), y = &quot;# Samples&quot;)
rm(x, y, a, b, r, beta)</code></pre>
<p><img src="/blog/2018-12-07-improving-deep-learning-networks_files/figure-html/sample_ewa_log_scale_r-1.png" width="672" /> <!-- beta = 1-10^r, where r in -3, -1.  -3 = 10^-3--></p>
</div>
<div id="hyperparameters-tuning-in-practice-pandas-vs.caviar" class="section level3">
<h3>Hyperparameters tuning in practice: Pandas vs. Caviar</h3>
<ul>
<li>Tips on organising search process</li>
<li>Intuition about hyperparameter setting might not transfer between application areas (NLP, vision, speech, ads, logistics), however cross-fertilisation is common</li>
<li>Intuitions get stale; i.e as data gradually changes or servers get upgraded. Recommends to retest/re-evaluate once every few months.</li>
<li>Schools of thought for searching:</li>
</ul>
<ol style="list-style-type: decimal">
<li>babysit one model (Panda)</li>
</ol>
<ul>
<li>Huge dataset, not many computational resources (CPU/GPU)</li>
<li>Watch one model and patiently nudge learning rate up or down as training model, e.g. if model training takes many day, may change hyperparameter on daily basis based on learning curve to date</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Train many models in parallel (Caviar)</li>
</ol>
<ul>
<li>Each model has different set of hyperparameter values</li>
<li>Choose model based on best curve</li>
</ul>
</div>
</div>
<div id="batch-normalisation" class="section level2">
<h2>Batch Normalisation</h2>
<div id="normalising-activations-in-a-network" class="section level3">
<h3>Normalising activations in a network</h3>
<ul>
<li>Sergey Loffe and Christian Szegedy</li>
<li>Makes hyperparameter search problem easier and NN more robust</li>
<li>Recall that normalising input features of logistic regression can speed up learning (i.e. subtract mean and divide by variance), transforming learning problem contours from elongated to round</li>
<li>In deeper mode have input parameters but also activations, how about normalising any hidden layer, e.g. <span class="math inline">\(a^{[2]} = g(z^{[2]})\)</span> so as to speed up training of <span class="math inline">\(w^{[3]}\)</span> and <span class="math inline">\(b^{[3]}\)</span>. Although, note that it is more common practice to normalise <span class="math inline">\(z^{[2]}\)</span>, rather than it’s activated value. This is the premise of batch normalisation</li>
<li><p>Given hidden layer value <span class="math inline">\(z^{[l](1)}, \ldots, z^{[\ell](m)}\)</span>, compute: <span class="math display">\[\begin{aligned}
\mu &amp;= \frac{1}{m}\sum_{i}z^{[\ell](i)} \\
\sigma^2 &amp;= \frac{1}{m} \sum_{i} (z^{[\ell](i)} - \mu)^2\\
z^{[\ell](i)}_\text{norm} &amp;= \frac{z^{[\ell](i)} - \mu}{\sqrt{\sigma^2 + \varepsilon}}\\
\tilde{z}^{[\ell](i)} &amp;= \gamma z^{[\ell](i)}_\text{norm} + \beta,
\end{aligned}\]</span> where <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are learnable parameters</p></li>
<li>Note that <span class="math inline">\(\tilde{z}^{[\ell](i)} = z^{[\ell](i)}\)</span> for <span class="math inline">\(\gamma = \sqrt{\sigma^2 + \varepsilon}\)</span> and <span class="math inline">\(\beta = \mu\)</span>.</li>
<li>Use <span class="math inline">\(\tilde{z}^{[\ell](i)}\)</span> as unlikely want mean 0 and variance 1 for sigmoid activation function, this way can adjust mean and variance.</li>
<li><p>So hidden units have standardised mean and variance</p></li>
</ul>
</div>
<div id="fitting-batch-norm-into-a-neural-network" class="section level3">
<h3>Fitting Batch Norm into a neural network</h3>
<p><span class="math display">\[X \xrightarrow{W^{[1]}, b^{[1]}}  Z^{[1]} \xrightarrow[\text{Batch Norm (BN)}]{\beta^{[1]}, \gamma^{[1]}} \rightarrow \tilde{Z}^{[i]} \rightarrow  g^{[1]}(\tilde{Z}^{[1]}) = a^{[1]}
  \xrightarrow{W^{[2]}, b^{[2]}}  Z^{[2]} \xrightarrow[\text{BN}]{\beta^{[2]}, \gamma^{[2]}} \tilde{Z}^{[2]} \rightarrow a^{[2]} \rightarrow \ldots\]</span></p>
<p>Parameters:</p>
<ul>
<li><span class="math inline">\(W^{[1]}, b^{[1]}, \ldots, W^{[L]}, b^{[L]}\)</span></li>
<li><span class="math inline">\(\beta^{[1]}, \gamma^{[1]}, \ldots, \beta^{[L]}, \gamma^{[L]}\)</span></li>
</ul>
<p>Learn parameters applying either gradient descent or any optimisation you want (Adam, RMS prop, momentum), e.g.,</p>
<ul>
<li>Compute gradients, <span class="math inline">\(d\beta^{[\ell]}\)</span></li>
<li>Update parameters, <span class="math inline">\(\beta^{[\ell]} := \beta^{[\ell]} - \alpha d\beta^{[\ell]}\)</span></li>
</ul>
<p>In reality, implementing batch norm using a programming framework is often a one-liner (and hides implementation details), e.g. in TensorFlow <code>tf.nn.batch-normalisation</code></p>
<p>To date, discussing in context of batch gradient descent, but in practice applied to mini-batch.</p>
<ul>
<li><p>Take first minibatch, <span class="math inline">\(X^{\{1\}}\)</span> and apply transformations: <span class="math display">\[\begin{aligned}
&amp; X^{\{1\}} \xrightarrow{W^{[1]}, b^{[1]}}  Z^{[1]} \xrightarrow[\text{BN}]{\beta^{[1]}, \gamma^{[1]}} \rightarrow \tilde{Z}^{[1]} \rightarrow  g^{[1]}(\tilde{Z}^{[1]}) = a^{[1]}
\xrightarrow{W^{[2]}, b^{[2]}}  Z^{[2]} \xrightarrow[\text{BN}]{\beta^{[2]}, \gamma^{[2]}} \tilde{Z}^{[2]} \rightarrow  \ldots\\
&amp; X^{\{2\}} \rightarrow \ldots \\
&amp; x^{\{\}} \rightarrow \ldots
\end{aligned}\]</span></p></li>
<li><p>In second minibatch, normalise based on data in just that mini-batch.</p></li>
</ul>
<p>Previously, said that parameters were <span class="math inline">\(W^{[\ell]}, b^{[\ell]},\beta^{[\ell]}, \gamma^{[\ell]}\)</span></p>
<ul>
<li>In batch norm, any constant you add, i.e., <span class="math inline">\(b^{[\ell]}\)</span> will get subtracted out in batch norm (when zero-ing out mean) and so this parameter is useless, this parameter is essentially replaced by <span class="math inline">\(\beta^{[\ell]}\)</span> which controls the shift / bias terms.</li>
</ul>
<p>Implementing gradient descent (with mini-batch)</p>
<ul>
<li>for <span class="math inline">\(t = 1\ldots \text{num MiniBatches}\)</span>
<ul>
<li>Compute forward prop on <span class="math inline">\(X^{\{t\}}\)</span></li>
<li>In each hidden layer, use BN to replace <span class="math inline">\(z^{[\ell]}\)</span> with <span class="math inline">\(\tilde{z}^{[\ell]}\)</span></li>
<li>Use backprop to compute <span class="math inline">\(dW^{[\ell]}, d\beta^{[\ell]}, d\gamma^{[\ell]}\)</span><br />
</li>
<li>Update parameters:
<ul>
<li><span class="math inline">\(W^{[\ell]} := W^{[\ell]} - \alpha dW^{[\ell]}\)</span></li>
<li><span class="math inline">\(\beta^{[\ell]} := \beta^{[\ell]} - \alpha d\beta^{[\ell]}\)</span></li>
<li><span class="math inline">\(\gamma^{[\ell]} := \gamma^{[\ell]} - \alpha d\gamma^{[\ell]}\)</span></li>
</ul></li>
</ul></li>
</ul>
<p>Noting that this also works with momentum, RMSprop or Adam, in addition to gradient descent (as detailed here) This is how you would implement from scratch, although not required if using programming framework such as TensorFlow</p>
</div>
<div id="why-does-batch-norm-work" class="section level3">
<h3>Why does Batch Norm work?</h3>
<ul>
<li>Doing similar thing to normalising features (but for hidden units instead of input features)</li>
<li>Makes weights more robust to changes to weights in earlier layers of the neural network
<ul>
<li>Covariate shift: If have learned <span class="math inline">\(X \rightarrow Y\)</span>, when distribution of <span class="math inline">\(X\)</span> changes, need to re-learn <span class="math inline">\(f(X) \rightarrow Y\)</span></li>
<li>Parameters in preceding hidden layers changing all the time and so subjecting subsequent layers to the problem of covariate shift.<br />
</li>
<li>Batch norm ensures that no matter how the inputs of a layer change, the mean and variance of the parameters of that layer will remain the same. It therefore limits the amount to which updating parameters in earlier layers impacts subsequent layers, i.e., it causes input values to be more stable, so later layers have more firm ground to stand on. Batch norm therefore weakens coupling between earlier and later layers; each layer can learn more independently and therefore speed up learning.<br />
</li>
</ul></li>
<li>Batch norm also has a slight regularisation effect on the mini-batches
<ul>
<li>With batch-norm, each mini-batch is scaled by the mean/variance computed on just that mini-batch. As the mean/variance will differ across mini-batches, batch norm is therefore adding noise to each layer’s activations <span class="math inline">\(z^{[\ell]}\)</span>. So similar to dropout it adds some noise to each hidden layer’s activations (recall dropout takes hidden unit and multiplies it by zero or one by some probability, whereas batch-norm adds noise by scaling).</li>
<li>By adding noise to hidden layers, it forces downstream hidden units not to rely too much on any one hidden unit.<br />
</li>
<li>Note, however, that noise added by batch norm is very slight and so should not be used for regularisation purposes but rather as a way to normalise hidden unit values and speed up learning.</li>
</ul></li>
</ul>
</div>
<div id="batch-norm-at-test-time" class="section level3">
<h3>Batch Norm at test time</h3>
<p>During training, batch norm processes one mini-batch at a time, <span class="math display">\[\begin{aligned}
\mu &amp;=\frac{1}{m}\sum_{i}z^{(i)}\\
\sigma^2 &amp;= \frac{1}{m} \sum_{i} (z^{(i)} - \mu)^2\\
z^{(i)}_\text{norm} &amp;= \frac{z^{(i)} - \mu}{\sqrt{\sigma^2 + \varepsilon}}\\
\tilde{z}^{(i)} &amp;= \gamma z^{(i)}_\text{norm} + \beta
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(m\)</span> = number of examples in mini-batch.</p>
<p>So during training, use the examples within the mini-batch to estimate <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> within that mini-batch.</p>
<p>At test time, may only have one example and may not be possible (or make sense) to estimate a mean and variance.</p>
<p>Instead, use a running mean and variance during training. Generally use exponentially weighted values to give more weight to later mini-batches.</p>
</div>
</div>
<div id="multi-class-classification" class="section level2">
<h2>Multi-class classification</h2>
<div id="softmax-regression" class="section level3">
<h3>Softmax Regression</h3>
<ul>
<li>To date, using binary classification (cat vs non-cat)</li>
<li>With multiple possible classes (koala, cat, dog, chick), use generalisation of logistic regression called Softmax regression</li>
<li>Use <span class="math inline">\(C\)</span> = Number of classes, e.g. <span class="math inline">\(C=4\)</span>, in which case index classes as $(0, , 3)</li>
<li><span class="math inline">\(n^{[L]} = C\)</span> and <span class="math inline">\(\hat{y} \in \mathbb{R}^{(4, 1)}\)</span> and values of <span class="math inline">\(\hat{y}\)</span> sum to one.</li>
<li>Each value in <span class="math inline">\(y\)</span> gives the probability a given classes</li>
<li>For Softmax layer (final layer <span class="math inline">\(L\)</span>) <span class="math display">\[ z^{[\ell]} = w^{[\ell]}a^{[\ell-1]}+ b^{[\ell]}\]</span></li>
</ul>
<p>Activation function for layer <span class="math inline">\(L\)</span> <span class="math display">\[\begin{aligned}
t_j &amp;= e^{z^{[j]}}\\
\hat{y} = a^{[j]} &amp;= \frac{t_j}{\sum^C_j=1 t_j}
\end{aligned}\]</span></p>
<p>For example, with <span class="math inline">\(Z^{[L]} = \begin{bmatrix} 5 \\ 2 \\ -1 \\ 3 \end{bmatrix}\)</span> have that</p>
<pre class="r"><code>z &lt;- c(5, 2, -1, 3)
t &lt;- exp(z)
t</code></pre>
<pre><code>## [1] 148.4131591   7.3890561   0.3678794  20.0855369</code></pre>
<pre class="r"><code>z &lt;- t / sum(t)
z</code></pre>
<pre><code>## [1] 0.842033572 0.041922383 0.002087193 0.113956852</code></pre>
<pre class="r"><code>sum(z)</code></pre>
<pre><code>## [1] 1</code></pre>
<p>Previously, activation function took a single real number as input and output a single real number. Now take in a vector of size <span class="math inline">\(C\)</span> and output vector of size <span class="math inline">\(C\)</span>.</p>
<p>Softmax examples with two input classes, decision boundaries are linear with no hidden layer <img src="/img/ImprovingDNN/softmax_nohidden.png" alt="Softmax Classification with no hidden layer" /></p>
</div>
<div id="training-a-softmax-classifier" class="section level3">
<h3>Training a softmax classifier</h3>
<ul>
<li>Note that <strong>Hardmax</strong> takes a vector <span class="math inline">\(Z\)</span> and places an element of 1 in the most probable, every other element has value 0.</li>
<li>Softmax regression generalises logistic regression to <span class="math inline">\(C\)</span> classes</li>
<li>If <span class="math inline">\(C = 2\)</span> then softmax reduces to logistic regression.</li>
<li>Target output is <span class="math display">\[y =  \begin{bmatrix}
0 \\
1 \\
0 \\
0
\end{bmatrix}\]</span> noting that <span class="math inline">\(y_1 = y_3 = y_4 = 0\)</span>. Example have output is <span class="math display">\[a^{L} = \hat{y} =  \begin{bmatrix}
0.3 \\
0.2 \\
0.1 \\
0.4
\end{bmatrix}\]</span></li>
<li>Loss function is <span class="math display">\[\mathcal{L}(\hat{y}, y) = -\sum^4_{j = 1} y_j \log \hat{y}_j\]</span> For given example this is <span class="math inline">\(-\log \hat{y}_2\)</span>. To minimise loss, want to make <span class="math inline">\(\hat{y}_2\)</span>, which is a probability, as large as possible. This is equivalent to MLE.</li>
</ul>
<p>Cost on entire training set: <span class="math display">\[J(w^{[1]}, b^{[1]}, \ldots) = \frac{1}{m} \sum^m_{i = 1}\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) \]</span></p>
<p>Noting that <span class="math display">\[\begin{aligned}
Y &amp;= [y^{(1)} y^{(2)}, \ldots, y^{(m)}] \\
&amp; = \begin{bmatrix}
0 &amp; 0 &amp; 1 &amp;\\
1 &amp; 0 &amp; 0 &amp; \\
0 &amp; 1 &amp; 9 &amp; \ldots\\
&amp; &amp; \vdots &amp; 
    \end{bmatrix} \\
&amp; \in \mathbb{R}^{C, m} \\
\hat{Y} ^= [\hat{y}^{(1)}, \hat{y}^{(2)}, \ldots, \hat{y}^{(m)}]
&amp; \in \mathbb{R}^{C, m} 
\end{aligned}\]</span></p>
<p>Gradient descent with softmax</p>
<ul>
<li>Forward prop: <span class="math inline">\(z^{[L]} \rightarrow a^{[L]} = \hat{y} \rightarrow \mathcal{L}(\hat{y}, y)\)</span>
<ul>
<li>Backprop: <span class="math inline">\(dz^{[L]} == \hat{y} - y\)</span></li>
</ul></li>
</ul>
</div>
</div>
<div id="introduction-to-programming-frameworks" class="section level2">
<h2>Introduction to programming frameworks</h2>
<div id="deep-learning-frameworks" class="section level3">
<h3>Deep learning frameworks</h3>
<ul>
<li>As you implement large models or more complex models (e.g. CNNs or recurring neural networks), it is not practical to implement from scratch.</li>
<li>e.g., whilst you understand how to do matrix multiplication, it is unlikely you will implement yourself.</li>
<li>Similarly, as a numerical linear algebra library will make you more efficient for large matrix multiplication, a programming framework will make you more efficient as you develop machine learning applications.</li>
</ul>
<p>Frameworks:</p>
<ul>
<li>Caffe/Caffe2</li>
<li>CNTK</li>
<li>DL4J</li>
<li>Keras</li>
<li>Lasagne</li>
<li>mxnet</li>
<li>PaddlePaddle</li>
<li>TensorFlow</li>
<li>Theano</li>
<li>Torch</li>
</ul>
<p>When choosing amongst frameworks, consider</p>
<ul>
<li>Ease of programming (development and deployment)</li>
<li>Running speed</li>
<li>Truly open (open source with good governance) (some companies may maintain single corporation control and then close off what was open source)</li>
</ul>
</div>
<div id="tensorflow" class="section level3">
<h3>TensorFlow</h3>
<ul>
<li>TensorFlow is one of the many great deep learning programming frameworks</li>
<li><p>Motivating problem: minimise cost function <span class="math display">\[J(w) = w^2 - 10w + 25,\]</span> which, using algebra, you see you can minimised at <span class="math inline">\(w = 5\)</span></p>
<pre class="python"><code>import numpy as np
import tensorflow as tf

coefficients = np.array([[1], [-10], [25]])

w = tf.Variable(0, dtype = tf.float32)
x = tf.placeholder(tf.float32, [3, 1])

#cost = tf.add(w**2, tf.multiply(-10., w), 25)
#cost = w**2 - 10*w + 25
cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
init = tf.global_variables_initializer()
session = tf.Session()
session.run(init)
print(session.run(w))</code></pre>
<pre><code>## 0.0</code></pre>
<pre class="python"><code>for i in range(1000):
  session.run(train, feed_dict = {x:coefficients})
print(session.run(w))</code></pre>
<pre><code>## 4.9999886</code></pre></li>
</ul>
<p>Note:</p>
<ul>
<li>Trying to optimise <span class="math inline">\(w\)</span> so declare as variable</li>
<li>Just had to define a cost function</li>
<li>Framework will work out derivatives, and how to minimise cost</li>
<li>Placeholder used to define data that will be defined later</li>
<li>feed_dict useful for mini-batches</li>
<li>cost equation allows TensorFlow to calculate a computation graph</li>
</ul>
<p>Can replace</p>
<pre class="python"><code>session = tf.Session()
session.run(init)
print(session.run(w))</code></pre>
<p>with</p>
<pre class="python"><code>with tf.Session() as session:
    session.run(init)
print(session.run(w))</code></pre>
</div>
</div>
</div>
