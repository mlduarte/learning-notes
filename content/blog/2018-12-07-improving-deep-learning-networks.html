---
title: Improving Deep Learning Networks
author: ''
date: '2018-12-07'
slug: improving-deep-learning-networks
categories: []
tags: []
banner: img/banners/improvingDNN.png
---



<div id="week1" class="section level1">
<h1>Practical aspects of deep learning</h1>
<div id="setting-up-your-machine-learning-application" class="section level2">
<h2>Setting up your machine learning application</h2>
<div id="traindevtest-sets" class="section level3">
<h3>Train/Dev/Test Sets</h3>
<p>Decisions to make</p>
<ul>
<li>No. of layers</li>
<li>No. of hidden units</li>
<li>Learning rates</li>
<li>Activation functions</li>
</ul>
<p>Impossible to correctly guess. In practice, it is iterative. Idea -&gt; code -&gt; experiment -&gt; Refine Idea -&gt; repeat …</p>
<p>Applied to NLP, computer vision, speech processing and structured data (advertisements, web search, computer security, logistics)</p>
<p>Intuitions from one domain/application area often do not transfer to another</p>
<p>Best choice = f(computer (GPU/CPU), # input features)</p>
<p>Need to be able to go around cycle quickly to be efficient.</p>
<p>Previous Era of Machine Learning (with small data sets): Data = Training (60%) + Hold-Out/Cross-Validation/Development Set/Dev (20%) + Test (20%). Or alternatively 70% training + 30% test</p>
<ul>
<li>Train multiple model</li>
<li>Compare using Dev</li>
<li>Use test for final model</li>
</ul>
<p>In modern big data era (e.g. 1,000,000 training examples), this is no longer best practice. May only need 10,000 examples for dev and test, making a ratio of 98% train, 1% dev, 1% test. With more than 1,000,000 examples, may have dev and test of .4% and .1%, respectively.</p>
<p>More specific guidelines will be given later in course.</p>
<p>Mismatched train/test distributions:</p>
<ul>
<li>E.g to identify cat may have different sources for dev/test and training.</li>
<li>Training set: Cat pictures from webpages (professional)</li>
<li>Dev/Test: Cat pictures from users using your app (blurrier, low-res)</li>
</ul>
<p>Guideline: Ensure dev and test sets come from same distribution. Okay to get training set from different sources</p>
<p>It might be okay not to have a test set. (Goal: unbiased estimate for final)</p>
<p>Otherwise, train on training set. Eval on dev set and try to get to good model, but will no longer have unbiased estimate of performance. Usually people say have train/test but what are really referring to is a train/dev set.</p>
</div>
<div id="biasvariance" class="section level3">
<h3>Bias/Variance</h3>
<p>Bias/Variance - easy to learn. difficult to master. Important Less discussion in deep learning era. Less trade-off in deep learning era.</p>
<p>High bias = underfitting High variance = overfitting</p>
<p>High dimensional problems - difficult to plot decision boundary</p>
<p>Key Metrics:</p>
<ul>
<li>Train set error</li>
<li>Dev set error</li>
</ul>
<p>If train set error &lt; dev set error then <strong>high variance</strong> (overfitting) If train set error <span class="math inline">\(\approx\)</span> dev set error and train set error high then <strong>high bias</strong></p>
<p>Example when considering classification of image as cat:</p>
<table style="width:88%;">
<colgroup>
<col width="15%" />
<col width="18%" />
<col width="18%" />
<col width="18%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th>Data set</th>
<th>Error Eg 1</th>
<th>Error Eg 2</th>
<th>Error Eg 3</th>
<th>Error Eg 4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Train:</td>
<td>1%</td>
<td>15%</td>
<td>15%</td>
<td>0.5%</td>
</tr>
<tr class="even">
<td>Test:</td>
<td>11%</td>
<td>16%</td>
<td>30%</td>
<td>1%</td>
</tr>
<tr class="odd">
<td>Diagnosis:</td>
<td>High variance</td>
<td>High bias</td>
<td>High bias &amp; high variance</td>
<td>Low bias and low variance</td>
</tr>
</tbody>
</table>
<p>Subtlety: Diagnosis depends on optimal error (Bayes error) = <span class="math inline">\(\approx\)</span> 0% (i.e. human can correctly detect cat pictures with almost 0 error). If baseline error around 15%, then the second example would be diagnosed as low bias and low variance.</p>
<p>Assumption: Training and dev sets draw from same distribution, otherwise require a more sophisticated analysis (see later video)</p>
</div>
<div id="basic-recipe-for-machine-learning" class="section level3">
<h3>Basic recipe for machine learning</h3>
<p>Following is a recipe for machine learning to systematically improve machine learning algorithm depending on whether high bias or high variance :-)</p>
<p>After training initial model:</p>
<p>Ask: Does it have high bias (look at training data)?</p>
<div class="figure">
<img src="/img/ImprovingDNN/bias_variance_decisions.png" alt="Deep NN Forward Propagation" />
<p class="caption">Deep NN Forward Propagation</p>
</div>
<p>Pre: Deep learning, it was a <strong>bias-variance tradeoff</strong>. But now there are tools (e.g. ability to use bigger network or more data) to improve both bias and variance. So really don’t need to trade-off anymore.</p>
</div>
</div>
<div id="regularising-your-neural-network" class="section level2">
<h2>Regularising your neural network</h2>
<div id="regularisation" class="section level3">
<h3>Regularisation</h3>
<p>If suspect overfitting (high variance), may not be possible to get more data. Regularisation will help.</p>
<div id="logistic-regression" class="section level4">
<h4>Logistic regression</h4>
<p>Aim <span class="math inline">\(\min\limits_{w, b} J(w,b)\)</span> where <span class="math inline">\(w \in \mathbb{R}^{n_x}, b \in \mathbb{R}\)</span>.</p>
<p><span class="math display">\[J(w,b) = \frac{1}{m} \sum^{m}_{i=1}\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\sum_{j=1}^{n_x}||w^{[j]}||_2^2\]</span></p>
<p><span class="math inline">\(L_2\)</span> regularisation: <span class="math display">\[\frac{\lambda}{2m}\sum_{j=1}^{n_x}||w||_2^2\]</span> <span class="math inline">\(L_1\)</span> regularisation: <span class="math display">\[\frac{\lambda}{2m}\sum_{j=1}^{n_x}|w|_j\]</span></p>
<p><span class="math inline">\(\lambda\)</span> is a hyperparameter.</p>
<ul>
<li>reserved word in Python (use instead <em>lambd</em>)</li>
<li>Needs to be tuned using cross-validation</li>
</ul>
<p>Typically <span class="math inline">\(L_2\)</span> used; <span class="math inline">\(L_1\)</span> results in sparse matrix for <span class="math inline">\(w\)</span> which some say is an advantage but in practice not used often.</p>
<p>Could regularise <span class="math inline">\(b\)</span> but just a single number, whereas <span class="math inline">\(w\)</span> has a lot of parameters.</p>
</div>
<div id="neural-network" class="section level4">
<h4>Neural Network</h4>
<p><span class="math display">\[J(w^{[1]},b^{[1]} \ldots, w^{[L]}, b^{[L]}) = \frac{1}{m} \sum^{m}_{i=1}\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\sum_{l=1}^{L}||w^{[l]}||_F^2\]</span></p>
<p>Matrix norm = Frobenius norm <span class="math display">\[||w^{[l]}||_F^2 = \sum_{i=1}^{n^{[l-1]}}\sum_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^2\]</span></p>
<p>Denoted by <span class="math inline">\(F\)</span> because it is <em>not</em> an <span class="math inline">\(L_2\)</span> norm</p>
<p>Impact on calculations</p>
<p><span class="math display">\[\begin{aligned}
    dw^{[l]} &amp;= \text{(from backprop)} + \frac{\lambda}{m}w^{[l]} \\
    w^{[l]} &amp;:= w^{[l]} - \alpha dw^{[l]} \\
            &amp;:= w^{[l]} - \alpha \left[ \text{(from backprop)} + \frac{\lambda}{m}w^{[l]}\right] \\
            &amp;:= w^{[l]}(1-\frac{\alpha \lambda}{m}) - \alpha\text{(from backprop)}
        \end{aligned}\]</span></p>
<p>L2 regularisation is sometimes called <strong>weight decay</strong> as it is like ordinary gradient descent, where you update <span class="math inline">\(w\)</span> by subtracting alpha times the original gradient you got from backprop, but you now multiply <span class="math inline">\(w\)</span> by <span class="math inline">\((1-\frac{\alpha \lambda}{m})\)</span>, which is a little bit less than 1.</p>
</div>
</div>
<div id="why-regularisation-reduces-overfitting" class="section level3">
<h3>Why regularisation reduces overfitting</h3>
<p>Attempt 1 at Explanation.</p>
<p>With large <span class="math inline">\(\lambda\)</span>, will set weights to zero to zero out a lot hidden units (approaches logistic regression). Although in reality, none are set to zero, but many almost zero. Hopefully intermediate <span class="math inline">\(\lambda\)</span> that gets to “just right”</p>
<p>Attempt 2 at Explanation : If z is quite small, <span class="math inline">\(\tanh(z)\)</span> is almost linear. So by increasing <span class="math inline">\(\lambda\)</span> will decrease <span class="math inline">\(w^{[l]}\)</span>. As <span class="math inline">\(z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}\)</span> then <span class="math inline">\(z^{[l]}\)</span> will be small Every layer will be roughly linear and so network is just linear network and even deep network will be linear.</p>
<p>Implementation Tip: Recall: <span class="math inline">\(J(\centerdot) = \frac{1}{m}\sum\limits_{i=1}^m \mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\sum\limits_{l=1}^{L}||w^{[l]}||^2_F\)</span></p>
<p>Need to plot the new definition of <span class="math inline">\(J\)</span> which includes the regularisation parameter, otherwise will not see <span class="math inline">\(J\)</span> monotonically decreasing.</p>
</div>
<div id="dropout-regularisation" class="section level3">
<h3>Dropout regularisation</h3>
<p>Go through each layer, for each node toss a coin and have .5 probability of removing. Then remove all ingoing and outgoing links from those nodes. Then do back-propagation. On a different training example, you repeat with the toss.</p>
<div id="implementing-dropout-inverted-dropout" class="section level4">
<h4>Implementing dropout (“Inverted dropout”)</h4>
<p>Forward Prop:</p>
<ul>
<li>Illustrate with layer <span class="math inline">\(\ell = 3\)</span></li>
<li>Set vector <span class="math inline">\(d3\)</span> = dropout vector for layer 3 = np.random.rand(a3.shape[0], a3.shape[1]) &lt; keep.prob</li>
<li>Take activations from third layer and set a3 := np.multiply(a3, d3)</li>
<li>Scale <span class="math inline">\(a3\)</span> up by setting a3/= keep.prob. This is so not to reduce the expected value of <span class="math inline">\(a^{[3]}\)</span>. Some earlier algorithms for dropout did not do this step.</li>
</ul>
<p>Noting that on iteration 1 might zero out some hidden units, but on second iteration might zero out different hidden units.</p>
<p>Backward Prop:</p>
<ul>
<li>Shut down the same neurons that were shut down during forward propagation by reapplying the same mask; i.e. set da3 := np.multiply(da3, d3)</li>
<li>Ensure to scale <span class="math inline">\(da3\)</span> up by setting da/= keep_prob.</li>
</ul>
<p>Making predictions at test time:</p>
<p>At test time, don’t use drop out.</p>
<p><span class="math display">\[\begin{aligned}
z^{[0]} &amp;= X\\
z^{[1]} &amp;= w a^{[0]} + b^{[1]}\\
a^{[1]} &amp;= g^{[1]}z^{[1]})\\
z^{[2]} &amp;= w a^{[1]} + b^{[2]}\\
\vdots\\
\hat{y}
\end{aligned}\]</span></p>
</div>
</div>
<div id="understanding-dropout" class="section level3">
<h3>Understanding dropout</h3>
<p>Can’t rely on any one feature, so have to spread out weights, which in turn has an effect of shrinking the weights (similar to L2 regularisation)</p>
<p>Note that can alter keep.prob by layer. e.g. in network with</p>
<ul>
<li>Input layer; <span class="math inline">\(n^{[0]}\)</span> = 3. Usually set keep.prob = 1.0 (i.e.. don’t apply)</li>
<li>Hidden Layer 1; <span class="math inline">\(n^{[1]} = 7\)</span>.</li>
<li>Hidden Layer 2; <span class="math inline">\(n^{[2]} = 7\)</span>. As dimension of weights is <span class="math inline">\(7 \times 7\)</span>, might want lower keep-prob in this layer.</li>
<li>Hidden Layer 3; <span class="math inline">\(n^{[3]} = 3\)</span>.</li>
<li>Hidden Layer 4; <span class="math inline">\(n^{[4]} = 2\)</span>.</li>
<li>Output Layer; <span class="math inline">\(n^{[5]} = 1\)</span></li>
</ul>
<p>Downside of using different keep.probs per layer: More hyperparameters to tune using cross-validation</p>
<p>Alternative: Have some layers where you apply drop out and some where you don’t, therefore have just one hyper-parameter which is the keep.prob for the layers for which you do apply the drop out.</p>
<p>Implementation Notes:</p>
<ul>
<li>First used for computer vision (input many pixels)</li>
<li>Unless algorithm is overfitting, then don’t bother with dropout regularisation. In computer vision, however, often have many pixels and not enough data so often overfit.</li>
</ul>
<p>Downside: Cost function <span class="math inline">\(J\)</span> is less well-defined, then lose debugging team of checking that <span class="math inline">\(J\)</span> is monotonically decreasing with number of iterations. Usually sets keep.prob to 1 and tests that <span class="math inline">\(J\)</span> is monotonically decreasing and that hasn’t added bug when using keep.prob</p>
</div>
<div id="other-regularisation-methods" class="section level3">
<h3>Other regularisation methods</h3>
<p>In addition to</p>
<ul>
<li>L2</li>
<li>Dropout</li>
</ul>
<p>there are other techniques to help with overfitting:</p>
<ul>
<li>Getting more training data (but can be expensive)</li>
<li>Augment training set by, for example,
<ul>
<li>Image detection
<ul>
<li>Flipping training set horizontally (when using computer vision)</li>
<li>Take random crops (rotate, zoom) You wouldn’t flip vertically! These don’t add so much extra info as would a new (independent) picture<br />
</li>
</ul></li>
<li>Optical character recognition can impose random rotations and distortions</li>
</ul></li>
<li>Early stopping. As run gradient descent usually plot training error or cost function <span class="math inline">\(J\)</span> against number of iterations. Should decrease with number of iterations. Also plot dev set error; this usually decreases then increases. Stop at inversion. Andrew Ng sometimes uses but disadvantage is that it takes away from orthogonalisation (i.e. the following two tasks/problems get coupled together; whereas with orthogonality keep as two separate tasks)
<ol style="list-style-type: decimal">
<li>Optimise cost function <span class="math inline">\(J\)</span> via gradient descent, momentum, adam, etc.</li>
<li>Not overfit via regularisation, more data, etc. So early stopping means you are not doing optimisation of cost function very well. Andrew Ng prefers L2 regularisation (although may need to try many values which is more computationally expensive, but on the plus side), Early stopping works similar to L2 regularisation, at earlier iterations <span class="math inline">\(w \approx 0\)</span> whereas for larger iterations, <span class="math inline">\(w\)</span> large.</li>
</ol></li>
</ul>
<p>Note that regularization hurts training set performance! This is because it limits the ability of the network to overfit to the training set. But since it ultimately gives better test accuracy, it is helping your system.</p>
</div>
</div>
<div id="setting-up-your-optimisation-problem" class="section level2">
<h2>Setting up your optimisation problem</h2>
<div id="normalising-inputs" class="section level3">
<h3>Normalising inputs</h3>
<p>Normalising input will speed up training</p>
<ol style="list-style-type: decimal">
<li>Subtract mean to get zero mean; <span class="math display">\[x := x - \mu\]</span> where <span class="math inline">\(\mu = \frac{1}{m}\sum^m_{i = 1}x^{(i)}\)</span></li>
<li>Normalise variance to get variance equal to 1; <span class="math display">\[x := x/\sigma^2\]</span> where <span class="math inline">\(\sigma^2 = \frac{1}{m} \sum^m_{i = 1} {(x^{(i)})}^2\)</span>.</li>
</ol>
<p>Note that do not need to subtract <span class="math inline">\(\mu\)</span> in variance formula here as <span class="math inline">\(\mu\)</span> is already set to zero.</p>
<p>Make sure to use same <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> for training and test set. Data has to go through the same transformation.</p>
<p>Normalisation is done because:</p>
<ul>
<li>With unnormalised, cost function (and therefore contours) is elongated bowl. Will need small learning rate, will oscillate.</li>
<li>With normalisation, get symmetric/spherical, can take much greater steps.</li>
</ul>
<p>Normalisation not so important when features on similar scales; but normalisation won’t hurt so may as well do.</p>
</div>
<div id="vanishingexploding-gradients" class="section level3">
<h3>Vanishing/exploding gradients</h3>
<p>Problem with training deep NN is vanishing/exploding gradients - slopes can get very very big or very very small which makes training small.</p>
<p>Making careful choice for random weight initialisation can minimise problem.</p>
<div class="figure">
<img src="/img/ImprovingDNN/ImprovingDNN_explodingGradients.JPG" alt="Deep NN Exploding Gradients" />
<p class="caption">Deep NN Exploding Gradients</p>
</div>
<p>For simplicity:</p>
<ul>
<li>Two hidden units per layer</li>
<li>Assume linear activation function, <span class="math inline">\(g(z) = z\)</span></li>
<li>Ignore <span class="math inline">\(b\)</span>, i.e. set <span class="math inline">\(b = 0\)</span></li>
<li>Therefore <span class="math inline">\(a{[1]} = g(z{[1]}) = z{[1]}\)</span></li>
</ul>
<p>Then</p>
<p><span class="math display">\[\begin{aligned}
    a^{[1]} &amp;= g(z^{[1]}) = g(w^{[1]}X + 0) = w^{[1]}X \\
    a^{[2]} &amp;= g(z^{[2]}) = g(w^{[2]} a^{[1]} + 0) =  w^{[2]} a^{[1]} =  w^{[2]}  w^{[1]}X\\
    a^{[3]} &amp;= g(z^{[3]}) = g(w^{[3]} a^{[2]} + 0) =  w^{[3]} a^{[2]} = w^{[3]}  w^{[2]}  w^{[1]}X\\
     \vdots\\
    \hat{y} &amp;= w^{[l]}w^{[l-1]}w^{[l-2]} \ldots w^{[2]}w^{[1]}X 
\end{aligned}\]</span></p>
<p>Assume <span class="math inline">\(w^{[l]} = \left[\begin{array}{cc} 1.5 &amp; 0 \\  0 &amp; 1.5  \end{array}\right], \forall l\)</span> except last one as it will be of a different dimension</p>
<p>then <span class="math display">\[\hat{y} = w^{[1]}\left[\begin{array}{cc}
1.5 &amp; 0 \\
  0 &amp; 1.5   
 \end{array}\right]^{L-1}X\]</span></p>
<p>So if weights are just larger than identity then activations increase exponentially. Conversely, if weights are just a smaller than identity then activations increase exponentially.</p>
<p>Similar argument can be used to show that gradients also increase/decrease exponentially. With deep learning matrix (i.e. a 152 layer network), then this can cause problems for gradient descent.</p>
</div>
<div id="weight-initialisation-for-deep-networks" class="section level3">
<h3>Weight initialisation for deep networks</h3>
<p>Careful choice of random initialisation will help with problem of vanishing/exploding gradients</p>
<p>Single layer network with <span class="math inline">\(n\)</span> inputs:</p>
<ul>
<li>Assume <span class="math inline">\(b = 0\)</span></li>
<li>$z = w_1x_1 + w_2x_2 + + w_n x_n + 0</li>
<li>The larger <span class="math inline">\(n\)</span> is the smaller you want <span class="math inline">\(w_i\)</span>.</li>
<li>Can set <span class="math inline">\(\text{var}(w_i) = 1/n\)</span>. With ReLU, it is better to have <span class="math inline">\(\text{var}(w_i) = 2/n\)</span></li>
<li><p>With more layers, set <span class="math inline">\(w^{[l]} = \text{np.random.rand(shape) * np.sqrt}(\frac{2}{n^{[l-1]}})\)</span> which draws on the formula for the variance of the Gaussian function. Aim to be closer to one.</p></li>
<li>For tanh activation function, use <span class="math inline">\(\sqrt{\frac{1}{n^{[l-1]}}}\)</span>. Called Xavier initialisation.</li>
<li>Some authors also use <span class="math inline">\(\sqrt{\frac{2}{n^{[l-1]} + n^{[l]}}}\)</span></li>
<li><p>Could also tune variance parameter as a hyperparameter but would be at lower end of what to do (not very important when compared to other parameters you could tune)</p></li>
</ul>
</div>
<div id="numerical-approximation-of-gradients" class="section level3">
<h3>Numerical approximation of gradients</h3>
<ul>
<li>To build up to gradient checking, first look how to approximate gradients</li>
<li>On <span class="math inline">\(x\)</span> axis, start with <span class="math inline">\(\theta\)</span>, e.g. 1
<ul>
<li>Nudge to right <span class="math inline">\(\theta + \epsilon\)</span>, e.g. 1.01</li>
<li>Nudge to left <span class="math inline">\(\theta + \epsilon\)</span>, e.g. 1.01</li>
</ul></li>
<li>Check gradient by using two sides of <span class="math inline">\(epsilon\)</span></li>
<li>Gradient, <span class="math inline">\(g(\theta) \approx \frac{\Delta\text{y}}{\Delta\text{x}} = \frac{f(\theta + \epsilon) - f(\theta - \epsilon)}{2 \epsilon}\)</span></li>
<li>With <span class="math inline">\(f(\theta) = \theta^3\)</span> and <span class="math inline">\(\theta\)</span> = 1 <span class="math display">\[\begin{aligned}
g(\theta) &amp;= f&#39;(\theta) = 3\theta = 3 \\
g(\theta) &amp;\approx \frac{1.01^3 - 0.99^3}{0.02} = 3.0001
\end{aligned}\]</span></li>
</ul>
<p>Note (Calculus), approximation error of derivative is <span class="math inline">\(O(\epsilon^2)\)</span>. If instead used one-sided difference <span class="math inline">\(g(\theta) \approx \frac{f(\theta + \epsilon) - f(\theta)}{\epsilon}\)</span> then approximation error is greater at <span class="math inline">\(O(\epsilon)\)</span>.</p>
</div>
<div id="gradient-checking-grad-check" class="section level3">
<h3>Gradient checking (Grad check)</h3>
<p>Gradient checking will help verify and debug implementation of backpropagation and save LOTS of time.</p>
<p>Steps: 1. Take all parameters <span class="math inline">\(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}, \ldots, W^{[L]}, b^{[L]}\)</span> and reshape into big vector <span class="math inline">\(\theta\)</span>, so that cost function is a function of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(J(\theta)\)</span> 2. Take all derivatives, <span class="math inline">\(dW^{[1]}, db^{[1]}, dW^{[2]}, db^{[2]}, \ldots, dW^{[L]}, db^{[L]}\)</span> and reshape into big vector <span class="math inline">\(d\theta\)</span> which is the same dimension as <span class="math inline">\(\theta\)</span> 3. Is <span class="math inline">\(d\theta\)</span> the gradient, or slope of cost function <span class="math inline">\(J(\theta)\)</span>?</p>
<p>To implement grad check:</p>
<p>for each <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
    d\theta_\text{approx}^{[i]} &amp;= \frac{
        J(\theta_1, \theta_2, \ldots, \theta_{i + \epsilon}, \ldots) - 
        J(\theta_1, \theta_2, \ldots, \theta_{i - \epsilon}, \ldots)}{2\epsilon}\\
        &amp;\approx d\theta^{[i]} ?
\end{aligned}\]</span></p>
<p>Uses Euclidean distances (square root of sum of squares) to calculate difference between two vectors: <span class="math display">\[\frac{||d\theta_\text{approx} - d\theta||_2}{||d\theta_\text{approx}||_2 + ||d\theta||_2}\]</span></p>
<p>With <span class="math inline">\(\epsilon = 10^{-7}\)</span>, if Euclidean distance <span class="math inline">\(\approx\)</span></p>
<ul>
<li><span class="math inline">\(10^{-7}\)</span> then great!<br />
</li>
<li><span class="math inline">\(10^{-5}\)</span> might be okay but take a very careful look.</li>
<li><span class="math inline">\(10^{-3}\)</span> be worried … look at the differences for each value of <span class="math inline">\(i\)</span> to determine where error might be.</li>
</ul>
</div>
<div id="gradient-checking-implementation-notes" class="section level3">
<h3>Gradient checking implementation notes</h3>
<ul>
<li>Don’t use in training in place of <span class="math inline">\(d\theta\)</span>- this is very slow</li>
<li>Use only to debug, once debugged then turn off</li>
<li>If algorithm fails grad check, look at components to try to identify bug, e.g. if <span class="math inline">\(db^{[l]}\)</span> is different, but <span class="math inline">\(dw^{[l]}\)</span> close then bug might be with <span class="math inline">\(db^{[l]}\)</span></li>
<li>Remember regularisation</li>
<li>Doesn’t work with dropout; best to implement grad check without dropout (set keep.prob to 1.0 then turn off), Could otherwise fix pattern and check, but doesn’t recommend.</li>
<li>Run at random initialisation; perhaps again after training. Could be close at <span class="math inline">\(w,b \approx 0\)</span>. Could run grad check at random initialisation, run for a while (train for some number of iterations) then run grad check.</li>
</ul>
</div>
</div>
</div>
<div id="week2" class="section level1">
<h1>Optimization algorithms</h1>
<div id="mini-batch-gradient-descent" class="section level2">
<h2>Mini-batch gradient descent</h2>
<ul>
<li>Vectorisation allows you to efficiently compute on <span class="math inline">\(m\)</span> examples</li>
</ul>
<p><span class="math display">\[\begin{aligned}
X &amp;= x^{(1)} x^{(2)} x^{(3)} \ldots x^{(m)}, X \in \mathbb{R}^{n_x \times m}\\
Y &amp;= y^{(1)} y^{(2)} y^{(3)} \ldots, y  \in \mathbb{R}^{1\times m}
\end{aligned}\]</span></p>
<p>But what if <span class="math inline">\(m\)</span> = 5,000,000?</p>
<p>Will need to train entire training set before take a little step of gradient descent, and then you need to train the entire training step again before taking another step.</p>
<p>Instead, split up training set into mini-batches (baby training set), e.g. with 1000 examples each.</p>
<p>Denote mini-batch <span class="math inline">\(t\)</span> by <span class="math inline">\(X^{\{t\}}, Y^{\{t\}}\)</span>,</p>
<p>where <span class="math display">\[X^{\{1\}}= [x^{(1)} x^{(2)} x^{(3)} \ldots x^{(1000)}], \ \ X^{\{1\}} \in \mathbb{R}^{n_x \times 1000}\]</span>, and <span class="math display">\[Y^{\{1\}}= [y^{(1)} y^{(2)} y^{(3)} \ldots y^{(1000)}], \ \ y^{\{1\}} \in \mathbb{R}^{1 \times 1000}\]</span></p>
<p>So recall:</p>
<ul>
<li>[]: layer index</li>
<li>{}: mini-batch index</li>
<li>(): training index</li>
</ul>
<p>Batch gradient descent refers to gradient descent algorithm used to date.</p>
<p>Mini-batch gradient descent algorithm:</p>
<p>for <span class="math inline">\(t = 1, \ldots, 5000\)</span> (the number of mini-batches)</p>
<ul>
<li><p>Forward prop on <span class="math inline">\(X^{\{t\}}\)</span>, using vectorisation implementation that processes 1000 examples at a time <span class="math display">\[\begin{aligned}
Z^{[1]} &amp;= W^{[1]}X^{\{t\}} + b^{[1]}\\
A^{[1]} &amp;= g^{[1]}(Z^{[1]})\\
\vdots\\
A^{[l]} &amp;= g^{[2]}(Z^{[2]})
\end{aligned}\]</span></p></li>
<li><p>Compute cost function <span class="math display">\[J^{\{t\}} = \frac{1}{1000}  \sum^{1000}_{i=1}\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2 \centerdot 1000}\sum_{l=1}^{L}||w^{[l]}||_F^2\]</span></p></li>
<li><p>Backprop to compute gradients w.r.t. <span class="math inline">\(J^{t}\)</span>, using (<span class="math inline">\(X^{\{t\}}\)</span>, <span class="math inline">\(Y^{\{t\}}\)</span>) <span class="math display">\[\begin{aligned}
w^{[l]} := w^{[l]} - \alpha dw^{[l]} \\
b^{[l]} := b^{[l]} - \alpha db^{[l]} 
\end{aligned}\]</span></p></li>
</ul>
<p><strong>1 epoch</strong>: Going through the 5000 mini-batches (meaning have gone through 5,000,000 training examples); this means you have done 5000 gradient descents. If you had used batch gradient descent you would have only done a single gradient descent with a single pass through the training set.</p>
<p>NB. May want to do more than 1 epoch…</p>
</div>
<div id="understanding-mini-batch-gradient-descent" class="section level2">
<h2>Understanding mini-batch gradient descent</h2>
<ul>
<li>With batch gradient descent, expect cost to go down on every iteration.</li>
<li>With mini-batch gradient descent, cost may not decrease on every iteration. This is because on every iteration, you are using a different training set to plot <span class="math inline">\(J^{\{t\}}\)</span> which is computed using <span class="math inline">\(X^{\{t\}}\)</span>, <span class="math inline">\(Y^{\{t\}}\)</span>. You should expect a downward trend.</li>
</ul>
<div class="figure">
<img src="/img/ImprovingDNN/cost_curves.png" alt="Cost Curves" />
<p class="caption">Cost Curves</p>
</div>
<p>Choosing your mini-batch size:</p>
<ul>
<li><strong>Blue</strong> Batch Gradient Descent: mini-batch size = <span class="math inline">\(m\)</span>
<ul>
<li>Disadvantage: Too long per iteration (with large training set)</li>
</ul></li>
<li><strong>Purple</strong> Stochastic Gradient Descent: mini-batch size = 1
<ul>
<li>Will never converge</li>
<li>Disadvantage: Lose speed-up from vectorisation</li>
</ul></li>
<li><strong>Green</strong> In-between
<ul>
<li>Advantage: Vectorisation + Make progress without needing to wait until process entire training set</li>
</ul></li>
</ul>
<div class="figure">
<img src="/img/ImprovingDNN/batch_contours.png" alt="Batch Contours" />
<p class="caption">Batch Contours</p>
</div>
<p>Guideline:</p>
<ul>
<li>If small training set (<span class="math inline">\(m \leq 2000\)</span>): Use batch gradient descent</li>
<li>Else typical mini-batch size one of {2^6=64, 2^7=128, 2^8=256, 2^9=512}; 2^10}; 1024 is rare. Ensure that mini-batch size fits in CPU/GPU memory, otherwise performance will “fall off the cliff”</li>
<li>Can use a hyperparameter to determine the power of 2 to use.</li>
</ul>
</div>
<div id="exponentially-moving-weighted-averages" class="section level2">
<h2>Exponentially (moving) weighted averages</h2>
<p>Initialise <span class="math inline">\(v_0 = 0\)</span> Set <span class="math display">\[v_t = \beta v_{t-1} + (1-\beta) x_t\]</span></p>
<p><span class="math inline">\(v_t\)</span> approximately averages over <span class="math inline">\(\frac{1}{1-\beta}\)</span> values of <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[\begin{aligned}
\beta = 0.9 &amp;:\approx 10    \; X \text{ values}\\
\beta = 0.98 &amp;:\approx 50   \;X \text{ values}\\
\beta = 0.5 &amp;:\approx 2     \;X \text{ values}
\end{aligned}\]</span></p>
</div>
<div id="understanding-exponentially-weighted-averages" class="section level2">
<h2>Understanding exponentially weighted averages</h2>
<p>Given; <span class="math display">\[\begin{aligned}
v_{100} &amp;= 0.9 v_{99} + 0.1x_{100}\\
v_{99} &amp;= 0.9 v_{98} + 0.1x_{99}\\
v_{98} &amp;= 0.9 v_{97} + 0.1x_{98}
\end{aligned}\]</span></p>
<p>have that <span class="math display">\[\begin{aligned}
v_{100} &amp;=  0.1x_{100} + 0.9 v_{99}\\
        &amp;=  0.1x_{100} + 0.9 (0.1x_{99} + 0.9 v_{98}) \\
        &amp;=  0.1x_{100} + 0.9 \left(0.1x_{99} + 0.9 [0.1x_{98} + 0.9 v_{97}]\right) \\
        &amp;=  0.1x_{100} + 0.1\times0.9 x_{99} + 0.1\times 0.9^2 x_{98} + 0.1\times 0.9^3 x_{97} + \ldots
        \end{aligned}\]</span></p>
<p>Note that all coefficients add up to almost 1, which is why this is an exponentially weighted average (bias correction)</p>
<p>Also, note that <span class="math inline">\(\varepsilon = 1-\beta\)</span> and <span class="math display">\[(1-\varepsilon)^{1/\varepsilon} = \frac{1}{e}\]</span> where <span class="math display">\[\frac{1}{1-\beta}\]</span> is the number of <span class="math inline">\(x\)</span> values over which the average is taken.</p>
<p>For example:</p>
<pre class="r"><code>data.frame(beta = c(0.9, 0.98, 0.5)) %&gt;%
    mutate(epsilon = 1 - beta,
           e_inv = beta^{1/epsilon},
           avg_over = 1/epsilon) %&gt;%
    arrange(beta) %&gt;%
    kable(col.names = c(&quot;$\\beta$&quot;, &quot;$\\epsilon$&quot;, &quot;$1/e$&quot;, &quot;Avg Over&quot;))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right"><span class="math inline">\(\beta\)</span></th>
<th align="right"><span class="math inline">\(\epsilon\)</span></th>
<th align="right"><span class="math inline">\(1/e\)</span></th>
<th align="right">Avg Over</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.50</td>
<td align="right">0.50</td>
<td align="right">0.2500000</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">0.90</td>
<td align="right">0.10</td>
<td align="right">0.3486784</td>
<td align="right">10</td>
</tr>
<tr class="odd">
<td align="right">0.98</td>
<td align="right">0.02</td>
<td align="right">0.3641697</td>
<td align="right">50</td>
</tr>
</tbody>
</table>
<p>In the following image, * <strong>Yellow<em>: <span class="math inline">\(\beta = 0.5\)</span> </em> </strong>Red<strong>: <span class="math inline">\(\beta = 0.9\)</span> * </strong>Green**: <span class="math inline">\(\beta = 0.98\)</span></p>
<div class="figure">
<img src="/img/ImprovingDNN/exp_weight_avg_betas.png" alt="Betas" />
<p class="caption">Betas</p>
</div>
<p>In general, the number of values over which the exponentially weighted average is averaging over, <em>Avg Over</em>, satisfies the relationship <span class="math inline">\(\beta^\text{Avg Over} \approx \frac{1}{e}\)</span>, i.e. after <em>Avg Over</em> days, the weight decays to less than about a third of the weight of the current day.</p>
<pre class="r"><code># This is showing that, for example, with beta = 0.9, after 10 days, the value of y is 0.34* value of current value of y.
betas &lt;- c(0.9, 0.98, 0.5)
dat &lt;- NULL
for (beta in betas){
    x &lt;- 1:100
    y &lt;- (1-beta)* (beta)^(1-x)
    dat &lt;- rbind(dat, data.frame(beta, x, y))
}
dat %&gt;% 
    group_by(beta) %&gt;%
    mutate(avg_over = 1/(1-beta)) %&gt;%
    mutate(e_inv = 1 - (y - lag(y, avg_over[1]))/y) %&gt;%
    filter(!is.na(e_inv)) %&gt;%
    top_n(n=1, wt = x) %&gt;%
    select(beta, avg_over, e_inv) %&gt;%
    arrange(beta) %&gt;%
    kable(col.names = c(&quot;$\\beta$&quot;, &quot;Avg Over&quot;, &quot;$1/e$&quot;))</code></pre>
<p>With <span class="math inline">\(\beta = 0.9\)</span>, after 10 <span class="math inline">\(x\)</span> values, weight of <span class="math inline">\(x_{i-11}\)</span> decays to less than a third of the current value.</p>
<p>Implementation: * Initialise <span class="math inline">\(v = 0\)</span> * Repeat to get next <span class="math inline">\(\theta_t\)</span>: <span class="math display">\[v_\theta := \beta v_\theta + (1-\beta) \theta_t\]</span></p>
<p>One line of code!.. not most accurate, could instead average over last 2, 10 or 50 days, but this is more computationally expensive.</p>
</div>
<div id="bias-correction-in-exponentially-weighted-averages" class="section level2">
<h2>Bias correction in exponentially weighted averages</h2>
<p>Bias correction can make computation of averages more accurate; this is correcting bias from initialisation.</p>
<p>For example with <span class="math inline">\(\beta = 0.98\)</span> and <span class="math inline">\(\theta_1 = 40\)</span> <span class="math display">\[\begin{aligned}
    v_t &amp;= \beta v_{t-1} + (1-\beta)\theta_t \\
    v_0 &amp;= 0 \\
    v_1 &amp;= 0.98 v_0 + 0.2 \theta_1 \\
        &amp;= 0 + 8 \\
    v_2 &amp;= 0.98 v_1 + 0.02 \theta_2 \\
        &amp;= 0.98 \times 0.02 \times \theta_1 + 0.02 \theta_2 \\
        &amp;= 0.0196 \theta_1 + 0.02 \theta_2
\end{aligned}\]</span> So both <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> are less than the average <span class="math inline">\(\theta\)</span>.</p>
<p>TO correct for bias use <span class="math display">\[\frac{v_t}{1-\beta^t}\]</span> Noting that at <span class="math inline">\(t=2\)</span>, <span class="math inline">\(1-\beta^t = (1-0.98^2) = 0.0396\)</span>, so that <span class="math display">\[\frac{v_t}{1-\beta^t} = \frac{0.0196 \theta_1 + 0.02 \theta_2}{0.0396},\]</span> i.e. the weights in the numerator add up to the denominator and so this is a weighted average.</p>
<p>When <span class="math inline">\(t\)</span> is large enough, the bias correction has no effect.</p>
<p>In machine learning, for exponentially weighted averages, most people don’t worry about bias correction as just wait initial period.</p>
</div>
<div id="gradient-descent-with-momentum" class="section level2">
<h2>Gradient descent with momentum</h2>
<p>Gradient descent with momentum:</p>
<ul>
<li>Is faster</li>
<li>Computes exponentially weighted average of gradients and uses them to update your weights instead.</li>
</ul>
<p>Algorithm: On iteration <span class="math inline">\(t\)</span>:</p>
<ul>
<li>Compute <span class="math inline">\(dW\)</span>, <span class="math inline">\(db\)</span> on current mini-batch</li>
<li><span class="math inline">\(v_{dW} = \beta v_{dW} + (1-beta) dW\)</span>,</li>
<li><span class="math inline">\(v_{db} = \beta v_{db} + (1-beta) db\)</span>,</li>
<li><span class="math inline">\(w := w - \alpha v_{dW}\)</span>, <span class="math inline">\(b := b - \alpha v_{db}\)</span></li>
</ul>
<p>So have two hyperparameters: <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>. In practice <span class="math inline">\(\beta = 0.9\)</span> works pretty well. in practice, people don’t use bias correction because after 10 iterations, fixed.</p>
<p><span class="math inline">\(v_{dW}\)</span> is initialised to a vector of zeros with the same dimension as <span class="math inline">\(dW\)</span> and <span class="math inline">\(w\)</span>. Similarly, <span class="math inline">\(v_{db}\)</span> is initialised to a vector of zeros with the same dimension as <span class="math inline">\(db\)</span> and <span class="math inline">\(b\)</span></p>
<p>Note that in the literature they scale <span class="math inline">\(\alpha\)</span> by <span class="math inline">\(1-\beta\)</span>, such that <span class="math inline">\(\alpha^\star = \frac{\alpha}{1-\beta}\)</span> and do following:</p>
<ul>
<li>Compute <span class="math inline">\(dW\)</span>, <span class="math inline">\(db\)</span> on current mini-batch</li>
<li><span class="math inline">\(v_{dW} = \beta v_{dW} + W\)</span>,</li>
<li><span class="math inline">\(v_{db} = \beta v_{db} + db\)</span>,</li>
<li><span class="math inline">\(w := w - \alpha^\star v_{dW}\)</span>, <span class="math inline">\(b := b - \alpha^\star v_{db}\)</span></li>
</ul>
<p>This initial formulation is preferred by Andrew Ng; this is because in the latter formulation, tuning Beta will impact scaling of <span class="math inline">\(v\times dW\)</span> and <span class="math inline">\(v \times db\)</span> and so you may need to return the learning rate alpha.</p>
<p>In the following image, * <strong>Blue<em>: Gradient descent </em> </strong>Red**: Gradient descent with momentum</p>
<div class="figure">
<img src="/img/ImprovingDNN/momentum.png" alt="Betas" />
<p class="caption">Betas</p>
</div>
</div>
<div id="rmsprop" class="section level2">
<h2>RMSprop</h2>
<p>Root mean square prop can also speed up gradient descent by dampening oscillations in gradient descent.</p>
<p>On iteration <span class="math inline">\(t\)</span>:</p>
<ul>
<li>Compute <span class="math inline">\(dW\)</span>, <span class="math inline">\(db\)</span> on current mini-batch</li>
<li><span class="math inline">\(S_{dW} = \beta_2 S_{dW} + (1-beta_2) dW^2\)</span> (using element-wise squaring),</li>
<li><span class="math inline">\(S_{db} = \beta_2 S_{db} + (1-beta_2) db ^2\)</span>,</li>
<li><span class="math inline">\(w := w - \alpha \frac{dW}{\sqrt{s_{dW}}}\)</span>, <span class="math inline">\(b := b - \alpha \frac{db}{\sqrt{s_{db}}}\)</span></li>
</ul>
<p>May therefore be able to use a larger learning rate <span class="math inline">\(\alpha\)</span></p>
<p>In practice, add a small epsilon to denominator to ensure numerical stability and don’t divide by almost zero.</p>
</div>
<div id="adam-optimisation-algorithm" class="section level2">
<h2>Adam optimisation algorithm</h2>
<p>Adam = Momentum + RMSprop</p>
<p>Algorithm:</p>
<ul>
<li><span class="math inline">\(v_{dW} = 0\)</span>, <span class="math inline">\(S_{dW} = 0\)</span>, <span class="math inline">\(v_{db} = 0\)</span>, $S_{db} = 0</li>
<li>On iteration <span class="math inline">\(t\)</span>:
<ul>
<li>Compute <span class="math inline">\(dW\)</span>, <span class="math inline">\(db\)</span> using current mini-batch</li>
<li><span class="math inline">\(v_{dW} = \beta_1 v_{dW} + (1-beta_1) dW\)</span>, <span class="math inline">\(v_{db} = \beta_1 v_{db} + (1-beta_1) db\)</span> (momentum)</li>
<li><span class="math inline">\(S_{dW} = \beta_2 S_{dW} + (1-beta_2) dW^2\)</span>, <span class="math inline">\(S_{db} = \beta_2 S_{db} + (1-beta_2) db ^2\)</span> (RMSprop)</li>
<li>Corrections:
<ul>
<li><span class="math inline">\(v_{dw}^\text{corrected} = v_{dw}/(1-\beta_1^t)\)</span></li>
<li><span class="math inline">\(v_{db}^\text{corrected} = v_{db}/(1-\beta_1^t)\)</span></li>
<li><span class="math inline">\(S_{dw}^\text{corrected} = S_{dw}/(1-\beta_2^t)\)</span></li>
<li><span class="math inline">\(S_{db}^\text{corrected} = S_{db}/(1-\beta_2^t)\)</span></li>
</ul></li>
<li>Updates:
<ul>
<li><span class="math inline">\(w := w - \alpha\frac{v_dw^\text{corrected}}{\sqrt{S_{dw}^\text{corrected}} + \varepsilon}\)</span></li>
<li><span class="math inline">\(b := b - \alpha\frac{v_db^\text{corrected}}{\sqrt{S_{db}^\text{corrected}} + \varepsilon}\)</span></li>
</ul></li>
</ul></li>
</ul>
<p>Hyperparameters:</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> - learning rate. needs to be tuned</li>
<li><span class="math inline">\(\beta_1\)</span> - momentum moving average of <span class="math inline">\(dw\)</span> and <span class="math inline">\(db\)</span> (first moment, mean of the derivatives). Typically 0.9.</li>
<li><span class="math inline">\(\beta_2\)</span> - moving weighted average of <span class="math inline">\(dw^2\)</span> and <span class="math inline">\(db^2\)</span> (second moment)</li>
<li><span class="math inline">\(\varepsilon\)</span> - doesn’t matter too much; doesn’t affect performance much at all</li>
</ul>
<p>Usually use default values for <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span> and <span class="math inline">\(\varepsilon\)</span></p>
<p>Adam = ADAptive Moment estimation</p>
<p>Note that do use bias correction in ADAM.</p>
</div>
<div id="learning-rate-decay" class="section level2">
<h2>Learning rate decay</h2>
<ul>
<li>Slowly reducing learning rate over time can help speed up learning rate</li>
<li>Recall 1 epoch = 1 pass through the data</li>
<li>Could set <span class="math display">\[\alpha = \frac{1}{1 + \text{decay rate} \times \text{epoch-num}} \alpha_0\]</span></li>
</ul>
<pre class="r"><code>alpha0 = 002
decay_rate = 1

data.frame(epoch = c(1, 2, 3, 4)) %&gt;%
    mutate(alpha = 1 /(1 + decay_rate * epoch) * alpha0) </code></pre>
<pre><code>##   epoch     alpha
## 1     1 1.0000000
## 2     2 0.6666667
## 3     3 0.5000000
## 4     4 0.4000000</code></pre>
<ul>
<li>Alternatively,
<ul>
<li>use exponential decay, set <span class="math inline">\(\alpha = 0.95^\text{epoch.num} \times \alpha_0\)</span></li>
<li>Set <span class="math inline">\(\alpha = \frac{k}{\text{epoch.num}} \alpha_0\)</span></li>
<li>Use step function to decrease alpha over time</li>
<li>Manual decay, watch as it is training a small number of models hour by hour or day by day.</li>
</ul></li>
</ul>
<p>This does help but is lower down Andrew Ng’s list on things to try.</p>
</div>
<div id="the-problem-of-local-optima" class="section level2">
<h2>The problem of local optima</h2>
<ul>
<li>people used to worry about optimisation algorithm getting stuck at local optima</li>
<li>In, e.g. 20000, dimension space would require all 20,000 to be convex / concave which is unlikely.<br />
</li>
<li>So intuition about low-dimension spaces doesn’t transfer to high-dimensional spaces</li>
<li>So, in deep learning, local optima isn’t a problem, but a plateau is a problem as they can really slow down learning</li>
<li>momentum, rmsProp or ADam can really help learning algorithm.</li>
<li>Because network is solving optimisation problems over such high dimensional spaces, no-one has great intuition about what these spaces really look like.</li>
</ul>
</div>
</div>
<div id="week3" class="section level1">
<h1>Hyperparameter tuning, Batch Normalization and Programming Frameworks</h1>
<div id="hyperparameter-tuning" class="section level2">
<h2>Hyperparameter Tuning</h2>
</div>
<div id="tuning-process" class="section level2">
<h2>Tuning process</h2>
</div>
<div id="using-and-approriate-scale-to-pick-hyperparameters" class="section level2">
<h2>Using and approriate scale to pick hyperparameters</h2>
</div>
<div id="hyperparameters-tuning-in-practice-pandas-vs.caviar" class="section level2">
<h2>Hyperparameters tuning in practice: Pandas vs. Caviar</h2>
</div>
<div id="batch-normalisation" class="section level2">
<h2>Batch Normalisation</h2>
<div id="normalising-activations-in-a-network" class="section level3">
<h3>Normalising activations in a network</h3>
</div>
<div id="fitting-batch-norm-into-a-neural-network" class="section level3">
<h3>Fitting Batch Norm into a neural network</h3>
</div>
</div>
<div id="why-does-batch-norm-work" class="section level2">
<h2>Why does Batch Norm work?</h2>
</div>
<div id="batch-norm-at-test-time" class="section level2">
<h2>Batch Norm at test time</h2>
</div>
<div id="multi-class-classification" class="section level2">
<h2>Multi-class classification</h2>
<div id="softmax-regression" class="section level3">
<h3>Softmax Regression</h3>
</div>
<div id="training-a-softmax-classifier" class="section level3">
<h3>Training a softmax classifier</h3>
</div>
</div>
<div id="introduction-to-programming-frameworks" class="section level2">
<h2>Introduction to programming frameworks</h2>
<div id="deep-learning-frameworks" class="section level3">
<h3>Deep learning frameworks</h3>
</div>
<div id="tensorflow" class="section level3">
<h3>Tensorflow</h3>
</div>
</div>
</div>
