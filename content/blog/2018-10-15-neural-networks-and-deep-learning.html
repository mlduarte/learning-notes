---
title: Neural Networks and Deep Learning
author: ''
date: '2018-10-15T13:39:46+02:00'
slug: neural-networks-and-deep-learning
categories: [Study-Notes]
tags: 
- Coursera
- Study-Notes
banner: "img/banners/NNandDL.png"
---



<div id="week1" class="section level1">
<h1>Introduction</h1>
<div id="introduction-to-deep-learning" class="section level2">
<h2>Introduction to Deep Learning</h2>
<p>Deep learning <span class="math inline">\(\equiv\)</span> training a neural network.</p>
<p>What is a Neural Network? Example: Housing price prediction</p>
<ul>
<li>Aim predict price = f(size of house)</li>
<li>Linear regression: Fit straight line, but know prices can never be 0, looks like piecewise linear regression, but simplest NN. <span class="math display">\[\text{Size} \rightarrow \bigcirc \rightarrow \text{Price (y)}\]</span>, where the neuron, computes the linear function, takes a max of zero and outputs the price. This function is called a <strong>ReLU</strong> (Rectified Linear Unit) function.<br />
</li>
<li>A larger NN is formed by stacking many of these NNs together.<br />
<img src="/img/DLS_NNIntro.png" alt="NN_eg" /></li>
</ul>
</div>
<div id="supervised-learning-with-neural-networks" class="section level2">
<h2>Supervised Learning with Neural Networks</h2>
<table>
<colgroup>
<col width="19%" />
<col width="27%" />
<col width="20%" />
<col width="32%" />
</colgroup>
<thead>
<tr class="header">
<th>Input(x)</th>
<th>Output (y)</th>
<th>Application</th>
<th>Type of NN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Home features</td>
<td>Price</td>
<td>Real Estate</td>
<td>Standard</td>
</tr>
<tr class="even">
<td>Ad and user info</td>
<td>Click on ad? (0/1)</td>
<td>Online Advertising</td>
<td>Standard</td>
</tr>
<tr class="odd">
<td>Image</td>
<td>Object (<span class="math inline">\(1, \ldots, 1000\)</span>)</td>
<td>Photo tagging</td>
<td>Convolution on NN (CNN)</td>
</tr>
<tr class="even">
<td>Audio</td>
<td>Text transcript</td>
<td>Speech recognition</td>
<td>Recurrent NN (RNN) (as sequential)</td>
</tr>
<tr class="odd">
<td>English</td>
<td>Chinese</td>
<td>Machine translation</td>
<td>RNN (as sequential)</td>
</tr>
<tr class="even">
<td>Image, Radar info</td>
<td>Position of other cars</td>
<td>Autonomous driving</td>
<td>(hybrid, incl. CNN)</td>
</tr>
</tbody>
</table>
<div class="figure">
<img src="/img/DLS_NNTypes.png" alt="NN_types" />
<p class="caption">NN_types</p>
</div>
<p>Success will depend on selecting the appropriate:</p>
<ul>
<li><span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span></li>
<li>Type of NN</li>
</ul>
<p>Deep learning can be applied to both types of data;</p>
<ul>
<li>Structured data: Essentially databases of data.</li>
<li>Unstructured data: Audio, images, text. Features might be pixel values in an image or individual words in a piece of text. Deep learning helping to interpret unstructured data as well.</li>
</ul>
</div>
<div id="why-is-deep-learning-taking-off" class="section level2">
<h2>Why is Deep Learning taking off?</h2>
<p>Basic technical ideas have been around for decades. Main drivers for rise of deep learning:</p>
<ul>
<li><p>Amount of data (computer, sensors, cell phones, etc.) <img src="/img/DLS_DrivenByScale.png" alt="NN_eg" /> Note that <span class="math inline">\(m\)</span> is used to denote the number of training examples.</p></li>
<li>Computation. Faster computation has helped increase the number of iterations through the Idea-Code-Experiment cycle as now can perform an iteration in 10 minutes / 1 day as opposed to a moth.</li>
<li><p>Algorithms (to make computation faster). Huge breakthrough: Sigmoid function -&gt; ReLU function. Gradient descent on the sigmoid function is very slow on the extreme values</p></li>
</ul>
<p>Course resources:</p>
<ul>
<li>Discussion forum; questions technical questions, etc.. Go to course home page. Otherwise at <a href="mailto:feedback@deeplearning.ai">feedback@deeplearning.ai</a></li>
</ul>
<p>Hero of Deep Learning: Geoffrey Hinton</p>
<ul>
<li>1982: Hinton, Rumelhart: seminal backpropagation algorithm, but it had been developed many times earlier.<br />
</li>
<li>Example “Mary has mother Victoria”, gave features such as nationality, generation, branch of family tree # Basics of Neural Network programming. Two views. Psychologist: Concept = bundle of features. AI: Concept = formal structuralist view<br />
</li>
<li>Most proud of work with Terry Segnowski (wake and sleep phases) vs back propagation (forward pass and backward pass) (Boltzmann machines) Restricted Boltzmann machines were ingredients of the winning entry for the Netflix competition.</li>
<li>Restricted Boltzmann machine. One layer of hidden features, can learn one layer. Then treat those features as data and do it again. And repeat. Uy Tay: Treat these as a single model; on top a restricted Boltzmann machine and below it use a Sigmoid belief net.</li>
<li>Recirculation algorithm: send information around loop so that information doesn’t change. You want to train it without having to do back propagation. Don’t want to change info. Relates to spike-timing dependent plasticity</li>
<li>Fast weights</li>
<li>Capsules</li>
</ul>
</div>
</div>
<div id="week2" class="section level1">
<h1>Neural Network Basics</h1>
<div id="logistic-regression-as-a-neural-network" class="section level2">
<h2>Logistic Regression as a Neural Network</h2>
</div>
<div id="binary-classification" class="section level2">
<h2>Binary Classification</h2>
<ul>
<li>When implement NN&lt; want to process entire training set without using an explicit for loop</li>
<li>Usually have forward propagation followed by backward propagation step</li>
<li>Will use logistic regression to convey ideas</li>
<li>Example of binary classification: y = {1 = cat; 0 = no cat}</li>
<li>Representation of image in computer
<ul>
<li>3 matrices, one each for red, green and blue pixel intensity values</li>
<li>If image is 64 pixels <span class="math inline">\(\times\)</span> 64 pixels, then would have 3 <span class="math inline">\(64 \times 64\)</span> matrices</li>
<li>In slide, the pictures are much smaller and so represented by 3 <span class="math inline">\(5\times4\)</span> matrices.</li>
</ul></li>
<li>Pixel intensity values are unrolled into one feature vector <span class="math inline">\(x\)</span>, which will have <span class="math inline">\(3 \times 64\times 64\)</span> rows, such that <span class="math inline">\(n = n_x = n\)</span> =12,288.</li>
</ul>
<p>Notation:</p>
<ul>
<li>training example: <span class="math inline">\((x,y), \; x \in \mathbb{R}^{n_x}, y \in {0,1}\)</span></li>
<li><span class="math inline">\(m = m_\text{train}\)</span> training examples: <span class="math inline">\((x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(m)}, y^{(m)})\)</span></li>
<li><span class="math inline">\(m_\text{test}\)</span> test examples</li>
<li>Data <span class="math inline">\(X = \left[\begin{array}{cccc} \mid &amp; \mid &amp; &amp; \mid \\ {\bf X}^{(1)} &amp; {\bf X}^{(2)} &amp; \cdots &amp; {\bf X}^{(m)}\\ \mid &amp; \mid &amp; &amp; \mid \\ \end{array} \right]\)</span>, where <span class="math inline">\(X\)</span> has <span class="math inline">\(n_x\)</span> rows (number of features) and <span class="math inline">\(m\)</span> columns (number of training examples), i.e. <span class="math inline">\(X \in \mathbb{R}^{n_x \times m}\)</span>. Python: <code>X.shape = (n, m)</code></li>
<li>Output (response value) is also stacked in columns, <span class="math display">\[Y = [y^{(1)}, y^{(2)}, \ldots, y^{(m)}],\]</span> i.e., <span class="math inline">\(Y \in \mathbb{R}^{1 \times m}\)</span>. Python: <code>Y.shape = (1, m)</code></li>
</ul>
<p>Refer to <em>notation guide</em> if you forget!</p>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic Regression</h2>
<ul>
<li>Given <span class="math inline">\(x\)</span>, want <span class="math inline">\(\hat{y} = P(y = 1 | x)\)</span></li>
<li>Features: <span class="math inline">\(x \in \mathbb{R}^{n_x}\)</span></li>
<li>Parameters: <span class="math inline">\(w \in \mathbb{R}^{n_x}\)</span>, <span class="math inline">\(b \in \mathbb{R}\)</span></li>
<li>How to generate <span class="math inline">\(\hat{y}\)</span>?
<ul>
<li>Linear regression: <span class="math inline">\(\hat{y} = w^Tx + b\)</span>. Not good for binary classification, as want to enforce <span class="math inline">\(0 \leq \hat{y} \leq 1\)</span>.</li>
<li>Logistic regression: <span class="math inline">\(\hat{y} = \sigma(w^Tx + b)\)</span>, where
<ul>
<li>Let <span class="math inline">\(z = w^Tx + b\)</span></li>
<li><span class="math inline">\(\sigma\)</span> is the sigmoid function, which cross the vertical axis at 0.5. <span class="math display">\[\sigma(z) = \frac{1}{1 + e^{-z}}\]</span></li>
</ul></li>
</ul></li>
</ul>
<pre class="r"><code>data.frame(z = seq(-10, 10, .001)) %&gt;%
    mutate(sigmoid = 1/(1 + exp(-z))) %&gt;%
    ggplot(aes(z, sigmoid)) + 
    geom_line()</code></pre>
<p><img src="/blog/2018-10-15-neural-networks-and-deep-learning_files/figure-html/sigmoid_function-1.png" width="576" /></p>
<p>Recall exponential function will be close to 0 for large negative values and approach infinity for large positive values.</p>
<pre class="r"><code>data.frame(x = seq(-1, 1, .01)) %&gt;%
    mutate(y = exp(x)) %&gt;%
    ggplot(aes(x, y)) + 
    geom_line() + 
    labs(y = &quot;exp(x)&quot;)</code></pre>
<p><img src="/blog/2018-10-15-neural-networks-and-deep-learning_files/figure-html/exp_function-1.png" width="576" /></p>
<p>Aim of logistic regression: Learn parameters <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> so that <span class="math inline">\(\hat{y}\)</span> is a good estimate of chance of <span class="math inline">\(y\)</span> being 1 or 0.</p>
<p>Note: In this course, treat W and B separately, where B corresponds to an intercept term.</p>
</div>
<div id="logistic-regression-cost-function" class="section level2">
<h2>Logistic Regression Cost Function</h2>
<p>First, recall log function:</p>
<pre class="r"><code>data.frame(x = seq(0,1,0.001)) %&gt;%
    mutate(y = log(x)) %&gt;%
    ggplot(aes(x,y)) + 
    geom_line() + 
    labs(y = &quot;y = log(x)&quot;)</code></pre>
<p><img src="/blog/2018-10-15-neural-networks-and-deep-learning_files/figure-html/log_function-1.png" width="576" /></p>
<p>Recall logistic regression model: <span class="math inline">\(\hat{y} = \sigma(w^Tx + b)\)</span> where <span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span></p>
<p>Given <span class="math inline">\({(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(m)}, y^{(m)})}\)</span>, want <span class="math inline">\(\hat{y}^{(i)} = y^{(i)}\)</span>.</p>
<p>Noting that <span class="math inline">\(z^{(i)} = w^Tx^{(i)} + b\)</span></p>
<p><strong>Loss (error) function</strong>:</p>
<ul>
<li>Need to define to measure how good <span class="math inline">\(\hat{y}\)</span> is when true label is <span class="math inline">\(y\)</span>.</li>
<li>Could do <span class="math inline">\(\mathcal{L}(\hat{y}, y) = \frac{1}{2}(\hat{y} - y)^2\)</span> but makes gradient descent not work well. Optimisation problem becomes non-convex and with lots of local optima.</li>
<li>Use <span class="math inline">\(\mathcal{L}(\hat{y}, y) = -\left(y \log \hat{y} + (1-y)\log(1-\hat{y})\right)\)</span></li>
</ul>
<p>if <span class="math inline">\(y=1\)</span>, <span class="math inline">\(\mathcal{L}(\hat{y}, y) = - \log \hat{y}\)</span> to make loss function small,</p>
<ul>
<li>want <span class="math inline">\(\log \hat{y}\)</span> large (because of negative), therefore</li>
<li>want <span class="math inline">\(\hat{y}\)</span> large</li>
</ul>
<pre class="r"><code>data.frame(yhat = seq(0,1,0.001)) %&gt;%
    mutate(cost = -log(yhat)) %&gt;%
    ggplot(aes(yhat,cost)) + 
    geom_line() + 
    xlab(latex2exp::TeX(&quot;$\\hat{y}$&quot;)) + 
    ylab(latex2exp::TeX(&quot;$-\\log \\;\\hat{y}$&quot;))</code></pre>
<p><img src="/blog/2018-10-15-neural-networks-and-deep-learning_files/figure-html/cost_term1-1.png" width="576" /></p>
<p>if <span class="math inline">\(y=0\)</span>, <span class="math inline">\(\mathcal{L}(\hat{y}, y) = -\log(1-\hat{y})\)</span></p>
<ul>
<li>Want <span class="math inline">\(\log (1-\hat{y})\)</span> (because of negative) large, therefore</li>
<li>Want <span class="math inline">\(\hat{y}\)</span> small</li>
</ul>
<pre class="r"><code>data.frame(yhat = seq(0,1,0.001)) %&gt;%
    mutate(cost = -log(1-yhat)) %&gt;%
    ggplot(aes(yhat,cost)) + 
    geom_line() + 
    xlab(latex2exp::TeX(&quot;$\\hat{y}$&quot;)) + 
    ylab(latex2exp::TeX(&quot;$-\\log \\;(1-\\hat{y})$&quot;))</code></pre>
<p><img src="/blog/2018-10-15-neural-networks-and-deep-learning_files/figure-html/cost_term2-1.png" width="576" /></p>
<p><strong>Cost function</strong>: <span class="math display">\[J(w,b) = \frac{1}{m}\sum^m_{i=1}\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) = -\frac{1}{m}\sum^m_{i=1}\left[y^{(i)} \log \hat{y}^{(i)} + (1-y^{(i)})\log(1-\hat{y}^{(i)})\right]\]</span></p>
<p>Loss function applied to just a single training example. Cost function is cost of parameters. In training logistic regression, want to find <span class="math inline">\(W\)</span> and <span class="math inline">\(B\)</span> to minimize overall cost function <span class="math inline">\(J\)</span>.</p>
<p>Logistic regression can be seen as a very very small neural network.</p>
</div>
<div id="gradient-descent" class="section level2">
<h2>Gradient Descent</h2>
<p>Used to learn the parameters <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> on your training algorithm.</p>
<p>Recap: <span class="math inline">\(\hat{y} = \sigma(w^Tx + b)\)</span>, <span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span></p>
<p>The cost function, <span class="math inline">\(J(w,b) = \frac{1}{m}\sum^m_{i=1}\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) = -\frac{1}{m}\sum^m_{i=1}\left[y^{(i)} \log \hat{y}^{(i)} + (1-y^{(i)})\log(1-\hat{y}^{(i)})\right]\)</span>, is convex.</p>
<p>Want to find <span class="math inline">\(w\)</span>, <span class="math inline">\(b\)</span> that minimize <span class="math inline">\(J(w,b)\)</span>.</p>
<p>For logistic regression, almost any initialisation of <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> works. Usually set to zero for LR (could do random)</p>
<p>Algorithm for just one parameter <span class="math inline">\(w\)</span>:</p>
<p>Repeat {</p>
<ul>
<li><span class="math inline">\(w := w - \alpha \frac{\partial J(w,b)}{\partial w}\)</span></li>
<li><span class="math inline">\(b := w - \alpha \frac{\partial J(w,b)}{\partial b}\)</span></li>
</ul>
<p>} where <span class="math inline">\(\alpha\)</span> is the learning rate.<br />
Recall definition of derivative = slope of function (height/width).</p>
<div class="figure">
<img src="/img/DLS_GradDesc.png" alt="Gradient Descent Algorithm" />
<p class="caption">Gradient Descent Algorithm</p>
</div>
</div>
<div id="computation-graph" class="section level2">
<h2>Computation graph</h2>
<p>Computations of NN organised in terms of</p>
<ul>
<li>Forward propagation step (compute output of NN) (left to right)</li>
<li>Back propagation step (compute gradients / derivatives) (right to left)</li>
</ul>
<p>Computation graph explains why organised this way.</p>
<p>Use computation graph when want to optimise J.</p>
<ul>
<li>Forward propagation: Compute value of J</li>
<li>Back propagation: next slides</li>
</ul>
<p><strong>Example</strong>: Find minimum of function <span class="math inline">\(J(a,b,c) = 3(a + bc)\)</span>.</p>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li>Initialisation. Let <span class="math inline">\(a = 5\)</span>, <span class="math inline">\(b = 3\)</span>, <span class="math inline">\(c = 2\)</span>.</li>
<li>Forward Propagation. Compute</li>
</ol>
<ul>
<li><span class="math inline">\(u = bc \rightarrow 6\)</span></li>
<li><span class="math inline">\(V = a + u \rightarrow 11\)</span></li>
<li><span class="math inline">\(J = 3v \rightarrow 33\)</span></li>
</ul>
<div class="figure">
<img src="/img/DLS_ComputationGraph.png" alt="Computation Graph" />
<p class="caption">Computation Graph</p>
</div>
<ol start="3" style="list-style-type: decimal">
<li>Back Propagation. Compute</li>
</ol>
<ul>
<li>Compute derivative of <span class="math inline">\(J\)</span> with respect to <span class="math inline">\(v\)</span>: <span class="math display">\[\frac{dJ}{dv} = \frac{d(3v)}{dv} = 3\]</span></li>
<li>Compute derivative of <span class="math inline">\(J\)</span> with respect to <span class="math inline">\(a\)</span>; use <strong>chain rule</strong> <span class="math inline">\(\frac{dJ}{da} = \frac{dJ}{dv}\frac{dv}{da}\)</span> such that <span class="math display">\[\begin{aligned}
\frac{dJ}{da}   &amp;= \frac{dJ}{dv}\frac{dv}{da} \\
                &amp;= \frac{d(3v)}{dv} \times \frac{d(a+u)}{da}\\
                &amp;= 3 \times 1 \\
                &amp; = 3
\end{aligned}\]</span></li>
<li>Compute derivative of <span class="math inline">\(J\)</span> with respect to <span class="math inline">\(u\)</span>; <span class="math display">\[\begin{aligned}
\frac{dJ}{du}   &amp;= \frac{dJ}{dv}\frac{dv}{du} \\
                &amp;= \frac{d(3v)}{dv} \times \frac{d(a+u)}{du}\\
                &amp;= 3 \times 1 \\
                &amp; = 3
\end{aligned}\]</span></li>
<li><p>Compute derivative of <span class="math inline">\(J\)</span> with respect to <span class="math inline">\(b\)</span>; <span class="math display">\[\begin{aligned}
\frac{dJ}{db}   &amp;= \frac{dJ}{du}\frac{du}{db} \\
                &amp;= 3 \times \frac{d(bc)}{db}\\
                &amp;= 3 \times c \\
                &amp; = 6
\end{aligned}\]</span></p></li>
<li><p>Compute derivative of <span class="math inline">\(J\)</span> with respect to <span class="math inline">\(c\)</span>; <span class="math display">\[\begin{aligned}
\frac{dJ}{dc}   &amp;= \frac{dJ}{du}\frac{du}{dc} \\
                &amp;= 3 \times \frac{d(bc)}{dc}\\
                &amp;= 3 \times b \\
                &amp; = 9
\end{aligned}\]</span></p></li>
</ul>
<p>In code use <strong>dvar</strong> for derivative of final output variable, such as <span class="math inline">\(J\)</span>, with respect to various intermediate quantities.</p>
</div>
<div id="logistic-regression-gradient-descent" class="section level2">
<h2>Logistic Regression Gradient Descent</h2>
<p>Recap: <span class="math display">\[\begin{aligned}
     z &amp;= w^Tx + b \\
     \hat{y} &amp;= a = \sigma(z)  = \frac{1}{1 + e^{-z}}\\
     \mathcal{L}(a,y) &amp;= -\left(y \log (a) + (1-y)\log(1-a)\right)
\end{aligned}\]</span></p>
<p>Consider logistic regression with two features, <span class="math inline">\(w_1, w_2\)</span>, <img src="/img/DLS_ComputationGraphLR.png" alt="Computation Graph" /> Logistic Regression Derivatives:</p>
<p><span class="math display">\[\begin{aligned} 
 \text{&quot;da&quot;} =    \frac{d\mathcal{L}(a,y)}{da} &amp;= \frac{d \left[-\left(y \log (a) + (1-y)\log(1-a)\right)\right]}{da}\\
                    &amp;= -\frac{y}{a} + \frac{1-y}{1-a} \\
   \text{&quot;dz&quot;} =   \frac{d\mathcal{L}(a,y)}{dz} &amp;= \frac{dL}{da}\frac{da}{dz} \\
                        &amp;= \frac{dL}{da} \times \left[\frac{d}{dz} \frac{1}{1 + e^{-z}}\right]\\
                        &amp;= \frac{dL}{da} \times \left[\frac{du}{dz}\frac{da}{du}\right] \;\;\text{where $u = 1 + e^{-z}$}\\
                        &amp; = \frac{dL}{da} \times \left[-e^{-z}(-u^{-2}\right]\\
                        &amp;= \frac{dL}{da} \times \left[e^{-z}(1 + e^{-z})^{-2}\right] \\
                        &amp; = \frac{dL}{da} \times \left[ \frac{e^{-z}}{(1 + e^{-z})^{-2}} \right]\\
                        &amp; =  \frac{dL}{da} \times \left[ a(1-a) \right] \\
                        &amp; =  \left[-\frac{y}{a} + \frac{1-y}{1-a}\right] \times \left[ a(1-a) \right] \\
                        &amp;= \left[-\frac{y(1-a)}{a(1-a)} + \frac{(1-y)a}{a(1-a)}\right] \times \left[ a(1-a) \right] \\
                        &amp;= -y(1-a) + (1-y)a \\
                        &amp;= -y +ya + a - ya \\
                        &amp;= a - y
\end{aligned}\]</span></p>
<p>Noting that <span class="math inline">\(a = \frac{1}{1 + e^{-z}}\)</span> and <span class="math inline">\(1-a = \frac{1 + e^{-z} - 1}{1 + e^{-z}} = \frac{e^{-z}}{1 + e^{-z}}\)</span></p>
<p>Now compute by how much you have to change <span class="math inline">\(w_1\)</span>, <span class="math inline">\(w_2\)</span> and <span class="math inline">\(b\)</span>. <span class="math display">\[\begin{aligned} 
    \text{&quot;dw1&quot;} = \frac{d\mathcal{L}(a,y)}{dw_1} &amp;= \frac{dz}{dw_1}\frac{dL}{dz} \\
                        &amp;= \frac{d(w_1x_1 + w_2x_2 + b)}{dw_1}\frac{dL}{dz} \\
                         &amp;= x_1 (a-y) \\
   \text{&quot;dw2&quot;} = \frac{d\mathcal{L}(a,y)}{dw_2} &amp;= \frac{dz}{dw_2}\frac{dL}{dz} \\
                        &amp;= \frac{d(w_1x_1 + w_2x_2 + b)}{dw_2}\frac{dL}{dz} \\
                         &amp;= x_2 (a-y) \\
   \text{&quot;db&quot;} = \frac{d\mathcal{L}(a,y)}{db} &amp;= \frac{dz}{db}\frac{dL}{dz} \\
                        &amp;= \frac{d(w_1x_1 + w_2x_2 + b)}{dw_1}\frac{dL}{dz} \\
                         &amp;=  (a-y) \\
\end{aligned}\]</span></p>
<p>The update steps are therefore: <span class="math display">\[\begin{aligned}
    w_1 := w_1 - \alpha dw_1 \\
    w_2 := w_2 - \alpha dw_2 \\
    b := b - \alpha db \\
\end{aligned}\]</span></p>
</div>
<div id="gradient-descent-on-m-examples" class="section level2">
<h2>Gradient Descent on <span class="math inline">\(m\)</span> Examples</h2>
<p>Previously, with 1 training example, used loss function. Here with <span class="math inline">\(m\)</span> training examples, use cost function</p>
<p><span class="math display">\[\begin{aligned}
J(w,b)  = \frac{1}{m} \sum^m_{i=1} \mathcal{L}(a^{(i)},y^{(i)}) \\
\rightarrow a^{(i)} = \hat{y}^{(i)} = \sigma(z^{(i)}) = \sigma(w^Tx^{(i)} + b)
\end{aligned}\]</span></p>
<p>The derivative of the cost function w.r.t. <span class="math inline">\(w_1\)</span> is: <span class="math display">\[\frac{d}{dw_1}J(w,b) = \frac{1}{m} \sum^m_{i=1} \frac{d}{dw_1}\mathcal{L}(a^{(i)},y^{(i)})\]</span></p>
<p>The derivatives <span class="math inline">\(\frac{d}{dw_1}\mathcal{L}(a^{(i)},y^{(i)})\)</span> were computed in previous section, therefore need to take average in order to obtain for cost function (as opposed to loss function)</p>
<p>To recap, one step of gradient descent (assuming just two features, i.e., <span class="math inline">\(n = 2\)</span>):</p>
<ol style="list-style-type: decimal">
<li>Initialise <span class="math inline">\(J=0\)</span>, <span class="math inline">\(dw_1 = 0\)</span>, <span class="math inline">\(dw_2 = 0\)</span>, <span class="math inline">\(db = 0\)</span></li>
<li>For <span class="math inline">\(i = 1\)</span> to <span class="math inline">\(m\)</span> (i.e., each training example)
<ul>
<li><span class="math inline">\(z^{(i)} = w^T x^{(i)} + b\)</span></li>
<li>The prediction <span class="math inline">\(a^{(i)} = \sigma(z^{(i)})\)</span></li>
<li><span class="math inline">\(J += -[y^{(i)}\log a^{(i)} + (1-y^{(i)})\log(1-a^{(i)})]\)</span></li>
<li><span class="math inline">\(dz^{(i)} = a^{(i)} - y^{(i)}\)</span></li>
<li><span class="math inline">\(dw_1 += x_1^{(i)} dz^{(i)}\)</span></li>
<li><span class="math inline">\(dw_2 += x_2^{(i)} dz^{(i)}\)</span></li>
<li><span class="math inline">\(db += dz^{(i)}\)</span></li>
</ul></li>
<li>Need to divide by m, such that
<ul>
<li><span class="math inline">\(J /=m\)</span></li>
<li><span class="math inline">\(dw_1 /= m\)</span></li>
<li><span class="math inline">\(dw_2 /= m\)</span></li>
<li><span class="math inline">\(db /= m\)</span> Noting that <span class="math inline">\(J\)</span>, <span class="math inline">\(dw_1\)</span>, <span class="math inline">\(dw_2\)</span>, <span class="math inline">\(db\)</span> are accumulators, but <span class="math inline">\($z^{(i)}\)</span> and <span class="math inline">\(a^{(i)}\)</span> are not accumulators, hence the superscript</li>
</ul></li>
<li>Update the parameters: <span class="math display">\[\begin{aligned}
w_1 := w_1 - \alpha dw_1 \\
w_2 := w_2 - \alpha dw_2 \\
b := b - \alpha db \\
\end{aligned}\]</span></li>
</ol>
<p>Weakness in calculation as implemented here: Two for-loops; (1) over training example, (2) over features (here only two, but otherwise have <span class="math inline">\(dw_1, dw_1, \ldots dw_n\)</span>)</p>
<p>In deep learning, without using explicit for-loops will help you scale. Better to use vectorisation techniques to remove for-loops (this is very important for deep learning)</p>
<p>This is one step of the gradient descent algorithm.</p>
</div>
<div id="python-and-vectorisation" class="section level2">
<h2>Python and Vectorisation</h2>
</div>
<div id="vectorisation" class="section level2">
<h2>Vectorisation</h2>
<p>Deep learning shines with large training sets (therefore very important for code to run quickly)</p>
<p>Calculation of <span class="math inline">\(z = w^T x + b\)</span>, for <span class="math inline">\(w \in \mathbb{R}^{n_x}\)</span>, <span class="math inline">\(x \in \mathbb{R}^{n_x}\)</span>:</p>
<ul>
<li>Non-vectorised. Set <span class="math inline">\(z=0\)</span> then for <span class="math inline">\(i\)</span> in range(<span class="math inline">\(n_x\)</span>), set <span class="math inline">\(z += w{i} \times x[i]\)</span>. At end of for-loop, set <span class="math inline">\(z +=b\)</span>.</li>
<li>Vectorised form of calculating in Python is as follows: <span class="math inline">\(z\)</span> = np.dot(<span class="math inline">\(w\)</span>, <span class="math inline">\(x\)</span>) + <span class="math inline">\(b\)</span>.</li>
</ul>
<p>Code:</p>
<pre class="python"><code>import numpy as np
a =  np.array([1,2,3,4])
print(a)</code></pre>
<pre><code>## [1 2 3 4]</code></pre>
<pre class="python"><code>import time
a = np.random.rand(1000000)
b = np.random.rand(1000000)

tic = time.time()
c = np.dot(a,b)
toc = time.time()

print(c)</code></pre>
<pre><code>## 249733.751553</code></pre>
<pre class="python"><code>print(&quot;Vectorised version:&quot; + str(1000*(toc-tic)) + &quot;ms&quot;)</code></pre>
<pre><code>## Vectorised version:38.537979126ms</code></pre>
<pre class="python"><code>c = 0
tic = time.time()
for i in range(1000000):
    c += a[i]*b[i]
toc = time.time()

print(c)</code></pre>
<pre><code>## 249733.751553</code></pre>
<pre class="python"><code>print(&quot;For loop:&quot; + str(1000*(toc-tic)) + &quot;ms&quot;)</code></pre>
<pre><code>## For loop:995.688915253ms</code></pre>
<p>Non-vectorised version took about 300 times faster</p>
<p>A lot of scaleable deep learning algorithms are don on a GPU (Graphics processing unit). Within Jupyter on CPU. Both GPU and CPU have parallelization instructions (SIMD = single instruction multiple data). Built-in functions such as np.function that don’t require a for loop are used, it enables Python to take better advantage of parallelism. GPU is very good at SIMD calculations, but CPU not bad either.</p>
</div>
<div id="more-vectorisation-examples" class="section level2">
<h2>More Vectorisation Examples</h2>
<p>Whenever possible avoid explicit for-loops.</p>
<p>Example: To calculate <span class="math inline">\(u = Av\)</span>, <span class="math inline">\(u_i = \sum_jA_{ij} v_j\)</span></p>
<ul>
<li><p>Non vectorised <span class="math display">\[\begin{aligned}
&amp; \text{for } i \ldots \\  
   &amp; \;\;\;\; \text{for } j \ldots \\  
   &amp; \;\;\;\; \;\;\;\;    u[i] += A[i][j] * v[j]
\end{aligned}\]</span></p></li>
<li><p>Vectorised: <span class="math inline">\(u = \text{np.dot}(A,v)\)</span></p></li>
</ul>
<p>Example: Given <span class="math inline">\(v =\left[\begin{array}{c} v_1 \\ \cdots \\ v_n \\ \end{array} \right]\)</span>, need to apply exponenetial operation on every element, such that <span class="math inline">\(u = \left[\begin{array}{c} e^{v_1} \\ \cdots \\ e^{v_n} \\ \end{array} \right]\)</span></p>
<ul>
<li><p>Non-vectorised:</p></li>
<li><p>Vectorised</p></li>
</ul>
<p>Note that the <code>numpy</code> library has many vectorised functions, including:</p>
<ul>
<li>np.log(v)</li>
<li>np.abs(v)</li>
<li>np.maximum(v, 0)</li>
<li>v**2</li>
<li>1/v</li>
</ul>
<p>Here remove the for-loop for the features. Now have just one left.</p>
<ol style="list-style-type: decimal">
<li><p>Initialise <span class="math inline">\(J=0\)</span>, <span class="math inline">\(db = 0\)</span>, <span class="math inline">\(dw = \text{np.zeros}(n_x, 1)\)</span></p></li>
<li><p>For <span class="math inline">\(i = 1\)</span> to <span class="math inline">\(m\)</span>,</p>
<ul>
<li><span class="math inline">\(z^{(i)} = w^T x^{(i)} + b\)</span></li>
<li>The prediction <span class="math inline">\(a^{(i)} = \sigma(z^{(i)})\)</span></li>
<li><span class="math inline">\(J += -[y^{(i)}\log a^{(i)} + (1-y^{(i)})\log(1-a^{(i)})]\)</span></li>
<li><span class="math inline">\(dz^{(i)} = a^{(i)} - y^{(i)}\)</span></li>
<li><code>$dw += x{(i)} dz{(i)}$</code></li>
<li><span class="math inline">\(db += dz^{(i)}\)</span></li>
</ul></li>
<li><p>Need to divide by m, such that</p>
<ul>
<li><span class="math inline">\(J /=m\)</span></li>
<li><code>$dw /= m$</code></li>
<li><span class="math inline">\(db /= m\)</span></li>
</ul></li>
</ol>
<p>Noting that <span class="math inline">\(J\)</span>, <span class="math inline">\(dw_1\)</span>, <span class="math inline">\(dw_2\)</span>, <span class="math inline">\(db\)</span> are accumulators, but <span class="math inline">\(z^{(i)}\)</span> and <span class="math inline">\(a^{(i)}\)</span> are not accumulators, hence the superscript</p>
<ol start="4" style="list-style-type: decimal">
<li>Update the parameters: <span class="math display">\[\begin{aligned}
w_1 := w_1 - \alpha dw_1 \\
w_2 := w_2 - \alpha dw_2 \\
b := b - \alpha db \\
\end{aligned}\]</span></li>
</ol>
</div>
<div id="vectorising-logistic-regression-predictions" class="section level2">
<h2>Vectorising Logistic Regression Predictions</h2>
<p>Vectorisation to compute predictions, <span class="math inline">\(a\)</span>:</p>
<p>Forward Propagation:</p>
<p><span class="math display">\[\begin{aligned}
z^{(1)} = w^Tx^{(1)} + b    &amp;&amp; z^{(2)} = w^Tx^{(2)} + b &amp;&amp;&amp;  z^{(3)} = w^Tx^{(3)} + b \\ 
a^{(1)} = \sigma(z^{(1)})   &amp;&amp; a^{(2)} = \sigma(z^{(2)}) &amp;&amp;&amp; a^{(3)} = \sigma(z^{(3)}) 
\end{aligned}\]</span></p>
<p>Let</p>
<ul>
<li><span class="math inline">\(Z = [z^{(1)} z^{(2)} \ldots z^{(m)}\)</span>, where <span class="math inline">\(z \in \mathbb{R}^{1 \times m}\)</span></li>
<li>Training inputs: <span class="math inline">\(X = \left[\begin{array}{cccc} \mid &amp; \mid &amp; &amp; \mid \\ {\bf X}^{(1)} &amp; {\bf X}^{(2)} &amp; \cdots &amp; {\bf X}^{(m)}\\ \mid &amp; \mid &amp; &amp; \mid \\ \end{array} \right]\)</span>, where <span class="math inline">\(X\)</span> has <span class="math inline">\(n_x\)</span> rows (number of features) and <span class="math inline">\(m\)</span> columns (number of training examples), i.e. <span class="math inline">\(X \in \mathbb{R}^{n_x \times m}\)</span></li>
<li><span class="math inline">\(\mathbf{b} = [b b \ldots b]\)</span>, where <span class="math inline">\(b \in \mathbb{R}^{1 \times m}\)</span></li>
</ul>
<p>So that <span class="math inline">\(Z = w^T \mathbf{X} + \mathbf{b}\)</span>, achieved in python using <code>z = np.dot(w.T, x) + b</code>, but in python <span class="math inline">\(b\)</span> is a raw number, <span class="math inline">\(1 \times 1\)</span>. This operation is called <strong>broadcasting</strong></p>
<p>Similarly, let:</p>
<ul>
<li><span class="math inline">\(A = [a^{(1)} a^{(2)} \ldots a^{(m)}]\)</span> = <span class="math inline">\(\sigma(Z)\)</span></li>
</ul>
</div>
<div id="vectorising-logistic-regression-output" class="section level2">
<h2>Vectorising Logistic Regression Output</h2>
<p>How to use vectorisation to perform gradient computations for all <span class="math inline">\(M\)</span> training samples:</p>
<p>Recall for gradient computation, calculated</p>
<p><span class="math display">\[
dz^{(1)} = a^{(1)} - y ^{(1)}\\
dz^{(2)} = a^{(2)} - y ^{(2)}\\
\vdots
\]</span></p>
<p>If</p>
<ul>
<li><span class="math inline">\(dZ = [dz^{(1)} d^{(2)} \ldots d^{(m)}]\)</span></li>
<li><span class="math inline">\(A = [a^{(1)} a^{(2)} \ldots a^{(m)}]\)</span></li>
<li><span class="math inline">\(Y = [y^{(1)} y^{(2)} \ldots y^{(m)}]\)</span></li>
</ul>
<p>Then <span class="math inline">\(dZ = A - Y\)</span></p>
<p>In previous implementation still had for loop for <span class="math inline">\(dw\)</span> and <span class="math inline">\(db\)</span> over the training examples, i.e. for the <span class="math inline">\(dw\)</span> <span class="math display">\[
dw = 0 \\
dw += x^{(1)} dz^{(1)}\\
dw += x^{(2)} dz^{(2)}\\
\vdots\\
dw/=m
\]</span></p>
<p>and for the <span class="math inline">\(db\)</span>: <span class="math display">\[
db = 0 \\
db +=  dz^{(1)}\\
db += dz^{(2)}\\
\vdots\\
db/=m
\]</span></p>
Vectorised version: <span class="math display">\[db = \frac{1}{m}  \sum^m_{i=1}dz^{(i)} = \frac{1}{m}  \text{np.sum}(dz)\]</span> and $$
<span class="math display">\[\begin{aligned}
dw &amp;= \frac{1}{m} \mathbf{X} dz^T  \\
    &amp;= \frac{1}{m} \left[\begin{array}{cccc} \mid &amp; \mid &amp;   &amp; \mid\\
    {\bf X}^{(1)} &amp; {\bf X}^{(2)}   &amp; \cdots    &amp; {\bf X}^{(m)} \\
    \mid        &amp; \mid          &amp;           &amp; \mid
    \end{array}\right]
    
    \left[\begin{array}{c}
    dz^{(1)}\\
    dz^{(2)}\\
    \vdots\\
    dz^{(m)}\\
\end{array} \right]\\
&amp;= \frac{1}{m}[x^{(1)}dz^{(1)} + \ldots + x^{(m)}dz^{(m)}]
  
 \end{aligned}\]</span>
<p>$$</p>
<p>Implementing Vectorised Logistic Regression 1. Initialise <span class="math inline">\(J=0\)</span>, <span class="math inline">\(db = 0\)</span>, <span class="math inline">\(dw = \text{np.zeros}(n_x, 1)\)</span></p>
<p>2 Forward &amp; back Propagation on all <span class="math inline">\(m\)</span> training examples a. <span class="math inline">\(Z = w^T \mathbf{X} + \mathbf{b}\)</span>, achieved in python using <code>z = np.dot(w.T, x) + b</code> b. <span class="math inline">\(A = \sigma(z)\)</span> c. <span class="math inline">\(dZ = A - Y\)</span> d. <span class="math inline">\(dw = \frac{1}{m} X dZ^T\)</span> e. <span class="math inline">\(db = \frac{1}{m} \text{np.sum}(dz)\)</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Gradient descent update w := w - dw b := b - db</li>
</ol>
<p>If want multiple iterations of gradient descent, then will still need for-loop</p>
</div>
<div id="broadcasting-in-python" class="section level2">
<h2>Broadcasting in Python</h2>
<p>Technique that python and numpy allows you to use to make code more efficient</p>
<p>Broadcasting example. Aim to calculate percentage of calories from carbs, proteins and fats, i.e. % in carbs in apples is 56 / (56 + 1.2 + 1.8). Can this be done without an explicit for-loop?</p>
<div class="figure">
<img src="/img/DLS_broadcastingEg.png" alt="Broadcasting Eg" />
<p class="caption">Broadcasting Eg</p>
</div>
<pre class="python"><code>import numpy as np
A = np.array([  [56.0, 0.0, 4.4, 68.0],
                [1.2, 104.0, 52.0, 8.0],
                [1.8, 135.0, 99.0, 0.9]])
print(A)</code></pre>
<pre><code>## [[  56.     0.     4.4   68. ]
##  [   1.2  104.    52.     8. ]
##  [   1.8  135.    99.     0.9]]</code></pre>
<pre class="python"><code>cal = A.sum(axis = 0) #to sum vertically
print(cal)</code></pre>
<pre><code>## [  59.   239.   155.4   76.9]</code></pre>
<pre class="python"><code>percentage = 100 * A/cal.reshape(1,4) # example of python broadcasting.  Dividing 3x4 by 1x4 matrix.
print(percentage)</code></pre>
<pre><code>## [[ 94.91525424   0.           2.83140283  88.42652796]
##  [  2.03389831  43.51464435  33.46203346  10.40312094]
##  [  3.05084746  56.48535565  63.70656371   1.17035111]]</code></pre>
<p>Broadcasting allows you to divide a <span class="math inline">\(3 \times 4\)</span> matrix by a <span class="math inline">\(1 \times 4\)</span> matrix. Python will auto-expand vectors / matrices to allow calculations of vectors and matrices.</p>
<p><strong>Examples</strong> <img src="/img/DLS_broadcastingEg2.png" alt="Broadcasting Eg" /></p>
<p>For more information, see <code>numpy</code> documentation. Equivalent to <code>bsxfun</code> in Matlab and Octave.</p>
</div>
<div id="a-note-on-pythonnumpy-vectors" class="section level2">
<h2>A note on python/numpy vectors</h2>
<p>Flexibility + broadcasting = strength and weakness</p>
<ul>
<li>Strength: can get a lot of done with just one line of code</li>
<li>Weakness: with flexibility can add subtle bugs. E.g. Column + row vector may give matrix, rather than perhaps wanted a column or row vector</li>
</ul>
<pre class="python"><code>import numpy as np
a = np.random.randn(5) # 5 random gaussian
print(a)</code></pre>
<pre><code>## [ 1.65771815 -0.10518511  1.7276875  -0.0872782   0.71822384]</code></pre>
<pre class="python"><code>print(a.shape) # rank 1 array, neither row nor column vector</code></pre>
<pre><code>## (5,)</code></pre>
<pre class="python"><code>print(a.T)     # looks the same as a</code></pre>
<pre><code>## [ 1.65771815 -0.10518511  1.7276875  -0.0872782   0.71822384]</code></pre>
<pre class="python"><code>print(np.dot(a, a.T)) #would expect matrix, instead get a number

# recommends not using data structures of rank 1 array.
# instead commit to making it a row or column vector</code></pre>
<pre><code>## 6.26746046825</code></pre>
<pre class="python"><code>a = np.random.randn(5,1)
print(a)</code></pre>
<pre><code>## [[-0.51856197]
##  [ 1.03962404]
##  [-2.18089677]
##  [ 0.67501724]
##  [ 0.09221786]]</code></pre>
<pre class="python"><code>print(a.T)</code></pre>
<pre><code>## [[-0.51856197  1.03962404 -2.18089677  0.67501724  0.09221786]]</code></pre>
<pre class="python"><code>print(np.dot(a, a.T))</code></pre>
<pre><code>## [[ 0.26890652 -0.5391095   1.13093013 -0.35003827 -0.04782068]
##  [-0.5391095   1.08081815 -2.26731272  0.70176415  0.09587191]
##  [ 1.13093013 -2.26731272  4.75631074 -1.47214292 -0.20111764]
##  [-0.35003827  0.70176415 -1.47214292  0.45564827  0.06224865]
##  [-0.04782068  0.09587191 -0.20111764  0.06224865  0.00850413]]</code></pre>
<p>Recommendations:</p>
<ul>
<li>Don’t use <em>rank 1 arrays</em>; instead use row or column vectors. If do end up with a rank 1 array can use <code>a = a.reshape((5,1))</code> to reshape<br />
</li>
<li>Use <code>assert(a.shape == (5,1))</code> type statements to ensure shape</li>
</ul>
</div>
<div id="quick-tour-of-jupyteripython-notebooks" class="section level2">
<h2>Quick tour of Jupyter/iPython Notebooks</h2>
<ul>
<li>Shift + Enter to run (but could be different)</li>
<li>Cell &gt; Run Cells</li>
<li>Markdown cell</li>
<li>Code is running on a kernel; with excessively large job, the kernel might die. Just click Kernel &gt; Restart</li>
</ul>
</div>
<div id="explanation-of-logistic-regression-cost-function" class="section level2">
<h2>Explanation of logistic regression cost function</h2>
</div>
<div id="hero-of-deep-learning-pieter-abbeel" class="section level2">
<h2>Hero of Deep Learning: Pieter Abbeel</h2>
<p>Deep reinforcement learning. Many more questions than in supervised learning. Where does the data come from? What actions you took early got you the reward later? Safety issues. Still need to solve longer horizon problems. Wants to run the reinforcement learning in the inner loop to try to improve it. See also Andrej Karpathy’s deep learning course which has videos online. Berkeley has a deep reinforcement learning course which has all of the lectures online.</p>
<p>Ensure to try things out with frameworks like TensorFlow, Chainer, Theano, PyTorch etc.; it’s very easy to get going and get something up and running very quickly. To get to practice yourself, right?</p>
<p>Discusses article in <a href="https://mashable.com/2017/07/28/16-year-old-ai-genius/#0Htj4Vt2IiqO">Mashable</a> about a 16-year-old who is one of the leaders on Kaggle competitions. And it just said, he just went out and learned things, found things online, learned everything himself and never actually took any formal course per se.</p>
<p>At present, deep reinforcement learning always starts from scratch, would be better if it could learn from the past.</p>
</div>
</div>
<div id="week3" class="section level1">
<h1>Shallow Neural Network</h1>
<div id="neural-networks-overview" class="section level2">
<h2>Neural Networks Overview</h2>
<ul>
<li>Logistic regression had one hidden layer; which calculates both <span class="math inline">\(z^{[1]}\)</span> and <span class="math inline">\(a^{[1]} = \sigma(z^{[1]})\)</span></li>
<li>Neural networks can consider of more than one hidden layer</li>
<li>Square brackets [] used to reference hidden layer</li>
<li>Round brackets () used to reference training example</li>
</ul>
<div class="figure">
<img src="/img/ShallowNN_ComputingOutput1.JPG" alt="NN Representation" />
<p class="caption">NN Representation</p>
</div>
</div>
<div id="neural-networks-representation" class="section level2">
<h2>Neural Networks Representation</h2>
<p>Example: Single hidden layer consists (<span class="math inline">\(\equiv\)</span> 2 layer neural network)</p>
<ul>
<li>Input layer, <span class="math inline">\(a^{[0]} = \mathbf{X} = {x_1, x_2, x_3}\)</span> within training set. Use <span class="math inline">\(a\)</span> for activation. Called layer 0 as not counted as not counted as an “official layer”.</li>
<li>Hidden layer(s) generates activations. The first hidden layer, for example, with 4 nodes, is denoted <span class="math inline">\(a^{[1]} = \{a_1^{[1]}, a_2^{[1]}, a_3^{[1]}, a_{4}^{[1]}\}\)</span>. Called <strong>hidden</strong> as don’t see true values in training set. First hidden layer will have associated parameters, for example when <span class="math inline">\(X \in \mathbb{R}^{1 \times 3}\)</span> and <span class="math inline">\(a^{[1]} \in \mathbb{R}^{4 \times 1}\)</span> then will have parameters
<ul>
<li><span class="math inline">\(w^{[1]} \in \mathbb{R}^{4 \times 3}\)</span></li>
<li><span class="math inline">\(b^{[1]} \in \mathbb{R}^{4 \times 1}\)</span></li>
</ul></li>
<li>Output layer (<span class="math inline">\(y\)</span> within training set). With one hidden layer, this is referred to as a the second layer and have <span class="math inline">\(\hat{y} = a^{[2]}\)</span> In this example of the neural network with one hidden layer of 4 units, the parameters associated with the output layer are:
<ul>
<li><span class="math inline">\(w^{[2]} \in \mathbb{R}^{1 \times 4}\)</span></li>
<li><span class="math inline">\(b^{[2]} \in \mathbb{R}^{1 \times 1}\)</span></li>
</ul></li>
</ul>
</div>
<div id="computing-a-neural-networks-output" class="section level2">
<h2>Computing a Neural Network’s Output</h2>
<ul>
<li>Use matrix multiplication to calculate nodes each activation layer to avoid use of for loops</li>
</ul>
<div class="figure">
<img src="/img/ShallowNN_ComputingOutput2.JPG" alt="NN Output Computation" />
<p class="caption">NN Output Computation</p>
</div>
<ul>
<li>For a 2 layer neural network will need just 4 equations:</li>
</ul>
<div class="figure">
<img src="/img/ShallowNN_ComputingOutput3.JPG" alt="NN Output Computation" />
<p class="caption">NN Output Computation</p>
</div>
</div>
<div id="vectorizing-across-multiple-examples" class="section level2">
<h2>Vectorizing across multiple examples</h2>
<ul>
<li>Last example, for one single training example</li>
<li>This example, for <span class="math inline">\(m\)</span> training examples</li>
<li>For each training example, <span class="math inline">\(i = 1 \ldots m\)</span>
<ul>
<li><span class="math inline">\(z^{[1](i)} = w^{[1]}x^{(i)} + b^{[1]}\)</span></li>
<li><span class="math inline">\(a^{[1]} = \sigma(z^{[1](i)})\)</span></li>
<li><span class="math inline">\(z^{[2](i)} = w^{[2]}a^{[1](i)} + b^{[2]}\)</span></li>
<li><span class="math inline">\(a^{[2]} = \sigma(z^{[2]})\)</span></li>
</ul></li>
<li>But want to get rid of for loop and so stack <span class="math inline">\(x\)</span>,<span class="math inline">\(z\)</span> and <span class="math inline">\(a\)</span> vertically (in columns) such that
<ul>
<li><span class="math inline">\(Z^{[1]} = W^{[1]}X + b^{[1]}\)</span></li>
<li><span class="math inline">\(A^{[1]} = \sigma(Z^{[1]})\)</span></li>
<li><span class="math inline">\(Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}\)</span></li>
<li><span class="math inline">\(A^{[2]} = \sigma(Z^{[2]})\)</span></li>
</ul></li>
<li><span class="math inline">\(X \in \mathbb{R}^{(n_x, m)}\)</span></li>
<li>Horizontal index = index to training example</li>
<li>Vertical index = index to node in neural network</li>
</ul>
</div>
<div id="explanation-for-vectorized-implementation" class="section level2">
<h2>Explanation for Vectorized Implementation</h2>
<ul>
<li>First training example, compute: <span class="math inline">\(z^{[1](1)} = w^{[1]}x^{(1)} + b^{[1]}\)</span></li>
<li>Second training example, compute: <span class="math inline">\(z^{[1](2)} = w^{[1]}x^{(2)} + b^{[1]}\)</span></li>
<li>Third training example, compute: <span class="math inline">\(z^{[1](3)} = w^{[1]}x^{(3)} + b^{[1]}\)</span></li>
</ul>
<p>Now ignore <span class="math inline">\(b\)</span> for now (i.e. assume 0), such that:</p>
<ul>
<li>First training example, compute: <span class="math inline">\(z^{[1](1)} = w^{[1]}x^{(1)}\)</span></li>
<li>Second training example, compute: <span class="math inline">\(z^{[1](2)} = w^{[1]}x^{(2)}\)</span></li>
<li>Third training example, compute: <span class="math inline">\(z^{[1](3)} = w^{[1]}x^{(3)}\)</span></li>
</ul>
<div class="figure">
<img src="/img/ShallowNN_Vectorized_Implementation.JPG" alt="NN Output Computation" />
<p class="caption">NN Output Computation</p>
</div>
<p>Note that <span class="math inline">\(W\)</span> is a matrix</p>
<ul>
<li><span class="math inline">\(w^{[1]} \in \mathbb{R}^{4 \times 3}\)</span></li>
<li><span class="math inline">\(w^{[2]} \in \mathbb{R}^{1 \times 4}\)</span></li>
</ul>
<p>Makes use of Python broadcasting.</p>
</div>
<div id="activation-functions" class="section level2">
<h2>Activation Functions</h2>
<p>So far been using sigmoid function as activation function <span class="math display">\[a = \frac{1}{1 + e^{-z}}\]</span>, but this is not the best choice; there are other options available.</p>
<p>In general case, replace <span class="math inline">\(\sigma(z^{[k]})\)</span> by <span class="math inline">\(g(z^{[1]})\)</span>.</p>
<p>Other options:</p>
<div id="hyperbolic-tangent-function-a-tanhz-fracez---e-zez-e-z" class="section level5">
<h5>1. Hyperbolic Tangent function <span class="math display">\[a = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\]</span></h5>
<p>It is a shifted sigmoid function that goes from -1 to 1 instead of from 0 to 1. Crosses origin. Almost always works better. As means closer to zero (similar to centring data)</p>
<p>Pretty much never uses sigmoid function; with <em>exception</em> for output layer in binary classification. as output <span class="math inline">\(y \in {0,1}\)</span>, i.e. <span class="math inline">\(0 \leq \hat{y} \leq 1\)</span>. So would use tanh function in hidden layer and sigmoid function activation in output layer.</p>
<p>Downside: if <span class="math inline">\(z\)</span> is very small or very large then derivative becomes very small and so can slow down gradient descent (similar to sigmoid function)</p>
</div>
<div id="relu-rectified-linear-unit-function-a-max0-z" class="section level5">
<h5>2. ReLU (rectified linear unit) function <span class="math display">\[a = \max(0, z)\]</span></h5>
<pre><code>* If $z &gt; 0$ then derivative is positive
* If $z &lt; 0$ then derivative is 0
* If $z = 0$, then derivative not defined, but odds very small.  In practice, can pretend that either 0 or 1.</code></pre>
<p>This is becoming increasingly the default choice for activation function.</p>
<p>Advantage: Derivative is different from zero when <span class="math inline">\(z &gt; 0\)</span> and so does not slow down loading</p>
<p>Disadvantage: Derivative is zero when <span class="math inline">\(z &lt; 0\)</span></p>
</div>
<div id="leaky-relu-a-max0.01z-z" class="section level5">
<h5>3. Leaky ReLU <span class="math display">\[a = \max(0.01z, z)\]</span></h5>
<ul>
<li>Usually works better but not used as much in practice Andrew Ng usually uses</li>
<li>Can use a training parameter instead of 0.01</li>
</ul>
</div>
</div>
<div id="why-you-need-non-linear-activation-functions" class="section level2">
<h2>Why You Need Non-Linear Activation Functions</h2>
<p>If you use a linear activation function (identity), then all you calculate is <span class="math display">\[\begin{aligned}
    a^{[2]}  = z^{[2]} &amp; = w^{[2]}a^{[1]}x + b^{[2]} \\
                        &amp; = w^{[2]}(w^{[1]}a^{[0]} + b^{[1]})x + b^{[2]} \\
                        &amp; = (w^{[2]}w^{[1]})x + (w^{[2]} b^{[1]} + b^{[2]} ) \\ 
                        &amp;= w^\prime x + b^\prime
\end{aligned}\]</span></p>
<p>So no matter how many layers, will just be computing a linear activation function, so may as well not have any hidden layers.</p>
<p>Linear hidden layer useless. <strong>Exception</strong> for output unit only of regression problem, i.e. when <span class="math inline">\(y \in \mathbb{R}\)</span>. But even then, if <span class="math inline">\(y &gt; 0\)</span> then probably better to use a ReLU function.</p>
</div>
<div id="derivatives-of-activation-functions" class="section level2">
<h2>Derivatives of Activation Functions</h2>
<p>Required for back-propagation of gradient descent.</p>
<p>Sigmoid activation function: <span class="math display">\[\begin{aligned}
    g(z) &amp;= \frac{1}{1 + e^{-z}} \\
    \frac{d}{dz}g(z) &amp;= \frac{1}{1 + e^{-z}}(1 - \frac{1}{1 + e^{-z}}) \\
    &amp;= g(z)\left(1 - g(z)\right) \\
    &amp;= a\left(1 - a\right)
\end{aligned}\]</span></p>
<p>Values: * If <span class="math inline">\(z = 10, g(z) \approx 1. g^\prime(z) \approx 1(1-1) \approx 0\)</span> * If <span class="math inline">\(z = -10, g(z) \approx 0. g^\prime(z) \approx 0(1-0) \approx 0\)</span> * If <span class="math inline">\(z = 0, g(z) = 0.5. g^\prime(z) = 0.5(1-0.5) = 0.25\)</span></p>
<p>Tanh activation function <span class="math display">\[\begin{aligned}
    g(z) &amp;= \tanh(z) \\
    \frac{d}{dz}g(z) &amp;= \frac{e^z - e^{-z}}{e^z + e^{-z}}\\
    &amp;= 1 - \left(\tanh(z)\right)^2 \\
    &amp;= 1 - a^2
\end{aligned}\]</span></p>
<p>Values:</p>
<ul>
<li>If <span class="math inline">\(z = 10, g(z) \approx 1. g^\prime(z) \approx 1-1^2 \approx 0\)</span></li>
<li>If <span class="math inline">\(z = -10, g(z) \approx -1. g^\prime(z) \approx 1-(-1)^2 \approx 0\)</span></li>
<li>If <span class="math inline">\(z = 0, g(z) = 0. g^\prime(z) = 1-0 = 1\)</span></li>
</ul>
<p>ReLU <span class="math display">\[\begin{aligned}
    g(z) &amp;= \max(0, z) \\
    g^\prime(z) &amp;= \begin{cases} 0 &amp;\text{if $z &lt; 0$}\\
                                1 &amp; \text{if $z &gt; 0$}\end{cases}
\end{aligned}\]</span> Gradient not technically defined at <span class="math inline">\(z = 0\)</span> but can set to 1.</p>
<p>Leaky ReLU <span class="math display">\[\begin{aligned}
    g(z) &amp;= \max(0.01z, z) \\
    g^\prime(z) &amp;= \begin{cases} 0.01 &amp;\text{if $z &lt; 0$}\\
                                1 &amp; \text{if $z \geq 0$}\end{cases}
\end{aligned}\]</span> Gradient not technically defined at <span class="math inline">\(z = 0\)</span> but can set to 1.</p>
</div>
<div id="gradient-descent-for-neural-networks" class="section level2">
<h2>Gradient descent for Neural Networks</h2>
<p>Parameters for single hidden layer:</p>
<ul>
<li><span class="math inline">\(w^{[1]} \in \mathbb{R}^{(n^{[1]},n^{[0]})}\)</span></li>
<li><span class="math inline">\(b^{[1]} \in \mathbb{R}^{(n^{[1]},1)}\)</span></li>
<li><span class="math inline">\(w^{[2]} \in \mathbb{R}^{((n^{[2]},n^{[1]}))}\)</span></li>
<li><span class="math inline">\(b^{[2]} \in \mathbb{R}^{(n^{[2]}, 1)}\)</span></li>
</ul>
<p>where</p>
<ul>
<li><span class="math inline">\(n_x = n^{[0]}\)</span> is the number of input parameters</li>
<li><span class="math inline">\(n^{[1]}\)</span> is the number of units in the single hidden layer</li>
<li><span class="math inline">\(n^{[2]} = 1\)</span> is the output unit</li>
</ul>
<p>Assuming binary classification, loss function is<span class="math display">\[J(w^{[1]}, b^{[1]}, w^{[2]}, b^{[2]}) = \frac{1}{m}\sum^n_{i=1}\mathcal{L}(\hat{y}, y)\]</span></p>
<p>To train parameters, need gradient descent. After initialising parameters <em>randomly</em> (important not to set all to zero),</p>
<p>Repeat {</p>
<ul>
<li>Compute prediction (<span class="math inline">\(\hat{y}^{(i)}, i = 1, \ldots, m\)</span>)</li>
<li>Compute derivative <span class="math inline">\(dW^{[1]} = \frac{dJ}{dw^{[1]}}\)</span>, <span class="math inline">\(db^{[1]} = \frac{dJ}{db^{[1]}}\)</span>, <span class="math inline">\(\ldots\)</span></li>
<li>Update phase</li>
</ul>
<p><span class="math display">\[\begin{aligned}
        w^{[1]} &amp;:= W^{[1]} - \alpha dW^{[1]}\\
        b^{[1]} &amp;:= b^{[1]} - \alpha db^{[1]}\\
        \vdots\\
\end{aligned}\]</span><br />
}</p>
<p>Formulas for computing partial derivatives.</p>
<p>Forward propagation: <span class="math display">\[\begin{aligned}
    Z^{[1]} &amp;= W^{[1]}X + b^{[1]}\\
    A^{[1]} &amp;= g^{[1]}(Z^{[1]})\\
    Z^{[2]} &amp;= W^{[2]}A^{[1]} + b^{[2]}\\
    A^{[2]} &amp;= g^{[2]}(Z^{[2]})
\end{aligned}\]</span></p>
<p>Back propagation <span class="math display">\[\begin{aligned}
    dZ^{[2]} &amp;= A^{[2]} - Y \\
    dW^{[2]} &amp;= \frac{1}{m} dZ^{[2]}A^{[1]T}\\
    db^{[2]} &amp;= \frac{1}{m}\text{np.sum($dZ^{[2]}$, axis = 1, keepdens = True}\\
    dZ^{[1]} &amp;=  W^{[2]T}dZ^{[2]} *  g^{[1]\prime}(Z^{[1]})
\end{aligned}\]</span></p>
</div>
<div id="backpropagation-intuition" class="section level2">
<h2>Backpropagation Intuition</h2>
<div class="figure">
<img src="/img/DLS_ComputationGraphLR.png" alt="Computation Graph" />
<p class="caption">Computation Graph</p>
</div>
<p>Logistic Regression <span class="math display">\[\begin{aligned} 
 \text{da} =    \frac{d\mathcal{L}(a,y)}{da} &amp;= \frac{d \left[-\left(y \log (a) + (1-y)\log(1-a)\right)\right]}{da}\\
                    &amp;= -\frac{y}{a} + \frac{1-y}{1-a} \\
   \text{dz} =   \frac{d\mathcal{L}(a,y)}{dz} &amp;= \frac{d\mathcal{L}}{da}\frac{da}{dz} \\
                    &amp;= \text{da} \frac{d}{dz}g(z) \\
                    &amp;= \text{da} \; g^\prime(z)\\
 \text{dw} = \frac{d\mathcal{L}(a,y)}{dw} 
                        &amp;=\frac{d\mathcal{L}(a,y)}{dz} \frac{dz}{dw}\\
                        &amp;= \text{dz} \; \frac{d}{dw}(w*x + b)\\
                        &amp;= \text{dz} \;  x\\
            \text{db}  \frac{d\mathcal{L}(a,y)}{db} &amp;= \frac{d\mathcal{L}(a,y)}{dz} \frac{dz}{db}\\
            &amp;= \text{dz} \frac{d}{db}(w*x + b) \\
            &amp;= \text{dz}
\end{aligned}\]</span></p>
Neural Network
<span class="math display">\[\begin{aligned} 
    \text{dz}^{[2]} &amp; =   a^{[2]} - y\\
    \text{dw}^{[2]} &amp;=\frac{d\mathcal{L}}{dz^{[2]}} \frac{dz^{[2]}}{dw^{[2]}}\\
                    &amp;= \text{dz}^{[2]} \; \frac{d}{dw^{[2]}}(W^{[2]}*a^{[1]} + b^{[2]})\\
                    &amp;= \text{dz}^{[2]} \;  a^{[1]}\\
    \text{db}^{[2]} &amp;= \frac{d\mathcal{L}}{dz^{[2]}} \frac{dz^{[2]}}{db^{[2]}}\\
                    &amp;= \text{dz}^{[2]} \frac{d}{db^{[2]}}(W^{[2]}*x + b^{[2]}) \\
                    &amp;= \text{dz}^{[2]} \\
            
    \text{dz}^{[1]}  &amp;= \frac{d\mathcal{L}}{dz^{[2]}}\frac{dz^{[2]}}{dz^{[1]}} \\
                    &amp;= \text{dz}^{[2]} \; \frac{d}{dz^{[1]}}\left(W^{[2]}a^{[1]} + b^{[2]}\right)\\
                    &amp;= \text{dz}^{[2]} \; \frac{d}{dz^{[1]}}\left(W^{[2]}g^{[1]}(z^{[1]}) + b^{[2]}\right)\\
                    &amp;= \text{dz}^{[2]} \; \left(W^{[2]}g^{[1]\prime}(z^{[1]})\right)\\
                    &amp;= w^{[2]T} dz^{[2]} * g^{[1]\prime}(z^{[1]})\\
    \text{dw}^{[1]} &amp;=\frac{d\mathcal{L}}{dz^{[1]}} \frac{dz^{[1]}}{dw^{[1]}}\\
                    &amp;= \text{dz}^{[1]} \; \frac{d}{dw^{[1]}}(w^{[1]}*x + b^{[1]})\\
                    &amp;= \text{dz}^{[1]} \;  x^T\\
    \text{db}^{[1]}  &amp;= \frac{d\mathcal{L}}{dz^{[1]}} \frac{dz^{[1]}}{db^{[1]}}\\
                    &amp;= \text{dz}^{[1]} \frac{d}{db^{[1]}}(w^{[1]}*x + b^{[1]}) \\
            &amp;= \text{dz}^{[1]}
\end{aligned}\]</span>
<p><span class="math display">\[\begin{aligned}
    dZ^{[2]} &amp;= A^{[2]} - Y \\
    dW^{[2]} &amp;= \frac{1}{m} dZ^{[2]}A^{[1]T}\\
    db^{[2]} &amp;= \frac{1}{m}\text{np.sum($dZ^{[2]}$, axis = 1, keepdens = True}\\
    dZ^{[1]} &amp;=  W^{[2]T}dZ^{[2]} *  g^{[1]\prime}(Z^{[1]})\\
    dW^{[1]} &amp;= \frac{1}{m} dZ^{[1]}X^{T}\\
    db^{[1]} &amp;= \frac{1}{m}\text{np.sum($dZ^{[1]}$, axis = 1, keepdens = True}
\end{aligned}\]</span></p>
<p>See alternative explanations of backpropagation refer to:</p>
<ul>
<li><a href="https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/" class="uri">https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/</a></li>
<li><a href="http://colah.github.io/posts/2015-08-Backprop/" class="uri">http://colah.github.io/posts/2015-08-Backprop/</a></li>
</ul>
</div>
<div id="random-initialisation" class="section level2">
<h2>Random Initialisation</h2>
<ul>
<li>Logistic regression can initialise weights to zero</li>
<li>Neural network, if initialise weights to zero, gradient descent will not work. Initialising bias (<span class="math inline">\(b\)</span>) to zero is okay.</li>
<li>If weights and bias are zero, then
<ul>
<li><span class="math inline">\(a_1^{[1]} = a_2^{[1]}\)</span></li>
<li><span class="math inline">\(dz_1^{[1]} = dz_2^{[1]}\)</span></li>
<li>The first row will always equal the second row. This is called the symmetry breaking problem.</li>
<li>Then may as well just have one unit</li>
<li>Proof can be performed via induction</li>
</ul></li>
<li>Solution, e.g. for 1-layer network with 2 units in hidden layer:
<ul>
<li>Set <span class="math inline">\(w^{[1]}\)</span> = np.random.randn((2,2)) * 0.01</li>
<li>Set <span class="math inline">\(b^{[1]}\)</span> = np.zero((2,1))</li>
<li>Set <span class="math inline">\(w^{[1]}\)</span> = np.random.randn((1,2)) * 0.01</li>
<li>Set <span class="math inline">\(b^{[2]}\)</span> = 0</li>
</ul></li>
</ul>
<p>Use 0.01 as want to avoid large values of z (which will happen if have large w) <span class="math display">\[\begin{aligned}
    z^{[1]} &amp;= W^{[1]}x + b^{[1]}\\
    a^{[1]} &amp;= g^{[1]}(z^{[1]})
\end{aligned}\]</span><br />
Less of issue if not using sigmoid or tanh activation functions</p>
<p>For deep neural networks may be a better option than 0.01.</p>
</div>
<div id="hero-of-deep-learning-ian-goodfellow" class="section level2">
<h2>Hero of Deep Learning: Ian Goodfellow</h2>
<ul>
<li>Wrote first textbook on modern version of deep learning</li>
<li>GAN = generative algorithm</li>
</ul>
</div>
</div>
<div id="week4" class="section level1">
<h1>Deep Neural Network</h1>
<div id="deep-l-layer-neural-network" class="section level2">
<h2>Deep L-layer neural network</h2>
<ul>
<li>To assess quality of class probabilities</li>
<li>Shows observed probability vs predicted class probability</li>
<li>Method:
<ul>
<li>Obtain scores for test set using classification model</li>
<li>Bin data into, e.g. 10, groups based on class probabilities</li>
<li>For each bin, determine observed event rate</li>
<li>Plot bin midpoint on x-axis and observed event rate on y-axis</li>
<li>If points fall on 45<span class="math inline">\(^{\circ}\)</span> line then well-callibrated</li>
</ul></li>
<li>Can set layer as hyperparameter for tuning<br />
</li>
<li><span class="math inline">\(L\)</span>: Number of layers</li>
<li><span class="math inline">\(n^{[\ell]}\)</span>: Number of units in layer <span class="math inline">\(\ell\)</span>. Recall that input layer is layer 0, but is not counted in number of layers.</li>
<li><span class="math inline">\(a^{[\ell]} = g^{[\ell]}(z^{[\ell]})\)</span>: Activations in layer <span class="math inline">\(\ell\)</span></li>
<li><span class="math inline">\(w^{[\ell]}\)</span>: Weights for computing values <span class="math inline">\(z^{[\ell]}\)</span></li>
<li><span class="math inline">\(b^{[\ell]}\)</span>: Bias vector for layer <span class="math inline">\(\ell\)</span></li>
<li><span class="math inline">\(X = a^z{[\ell]}\)</span> = input layer</li>
<li><span class="math inline">\(a^{[L]} = \hat{y}\)</span> = output layer</li>
</ul>
</div>
<div id="forward-propagation" class="section level2">
<h2>Forward Propagation</h2>
<div class="figure">
<img src="/img/DeepNN_ForwardPropagation.JPG" alt="Deep NN Forward Propagation" />
<p class="caption">Deep NN Forward Propagation</p>
</div>
</div>
<div id="getting-your-matrix-dimensions-right" class="section level2">
<h2>Getting your matrix dimensions right</h2>
<ul>
<li>Recall: <span class="math inline">\(z^{[1]} = a^{[0]} + b^{[1]}\)</span> of the first hidden layer has dimension (<span class="math inline">\(n^{[1]}\)</span>,1)</li>
</ul>
<p>Dimensions:</p>
<ul>
<li><span class="math inline">\(w^{[\ell]}: (n^{[\ell]}, n^{[\ell-1]})\)</span>. Same dimension for <span class="math inline">\(dw^{[\ell]}\)</span> in backpropagation.</li>
<li><span class="math inline">\(b^{[\ell]}: (n^{[\ell]}, 1)\)</span>. Same dimension for <span class="math inline">\(db^{[\ell]}\)</span> in backpropagation.</li>
<li>The activation <span class="math inline">\(a^{[\ell]} = g^{[\ell]}(z^{[\ell]})\)</span> has the same dimension as <span class="math inline">\(z^{[\ell]}\)</span></li>
</ul>
<p>For vectorisation</p>
<ul>
<li>Stack <span class="math inline">\(z\)</span> horizontally for each training example, so <span class="math inline">\(Z^{[\ell]}: (n^{[\ell]},m)\)</span></li>
<li>Stack <span class="math inline">\(A\)</span> horizontally for each training example, so <span class="math inline">\(A^{[\ell-1]}: (n^{[\ell-1]},m)\)</span>.<br />
</li>
<li><span class="math inline">\(b\)</span> remains <span class="math inline">\((n^{[\ell]},1)\)</span> but with python broadcasting this becomes <span class="math inline">\((n^{[\ell]},m)\)</span></li>
</ul>
<p>In summary:</p>
<ul>
<li><span class="math inline">\(Z^{[\ell]}, A^{[\ell]}: (n^{[\ell]},m)\)</span>, but with <span class="math inline">\(\ell = 0, A^{[0]} = X: (n^{[0]},m)\)</span></li>
<li>Similarly for backpropagation, <span class="math inline">\(dZ^{[\ell]}, dA^{[\ell]}: (n^{[\ell]},m)\)</span></li>
</ul>
</div>
<div id="why-deep-representations" class="section level2">
<h2>Why deep representations</h2>
<p>Deep network</p>
<ul>
<li>Face recognition/detection (visualisation will make more sense with convolutional neural network (CNN))
<ul>
<li>First layer - where are the edges? (small edges of image)</li>
<li>Second layer - detect eye, nose</li>
<li>Later layer - detect (larger areas of image)</li>
</ul></li>
<li>Speech recognition
<ul>
<li>First layer - low level audio-waveform</li>
<li>Second layer - basic units of sound, phoneme</li>
<li>Third layer - Words</li>
<li>Last layer - Sentences/ Phases</li>
</ul></li>
<li>Could be dangerous to make analogy between deep learning and how the human brain works</li>
</ul>
<p>Circuit theory and deep learning</p>
<blockquote>
<p>Informally, there are functions you can compute with a small L-layer deep nueral network that shallower networks require exponentially more hidden units to compute.</p>
</blockquote>
<p>Re-branding: Neural networks with many layers re-branded as “Deep Learning”.</p>
<p>Usually starts with logistic regression and then tries 1-2 hidden layers and considers the number of hidden layers as a tuning parameters.</p>
</div>
<div id="building-blocks-of-deep-neural-networks" class="section level2">
<h2>Building blocks of deep neural networks</h2>
<ul>
<li>Usually cache W and B for convenience.</li>
</ul>
<div class="figure">
<img src="/img/DeepNN_BuildingBlockFlow.JPG" alt="Deep NN Forward Propagation" />
<p class="caption">Deep NN Forward Propagation</p>
</div>
</div>
<div id="forward-and-backward-propagation" class="section level2">
<h2>Forward and Backward Propagation</h2>
<p><img src="/img/DeepNN_FandBPropagation1.JPG" alt="Forward Propagation" /> <img src="/img/DeepNN_FandBPropagation2.JPG" alt="Back Propagation" /> <img src="/img/DeepNN_FandBPropagation3.JPG" alt="Forward and Back Propagation" /></p>
</div>
<div id="parameters-and-hyperparameters" class="section level2">
<h2>Parameters and Hyperparameters</h2>
<p>Parameters: <span class="math inline">\(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}, \ldots\)</span></p>
<p>Hyperparameters: Control parameters (i.e. determine their values)</p>
<ul>
<li>Learning rate <span class="math inline">\(\alpha\)</span> (previously may have been sloppy and referred to as parameter)</li>
<li>Number of iterations</li>
<li>Number of hidden layers <span class="math inline">\(L\)</span></li>
<li>Number of hidden units <span class="math inline">\(n^{[1]}, n^{[2]}, \dots\)</span></li>
<li>Choice of activation function</li>
<li><span class="math inline">\(\vdots\)</span></li>
<li>Momentum</li>
<li>Mini-batch size</li>
<li>Regularization</li>
</ul>
<p>Empirical process: Idea <span class="math inline">\(\rightarrow\)</span> Code <span class="math inline">\(\rightarrow\)</span> Experiment <span class="math inline">\(\rightarrow\)</span> Idea <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\ldots\)</span>. i.e. need to try a lot and see how it works</p>
<p>Very difficult to know in advance the hyperparameters. Need to go around cycle and iterate. But in next course will discuss systematic way for trying out a range of values. Note that even if tune to best hyper-parameter today may change in a year or so for the same problem (could be CPU..)</p>
</div>
<div id="what-does-it-have-to-do-with-the-human-brain" class="section level2">
<h2>What does it have to do with the human brain?</h2>
<p>Not a whole lot… but analogy is made because of: <img src="/img/DeepNN_BrainAnalogy.JPG" alt="Brain Analogy" /></p>
</div>
</div>
