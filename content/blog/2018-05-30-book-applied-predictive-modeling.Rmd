---
title: 'Book: Applied Predictive Modeling'
author: Marie
date: '2018-05-30'
slug: book-applied-predictive-modeling
draft: TRUE
categories:
  - Study-Notes
tags:
  - R
  - Study-Notes
  - Book
---

# Overview
This post includes my notes and reproduction of examples in the book [Applied Predictive Modeling](http://appliedpredictivemodeling.com) (2013), by Max Kuhn and Kjell Johnson.

# Notes

## Introduction
Common reasons for predictive model failure:

* Inadequate pre-processing of data
* Inadequate model validation
* Unjustified extrapolation
* Overfitting 
* Insufficient number of models explorer

When prediction accuracy is the primary goal, should not choose a second-rate model for interpretability.
To improve accuracy, generally need a more complex model which is more difficult to interpret.  

Foundation for effective predictive model:

* intuition and deep knowledge (to obtain relevant data, eliminate noise)
* Relevant data
* Versatile computation toolbox (pre-processing, modelling, visulations)
The combined force of predictive modelling and intuition will be better than the parts.


```{r load_libraries, include=FALSE}

if (!requireNamespace("AppliedPredictiveModeling")) install.packages("AppliedPredictiveModeling")
require("AppliedPredictiveModeling")

# Source: https://github.com/gimoya/theBioBucket-Archives/blob/master/R/Functions/instant_pkgs.R#L7
# e.g. loadPackages(c("base", "jpeg"))
loadPackages <- function(pkgs) { 
  # determine which packages are not installed
  pkgs_miss <- pkgs[which(!pkgs %in% installed.packages()[, 1])]
  #return(pkgs_miss)
  
  #install and load packages not installed
  if (length(pkgs_miss) > 0) {
    install.packages(pkgs_miss, dependencies = c("Depends", "Suggests"))
  }

  # load packages not already loaded
  attached <- search()
  attached_pkgs <- attached[grepl("package", attached)]
  need_to_attach <- pkgs[which(!pkgs %in% gsub("package:", "", attached_pkgs))]
  #return(need_to_attach)
  
  if (length(need_to_attach) > 0) {
    for (i in 1:length(need_to_attach)) suppressPackageStartupMessages(require(need_to_attach[i],character.only=TRUE))
  }
}


# alternative use: AppliedPredictiveModeling:getPackages()
loadPackages(c(
               "caret"
              , "CORElearn"
              , "corrplot"
              , "Cubist"
              , "C50"
              , "DMwR"
              #, "DWD" # Disstance Weighted Discrimination; not current on CRAN
              , "earth"
              , "elasticnet"
              , "ellipse"
              , "e1071"
              , "gbm"
              , "glmnet"
              , "Hmisc"
              , "ipred"
              , "kernlab"
              , "klaR"
              , "latticeExtra"
              , "MASS"
              , "mda"
              , "minerva"
              , "nnet"
              , "pamr"
              , "party"
              , "partykit"
              , "pls"
              , "pROC"
              , "randomForest"
              , "RColorBrewer"
              , "rpart"
             # , "RWeka" # requires Java; http://www.oracle.com/technetwork/java/javase/downloads/jdk10-downloads-4416644.html
              , "sparseLDA"
              , "tabplot"
              , "tidyverse"
               ))

```

## Predictive Modeling Process

Steps:

  1. Understand data and modelling objectives; critical for a reliable and trustworthy model for predicting new samples; necessary before moving to the next steps.
  2. Preprocess and split the data
  3. Build & evaluate model
  
    * Split dataset into training set and validation set.     Using MPG for 2010-2011 model year cars, if goal is to predict MPG for a new car line, then model could be created using all 2010 model cars and tested on the new 2011 cars.  This, in contrast, to taking a random sample fo the data for model building.  Use training set to try a number of techniques; only use validation set for a few strong candidate modesl.  Repeatedly using test set negates its utility as  final arbitrator
    * Alternatively can use resampling (cross-validation) to evaludate model
  4. Select model
  
Themes

* Data splitting.  How the model will be applied (e.g. whether it will be extrapolated to a new population) should determine how the training and test sets are determined.  The amount of data available will also influence data splitting decisions.  With a small dataset (and therefore test), resampling advised.
* Predictors: feature selection
* Estimating performance: Statistics (e.g. RMSE) and visualisations.  Both are important.
* Evaluating several models: There is no single model that will always do better than another.
* Model selection: Involves choosing between models and selecting tuning parametesr (within model)



