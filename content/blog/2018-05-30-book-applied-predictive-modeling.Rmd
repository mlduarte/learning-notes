---
title: 'Book: Applied Predictive Modeling'
author: Marie
date: '2018-05-30'
slug: book-applied-predictive-modeling
banner: "img/banners/kuhn.png"
categories:
  - Study-Notes
tags:
  - R
  - Study-Notes
  - Book
---

# Overview
This post includes my notes and reproduction of examples in the book [Applied Predictive Modeling](http://appliedpredictivemodeling.com) (2013), by Max Kuhn and Kjell Johnson.

# Notes

## Introduction
Common reasons for predictive model failure:

* Inadequate pre-processing of data
* Inadequate model validation
* Unjustified extrapolation
* Overfitting 
* Insufficient number of models explorer

When prediction accuracy is the primary goal, should not choose a second-rate model for interpretability.
To improve accuracy, generally need a more complex model which is more difficult to interpret.  

Foundation for effective predictive model:

* Intuition and deep knowledge (to obtain relevant data, eliminate noise)
* Relevant data
* Versatile computation toolbox (pre-processing, modelling, visualizations)
The combined force of predictive modelling and intuition will be better than the parts.


```{r load_libraries, include=FALSE}
if (!requireNamespace("AppliedPredictiveModeling")) install.packages("AppliedPredictiveModeling")
require("AppliedPredictiveModeling")

# Source: https://github.com/gimoya/theBioBucket-Archives/blob/master/R/Functions/instant_pkgs.R#L7
# e.g. loadPackages(c("base", "jpeg"))
loadPackages <- function(pkgs) { 
  # determine which packages are not installed
  pkgs_miss <- pkgs[which(!pkgs %in% installed.packages()[, 1])]
  #return(pkgs_miss)
  
  #install and load packages not installed
  if (length(pkgs_miss) > 0) {
    install.packages(pkgs_miss, dependencies = c("Depends", "Suggests"))
  }

  # load packages not already loaded
  attached <- search()
  attached_pkgs <- attached[grepl("package", attached)]
  need_to_attach <- pkgs[which(!pkgs %in% gsub("package:", "", attached_pkgs))]
  #return(need_to_attach)
  
  if (length(need_to_attach) > 0) {
    for (i in 1:length(need_to_attach)) suppressPackageStartupMessages(require(need_to_attach[i],character.only=TRUE))
  }
}


# alternative use: AppliedPredictiveModeling:getPackages()
# use scriptLocation() for chapter analysis in text
loadPackages(c(
               "caret"
              , "CORElearn"
              , "corrplot"
              , "Cubist"
              , "C50"
              , "DMwR"
              #, "DWD" # Disstance Weighted Discrimination; not current on CRAN
              , "earth"
              , "elasticnet"
              , "ellipse"
              , "e1071"
              , "gbm"
              , "glmnet"
              , "Hmisc"
              , "ipred"
              , "kernlab"
              , "klaR"
              , "latticeExtra"
              , "MASS"
              , "mda"
              , "minerva"
              , "nnet"
              , "pamr"
              , "party"
              , "partykit"
              , "pls"
              , "pROC"
              , "randomForest"
              , "RANN"
              , "RColorBrewer"
              , "rpart"
             # , "RWeka" # requires Java; http://www.oracle.com/technetwork/java/javase/downloads/jdk10-downloads-4416644.html
              , "sparseLDA"
              , "tabplot"
              , "tidyverse"
               ))
# R.home()
# .Library
```

## Predictive Modeling Process

Steps:

  1. Understand data and modelling objectives; critical for a reliable and trustworthy model for predicting new samples; necessary before moving to the next steps.
  2. Preprocess and split the data
  3. Build & evaluate model
  
    * Split dataset into training set and validation set.     Using MPG for 2010-2011 model year cars, if goal is to predict MPG for a new car line, then model could be created using all 2010 model cars and tested on the new 2011 cars.  This, in contrast, to taking a random sample of the data for model building.  Use training set to try a number of techniques; only use validation set for a few strong candidate models.  Repeatedly using test set negates its utility as  final arbitrator
    * Alternatively can use resampling (cross-validation) to evaluate model
  4. Select model
  
Themes

* Data splitting.  How the model will be applied (e.g. whether it will be extrapolated to a new population) should determine how the training and test sets are determined.  The amount of data available will also influence data splitting decisions.  With a small dataset (and therefore test), resampling advised.
* Predictors: feature selection
* Estimating performance: Statistics (e.g. RMSE) and visualisations.  Both are important.
* Evaluating several models: There is no single model that will always do better than another.
* Model selection: Involves choosing between models and selecting tuning parameters (within model)

## Data Pre-Processing

* Addition, deletion or transformation of **training set** data
* Can make or break model's predictive ability
* Feature extraction/feature engineering: how predictors are encoded, e.g. combinations, ratios
* Feature selection: Include predictors to maximise accuracy
* Method depends on model being used and true relationship with outcome
* Model may require:
  * Predictors have common scale (e.g PLS)
  * Removal of outliers


### Data Tranformations (Individual Predictors)
* Pre-processing techniques include:
  * Centering: Subtract average $\rightarrow \bar{x} = 1$
  * Scaling: Divide by standard deviation  $\rightarrow  s = 1$
  * Skewness transformations.  
* Disadvantage of transformations is loss of interpretability  

**Centering and Scaling**

**Skewness**
Skewed if $\frac{x_\max}{x_\min} > 20$, or if |skewness statistic| >> 0 
    $$
    \begin{align}
      \text{skewness} & = \frac{\sum (x_i - \bar{x})^3}{(n-1)v^{2/3}} \\
      v & = \frac{\sum(x_i - \bar{x})^2}{(n-1)}
   \end{align}$$

Skewness can be removed by replacing data with log, square root or inverse, or by using the Box and Cox family of transformations and determining the appropriate parameter, $\lambda$,

\begin{align}
  x^\star =  \begin{cases}\frac{x^\lambda - 1 }{\lambda} & \text{if } \lambda \neq 0 \\
                    \log(x) & \text{if } \lambda = 0
              \end{cases}
        
\end{align}

Note

  * Square, $\lambda = 2$
  * Square root, $\lambda = 0.5$
  * Inverse,  $\lambda = -1$

$\lambda$ can be estimated using training data for each feature with skewness.  Transformations should only be applied if $\lambda$ outside $1 \pm 0.02$.


The `MASS::boxcox` function can estimate $\lambda$ but will not create the transformed variables.
The `caret::BoxCoxTrans` function will find the appropriate transformation and apply them to the new data.

**Example**: Segmentation data

```{r segmentation_preprocess}
# load case study training data
data("segmentationOriginal")
segTrain <- subset(segmentationOriginal, Case == "Train")

## Remove response variables (first three columns)
segTrainId <- segTrain$Cell
segTrainClass <- segTrain$Class
segTrainCase <- segTrain$Case
segTrainX <- select(segTrain, -c(Cell, Class, Case)) 
#%>% select(-contains("Status"))

# https://topepo.github.io/caret/pre-processing.html#the-preprocess-function

# Summarise the predictors
segMetrics <- segTrainX %>%
  select_if(is.numeric) %>%
  summarise_all(funs(skewness, n_distinct, max, min)) %>%
  gather(key="key", value="value") %>%
  extract(key, c("variable", "metric"), "(.*)_(skewness|n_distinct|max|min)") %>%
  tidyr::spread(metric, value) %>%
  mutate(max_min_ratio = ifelse(min==0, (max+1)/(min+1), max/min)) %>%
  select(variable, n_distinct, skewness, max_min_ratio, min, max)

## Use caret's preProcess function to transform for skewness
segPP <- caret::preProcess(segTrainX, method = "BoxCox")

## Apply the transformations
segTrainTrans <- predict(segPP, segTrainX)

#Transformation used
# pp2df <- function(object, ...) {
#   vars <- unlist(object$method)
#   nums <- vapply(object$method, length, c(num = 0))
#   meth <- rep(names(nums), nums)
#   
#   data.frame(variable = unname(vars), method = meth)
# }

# Record results
res <- data.frame(lambda = sapply(segPP$bc, `[[`, 1))
res <- data.frame(variable = rownames(res), res, row.names = NULL, stringsAsFactors = FALSE)

segMetrics <- segMetrics %>% 
  left_join(res, by="variable") %>% 
  arrange(lambda)

head(segMetrics)

rm(res)


```
In this data set there were a total of `r nrow(segMetrics)` features, for which:

* `r nrow(filter(segMetrics, min <= 0 ))` were not transformed because they had a minimum value of $\leq 0$

```{r segmentation_boxcox_neg, eval=FALSE, include=FALSE}
segMetrics %>%
  filter(min <= 0)
```

* the remaining features had a $\lambda$ between `r min(segMetrics$lambda, na.rm = TRUE)` and `r max(segMetrics$lambda, na.rm = TRUE)`
* Features with a lambda between 0.98 and 1.02 would not be transformed 


**Question**: Why not add an offset variable to ensure all variables are positive?  Note, that instead of using the log transformation, or Box-Cox transformations when predictors have values of zero, can instaed use the Yeo-Johnson family of transformations.  This family is similar to the Box-Cos transformations but can handle zero or negative predictor values.


As an example, the feature _VarIntenCh3_, which records the standard deviation of pix intensity in actin filaments, had the following properties:

* Metrics
```{r segmentation_featureEG}
filter(segMetrics, variable == "VarIntenCh3")
```
* Strong right skewness
* Original data distribution
```{r segmentation_featureEG_orig}
histogram(~segTrainX$VarIntenCh3,
          xlab = "Natural Units",
          type = "count")
```
* Transformed data distribution
```{r segmentation_featureEG_trans}
histogram(~log(segTrainX$VarIntenCh3),
          xlab = "Log Units",
          ylab = " ",
          type = "count")
```





### Data Transformations (Multiple Predictors)

**Outliers**

* Is value valid or has recording error occurred?
* Take care to remove or change values, especially if sample size is small (as could be a result of a skewed distribution without enough data to see the skewness, or could be an indication of a special part of the population)
* Decision trees and SVMs are insensitive to outliers
* The _spatial sign_ transformation can minimize the problem of a model's sensitivity to outliers

Spatial Sign Transformation

* Projects predictor values onto a multidimensional sphere
* Makes all samples the same distance from the centre of the sphere
* Each sample is divided by its squared norm:
  \[ x_{ij}^* = \frac{x_{ij}}{\sum^P_{j=1} x^2_{ij}}\]
* The denominator measures the squared distance to the centre of the predictors distribution
* It is important to center and scale the predictor data prior to using this transformation
* The predictors are transformed as a group, making removal of a predictor problematic
  
Example:

* Investigation of ~ 8 outliers shows valid but poorly sampled population (e.g. highly profitable customers)
* Spatial sign transformation applied; outliers reside in northwest section of distribution but contracted inwards
* Mitigates effect on model training

```{r spatialsign_egIris}
trellis.par.set(caretTheme())
featurePlot(x=iris[,-5], y=iris[,5], "pairs")
featurePlot(spatialSign(scale(iris[,-5])), iris[,5], "pairs")
```
```{r spatialsign_eg_rand}
set.seed(1)
n <- 10000
tmp <- data.frame(x=c(rnorm(n, 0, 0.02), -1, 1, 0.5),
                  y=c(rnorm(n, 0, 0.2), -1, 1, -2))

plot(tmp, asp=1, col=c(rep(1,n), 2, 3, 4), pch=19)
grid()
plot(spatialSign(tmp), asp=1, col=c(rep(1,n), 2, 3, 4), pch=19)
grid()
```


**Data Reduction and Feature Extraction**

Data reduction techniques

* Generate smaller set of predictors that capture most of the information within the original variables
* _Signal (Feature) Extraction_ techniques: New predictors are functions of original variables (therefore all original variables required)
* PCA
    + Finds linear combinations of predictors (principal components) to capture most possible variance
    + First PC captures more variability than any other linear combination
    + Subsequent PCs are uncorrelated with all previous PCs
    + Principal Component can be written as:
  \[PC_j = (a_{j1} x \text{Predictor} 1) + (a_{j2} x \text{Predictor} 2) + \ldots + (a_{jP} x \text{Predictor} P)\]
  where P = # predictors, coefficients = component weights (or loadings) that show which predictors are important for given PC.
    + Advantage: creates uncorrelated components, which is required for stability of some models
    + Disadvantages: Seeks predictor-set variation without regard to predictor measurement scales/distributions or response variable; without guidance can summarize data characteristics that are irrelevant to structure of data and modelling objective
  
        - Seeks linear combinations that maximize variability, and therefore will first summarise predictors with more variation.  If original predictors are on measurements scales then first few components will summarise higher magnitude predictors; consequently it will focus on identifying data structure based on measurement scales rather than based on important relationships within the data for the current problem.
        - Unsupervised technique; does not consider consider modelling objective / response variability.  
      
  + Should first transform skewed predictors and then center and scale prior to performing PCA; prevent PCA being influenced by original measurement scales
  + Consider Partial Least Squares (PLS) to derive components with response variable in mind.
  + Use scree plot to decide number of components to keep; in automated model building process, optimal number can be determined by cross-validation
  + Should also visually examine the PCs; plot first few PCs against each other and color points by, e.g., class labels.  If PCA has captured sufficient amount of information in data, may demonstrate clusters of samples/outliers requiring closer examination.  Ensure to use the same scale as later components will often have smaller data ranges and plotting on separate scales may lead to potential to over-interpret patterns.
  
Example: PCA applied to two features

  * For channel 1, Intensity Entropy is highly correlated with Fiber Width (`r round(with(segTrainTrans, cor(AvgIntenCh1, EntropyIntenCh1)),2)`)
  * Could use just one predictor, or could use PCA to instead use a linear combination of these two predictors
  
```{r segPCS_EGplot}
## R's prcomp is used to conduct PCA
pr <- prcomp(~ AvgIntenCh1 + EntropyIntenCh1, 
             data = segTrainTrans, #already pre-preprocessed for skewness
             scale. = TRUE)


transparentTheme(pchSize = .7, trans = .3)

xyplot(AvgIntenCh1 ~ EntropyIntenCh1,
       data = segTrainTrans,
       groups = segTrain$Class,
       xlab = "Channel 1 Fiber Width",
       ylab = "Intensity Entropy Channel 1",
       auto.key = list(columns = 2),
       type = c("p", "g"),
       main = "Original Data",
       aspect = 1)

xyplot(PC2 ~ PC1,
       data = as.data.frame(pr$x),
       groups = segTrain$Class,
       xlab = "Principal Component #1",
       ylab = "Principal Component #2",
       main = "Transformed",
       xlim = extendrange(pr$x),
       ylim = extendrange(pr$x),
       type = c("p", "g"),
       aspect = 1)
```
  
  * Because first PC summarises `r scales::percent(summary(pr)$importance[2,1])` variation and the second `r scales::percent(summary(pr)$importance[2,2])`, in this case could just use the first PC.
  
```{r pca_importance, eval=FALSE, include=FALSE}
summary(pr)
# importance also available:
eigs <- pr$sdev^2
eigs/sum(eigs)
```
  
Example: PCA applied to all features

The scree plot shows that four PCs would be retained.
 
```{r segPCS_EGscree}

## Apply PCA to the entire set of predictors.

## There are a few predictors with only a single value, so we remove these first
## (since PCA uses variances, which would be zero)
isZV <- apply(segTrainX, 2, function(x) length(unique(x)) == 1)

segPP <- preProcess(segTrainX[, !isZV], c("BoxCox", "center", "scale"))
segTrainTrans <- predict(segPP, segTrainX[, !isZV])

segPCA <- prcomp(segTrainTrans, center = TRUE, scale. = TRUE)

tab <- summary(segPCA)$importance[2,]
tab <- data.frame(PC = names(tab), Var = tab, row.names=NULL, stringsAsFactors = FALSE) %>% mutate(PC=parse_number(PC))
ggplot(tab, aes(x = PC, y=Var))+ 
         geom_line() + 
  geom_point()
```

Scatterplot matrix of first 3 PCs (with points coloured by class): 

* Appears to be some separation between classes when plotting first and second component (but remember that these two components only explain `r scales::percent(sum(tab$Var[1:2]))` of variance and so don't over-interpret!).  However distribution of well-segmented cells roughly contained within poorly identified cells; cell types don't appear to be easily separated.  Don't despair!; it does not mean other models, e.g. that can accommodate non-linear relationships, will reach the same conclusions.
```{r segPCS_EGscat}
## Plot a scatterplot matrix of the first three components
transparentTheme(pchSize = .8, trans = .3)
panelRange <- extendrange(segPCA$x[, 1:3])
splom(as.data.frame(segPCA$x[, 1:3]),
      groups = segTrainClass,
      type = c("p", "g"),
      as.table = TRUE,
      auto.key = list(columns = 2),
      prepanel.limits = function(x) panelRange)
```

* Can also visualise which predictor is associated with each principal component.  A coefficient (loading) close to zero within the PC linear equation indicates that that predictors did not contribute much to that component.  In the following figure, each point corresponds to a predictor variable and is coloured by thte opitcal channel used in the exerpiment.  For the first PC, channel 1 (cell body) loadings are greater and therefore have largest effect on PC.  However, even though cell body measurements account fo rmore variation in the data, this does not imply that these variables will be associated with predicting segmentation quality. 

```{r segPCS_EGrot}
## Format the rotation values for plotting
segRot <- as.data.frame(segPCA$rotation[, 1:3])

## Derive the channel variable
vars <- rownames(segPCA$rotation)
channel <- rep(NA, length(vars))
channel[grepl("Ch1$", vars)] <- "Channel 1"
channel[grepl("Ch2$", vars)] <- "Channel 2"
channel[grepl("Ch3$", vars)] <- "Channel 3"
channel[grepl("Ch4$", vars)] <- "Channel 4"

segRot$Channel <- channel
segRot <- segRot[complete.cases(segRot),]
segRot$Channel <- factor(as.character(segRot$Channel))

## Plot a scatterplot matrix of the first three rotation variable
transparentTheme(pchSize = .8, trans = .7)
panelRange <- extendrange(segRot[, 1:3])
upperp <- function(...)
  {
    args <- list(...)
    circ1 <- ellipse(diag(rep(1, 2)), t = .1)
    panel.xyplot(circ1[,1], circ1[,2],
                 type = "l",
                 lty = trellis.par.get("reference.line")$lty,
                 col = trellis.par.get("reference.line")$col,
                 lwd = trellis.par.get("reference.line")$lwd)
    circ2 <- ellipse(diag(rep(1, 2)), t = .2)
    panel.xyplot(circ2[,1], circ2[,2],
                 type = "l",
                 lty = trellis.par.get("reference.line")$lty,
                 col = trellis.par.get("reference.line")$col,
                 lwd = trellis.par.get("reference.line")$lwd)
    circ3 <- ellipse(diag(rep(1, 2)), t = .3)
    panel.xyplot(circ3[,1], circ3[,2],
                 type = "l",
                 lty = trellis.par.get("reference.line")$lty,
                 col = trellis.par.get("reference.line")$col,
                 lwd = trellis.par.get("reference.line")$lwd)
    panel.xyplot(args$x, args$y, groups = args$groups, subscripts = args$subscripts)
  }
splom(~segRot[, 1:3],
      groups = segRot$Channel,
      lower.panel = function(...){}, upper.panel = upperp,
      prepanel.limits = function(x) panelRange,
      auto.key = list(columns = 2))
```


### Missing Values

Types of missing data:

* Structurally missing, e.g. number of children man has given birth to
* Informative missingness: if related to outcome, can induce bias, e.g. customer rating are using polarised
* Censored (this is not missing data, as something isknown about it), but for predictive models, it may be treated as missing, or hte censored value may be used as the observed value.  E.g., for labatory test which cannot measure below a limit, may use a random value between 0 and the limit as the observed value.  

Options:

* Remove samples (if small subset of large data set)
* Specifically account for the missingness (e.g. tree-based techniques)
* Impute by using information in the training set of predictors (i.e. predictive model within a predictive model).  
    * Note that imputation for statistical inference is not the same as inference for predictive models (references given for the latter).  
    * Incorporate imputation in resampling if being used to select tuning parmater values.   
    * If number of predictors affected by missing values is small, best to perform exploratory analysis of relationships between predictors, e.g. using PCA or visualisations.  If a variable with missing values is highly correlated with another thne can use a focussed model
    * KNN popular to impute by finding samples in training set closest to it and averages nearby points to fill it.  Advantage: Confined to training set. Disadvantage: KNN requires entire training set and number of neighbours and method of determinig "closeness" are tuning parameters.
    
```{r imputationViaKNN}
require(AppliedPredictiveModeling)
data(segmentationOriginal)

## Retain the original training set
segTrain <- subset(segmentationOriginal, Case == "Train")

## Remove the first three columns (identifier columns)
segTrainX <- segTrain[, -(1:3)]
segTrainX <- segTrainX[, -nearZeroVar(segTrainX)]

# Randomly sample 50 to be missing
set.seed(15103930)
ind_miss <- sample(1:nrow(segTrainX), 50, replace = FALSE)
obs <- segTrainX[ind_miss, ]
segTrainX$PerimCh1[ind_miss] <- NA
summary(segTrainX$PerimCh1)

# Predict the missing values
set.seed(100)
knnTrans <- preProcess(segTrainX, method = c("center", "scale", "knnImpute"))
knnPred <- predict(knnTrans, segTrainX)
pred <- knnPred$PerimCh1[ind_miss]
obsTrans <- predict(knnTrans, obs)$PerimCh1

ggplot(data.frame(obs = obsTrans, pred), aes(x=obs, y=pred)) + 
  geom_point() + 
  geom_smooth(method='lm')

cor(obsTrans, pred)
#if("PerimCh1" %in% preProcValues$method$scale) pred <- pred * preProcValues$std["PerimCh1"]
#if("PerimCh1" %in% preProcValues$method$center) pred <- pred + preProcValues$mean["PerimCh1"]

```
    

TODO: Simple regression method ..need to first find correlated variables
```{r imputationViaRegression}
# use cell fiber lengtha nd cell size. to predict.  
segCorr <- cor(segTrainX, use = "pairwise.complete.obs")[,"PerimCh1"]
sort(segCorr, decreasing = TRUE)[2:3]
mod <- lm(PerimCh1 ~ FiberLengthCh1 + LengthCh1, data = segTrainX[-ind_miss, ])
pred <- predict(mod, segTrainX[ind_miss, ])
ggplot(data.frame(obs = obsTrans, pred), aes(x=obs, y=pred)) + 
  geom_point() + 
  geom_smooth(method='lm')

cor(pred, obsTrans)
```


### Removing Predictors

Advantage of less predictors:

* Decreased computation time
* Decreased complexity / More pasimonious / More interpretable
* Highly correlated predictors are measring the same thing and so have no extra information
* Better model performance / stability without problematic and correlated variabels

Predictors to remove:

### Zero Variance and Near-Zero Variance Predictors

* **Zero variance predictors** : Those with a single unique value
* **Near-zero variance predictors** : Thos with a single value for most samples

To determine:
1.  Calculate number of unique variables / number of samples
2. Calculate frequency of unique values (or ratio of most common frequent to second most frequent)

If  (1) faction of unique values over sample size is loq, e.g. $\leq$ 10% and (2) ratio of frequency of most prevalent value to the second most prevalent value is large, e.g. $\geq$ 20

### Correlated Variables

* **Collinearity**: Correlated pair of predictor variables
* **Multicollinearity**: Relationships between multiple predictors at once

To detect:
1. Visually.  TODO: Review Everitt 
```{r plotCorrelationMatrix}
segData <- subset(segmentationOriginal, Case == "Train")[, -(1:3)]
isZV <- apply(segData, 2, function(x) length(unique(x)) == 1)
segData <- segData[, !isZV]
correlations <- cor(segData)
corrplot::corrplot(correlations, order = "hclust", tl.cex = 0.2)
```


2. PCA: If first PCA accounts for large percentage of variance, then there is at least one group of predictors that represent the same information.  Use PCA loadings to understand which predictos are associated with each component.  Note though that as unsupervised, no guarnatee that resulting predictors will have relationship with the outcome.

TODO: Need to understand PCA better

3. Variance Inflation Factor (VIF) statistic available as part of classical regression anallysis, however only useful for linear regression. Whilst it determine collinear predictors it does not determine which should be removed to resolve the problem
4. Remove the minimum number of predictors to determien that all pairwise correlations are below a certain threshold.  This only identifies collineariites in two dimensions, but can improve model performance. Algorithm is as follows:
  1. Calculate correlation matrix
  2. Determine pair of predictors with largest absolute correlation
  3. Determine average correlation between A and other variables.  Repeat for B.
  4. Remove the predictor (A or B) with the largest average corelation
  5. Repeat steps 2-3 until no absolute correlations above the threshold
  
This algorithm is implemented using `caret::findCorrelation`
  
```{r removeCorrelatedVars}
segTrain <- subset(segmentationOriginal, Case == "Train")
## Remove the first three columns (identifier columns)
segTrainX <- segTrain[, -(1:3)]
isZV <- apply(segTrainX, 2, function(x) length(unique(x)) == 1)
segTrainX <- segTrainX[, !isZV]

segTrainClass <- segTrain$Class



segPP <- preProcess(segTrainX, c("BoxCox", "center", "scale"))
segTrainTrans <- predict(segPP, segTrainX)


## Use caret's preProcess function to transform for skewness
segPP <- preProcess(segTrainX, method = "BoxCox")
## Apply the transformations
segTrainTrans <- predict(segPP, segTrainX)
segCorr <- cor(segTrainTrans)
#corrplot(segCorr, order = "hclust", tl.cex = .35)

## caret's findCorrelation function is used to identify columns to remove.
highCorr <- findCorrelation(segCorr, .75)
length(highCorr)
```



### Adding Predictors

* It is common to decompose categorical predictors usng dummy variables (indicator with zero/one).  With 5 categories, only 4 dummy variables needed.  If 5 were included in regression then would have numerical isseus (intercept replaces the other); for other models, including all 5 may aid interpretation

* Non-linear transformations, e.g. $B^2$, where $B$ is a predictor
* Combinations of data (e.g. class centroids, center of predictor data to each class)

### Binning Predictors

Disadvantages of manual binning:

* Loss of performance in the model
* Loss in predicion in the predictions
* Can lead to high rate of false positives (noisy predictors determined to be informative)

Advantage: More interpretable.  However the perceived improvement in interpretability by manual binning is usually offest by  siginficant loss in perofrmance (the goal of this book is prediction not interpretation and so manual binning not recommended).

Automatic Binning 
* E.G Classification / Regression Trees
* Multivariate adaptive regression splies
* Evaluate many variables simulatabiuosly, based on staticscally sound methodogloges

### Computing



### Transformations

* `e1071::sknewness`: calculates sample skewness statistics for each predictor.  Those that are highly skewed can be priorritsed for distribution visualations using `hist`, `lattice:histogram`
* `MASS::boxcox`: to determine type of tranformation to use; estimates $\lambda$ but will not create the transformed variable(s)
* `caret::BoxCoxTrans`: Find most appropriate transformation and apply to new data

Example:
```{r egBoxCoxTrans}
segData <- subset(segmentationOriginal, Case == "Train")[, -(1:3)]
Ch1AreaTrans <- BoxCoxTrans(segData$AreaCh1)
Ch1AreaTrans
data.frame(orig = head(segData$AreaCh1), auto.trans = predict(Ch1AreaTrans, head(segData$AreaCh1)), man.trans = ( head(segData$AreaCh1) ^ Ch1AreaTrans$lambda - 1 )/Ch1AreaTrans$lambda   )

```


* `prcomp`: For PCA

Example:
```{r egPCA}
segData <- subset(segmentationOriginal, Case == "Train")[, -(1:3)]
isZV <- apply(segData, 2, function(x) length(unique(x)) == 1)
segData <- segData[, !isZV]

pcaObject <- prcomp(segData, center = TRUE, scale. = TRUE)
# Calculate the cumulative percentage of variance which each component accounts for.
percentVariance <- pcaObject$sd^2/sum(pcaObject$sd^2)*100
percentVariance[1:3]
# Transformed variables stored in a `pcaObject` as a sub-object called x
head(pcaObject$x[ , 1:5])
# Loadings
head(pcaObject$rotation[ , 1:3])
```

* `caret::spatialSign`: Spatial sign transfomration
* `impute::impute.knn` to estimate missing data; also possible using `caret::PreProcess` which applies imputation methods based on KNN or bagged trees
* `caret::PreProcess`: Has the ability to
  * Transform
  * Center
  * Scale
  * Impute values
  * Feature extraction
  * Apply spatial sign transformation

  (in this order) After calling the `preProcess` function, the `predict` method applies the tranformationr esults to a set of data.
  
### Filtering

* `caret::nearZeroVar`: To determine predictors with unique variables
* `cor`: To determine correlations
* `corrplot:corrplot`: Includes option to reorder variables in a way that reveals clusters of highly correlated predictors
* `caret::findCorrelation`: Recommends columns for deleteion based on a given threshold of pairwise correlations
* `subselect` package also has methods for selecting predictors

### Creating Dummy Variables

* `caret::dummyVars`: To determine encodings for clategorical predictors
* All dummy variables recommended when using tree-based model

```{r egDummyVariableCreation}
data(cars)
carSubset <- select(cars, Price, Mileage)
type <- c("convertible", "coupe", "hatchback", "sedan", "wagon")
carSubset$Type <- factor(apply(cars[, 14:18], 1, function(x) type[which(x == 1)]))

simpleMod <- dummyVars(~Mileage + Type,
                       data = carSubset,
                       ## Remove the variable name from the
                       ## column name
                       levelsOnly = TRUE)
simpleMod

# To generate the dummy variables fo rhte training set or any new samples, use the predict method with teh dummyVars objects
predict(simpleMod, head(carSubset))

withInteraction <- dummyVars(~Mileage + Type + Mileage:Type,
                             data = carSubset,
                             levelsOnly = TRUE)
withInteraction
predict(withInteraction, head(carSubset))
```


### Chapter 3 Exercises

#### Exercise 3.1
The UC Irvine Machine Learning Repository contains a data set related
to glass identification. The data consist of 214 glass samples labeled as one
of seven class categories. There are nine predictors, including the refractive
index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.

```{r ex31_loaddata}
require(mlbench)
data(Glass)
str(Glass)

GlassPredictors <- select(Glass, -Type) %>%
  gather(Variable, Value)
```

(a) Using visualizations, explore the predictor variables to understand their
distributions as well as the relationships between predictors.
```{r ex31_hist}
ggplot(GlassPredictors, aes(x = Value)) +  
  geom_histogram() + 
  facet_wrap(~Variable, scales = "free")
```


```{r ex31_dens}
ggplot(GlassPredictors, aes(x = Value)) +  
  geom_density() + 
  facet_wrap(~Variable, scales = "free")
```

```{r ex31_scatter, message=FALSE, warning=FALSE}
require(GGally)
ggpairs(select(Glass, -Type))
```




```{r ex31_corplot, eval=FALSE, include=FALSE}
x <- select(Glass, -Type) %>%
  select_if(function(x) length(nearZeroVar(x)) == 0) 
correlations <- cor(x)
corrplot::corrplot(correlations, order = "hclust", tl.cex = 1)

```
(b) Does there appear to be any outliers in the data?  Are predictors skewed?

* Several variables show signs of skewness (BA, Ca, FE, RI)
* Variable K could be skewed or have outliers
* Variables K and MG have possible second modes around zero, whereas Fe and Ba also have a high distribution of values around zero
* Variables Ca, NA, RI and SI have concentrations of samples in the middle of the scale and a small number of data points at the edges
* Variables Ca and Ri are positively correlated, and Ca and NA
* Visually, Ca and Na appear to be negatively correlated, however the correlation score is -0.275

(c) Are there any relevant transformations of one or more predictors that
might improve the classification model?

Due to variables containing zero, use the Yeo-Johnson family of transformations
```{r ex31_yeo}
glassTrans <- preProcess(select(Glass, -Type), method = "YeoJohnson")
glassTransData <- predict(glassTrans, select(Glass, -Type))
glassTransData %>%
  gather(Variable, Value) %>%
  mutate(Transformation = "Yeo-Johnson") %>%
  bind_rows(data.frame(GlassPredictors, Transformation = "NA")) %>%
ggplot(aes(x = Value, color = Transformation)) +  
  geom_density() + 
  facet_wrap(~Variable, scales = "free")
```
However, they don't really help in terms of skewness ... 

To mitigate outliers, use the spatial sign transformation



# New R commands
`apropos`: search R packages for a given term in currently loaded packages
```{r apropos, eval=FALSE, include=FALSE}
apropos("confusion")
```

`RSiteSearch`: find function in any package
```{r eval=FALSE, include=FALSE}
RSiteSearch("confusion", restrict="functions")
```

