---
title: 'Book: Applied Predictive Modeling (Part I)'
author: "Marie"
date: '2018-05-30T13:39:46+02:00'
categories: Study-Notes
slug: book-applied-predictive-modeling
tags:
- R
- Study-Notes
- Book
banner: img/banners/kuhn.png
---

[Preprocessing](#preprocessing)
[Over-Fitting and Model Tuning](#tuning)

# Overview
This post includes my notes and reproduction of examples of **Part I: General Strategies** of the book [Applied Predictive Modeling](http://appliedpredictivemodeling.com) (2013), by Max Kuhn and Kjell Johnson


# Introduction
Common reasons for predictive model failure:

* Inadequate pre-processing of data
* Inadequate model validation
* Unjustified extrapolation
* Overfitting 
* Insufficient number of models explorer

When prediction accuracy is the primary goal, should not choose a second-rate model for interpretability.
To improve accuracy, generally need a more complex model which is more difficult to interpret.  

Foundation for effective predictive model:

* Intuition and deep knowledge (to obtain relevant data, eliminate noise)
* Relevant data
* Versatile computation toolbox (pre-processing, modelling, visualizations)
The combined force of predictive modelling and intuition will be better than the parts.


```{r load_libraries, include=FALSE}
if (!requireNamespace("AppliedPredictiveModeling")) install.packages("AppliedPredictiveModeling")
library("AppliedPredictiveModeling")

# Source: https://github.com/gimoya/theBioBucket-Archives/blob/master/R/Functions/instant_pkgs.R#L7
# e.g. loadPackages(c("base", "jpeg"))
loadPackages <- function(pkgs) { 
  # determine which packages are not installed
  pkgs_miss <- pkgs[which(!pkgs %in% installed.packages()[, 1])]
  #return(pkgs_miss)
  
  #install and load packages not installed
  if (length(pkgs_miss) > 0) {
    install.packages(pkgs_miss, dependencies = c("Depends", "Suggests"), repos = "http://cran.us.r-project.org")
  }

  # load packages not already loaded
  attached <- search()
  attached_pkgs <- attached[grepl("package", attached)]
  need_to_attach <- pkgs[which(!pkgs %in% gsub("package:", "", attached_pkgs))]
  #return(need_to_attach)
  
  if (length(need_to_attach) > 0) {
    for (i in 1:length(need_to_attach)) suppressPackageStartupMessages(library(need_to_attach[i],character.only=TRUE))
  }
}


# use scriptLocation() for chapter analysis in text
#AppliedPredictiveModeling::getPackages(1:4)


loadPackages(c(
               "caret"
              , "CORElearn" # Classification, Regression and Feature Evaluation
              , "corrplot"  # Correlation plots
            #  , "Cubist"    # Rule and distance based regression modelling
            #  , "C50"       # Decision trees
              , "DMwR"      # Data mining with R book
              #, "DWD" # Distance Weighted Discrimination; not current on CRAN
              , "earth" # MARS model
              , "elasticnet" # elastic net
              , "ellipse"   # for drawing ellipse like confidence intervals
              , "e1071"     # Misc functions incl. skewness
            #  , "gbm"       # gradient boosting
            #  , "glmnet"    # lassoo and elastic-net
              , "Hmisc"     # miscelaneous
            #  , "ipred"     # Improved predictive models by indirect classification and bagging for classification, regression and survival
            #  , "kernlab"  # SVM, spectral clustering, kernal pca, ...
            #  , "klaR"     # classification and visualisation
              , "lars"
              , "latticeExtra"  # lattice
            #  , "MASS"     # Modern applied statistics with S book
            #  , "mda"      # Mixture and flexible discriminant analysis
            #  , "minerva"  # Maximal Information-Based Nonparametric Exploration for Variable Analysis
            #  , "nnet"     # Neural net
            #  , "pamr"     # prediction analysis for microarray
            #  , "party"    # A Laboratory for Recursive Partytioning
            #  , "partykit" # A Toolkit for Recursive Partytioning
            #  , "pls"      # Partial Least Squares and Principal Component Regression
            #  , "pROC"     # Title Display and Analyze ROC Curves
            #  , "randomForest"  # random forest
              , "RANN"     # Fast Nearest Neighbour Search
              , "RColorBrewer" # colours
            #  , "rpart"    # ecursive Partitioning and Regression Tree
             # , "RWeka"    # requires Java; http://www.oracle.com/technetwork/java/javase/downloads/jdk10-downloads-4416644.html
            #  , "sparseLDA" #Sparse Discriminant Analysis
             # , "tabplot"  # Tableplot, a Visualization of Large Datasets
              , "tidyverse"
               ))
# R.home()
# .Library
```

## Predictive Modeling Process

Steps:

  1. Understand data and modelling objectives; critical for a reliable and trustworthy model for predicting new samples; necessary before moving to the next steps.
  2. Preprocess and split the data
  3. Build & evaluate model
  
    * Split dataset into training set and validation set.     Using MPG for 2010-2011 model year cars, if goal is to predict MPG for a new car line, then model could be created using all 2010 model cars and tested on the new 2011 cars.  This, in contrast, to taking a random sample of the data for model building.  Use training set to try a number of techniques; only use validation set for a few strong candidate models.  Repeatedly using test set negates its utility as  final arbitrator
    * Alternatively can use resampling (cross-validation) to evaluate model
  4. Select model
  
Themes

* Data splitting.  How the model will be applied (e.g. whether it will be extrapolated to a new population) should determine how the training and test sets are determined.  The amount of data available will also influence data splitting decisions.  With a small dataset (and therefore test), resampling advised.
* Predictors: feature selection
* Estimating performance: Statistics (e.g. RMSE) and visualisations.  Both are important.
* Evaluating several models: There is no single model that will always do better than another.
* Model selection: Involves choosing between models and selecting tuning parameters (within model)

# Data Pre-Processing <a id = preprocessing ></a>

* Addition, deletion or transformation of **training set** data
* Can make or break model's predictive ability
* Feature extraction/feature engineering: how predictors are encoded, e.g. combinations, ratios
* Feature selection: Include predictors to maximise accuracy
* Method depends on model being used and true relationship with outcome
* Model may require:
  * Predictors have common scale (e.g PLS)
  * Removal of outliers


## Data Tranformations (Individual Predictors)
* Pre-processing techniques include:
  * Centering: Subtract average $\rightarrow \bar{x} = 1$
  * Scaling: Divide by standard deviation  $\rightarrow  s = 1$
  * Skewness transformations.  
* Disadvantage of transformations is loss of interpretability  

**Centering and Scaling**

**Skewness**
Skewed if $\frac{x_\max}{x_\min} > 20$, or if |skewness statistic| >> 0 
    $$
    \begin{align}
      \text{skewness} & = \frac{\sum (x_i - \bar{x})^3}{(n-1)v^{2/3}} \\
      v & = \frac{\sum(x_i - \bar{x})^2}{(n-1)}
   \end{align}$$

Skewness can be removed by replacing data with log, square root or inverse, or by using the Box and Cox family of transformations and determining the appropriate parameter, $\lambda$,

\begin{align}
  x^\star =  \begin{cases}\frac{x^\lambda - 1 }{\lambda} & \text{if } \lambda \neq 0 \\
                    \log(x) & \text{if } \lambda = 0
              \end{cases}
        
\end{align}

Note

  * Square, $\lambda = 2$
  * Square root, $\lambda = 0.5$
  * Inverse,  $\lambda = -1$

$\lambda$ can be estimated using training data for each feature with skewness.  Transformations should only be applied if $\lambda$ outside $1 \pm 0.02$.


The `MASS::boxcox` function can estimate $\lambda$ but will not create the transformed variables.
The `caret::BoxCoxTrans` function will find the appropriate transformation and apply them to the new data.

**Example**: Segmentation data

```{r preprocess_segmentation_data}
# load case study training data
data("segmentationOriginal")
segTrain <- subset(segmentationOriginal, Case == "Train")

## Remove response variables (first three columns)
segTrainId <- segTrain$Cell
segTrainClass <- segTrain$Class
segTrainCase <- segTrain$Case
segTrainX <- dplyr::select(segTrain, -c(Cell, Class, Case)) 
#%>% select(-contains("Status"))

# https://topepo.github.io/caret/pre-processing.html#the-preprocess-function

# Summarise the predictors
segMetrics <- segTrainX %>%
  select_if(is.numeric) %>%
  summarise_all(funs(skewness, n_distinct, max, min)) %>%
  gather(key="key", value="value") %>%
  extract(key, c("variable", "metric"), "(.*)_(skewness|n_distinct|max|min)") %>%
  spread(metric, value) %>%
  mutate(max_min_ratio = ifelse(min==0, (max+1)/(min+1), max/min)) %>%
  select(variable, n_distinct, skewness, max_min_ratio, min, max)

## Use caret's preProcess function to transform for skewness
segPP <- caret::preProcess(segTrainX, method = "BoxCox")

## Apply the transformations
segTrainTrans <- predict(segPP, segTrainX)

#Transformation used
# pp2df <- function(object, ...) {
#   vars <- unlist(object$method)
#   nums <- vapply(object$method, length, c(num = 0))
#   meth <- rep(names(nums), nums)
#   
#   data.frame(variable = unname(vars), method = meth)
# }

# Record results
res <- data.frame(lambda = sapply(segPP$bc, `[[`, 1))
res <- data.frame(variable = rownames(res), res, row.names = NULL, stringsAsFactors = FALSE)

segMetrics <- segMetrics %>% 
  left_join(res, by="variable") %>% 
  arrange(lambda)

head(segMetrics)

rm(res)


```
In this data set there were a total of `r nrow(segMetrics)` features, for which:

* `r nrow(filter(segMetrics, min <= 0 ))` were not transformed because they had a minimum value of $\leq 0$

```{r nontrans_segmentation_metrics, eval=FALSE, include=FALSE}
segMetrics %>%
  filter(min <= 0)
```

* the remaining features had a $\lambda$ between `r min(segMetrics$lambda, na.rm = TRUE)` and `r max(segMetrics$lambda, na.rm = TRUE)`
* Features with a lambda between 0.98 and 1.02 would not be transformed 


**Question**: Why not add an offset variable to ensure all variables are positive?  Note, that instead of using the log transformation, or Box-Cox transformations when predictors have values of zero, can instead use the Yeo-Johnson family of transformations.  This family is similar to the Box-Cos transformations but can handle zero or negative predictor values.


It should be noted that transformations to reduce skewness might not always be successful.  Under such circumstances, one should use models that are not unduly affected by skewed distributions (e.g. tree-based methods)

As an example, the feature _VarIntenCh3_, which records the standard deviation of pix intensity in actin filaments, had the following properties:

* Metrics
```{r segmentation_varIntenCh3}
filter(segMetrics, variable == "VarIntenCh3")
```
* Strong right skewness
* Original data distribution
```{r original_segmentation_features}
histogram(~segTrainX$VarIntenCh3,
          xlab = "Natural Units",
          type = "count")
```
* Transformed data distribution
```{r trans_segmentation_features}
histogram(~log(segTrainX$VarIntenCh3),
          xlab = "Log Units",
          ylab = " ",
          type = "count")
```





## Data Transformations (Multiple Predictors)

**Outliers**

* Is value valid or has recording error occurred?
* Take care to remove or change values, especially if sample size is small (as could be a result of a skewed distribution without enough data to see the skewness, or could be an indication of a special part of the population)
* Decision trees and SVMs are insensitive to outliers
* The _spatial sign_ transformation can minimize the problem of a model's sensitivity to outliers

Spatial Sign Transformation

* Projects predictor values onto a multidimensional sphere
* Makes all samples the same distance from the centre of the sphere
* Each sample is divided by its squared norm:
  \[ x_{ij}^* = \frac{x_{ij}}{\sum^P_{j=1} x^2_{ij}}\]
* The denominator measures the squared distance to the centre of the predictors distribution
* It is important to center and scale the predictor data prior to using this transformation
* The predictors are transformed as a group, making removal of a predictor problematic
  
Example:

* Investigation of ~ 8 outliers shows valid but poorly sampled population (e.g. highly profitable customers)
* Spatial sign transformation applied; outliers reside in northwest section of distribution but contracted inwards
* Mitigates effect on model training

```{r iris_spatial_trans}
trellis.par.set(caretTheme())
featurePlot(x=iris[,-5], y=iris[,5], "pairs")
featurePlot(spatialSign(scale(iris[,-5])), iris[,5], "pairs")
```
```{r rand_spatial_trans}
set.seed(1)
n <- 10000
tmp <- data.frame(x=c(rnorm(n, 0, 0.02), -1, 1, 0.5),
                  y=c(rnorm(n, 0, 0.2), -1, 1, -2))

plot(tmp, asp=1, col=c(rep(1,n), 2, 3, 4), pch=19)
grid()
plot(spatialSign(tmp), asp=1, col=c(rep(1,n), 2, 3, 4), pch=19)
grid()
```


**Data Reduction and Feature Extraction**

Data reduction techniques

* Generate smaller set of predictors that capture most of the information within the original variables
* _Signal (Feature) Extraction_ techniques: New predictors are functions of original variables (therefore all original variables required)
* PCA
    + Finds linear combinations of predictors (principal components) to capture most possible variance
    + First PC captures more variability than any other linear combination
    + Subsequent PCs are uncorrelated with all previous PCs
    + Principal Component can be written as:
  \[PC_j = (a_{j1} x \text{Predictor} 1) + (a_{j2} x \text{Predictor} 2) + \ldots + (a_{jP} x \text{Predictor} P)\]
  where P = # predictors, coefficients = component weights (or loadings) that show which predictors are important for given PC.
    + Advantage: creates uncorrelated components, which is required for stability of some models
    + Disadvantages: Seeks predictor-set variation without regard to predictor measurement scales/distributions or response variable; without guidance can summarize data characteristics that are irrelevant to structure of data and modelling objective
  
        - Seeks linear combinations that maximize variability, and therefore will first summarise predictors with more variation.  If original predictors are on measurements scales then first few components will summarise higher magnitude predictors; consequently it will focus on identifying data structure based on measurement scales rather than based on important relationships within the data for the current problem.
        - Unsupervised technique; does not consider consider modelling objective / response variability.  
      
  + Should first transform skewed predictors and then center and scale prior to performing PCA; prevent PCA being influenced by original measurement scales
  + Consider Partial Least Squares (PLS) to derive components with response variable in mind.
  + Use scree plot to decide number of components to keep; in automated model building process, optimal number can be determined by cross-validation
  + Should also visually examine the PCs; plot first few PCs against each other and color points by, e.g., class labels.  If PCA has captured sufficient amount of information in data, may demonstrate clusters of samples/outliers requiring closer examination.  Ensure to use the same scale as later components will often have smaller data ranges and plotting on separate scales may lead to potential to over-interpret patterns.
  
Example: PCA applied to two features

  * For channel 1, Intensity Entropy is highly correlated with Fiber Width (`r round(with(segTrainTrans, cor(AvgIntenCh1, EntropyIntenCh1)),2)`)
  * Could use just one predictor, or could use PCA to instead use a linear combination of these two predictors
  
```{r segmentation_PCA}
## R's prcomp is used to conduct PCA
pr <- prcomp(~ AvgIntenCh1 + EntropyIntenCh1, 
             data = segTrainTrans, #already pre-preprocessed for skewness
             scale. = TRUE)


transparentTheme(pchSize = .7, trans = .3)

xyplot(AvgIntenCh1 ~ EntropyIntenCh1,
       data = segTrainTrans,
       groups = segTrain$Class,
       xlab = "Channel 1 Fiber Width",
       ylab = "Intensity Entropy Channel 1",
       auto.key = list(columns = 2),
       type = c("p", "g"),
       main = "Original Data",
       aspect = 1)

xyplot(PC2 ~ PC1,
       data = as.data.frame(pr$x),
       groups = segTrain$Class,
       xlab = "Principal Component #1",
       ylab = "Principal Component #2",
       main = "Transformed",
       xlim = extendrange(pr$x),
       ylim = extendrange(pr$x),
       type = c("p", "g"),
       aspect = 1)
```
  
  * Because first PC summarises `r scales::percent(summary(pr)$importance[2,1])` variation and the second `r scales::percent(summary(pr)$importance[2,2])`, in this case could just use the first PC.
  
```{r segmentation_pca_importance, eval=FALSE, include=FALSE}
summary(pr)
# importance also available:
eigs <- pr$sdev^2
eigs/sum(eigs)
```
  
Example: PCA applied to all features

The scree plot shows that four PCs would be retained.
 
```{r segmentation_pca_scree}

## Apply PCA to the entire set of predictors.

## There are a few predictors with only a single value, so we remove these first
## (since PCA uses variances, which would be zero)
isZV <- apply(segTrainX, 2, function(x) length(unique(x)) == 1)

segPP <- preProcess(segTrainX[, !isZV], c("BoxCox", "center", "scale"))
segTrainTrans <- predict(segPP, segTrainX[, !isZV])

segPCA <- prcomp(segTrainTrans, center = TRUE, scale. = TRUE)

tab <- summary(segPCA)$importance[2,]
tab <- data.frame(PC = names(tab), Var = tab, row.names=NULL, stringsAsFactors = FALSE) %>% mutate(PC=parse_number(PC))
ggplot(tab, aes(x = PC, y=Var))+ 
         geom_line() + 
  geom_point()
```

Scatterplot matrix of first 3 PCs (with points coloured by class): 

* Appears to be some separation between classes when plotting first and second component (but remember that these two components only explain `r scales::percent(sum(tab$Var[1:2]))` of variance and so don't over-interpret!).  However distribution of well-segmented cells roughly contained within poorly identified cells; cell types don't appear to be easily separated.  Don't despair!; it does not mean other models, e.g. that can accommodate non-linear relationships, will reach the same conclusions.
```{r segmentation_pca_scatter}
## Plot a scatterplot matrix of the first three components
transparentTheme(pchSize = .8, trans = .3)
panelRange <- extendrange(segPCA$x[, 1:3])
splom(as.data.frame(segPCA$x[, 1:3]),
      groups = segTrainClass,
      type = c("p", "g"),
      as.table = TRUE,
      auto.key = list(columns = 2),
      prepanel.limits = function(x) panelRange)
```

* Can also visualise which predictor is associated with each principal component.  A coefficient (loading) close to zero within the PC linear equation indicates that that predictors did not contribute much to that component.  In the following figure, each point corresponds to a predictor variable and is coloured by the optical channel used in the experiment.  For the first PC, channel 1 (cell body) loadings are greater and therefore have largest effect on PC.  However, even though cell body measurements account for more variation in the data, this does not imply that these variables will be associated with predicting segmentation quality. 

```{r segmentation_pca_splom}
## Format the rotation values for plotting
segRot <- as.data.frame(segPCA$rotation[, 1:3])

## Derive the channel variable
vars <- rownames(segPCA$rotation)
channel <- rep(NA, length(vars))
channel[grepl("Ch1$", vars)] <- "Channel 1"
channel[grepl("Ch2$", vars)] <- "Channel 2"
channel[grepl("Ch3$", vars)] <- "Channel 3"
channel[grepl("Ch4$", vars)] <- "Channel 4"

segRot$Channel <- channel
segRot <- segRot[complete.cases(segRot),]
segRot$Channel <- factor(as.character(segRot$Channel))

## Plot a scatterplot matrix of the first three rotation variable
transparentTheme(pchSize = .8, trans = .7)
panelRange <- extendrange(segRot[, 1:3])
upperp <- function(...)
  {
    args <- list(...)
    circ1 <- ellipse(diag(rep(1, 2)), t = .1)
    panel.xyplot(circ1[,1], circ1[,2],
                 type = "l",
                 lty = trellis.par.get("reference.line")$lty,
                 col = trellis.par.get("reference.line")$col,
                 lwd = trellis.par.get("reference.line")$lwd)
    circ2 <- ellipse(diag(rep(1, 2)), t = .2)
    panel.xyplot(circ2[,1], circ2[,2],
                 type = "l",
                 lty = trellis.par.get("reference.line")$lty,
                 col = trellis.par.get("reference.line")$col,
                 lwd = trellis.par.get("reference.line")$lwd)
    circ3 <- ellipse(diag(rep(1, 2)), t = .3)
    panel.xyplot(circ3[,1], circ3[,2],
                 type = "l",
                 lty = trellis.par.get("reference.line")$lty,
                 col = trellis.par.get("reference.line")$col,
                 lwd = trellis.par.get("reference.line")$lwd)
    panel.xyplot(args$x, args$y, groups = args$groups, subscripts = args$subscripts)
}
# note this requires ellipse
lattice::splom(~segRot[, 1:3],
      groups = segRot$Channel,
      lower.panel = function(...){}, upper.panel = upperp,
      prepanel.limits = function(x) panelRange,
      auto.key = list(columns = 2))
```


## Missing Values

Types of missing data:

* Structurally missing, e.g. number of children man has given birth to
* Informative missingness: if related to outcome, can induce bias, e.g. customer rating are using polarised
* Censored (this is not missing data, as something is known about it), but for predictive models, it may be treated as missing, or the censored value may be used as the observed value.  E.g., for laboratory test which cannot measure below a limit, may use a random value between 0 and the limit as the observed value.  

Options:

* Remove samples (if small subset of large data set)
* Specifically account for the missingness (e.g. tree-based techniques)
* Impute by using information in the training set of predictors (i.e. predictive model within a predictive model).  
    * Note that imputation for statistical inference is not the same as inference for predictive models (references given for the latter).  
    * Incorporate imputation in resampling if being used to select tuning parameter values.   
    * If number of predictors affected by missing values is small, best to perform exploratory analysis of relationships between predictors, e.g. using PCA or visualisations.  If a variable with missing values is highly correlated with another then can use a focused model
    * KNN popular to impute by finding samples in training set closest to it and averages nearby points to fill it.  Advantage: Confined to training set. Disadvantage: KNN requires entire training set and number of neighbours and method of determining "closeness" are tuning parameters.
    
```{r segmentation_imputation_knn}
library(AppliedPredictiveModeling)
data(segmentationOriginal)

## Retain the original training set
segTrain <- subset(segmentationOriginal, Case == "Train")

## Remove the first three columns (identifier columns)
segTrainX <- segTrain[, -(1:3)]
segTrainX <- segTrainX[, -nearZeroVar(segTrainX)]

# Randomly sample 50 to be missing
set.seed(15103930)
ind_miss <- sample(1:nrow(segTrainX), 50, replace = FALSE)
obs <- segTrainX[ind_miss, ]
segTrainX$PerimCh1[ind_miss] <- NA
summary(segTrainX$PerimCh1)

# Predict the missing values
set.seed(100)
knnTrans <- preProcess(segTrainX, method = c("center", "scale", "knnImpute"))
knnPred <- predict(knnTrans, segTrainX)
pred <- knnPred$PerimCh1[ind_miss]
obsTrans <- predict(knnTrans, obs)$PerimCh1

ggplot(data.frame(obs = obsTrans, pred), aes(x=obs, y=pred)) + 
  geom_point() + 
  geom_smooth(method='lm')

cor(obsTrans, pred)
#if("PerimCh1" %in% preProcValues$method$scale) pred <- pred * preProcValues$std["PerimCh1"]
#if("PerimCh1" %in% preProcValues$method$center) pred <- pred + preProcValues$mean["PerimCh1"]

```
    


```{r segmentation_imputation_regression}
# use cell fiber lengtha nd cell size. to predict.  
segCorr <- cor(segTrainX, use = "pairwise.complete.obs")[,"PerimCh1"]
sort(segCorr, decreasing = TRUE)[2:3]
mod <- lm(PerimCh1 ~ FiberLengthCh1 + LengthCh1, data = segTrainX[-ind_miss, ])
pred <- predict(mod, segTrainX[ind_miss, ])
ggplot(data.frame(obs = obsTrans, pred), aes(x=obs, y=pred)) + 
  geom_point() + 
  geom_smooth(method='lm')

cor(pred, obsTrans)
```


## Removing Predictors

Advantage of less predictors:

* Decreased computation time
* Decreased complexity / More parsimonious / More interpretable
* Highly correlated predictors are measuring the same thing and so have no extra information
* Better model performance / stability without problematic and correlated variables

Predictors to remove:

* Zero Variance and near-zero
* Correlated

## Zero Variance and Near-Zero Variance Predictors

* **Zero variance predictors** : Those with a single unique value
* **Near-zero variance predictors** : Those with a single value for most samples

To determine:

1. Calculate number of unique variables / number of samples
2. Calculate frequency of unique values (or ratio of most common frequent to second most frequent)

If  (1) fraction of unique values over sample size is low, e.g. $\leq$ 10% and (2) ratio of frequency of most prevalent value to the second most prevalent value is large, e.g. $\geq$ 20

## Correlated Variables

* **Collinearity**: Correlated pair of predictor variables
* **Multicollinearity**: Relationships between multiple predictors at once

To detect:
1. Visually.   
```{r segmentation_correlation_matrix}
segData <- subset(segmentationOriginal, Case == "Train")[, -(1:3)]
isZV <- apply(segData, 2, function(x) length(unique(x)) == 1)
segData <- segData[, !isZV]
correlations <- cor(segData)
corrplot::corrplot(correlations, order = "hclust", tl.cex = 0.2)
```


2. PCA: If first PCA accounts for large percentage of variance, then there is at least one group of predictors that represent the same information.  Use PCA loadings to understand which predictors are associated with each component.  

3. Variance Inflation Factor (VIF) statistic available as part of classical regression analysis, however only useful for linear regression. Whilst it determine collinear predictors it does not determine which should be removed to resolve the problem.

To remove correlated predictors:

1. Use a heuristic algorithm to remove the minimum number of predictors such that all pairwise correlations are below a certain threshold.  This only identifies colinearities in two dimensions, but can improve model performance. Algorithm is as follows:
  1. Calculate correlation matrix
  2. Determine pair of predictors with largest absolute correlation
  3. Determine average correlation between A and other variables.  Repeat for B.
  4. Remove the predictor (A or B) with the largest average correlation
  5. Repeat steps 2-3 until no absolute correlations above the threshold
  
This algorithm is implemented using `caret::findCorrelation`
  
```{r segmentatino_correlated_variables}
segTrain <- subset(segmentationOriginal, Case == "Train")
## Remove the first three columns (identifier columns)
segTrainX <- segTrain[, -(1:3)]
isZV <- apply(segTrainX, 2, function(x) length(unique(x)) == 1)
segTrainX <- segTrainX[, !isZV]

segTrainClass <- segTrain$Class



segPP <- preProcess(segTrainX, c("BoxCox", "center", "scale"))
segTrainTrans <- predict(segPP, segTrainX)


## Use caret's preProcess function to transform for skewness
segPP <- preProcess(segTrainX, method = "BoxCox")
## Apply the transformations
segTrainTrans <- predict(segPP, segTrainX)
segCorr <- cor(segTrainTrans)
#corrplot(segCorr, order = "hclust", tl.cex = .35)

## caret's findCorrelation function is used to identify columns to remove.
highCorr <- findCorrelation(segCorr, .75)
length(highCorr)
```

2. Feature extraction algorithms (e.g. PCA) can also be used to mitigate effect of correlations; the disadvantage is that they make connection between predictors more complex and as unsupervised no guarantee that resulting components will have relationship with outcome. 
3. Simulated Annealing (e.g., `subselect` package)
4. Genetic Algorithms

## Adding Predictors

* It is common to decompose categorical predictors using dummy variables (indicator with zero/one).  With 5 categories, only 4 dummy variables needed.  If 5 were included in regression then would have numerical issues (intercept replaces the other); for other models, including all 5 may aid interpretation

* Non-linear transformations, e.g. $B^2$, where $B$ is a predictor
* Combinations of data (e.g. class centroids, center of predictor data to each class)

## Binning Predictors

Disadvantages of manual binning:

* Loss of performance in the model
* Loss in prediction in the predictions
* Can lead to high rate of false positives (noisy predictors determined to be informative)

Advantage: More interpretable.  However the perceived improvement in interpretability by manual binning is usually offset by  significant loss in performance (the goal of this book is prediction not interpretation and so manual binning not recommended).

Automatic Binning 
* E.G Classification / Regression Trees
* Multivariate adaptive regression splines
* Evaluate many variables simultaneously, based on statistically sound methodologies

## Computing



### Transformations

* `e1071::sknewness`: calculates sample skewness statistics for each predictor.  Those that are highly skewed can be prioritised for distribution visualisations using `hist`, `lattice:histogram`
* `MASS::boxcox`: to determine type of transformation to use; estimates $\lambda$ but will not create the transformed variable(s)
* `caret::BoxCoxTrans`: Find most appropriate transformation and apply to new data

Example:
```{r segmentation_boxcox_trans}
segData <- subset(segmentationOriginal, Case == "Train")[, -(1:3)]
Ch1AreaTrans <- BoxCoxTrans(segData$AreaCh1)
Ch1AreaTrans
data.frame(orig = head(segData$AreaCh1), auto.trans = predict(Ch1AreaTrans, head(segData$AreaCh1)), man.trans = ( head(segData$AreaCh1) ^ Ch1AreaTrans$lambda - 1 )/Ch1AreaTrans$lambda   )

```


* `prcomp`: For PCA

Example:
```{r segmentation_pca_trans}
segData <- subset(segmentationOriginal, Case == "Train")[, -(1:3)]
isZV <- apply(segData, 2, function(x) length(unique(x)) == 1)
segData <- segData[, !isZV]

pcaObject <- prcomp(segData, center = TRUE, scale. = TRUE)
# Calculate the cumulative percentage of variance which each component accounts for.
percentVariance <- pcaObject$sd^2/sum(pcaObject$sd^2)*100
percentVariance[1:3]
# Transformed variables stored in a `pcaObject` as a sub-object called x
head(pcaObject$x[ , 1:5])
# Loadings
head(pcaObject$rotation[ , 1:3])
```

* `caret::spatialSign`: Spatial sign transformation
* `impute::impute.knn` to estimate missing data; also possible using `caret::PreProcess` which applies imputation methods based on KNN or bagged trees
* `caret::PreProcess`: Has the ability to

  * Transform
  * Center
  * Scale
  * Impute values
  * Feature extraction
  * Apply spatial sign transformation

  (in this order) After calling the `preProcess` function, the `predict` method applies the transformation results to a set of data.
  
### Filtering

* `caret::nearZeroVar`: To determine predictors with unique variables
* `cor`: To determine correlations
* `corrplot:corrplot`: Includes option to reorder variables in a way that reveals clusters of highly correlated predictors
* `caret::findCorrelation`: Recommends columns for deletion based on a given threshold of pairwise correlations
* `subselect` package also has methods for selecting predictors

### Creating Dummy Variables

* `caret::dummyVars`: To determine encoding for categorical predictors
* All dummy variables recommended when using tree-based model

```{r cars_dummy_vars}
data(cars)
carSubset <- select(cars, Price, Mileage)
type <- c("convertible", "coupe", "hatchback", "sedan", "wagon")
carSubset$Type <- factor(apply(cars[, 14:18], 1, function(x) type[which(x == 1)]))

simpleMod <- dummyVars(~Mileage + Type,
                       data = carSubset,
                       ## Remove the variable name from the
                       ## column name
                       levelsOnly = TRUE)
simpleMod

# To generate the dummy variables fo rhte training set or any new samples, use the predict method with teh dummyVars objects
predict(simpleMod, head(carSubset))

withInteraction <- dummyVars(~Mileage + Type + Mileage:Type,
                             data = carSubset,
                             levelsOnly = TRUE)
withInteraction
predict(withInteraction, head(carSubset))
```


## Chapter 3 Exercises

### Exercise 3.1
The UC Irvine Machine Learning Repository contains a data set related
to glass identification. The data consist of 214 glass samples labeled as one
of seven class categories. There are nine predictors, including the refractive
index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.

```{r load_glass_data, message=FALSE, warning=FALSE}
library(mlbench)
data(Glass)
str(Glass)

GlassPredictors <- select(Glass, -Type) %>%
  gather(Predictor, Value)
```

(a) Using visualizations, explore the predictor variables to understand their
distributions as well as the relationships between predictors.
```{r glass_hist, message=FALSE, warning=FALSE}
ggplot(GlassPredictors, aes(x = Value)) +  
  geom_histogram() + 
  facet_wrap(~Predictor, scales = "free")
```


```{r glass_dens}
ggplot(GlassPredictors, aes(x = Value)) +  
  geom_density() + 
  facet_wrap(~Predictor, scales = "free")
```

```{r glass_predictor_scatter, message=FALSE, warning=FALSE}
library(GGally)
ggpairs(select(Glass, -Type))
```


(b) Does there appear to be any outliers in the data?  Are predictors skewed?

* Several variables show signs of skewness (BA, Ca, FE, RI)
* Variable K could be skewed or have outliers
* Variables K and MG have possible second modes around zero, whereas Fe and Ba also have a high distribution of values around zero
* Variables Ca, NA, RI and SI have concentrations of samples in the middle of the scale and a small number of data points at the edges
* Variables Ca and Ri are positively correlated, and Ca and NA
* Visually, Ca and Na appear to be negatively correlated, however the correlation score is -0.275

(c) Are there any relevant transformations of one or more predictors that
might improve the classification model?

Due to variables containing zero, use the Yeo-Johnson family of transformations
```{r glass_yeo_trans, message=FALSE, warning=FALSE}
glassTrans <- preProcess(select(Glass, -Type), method = "YeoJohnson")
glassTransData <- predict(glassTrans, select(Glass, -Type))
dat <- glassTransData %>%
  gather(Predictor, Value) %>%
  mutate(Transformation = "Yeo-Johnson") %>%
  bind_rows(data.frame(GlassPredictors, Transformation = "NA"))


# Final result
res <- list()
# How many points we want 
nPoints <- 1e3

# Using simple loop to scale and calculate density
combinations <- expand.grid(unique(dat$Predictor), unique(dat$Transformation))
for(i in 1:nrow(combinations)) {
    # Subset data
    foo <- subset(dat, Predictor == combinations$Var1[i] & Transformation == combinations$Var2[i])
    # Perform density on scaled signal
    densRes <- density(x = scale(foo$Value), n = nPoints)
    # Position signal from 1 to wanted number of points
    res[[i]] <- data.frame(x = 1:nPoints, y = densRes$y, 
                           pred = combinations$Var1[i], trans = combinations$Var2[i])
}
res <- do.call(rbind, res)
ggplot(res, aes(x / nPoints, y, color = trans, linetype = trans)) +
    geom_line(alpha = 0.5, size = 1) +
    facet_wrap(~ pred, scales = "free")  +
  labs(x = "Value / Points", y = "Scaled Density", colour = "Transformation", linetype = "Transformation")
```

However, they don't really help in terms of skewness ... 

To mitigate outliers, use the spatial sign transformation
```{r glass_spatial_trans, message=FALSE, warning=FALSE}
glassCSSSTrans <- preProcess(select(Glass, -Type), method = c("center", "scale", "spatialSign"))
glassCSSSData <- predict(glassCSSSTrans, select(Glass, -Type))
ggpairs(glassCSSSData)
```

Spatial sign transformation was effective in removing outliers.

### Exercise 2
The soybean data can also be found at the UC Irvine Machine Learning
Repository. Data were collected to predict disease in 683 soybeans. The 35
predictors are mostly categorical and include information on the environmental
conditions (e.g., temperature, precipitation) and plant conditions (e.g., left
spots, mold growth). The outcome labels consist of 19 distinct classes.

```{r ex3_2_load_data, message=FALSE, warning=FALSE, include=FALSE}
library(mlbench)
data(Soybean)
#?Soybean
```


(a) Investigate the frequency distributions for the categorical predictors. Are
any of the distributions degenerate in the ways discussed earlier in this
chapter?

Note: By degenerate, I believe it is meant whether the following exist:

* Skewness
* Outliers
* Missing values
* Zero- and near-zero- variance

```{r soybean_hist, message=FALSE, warning=FALSE}
select(Soybean, -Class) %>%
  select_if(negate(is.numeric)) %>%
  gather(Variable, Value) %>%
  ggplot(aes(x = Value)) + 
  geom_bar() + 
  facet_wrap(~ Variable, scales = "free")
```

From these plots we see:

* Few observations in April (0)
* Precipitation usually greater than normal (2)
* Temperature usually normal
* Temperature, precipitation and hail are missing values.



A mosaic plot, fluctuation diagram, or faceted bar chart may be used to display two categorical variable, as detailed in [The Generalized Pairs Plot](https://vita.had.co.nz/papers/gpp.pdf)

```{r soybean_pairs, message=FALSE, warning=FALSE}
ggpairs(select(Soybean, date, precip, temp), diag = 'blankDiag', upper = list(discrete = "ratio"))
```

```{r soybean_mosaic, message=FALSE, warning=FALSE}
library(vcd)
#library(ggmosaic)
#detach("package:kernlab", unload=TRUE)
Soybean2 <- Soybean %>%
  mutate(date = fct_recode(date, Apr = "0", May = "1", Jun = "2", Jul = "3", Aug = "4", Sep = "5", Oct = "6"), 
         temp = fct_recode(temp, "lt-norm" = "0", "norm" = "1", "gt-norm" = "2"))
vcd::mosaic(~date + temp, data = Soybean2)

ggplot(Soybean2, aes(x = date, fill = temp)) + geom_bar()

Soybean2 %>%
  group_by(date, temp) %>%
  summarise(n = n()) %>%
  group_by(date) %>%
  mutate(pct = n/sum(n)) %>%
  ggplot(aes(x = date, fill = temp, y = pct)) + geom_bar(stat = "identity")

Soybean2 %>%
  group_by(date, temp) %>%
  summarise(n = n()) %>%
  group_by(temp) %>%
  mutate(pct = n/sum(n)) %>%
  ggplot(aes(x = temp, fill = date, y = pct)) + geom_bar(stat = "identity")

```

(b) Roughly 18% of the data are missing. Are there particular predictors that
are more likely to be missing? Is the pattern of missing data related to
the classes?

* Higher proportion of missing temperatures in April, although most missing temperatures are in July.
* The pattern of missing data by class is shown below

```{r soybean_missing_predictors}
n_nulls <- function(x) sum(is.na(x))
select(Soybean, -Class) %>%
    select_if(negate(is.numeric)) %>%
    summarise_all(funs(n_distinct, n_nulls))  %>%
    gather(key="key", value="value") %>%
    extract(key, c("variable", "metric"), "(.*)_(n_distinct$|n_nulls$)") %>%
    tidyr::spread(metric, value) %>%
    #filter(variable %in% c("date", "precip", "temp", "hail")) %>%
  mutate(n = nrow(Soybean), 
         prop_nulls = n_nulls/n) %>%
  arrange(desc(prop_nulls))

# Can also use YaleToolkit::whatis function
#YaleToolkit::whatis(select(Soybean, -Class))


```

```{r soybean_missing_cases}
table(Soybean$Class, complete.cases(Soybean))
```
Some classes have no complete cases, in particular: 2-4-d-injury, cyst-namatode,  diaporthe-pod-&-stem-blight, herbicide-injury  and phytophthora-rot.  Can check if there are just a few predictors causing this issue.

```{r soybean_missing_predictors_by_case}
n_nulls <- function(x) sum(is.na(x))
Soybean %>%
  filter(Class %in% c('2-4-d-injury', 'cyst-namatode',  'diaporthe-pod-&-stem-blight', 'herbicide-injury', 'phytophthora-rot')) %>%
  group_by(Class) %>%
    select_if(negate(is.numeric)) %>%
    summarise_all(funs(n_nulls, n=n()))  %>%
    gather(key="key", value="value", -Class) %>%
    extract(key, c("variable", "metric"), "(.*)_(n$|n_nulls$)") %>%
    spread(metric, value) %>%
  mutate(prop_nulls = n_nulls/n) %>%
  select(Class, variable, prop_nulls) %>%
  spread(Class, prop_nulls)


```

This shows that many predictors are completely missing for some cases, e.g. the class 2-4-d-injury has no observations of the predictor canker.lesion, crop.hist, ext.decay, fruit.pots, etc.

(c) Develop a strategy for handling missing data, either by eliminating
predictors or imputation.

In some cases, 100% of predictor values are missing and so imputation is unlikely to help. Options:

* Remove cases with high proportion of missing values
* Encode missing as another level

```{r soybean_sparsity}
dat <- Soybean %>%
  # consider only those that are complete cases
  filter_all(all_vars(!is.na(.))) %>%
  # convert ordered variable to factor  
  mutate_if(is.ordered, function(x) factor(as.character(x)))

# generate binary predictors
dummyInfo <- dummyVars(Class ~ ., data=dat)
dummies <- predict(dummyInfo, dat)

# Determine which variables are sparse
sparsity <- nearZeroVar(dummies, saveMetrics = TRUE)
head(sparsity)

# Number and percentage of predictors to remove
sparsity %>%
  summarise(n = n(), n_nzv = sum(nzv), pct_nzv = mean(nzv))
```

To bypass the need to remove 19% of predictors, could use models insensitive to sparsity, such as tree- or rule- based models, or naive Bayes.

### Exercise 3
Chapter 5 introduces Quantitative Structure-Activity Relationship
(QSAR) modeling where the characteristics of a chemical compound are used
to predict other chemical properties. The caret package contains a QSAR
data set from Mente and Lombardo (2005). Here, the ability of a chemical
to permeate the blood-brain barrier was experimentally determined for 208
compounds. 134 descriptors were measured for each compound.

(a) Start R and use these commands to load the data:
```{r bloodbrain_data}
#library(caret); library(tidyverse); library(e1071)
data(BloodBrain)
# use ?BloodBrain to see more details
```

The numeric outcome is contained in the vector logBBB while the predictors
are in the data frame bbbDescr.

(b) Do any of the individual predictors have degenerate distributions? i.e., Need to look for: Skewness, outliers, missing values and zero- and near-zero- variance


There are `ncol(bbbDescr)` descriptors and so will not plot all.

* Skewness is present if |skewness| is large or if the ratio of the maximum to minimum value is greater than 20.  
* (Near-)Zero Variance is present if percentage of unique values is $\leq$ 10 and frequency ratio (or most common over second-most common value) is $\geq$ 20.

```{r bloodbrain_dist}
zeroVar <- function(x) nearZeroVar(x, saveMetrics = TRUE)$zeroVar
nzv <- function(x) nearZeroVar(x, saveMetrics = TRUE)$nzv
freqRatio <- function(x) nearZeroVar(x, saveMetrics = TRUE)$freqRatio
percentUnique <- function(x) nearZeroVar(x, saveMetrics = TRUE)$percentUnique

  
bbbSumm <- bbbDescr %>% 
  summarise_all(funs(skewness, n_distinct, max, min, zeroVar, nzv, freqRatio, percentUnique)) %>%
  gather(key="key", value="value") %>%
  extract(key, c("variable", "metric"), "(.*)_(skewness|n_distinct|max|min|zeroVar|nzv|freqRatio|percentUnique)") %>%
  spread(metric, value) %>%
  mutate(max_min_ratio = ifelse(min==0, (max+1)/(min+1), max/min)) %>%
  select(variable, n_distinct, skewness, max_min_ratio, zeroVar, nzv, freqRatio, percentUnique, min, max) %>%
  arrange(desc(abs(skewness)))

head(bbbSumm, n = 15)
```

There are a number of variables with near-zero variance: 
`r cat(paste('-', filter(bbbSumm, nzv == 1)$variable), sep = '\n') `

These variables would be removed for models that are impacted detrimentally by near-zero variance predictors.

```{r bloodBrain_nsv_hist}
select(bbbDescr, filter(bbbSumm, nzv == 1)$variable) %>%
  gather(Measure, Value) %>%
  
  ggplot() + 
  geom_histogram(aes(x=Value)) +
  facet_wrap(~ Measure)
```

There are also a number of skewed variables.  Data pre- and post-transformation (Yeo-Johnson) are shown below.  May be best to determine manually which to transform.  
  
```{r bloodBrain_dens_glimpse}
bbb_top_vars <- filter(bbbSumm, ! variable %in% filter(bbbSumm, nzv == 1)$variable) %>% 
  top_n(n = 16, wt = abs(skewness)) %>%
  select(variable) %>%
  pull()


bbbTrans <- preProcess(select(bbbDescr, -c(filter(bbbSumm, nzv == 1)$variable)), method = "YeoJohnson")
bbbTransData <- predict(bbbTrans, select(bbbDescr, -c(filter(bbbSumm, nzv == 1)$variable))) 
dat <- bbbTransData %>%
  select(bbb_top_vars) %>%
  gather(Predictor, Value) %>%
  mutate(Transformation = "Yeo-Johnson") %>%
  bind_rows(data.frame(gather(select(bbbDescr, bbb_top_vars), Predictor, Value), Transformation = "NA", stringsAsFactors = FALSE)) 
  
# Final result
res <- list()
# How many points we want 
nPoints <- 1e3

# Using simple loop to scale and calculate density
combinations <- expand.grid(unique(dat$Predictor), unique(dat$Transformation))
for(i in 1:nrow(combinations)) {
    # Subset data
    foo <- subset(dat, Predictor == combinations$Var1[i] & Transformation == combinations$Var2[i])
    # Perform density on scaled signal
    densRes <- density(x = scale(foo$Value), n = nPoints)
    # Position signal from 1 to wanted number of points
    res[[i]] <- data.frame(x = 1:nPoints, y = densRes$y, 
                           pred = combinations$Var1[i], trans = combinations$Var2[i])
}
res <- do.call(rbind, res)
ggplot(res, aes(x / nPoints, y, color = trans, linetype = trans)) +
    geom_line(alpha = 0.5, size = 1) +
    facet_wrap(~ pred, scales = "free")  +
  labs(x = "Value / Points", y = "Scaled Density", colour = "Transformation", linetype = "Transformation")

```


(c) Generally speaking, are there strong relationships between the predictor
data? If so, how could correlations in the predictor set be reduced?
Does this have a dramatic effect on the number of predictors available for
modeling?

Samples in tails of skewed predictors may have significant effect on correlation structure and so best to review after transformation.  

```{r bloodBrain_correlation_plots}
# Correlation plot with no transformation
bbb_cor_raw <- cor(select(bbbDescr, -c(filter(bbbSumm, nzv == 1)$variable)))
corrplot(bbb_cor_raw, order = "hclust", addgrid.col = NA, tl.pos = "n")

# Correlation plot of Yeo-Johnson transformed data
bbb_cor_yeo <- cor(bbbTransData)
corrplot(bbb_cor_yeo, order = "hclust", addgrid.col = NA, tl.pos = "n")

# Correlation plot of spatial sign transformed data
bbb_cor_sps <- cor(spatialSign(scale(select(bbbDescr, -c(filter(bbbSumm, nzv == 1)$variable)))))
corrplot(bbb_cor_sps, order = "hclust", addgrid.col = NA, tl.pos = "n")

```


This shows that correlations lessen with increasing levels of transformations

```{r bloodBrain_corr_summ}
corrInfo <- function(x) summary(x[upper.tri(x)])
corrInfo(bbb_cor_raw)
corrInfo(bbb_cor_yeo)
corrInfo(bbb_cor_sps)
```

However, rather than transform data (e.g., using PCA), it may be better to remove correlated predictors, using `findCorrelation` function.

The following chart shows the number of variables that would be removed based on varying correlation thresholds 
```{r bloodBrain_corr_heuristic}
thresholds <- seq(.25, .95, by = 0.05)
size <- meanCorr <- rep(NA, length(thresholds))
removals <- vector(mode = "list", length = length(thresholds))

for(i in seq_along(thresholds)){
  removals[[i]] <- findCorrelation(bbb_cor_raw, thresholds[i]) # removed predictors
  subMat <- bbb_cor_raw[-removals[[i]], -removals[[i]]]       # remaining predictors
  size[i] <- ncol(bbb_cor_raw) -length(removals[[i]])         # number of remaining predictors
  meanCorr[i] <- mean(abs(subMat[upper.tri(subMat)]))         # mean correlation of remaining predictors
}

corrData <- data.frame(value = c(size, meanCorr),
                       threshold = c(thresholds, thresholds),
                       what = rep(c("Predictors", 
                                    "Average Absolute Correlation"),
                                  each = length(thresholds))) 

# xyplot(value~ threshold|what, data = corrData, 
#        scales = list(y = list(relation = "free")),
#        type = c("p", "g", "smooth"),
#        degree = 2,
#        ylab = "")

ggplot(corrData, aes(x = threshold, y = value)) +
  geom_point() + 
  geom_smooth(method = "loess", se = FALSE) + 
  facet_wrap(~ what, scales = "free")
  
```

Other methods to remove correlated variables and search for quality subsets include simulated annealing and genetic algorithms, e.g. via the `subselect` package.  These require that the correlation matrix is well-conditioned, which is possible using the `subselect::trim.matrix` function.  This function is a less greedy method to remove perfect pair-wise correlations and relationships between three or more predictors.  



```{r bloodBrain_trim_for_anneal_and_genetic, collapse = TRUE}
library(subselect)
ncol(bbb_cor_raw)
trimmed <- trim.matrix(bbb_cor_raw, tolval=1000*.Machine$double.eps)$trimmedmat
ncol(trimmed)
```

The Simulated Annealing algorithm seeks a k-variable subset and requires that the following variables are specified: 

* kmin: Smallest subset that is wanted.  Here kmin is set to the number of parameters (18) determined using the `caret::findCorrelation` solution at a threshold of 40%.
* kmax: Largest subset that is wanted (=kmin by default).  


```{r bloodBrain_anneal}
set.seed(702)
sa <- anneal(trimmed, kmin = 18, niter = 1000)
saMat <- bbb_cor_raw[sa$bestsets[1,], sa$bestsets[1,]]
```
The required parameters for the Genetic Algorithm algorithm are the same as for Simulated Annealing.
```{r bloodBrain_genetic}
set.seed(702)
ga <- genetic(trimmed, kmin = 18, nger = 1000)
gaMat <- bbb_cor_raw[ga$bestsets[1,], ga$bestsets[1,]]
```


```{r bloodBrain_remove_corr_alg_comparison} 
# caret::findCorrelation
fcMat <- bbb_cor_raw[-removals[size == 18][[1]], 
                 -removals[size == 18][[1]]]
corrInfo(fcMat) 

# Simulated Annealing
corrInfo(saMat) 

# Genetic Algorithm
corrInfo(gaMat) 
```

The main difference between these results is that the greedy approach of `findCorrelation` is much more conservative than the techniques found in the `subselect` package. 


# Chapter 4: Over-Fitting and Model Tuning <a id="tuning"></a>

Aim: Strategies to avoid overfitting so that model can predict new samples with similar degree of accuracy as for training data.  Will use model tuning (of model parameters) to maximise model performance.  Traditionally, training set to build and tune model and test set to estimate performance.  Modern approach is to split data into multiple training and test data sets.

## The Problem of Over-Fitting
Over-fit model:

  * Has learnt noise of data
  * Poor accuracy when applied to new data set
  
**Apparent Performance**: Error rate of model when applied to the training set

## Model Tuning

**Tuning parameter**: Has no analytical formula to calculate an appropriate value, e.g. k in knn.  Often control complexity and so poor choices can lead to over-fitting.

Methods for determining tuning parameters:

* Parameter Tuning Process:
  1. Define set of candidate values for tuning parameter(s)
  2. For each candidate set:
    a. Resample data
    b. Fit model
    c. Predictor hold-outs
  3. Aggregate resampling into performance profile
  4. Determine final tuning parameters
  6. Refit model with entire training set and final tuning parameters
* Genetic Algorithms
* Simplex search methods

## Data Splitting (Single Test Set)
Not generally recommended.

Methods:

* Nonrandom; appropriate for example, if need to test on a different sample population, point in time or new phenomenon
* Simple random sample
* Stratified random sampling; apply to disproportionate classes to obtain similar distribution between training and test sets
* Maximum dissimilarity sampling; distance between predictor values for two samples
  * Initialise with single sample
  * The second sample is chosen as the most dissimilar to the first
  * The third and subsequent samples are most dissimilar to the __group__ of prior samples (e.g. average or minimum of the dissimilarity)

Variants:

* Use stratified random sampling to select $k$ partitions so that the folds are balanced with respect to the outcome.
* Leave-one-out cross-validation (LOOCV), $k$ = # samples
* Repeated $k$-fold cross-validation; can increase precision while maintaining small bias

Choice of $k$ has a bias-variance trade-off;

**Bias**: difference between estimated and true values of performance

**variance**: Repeating the resampling procedure produces a very different value

Small $k$, e.g. 2-3: Decreased variance, increased bias.  (Bias is similar to bootstrap but with larger variance) <br>
Large $k$: Increased variance, decreased bias.  More computationally taxing.

Typically use $k$ = 5 or 10.  

## Resampling Techniques

### k-Fold Cross-Validation

Method:

* Partition sample into $k$ sets (folds) of roughly equal size.
* Repeat for each fold:
  * Fit model to all samples except the fold $i$
  * Predict held-out samples to estimate performance measures
* Summarise $k$ resampled estimates of performance, e.g. mean, std. error to understand relationship between tuning parameters and model utility


### Generalised Cross-Validation
For linear regression, can use generalised cross-validation (GCV) statistic to approximate the leave-one-out error rate, 
\[\text{GCV} = \frac{1}{n}\sum^n_{i=1}\left(\frac{y_i - \hat{y}_i}{1-\text{df}/n}\right)^2\]
Models with similar precision but higher complexity will have a larger GCV value.

### Repeated Training/Test Splits (a.k.a Leave-Group-Out CV or Monte Carlo CV)
Practitioner determines percent split between training (75-80%) and test, in addition to the number of repetitions (50-200).
Can increase proportion of data in training set with increased repetitions in order to decrease bias.  Increasing repetitions will decrease uncertainty of performance estimates.  
Unlike in $k$-fold cross validation, a sample can be in multiple hold-out sets.

### The Bootstrap
**Bootstrap Sample**: Random sample of data taken with replacement.  Bootstrap sample is same size as original data set.  Prediction is made on the **out-of-bag samples**, i.e. those samples not selected.

In comparison to cross-validation, bootstrap technique has:

* Less uncertainty than $k$-fold cv
* More bias (similar to 2-fold CV); bias will decrease as training set sample size becomes large (on average 63.2% of data points represented at least once)

Modifications:

* 632 method: combined bootstrap estimate and estimate from re-predicting the training set (the apparent error rate),  error rate = .632 x simple bootstrap estimate + 0.368 x apparent error rate


## Case Study: Credit Scoring
Aim: predict probability that applicants have good credit <br>
Dataset: German credit data set <br>
Samples: 1000  <br>
Class distribution: Good(70%), Bad(30%) <br>
Baseline accuracy: Predict all to be good, such that accuracy is 70% <br>
Predictors: Credit history, employment, account status, loan amount.  Majority categorical and so converted to dummy variables.  After conversion, there were 41 predictors.  

## Choosing Final Tuning Parameters

Approach to choose final settings:

* Numerically optimal: Pick those with numerically best performance estimates.  Disadvantage: May lead to choice of an overly complicated model.
* One-standard error method: Find the numerically optical model then the simplest model whose performance is within a single standard error.
* Tolerance method: Choose simpler model that has performance ($X$) within tolerance of best ($O$), i.e. such that 
\[(X-O)/O > \text{Tolerance}\]
e.g. if best accuracy is 75% then with 4% loss in accuracy acceptable, then can choose accuracy, $X$, at $O(1 +  \text{Tolerance}) = .75*(1 - .04) = 0.72$.

## Data Splitting Recommendations

Strong case to use resampling

* Single test set has limited ability to characterise uncertainty
* Proportionally large test sets increase bias
* With small sample sizes, test set uncertainty can be considerably large

No resampling method is uniformly better.

* Small sample size; recommend 10-fold cross-validation
* Large sample size; difference becomes less pronounced and computational efficiency increases.  Again 10-fold cross-validation recommended
* To choose between models as opposed to getting best indicator of performance, consider bootstrap procedures as have low variance

There may be some (often negligible) bias for final model chosen based on tuning parameter with smallest error rate.  

## Choosing Between Models

Once tuning parameters chosen for each model, how do we determine between multiple models?

Approaches:

* Determine performance ceiling, then simplify
  * Start with least interpretable and most flexible model, e.g. boosted trees or support vector machines, to determine performance ceiling
  * Investigate simpler models that are less opaque, e.g. MARS, partial least squares, gams or naive Bayes
  * Choose the simplest model that reasonable approximates the performance ceiling
* Use resampling results to compare models
  * Use identically resampled data sets
  * Use statistical methods for pair comparisons, e.g. use paired t-test to evaluate that models have equivalent accuracy or determine mean difference in accuracy and associated confidence interval

## Computing
### Data Splitting
```{r load_twoclassdata, include=FALSE}
# Data
data(twoClassData)
str(predictors) # predictors
str(classes) # response values
```
The following functions can be used to partition the data:

* `caret::sample`: simple random split
* `caret::createDataPartition`: stratified random split (based on the classes)
* `caret::maxdissim`: maximum dissimilarity sampling

The following code performs stratified random splitting:
```{r split_twoclassdata}
set.seed(1)
trainingRows <- createDataPartition(classes, p = 0.80, list = FALSE)
# training data
trainPredictors <- predictors[trainingRows, ]
trainClasses <- classes[trainingRows]
# test data
testPredictors <- predictors[-trainingRows, ]
testClasses <- classes[-trainingRows]
str(trainPredictors); str(testPredictors)
```

### Resampling

The following functions can be used to resample the data:

* `caret::createDataPartition`: Repeated training/test splits, utilises the `times` argument
* `caret::createResamples`: Bootstrap samples
* `caret::createFolds`: k-Fold cross-validation
* `caret::createMultiFolds`: Repeated cross-validation

The following code generates three resampled versions of the training set
```{r multi_split_twoclassdata}
set.seed(1)
repeatedSplits <- createDataPartition(trainClasses, p = 0.8, times = 3)
str(repeatedSplits)
```

The following code creates indicators for 10-fold cross-validation, where each fold is $(k-1)/k%$ of the data.
```{r fold_twoclassdata}
set.seed(1)
cvSplits <- createFolds(trainClasses, k = 10, returnTrain = TRUE)
str(cvSplits)

fold1 <- cvSplits[[1]]
cvPredictors1 <- trainPredictors[fold1, ]
cvClasses1 <- trainClasses[fold1]
scales::percent(nrow(cvPredictors1)/nrow(trainPredictors))
```

### Basic Model Building in R
To fit a 5-nearest neighbour classification model to the training data can use:

* `MASS:knn`
* `ipred:ipredknn`
* `caret::knn3`

Conventions for specifying models:

1. Formula interface. e.g. `modelFunction(response_var ~ predictors, data = data)`
<br>Advantages: Convenient; can specify transformations choose subset(s) of predictors in-line
<br>Disadvantage: Formula information not efficiently stored and can slow down computations with a large number of predictors
2. Matrix (non-formula) interface, e.g.`modelFunction(x = predictors_matrix, y = response_var_vector)`.  The predictors are typically a matrix or a dataframe.

Not all R functions have both interfaces.

The following code estimated 5-nn using `caret:knn3`.
```{r train_knn_twosample}
# Fit model
trainPredictros <- as.matrix(trainPredictors)
knnFit <- knn3(x = trainPredictors, y = as.factor(trainClasses), k = 5)
knnFit
```
```{r predict_knn_twosample}
testPredictions <- predict(knnFit, newdata = testPredictors, type  = "class")
head(testPredictions)
```


#### Determination of Tuning Parameters

R Packages / Functions:

* e1071::tune - can evaluate 4 types of models across a range of parameters
* ipred::errorest can resample single models
* caret::train - built in modules for 144 models with capability for different resampling methods, performance measures, and algorithms for choosing best model profile.  

SVM tuning parameters:

* Cost parameter
* Kernel parameter $\sigma$ which impacts smoothness of the decision boundary of the kernel (which may be linear, radial basis, ...) 

Options:

1. Try several combinations of both tuning parameters
2. There is an analytical formula for a reasonable estimate of $\sigma$, use this and only tune the cost parameter. 

```{r TuneGermanCreditSVM, results='hold'}
data(GermanCredit)

GermanCredit <- GermanCredit[, -nearZeroVar(GermanCredit)]
GermanCredit$CheckingAccountStatus.lt.0 <- NULL
GermanCredit$SavingsAccountBonds.lt.100 <- NULL
GermanCredit$EmploymentDuration.lt.1 <- NULL
GermanCredit$EmploymentDuration.Unemployed <- NULL
GermanCredit$Personal.Male.Married.Widowed <- NULL
GermanCredit$Property.Unknown <- NULL
GermanCredit$Housing.ForFree <- NULL


## Split the data into training (80%) and test sets (20%)
set.seed(100)
inTrain <- createDataPartition(GermanCredit$Class, p = .8)[[1]]
GermanCreditTrain <- GermanCredit[ inTrain, ]
GermanCreditTest  <- GermanCredit[-inTrain, ]

# Basic call with 
set.seed(1056)
svmFit <- train(Class ~ .,
    data = GermanCreditTrain,
    method = "svmRadial",
    preProc = c("center", "scale"),
    tuneLength = 10,  
    trControl = trainControl(method = "repeatedcv",   # default is bootstrap
                             repeats = 5,
                             classProbs = TRUE))
svmFit

rm(inTrain)





svmFit <- train(Class ~ .,
    data = GermanCreditTrain,
    method = "svmRadial",
    preProc = c("center", "scale"),
    tuneLength = 10,  
    trControl = trainControl(method = "cv",   # default is bootstrap
                             number = 2,
                             classProbs = TRUE))


```

Line plot of the average performance profile of the SVM classification model:
```{r plotGermanCreditSVMTuneFit}
plot(svmFit, scales = list(x = list(log = 2)))
```

To predict new samples:
```{r predictGermanCreditSVMTune, echo=TRUE, results = 'hold'}
predictedClasses <- predict(svmFit, GermanCreditTest)
str(predictedClasses)
rm(predictedClasses)
```

Class probabilities
```{r GermanCreditSvmClassProbs, echo=TRUE, results = 'hold'}
predictedProbs <- predict(svmFit, newdata = GermanCreditTest, type = "prob")
head(predictedProbs)
rm(predictedProbs)
```

Other R packages/functions to estimate performance via resampling:

* Design::validate
* ipred::errorest
* e1071::tune

##### Between-Model Comparisons

Logistic regression has no tuning parameters, but resampling can still be used to characterise the performance of the model.
```{r GermanCreditsLR}
set.seed(1056)
lrFit <- train(Class ~ .,
    data = GermanCreditTrain,
    method = "glm",
    trControl = trainControl(method = "cv",  
                             number = 2))



lrFit
```


* Use `resamples` function to compare models based on cross-validation statistics
* Use same seed so get same resamples as per SVM and therefore paired accuracy measurements exist.
* First, create `resamples` object from the models

```{r modelComparisonResample}
resamp <- resamples(list(SVM = svmFit, Logistic = lrFit))
summary(resamp)
```

Visualisation of paired values:

```{r modelComparisonResamplePlot, fig.height = 12, fig.width = 8, results = 'hold', fig.show='hold'}
p1 <- xyplot(resamp)
p2 <- parallelplot(resamp)
p3 <- splom(resamp)
p4 <- densityplot(resamp)
p5 <- bwplot(resamp)
p6 <- dotplot(resamp)
p7 <- ggplot(resamp)
gridExtra::grid.arrange(p1, p3, p2, p4, p5, p6, p7, ncol = 2)
rm(list = ls(pattern = "^p\\d$")) 
```

To assess possible differences:
```{r modelComparisonResampleDiff}
modelDifferences <- diff(resamp)
summary(modelDifferences)
```
The p-values for the model comparisons are `r round(modelDifferences$statistics$Accuracy$SVM.diff.Logistic$p.value,3)` for Accuracy and `r round(modelDifferences$statistics$Kappa$SVM.diff.Logistic$p.value,3)` for Kappa.  Note that the Kappa statistic (chapter 11:) was originally designed to assess the agreement between 2 raters 
\[\text{Kappa} = \frac{O-E}{1-E}\]
where $O$ = observed accuracy and $E$ is expected accuracy based on marginal totals of the confusion matrix.  Statistic takes values between -1 and 1; 

* 0 :no agreement between observed and predicted
* 1: perfect concordance
* -1: prediction is perfect opposite direction of truth

When class distributions equivalent, overall accuracy and Kappa are proportional.  Kappa values within 0.30 to 0.5 indicate reasonable agreement.

With high accuracy (90%) and high expected accuracy (85%), Kappa will show moderate agreement between observed and predicted classes
\[\text{Kappa} = \frac{90-85}{1-85} = 5/15 = 30%\]

Kappa statistic can be extended to evaluate concordance in problems with > 2 classes.  With ordinal classes, can use weighted Kappa to enact more substantial penalties to errors further away from true result.

```{r tuning_clean_up, include=FALSE}
rm(GermanCredit, GermanCreditTest, GermanCreditTrain, lrFit, modelDifferences, resamp, svmFit)
```



## Chapter 4 Exercises

### Exercise 4.1
Consider the music genre data set described in Sect. 1.4. The objective
for these data is to use the predictors to classify music samples into the
appropriate music genre.
(a) What data splitting method(s) would you use for these data? Explain.
(b) Using tools described in this chapter, provide code for implementing your
approach(es).


### Exercise 4.2
Consider the permeability data set described in Sect. 1.4. The objective
for these data is to use the predictors to model compounds’ permeability.
(a) What data splitting method(s) would you use for these data? Explain.
(b) Using tools described in this chapter, provide code for implementing your
approach(es).

### Exercise 4.3
Partial least squares (Sect. 6.3) was used to model the yield of a chemical
manufacturing process (Sect. 1.4). 

The objective of this analysis is to find the number of PLS components
that yields the optimal R2 value (Sect. 5.1). PLS models with 1 through 10
components were each evaluated using five repeats of 10-fold cross-validation
and the results are presented in the following table:
(a) Using the “one-standard error” method, what number of PLS components
provides the most parsimonious model?
(b) Compute the tolerance values for this example. If a 10% loss in R2 is
acceptable, then what is the optimal number of PLS components?
(c) Several other models (discussed in Part II) with varying degrees of complexity
were trained and tuned and the results are presented in Fig. 4.13.
If the goal is to select the model that optimizes R2, then which model(s)
would you choose, and why?
(d) Prediction time, as well as model complexity (Sect. 4.8) are other factors
to consider when selecting the optimal model(s). Given each model’s prediction
time, model complexity, and R2 estimates, which model(s) would
you choose, and why?

```{r}
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
```


### Exercise 4.4
Brodnjak-Vonina et al. (2005) develop a methodology for food laboratories
to determine the type of oil from a sample. In their procedure, they used
a gas chromatograph (an instrument that separate chemicals in a sample) to
measure seven different fatty acids in an oil. These measurements would then
be used to predict the type of oil in a food samples. To create their model,
they used 96 samples2 of seven types of oils.
These data can be found in the caret package using data(oil). The oil
types are contained in a factor variable called oilType. The types are pumpkin (coded as A), sunflower (B), peanut (C), olive (D), soybean (E), rapeseed (F)
and corn (G). 

(a) Use the sample function in base R to create a completely random sample
of 60 oils. How closely do the frequencies of the random sample match
the original samples? Repeat this procedure several times of understand
the variation in the sampling process.
(b) Use the caret package function createDataPartition to create a stratified
random sample. How does this compare to the completely random samples?

(c) With such a small samples size, what are the options for determining
performance of the model? Should a test set be used?
(d) One method for understanding the uncertainty of a test set is to use a
confidence interval. To obtain a confidence interval for the overall accuracy,
the based R function binom.test can be used. It requires the user
to input the number of samples and the number correctly classified to
calculate the interval. For example, suppose a test set sample of 20 oil
samples was set aside and 76 were used for model training. The width of the 95% confidence interval is 37.9%. Try different samples sizes and accuracy rates to understand the trade-off
between the uncertainty in the results, the model performance, and the
test set size.



# Measuring Performance in Regression Models

* Relying on single performance measure problematic
* Use of visualisations critical


## Quantitative Measures of Performance

* **RMSE**: How far (on average) residuals are from zero $\equiv$ average distance between observed values and model predictions
* **Coefficient of Determination ($R^2$)**: Proportion of variation in data explained by model; usually equal to squared value of correlation between observed and predicted values. 
    * Measure of correlation not accuracy.  Looking at plot of observed vs predicted values may show that is under- or over- predicts for subsets of the data
    * Dependent on sample variance (and therefore unit of measurement) of response variable , e.g. with RMSE = 1 and $s^2 = 4.2$ gives $R^2 = 1 - RMSE/s^2$ = `r scales::percent(1 - 1/4.2)`, but with $s^2 = 3$, the RSE is `r scales::percent(1 - 1/2)` 
* **Spearman's Rank Correlation**: Correlation coefficient between ranks of observed and predicted outcomes, when focus is on ranking ability of model rather than predictive accuracy.

## Variance-Bias Trade-Off

$$\begin{aligned}
\text{MSE} &= \frac{1}{n}\sum_{i = 1}^n (y_i - \hat{y}_i)^2 \\
  E(\text{MSE}) &= \sigma^2 (\text{Model Bias})^2 + \text{Model Variance}
\end{aligned}$$

* Irreducible noise ($\sigma^2$); cannot be eliminated by modelling.  Based on assumption that $e_i \sim ?(0, \sigma^2)$
* Bias(-squared): Closeness of model to true relationship between predictors and response
* Model Variance: Changes to model fit with small perturbations in data

**Overfitting**: Complex models usually have low bias and high variance

**Underfitting**: Simple models generally have high bias and low variance 

Note: Highly correlated predictors can lead to collinearity and therefore high model variance.  To mitigate, may be ideal to increase bias.

### Computing
```{r perf_load_data}
observed <- c(0.22, 0.83, -0.12, 0.89, -0.23, -1.30, -0.15, -1.4,0.62, 0.99, -0.18, 0.32, 0.34, -0.30, 0.04, -0.87,
0.55, -1.30, -1.15, 0.20)
predicted <- c(0.24, 0.78, -0.66, 0.53, 0.70, -0.75, -0.41, -0.43,0.49, 0.79, -1.19, 0.06, 0.75, -0.07, 0.43, -0.42, -0.25, -0.64, -1.26, -0.07)

residualValues <- observed - predicted
summary(residualValues)
```

Plot of observed vs predicted values: to understand how model fits
```{r perf_plot_obs_pred}
ggplot(data.frame(observed, predicted), aes(observed, predicted)) + 
  geom_point() + 
  expand_limits(
    x =  c(min(observed, predicted), max(observed, predicted)),
    y = c(min(observed, predicted), max(observed, predicted))) + 
  geom_abline(intercept = 0, slope = 1)
```

Plot of residuals vs predicted values: to check for any systematic patterns in predictions
```{r perf_plot_pred_resid}
ggplot(data.frame(predicted, residualValues), aes(predicted, residualValues)) + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 0)
```

Coefficient of Determination: 
```{r perf_R2, results='hold'}
caret::R2(predicted, observed)
cor(predicted, observed)^2
```

Root Mean Square Error:
```{r perf_RMSE, results='hold'}
caret::RMSE(predicted, observed)
sqrt(mean(residualValues^2))
```
Rank Correlation:
```{r perf_spearman}
cor(predicted, observed, method = "spearman")
```

```{r perf_cleanup, include=FALSE}
rm(observed, predicted, residualValues)
```


# Linear Regression and Its Cousins

Form:
\[y_i = b_0 + b_1x_{i1} + b_2x_{i2} + \ldots + b_P x_{iP} + e_i\]

Models that are linear in the parameters:

* Linear regression
* Partial least squares (PLS)
* Penalised models:
    * Ridge regression
    * The lasso
    * Elastic net
    
Modelling objective: Estimate parameters such that SSE minimised.

* Linear regression - minimises bias, higher model variance
* Penalised model - lower model variance

Advantage of linear models:

* Interpretable
* Relationships between predictors can be interpreted through coefficients ???
* Can compute standard errors of coefficients (if willing to make assumptions regarding the distribution of model residuals) and therefore assess statistical significance of the predictors.  However inference is not goal of this book, but rather model prediction

Disadvantage:

* Only appropriate when relationship between predictors and response falls along a hyperplane; although can augment with higher order terms to capture curvature but may not be appropriate (in which case, nonlinear regression models and regression trees should be investigated)


    
## Linear Regression

Aim: Find plane that minimizes SSE between observed and predicted response:
\[\text{SSE} = \sum^n_{i=1}(y_i-\hat{y}_i)^2\]

Ordinary Least Squares: Optimal plane that minimises the bias component of the bias-variance trade off, given by:
\[(X^T X)^{-1} X^T y\]

For proof and details around following theorem, see [OLS in Matrix Form, by Prof. Michael J Rosenfeld (Stanford University)](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf)

Gauss-Markov Theorem: Given the following assumptions, there will be no other linear and unbiased estimated of the coefficients that has a smaller sampling variance.  I.e., the OLS estimates or the Best Linear, Unbiased and Efficient estimated (BLUE).  Assumptions:

* There is a linear relationship between $y$ and $X$
* Identification condition: There is no perfect multicollinearities, i.e. the columns of $X$ are linearly independent
* Zero conditional mean assumption: Disturbances average out to 0 for any value of $X$.  
\[E(\epsilon|X) = 0\]
Implies that $E(y) = X\beta$ and therefore that get the mean function right
* Assumption of homoskedasticity and no autocorrelation, $E(\epsilon \epsilon^\prime|X) = \sigma^2I$.  Homoskedasticity states that the variance of $\epsilon_i$ is the same ($\sigma^2$) for all $i$, i.e., $\text{var}[\epsilon_i|X] = \sigma^2 \forall i$.  The assumption of no autocorrelation (uncorrelated errors) means that $\text{cov}[\epsilon_i, \epsilon_j|X] = 0 \forall i,j$, i.e. knowing something about the residual term for one observation tells nothing about the residual term for any other observation.
* $X$ must be generated by a mechanism that is unrelated to $\epsilon$
* Not required but typically assumed to make hypothesis testing easier: \[\epsilon|X \sim N(0, \sigma^2I)\]

Training and validation techniques required to understand predictive ability of data, even though there are no tuning parameters.

Disadvantages of linear regression:

##### 1.  Requires the following conditions for unique set of regression coefficient,
noting that optimal plane includes the term $(\mathbf{X}^T\mathbf{X})^{-1}$)

  a. No perfect multicollinearities (no predictor can be determined from a combination of $\geq$ ` other predictors).  If not, can:
    
   * Replace $(\mathbf{X}^T\mathbf{X})^{-1}$ with conditional inverse (???)
   * Remove predictors that are collinear.  By default, $R$ fits the largest identifiable model by removing variables the reverse order of appearance in the model formula.  
        
   Impact: Regression coefficients to determine predictions are not unique and therefore lose ability to meaningfully interpret coefficients.

   Solutions: 
   
   * With small number of predictors, can manually remove offending predictors.  But with many predictors or those with complex relationship, may be better to choose a model that can tolerate collinearity.   Additionally, linear combinations of predictors may still be correlated with other predictors
   * Use PCA, however new predictors will be linear combinations of original predictors which can impede practical understanding.  Note that this is known as principal component regression (PCR); see below..
   
  b.# samples > # predictors.  If not, 
    
* Use pre-processing techniques to remove pairwise correlated predictors and therefore reduce number of overall predictors
* If problem remains, consider PCA, simultaneous dimension
    reduction, regression via PLS or methods that shrink parameter
    estimates (ridge regression, the lasso, or the elastic net).
 
* Beware with Bootstrapping/cross-validation: Resampling may lead to less observations in training data than number of predictors.   
    
Note: After removing pairwise correlated predictors, regression may still be impacted by multicollinearity (which is when one ore more of the predictors are functions of $\geq$ 2 other predictors). 
        
The article [What Are the Effects of Multicollinearity and When Can I Ignore Them?](http://blog.minitab.com/blog/adventures-in-statistics-2/what-are-the-effects-of-multicollinearity-and-when-can-i-ignore-them) states the following:
    
* Moderate multicollinearity may not be problematic
* Severe multicollinearity is because it can increase the variance of the coefficient estimates and make the estimates very sensitive to minor changes in the model.  This makes the coefficient estimates unstable and difficult to interpret.  It can cause the coefficients to switch signs and makes it more difficult to specify the correct model.  
    
In short the article states that, multicollinearity

* Can make choosing the correct predictors more difficult
* Interferes in determining precise effect of each predictors
* Doesn't affect overall fit of the model or produce bad predictions. (so depending on goals, multicollinearity may not be a problem)
    
Multicollinearity can be diagnosed by investigating the variance inflation factors.  High VIFs (>5) suggest that coefficients are poorly estimated and that one should be wary of their p-values.  
        
Note:Higher-order terms (squared and cubed predictors) are correlated with main effect terms because they include the main effects term.  To reduce high VIFs produced by interaction and higher-order terms, you can standardise the continuous predictor variables. Centering the variables will remove the multicollinearity produced by interaction and higher-order terms and has the  added benefit of not changing the interpretation of coefficients; the coefficients continue to estimate the change in the mean response per unit increase in $X$ when all other predictors are held constant.
    
##### 2.  Solution is linear in parameters and therefore does not account for curvature or nonlinear structures.    

To identify, examine diagnostic plots, looking for curvature in predicted-versus-residual plot. Can then include quadratic , cubic and interactions, but may make data matrix have more predictors than observations and matrix won't be invertible.

##### 3. Impacted by "outliers" (_influencers_). 
OLS aims to minimise SSE; outliers will have exponentially large residuals and so linear regression will adjust parameter estimates to better accommodate the outliers.  

Robust regression has been developed to address these kind of issues:

* Use alternative metric to SSE that is less sensitive to large outliers, e.g. find parameter estimates that minimise the sum of absolute errors, or use Huber function that uses squared residuals when they are small and  the residuals themselves when above a threshold.  




## Principal Component Regression

Two step regression:

1. Dimension reduction via PCA
2. Regression on PCA components

Advantages:

* Components uncorrelated
* If variability in predictor space related to response variability then PCR has good change of identifying predictive relationship

Disadvantages:

* Not recommended; use instead PLS when there are correlated predictors and a linear regression-type solution desired
* New predictors will be linear combinations of original predictors which can impede practical understanding
* Components may not explain response variable; i.e. components are not selected with response in mind.  

## Partial Least Squares 


Based on nonlinear iterative partial least squares (NIPALS) algorithm:

Iteratively seeks to find underlying, or latent, relationships among predictors which are highly correlated with response.  For univariate response, each iteration:

* Assesses relationship between predictors $\mathbf{X}$ and response $\mathbf{y}$ and summarises with a vector of weights $\mathbf{w}$ (directions)
* Orthogonally projects predictor data  onto the direction to generate scores $\mathbf{t}$ 
* Scores used to generate loadings $\mathbf{p}$, which measure the correlation of the score vector to the original predictors
* Deflates predictors and the response by subtracting current estimate of predictor and response, respectively.  These are then used to generate next set of weights, scores and loadings.

PLSR | PCR
--------------------------------------| -----------------------------------------------
Simultaneously summarises variation of predictors while being optimally correlated with the outcome | Finds linear combinations of predictors to maximally summarise predictor space variability
Supervised dimension reduction | Unsupervised procedure

Preprocessing required for PLS:  Centre and scale; otherwise will be draw towards predictors with large variation

Tuning parameters: Number of components to retain; can be determined using resampling techniques.

Variable importance in the projection for $j$th predictor:
\[\text{VIP} = \sqrt{\frac{\sum^h_{k=1}R^2(y,t_k)(w_{kj}/||w_k||)^2}{(1/p)\sum^h_{k=1}R^2(y,t_k)}},\]

where 

* $p$ = # predictor variables 
* $h$ = # Components
* $w_{kj}$ = Weight of $j$th predictor variable in component $k$
* $R^2(y,t_k)$ = response variation explained by component $k$
* Numerator: Weighted sum of normalised weights corresponding to the $j$th predictor, where the $j$th normalised weight is scaled by the amount of variation in the response explained by the $k$th component
* Denominator: Total amount of response variation explained by all $k$ components
* The larger the normalised weight and amount of response variation explained by the component, the more important the predictor

Rule of thumb: Variables with VIP > 1 contain predictive information.
Variables with small PLS regression coefficients and small VIP values not likely important and should be considered as candidates for removal from the model

### Algorithmic Variations of PLS

#### Alternatives to address computational inefficiencies:

With $\geq$ 2500 samples or $\geq$ 20 predictors NIPALS becomes inefficient.  As both predictor matrix and response deflated, an $n \times P$ matrix and an $n \times 1$ vector must be recomputed, operated on, and stored in each iteration

Alternative approaches:

* kernel matrix approach
* SIMPLS
* etc


#### Alternatives to consider nonlinearity
Recall: PLS components summarise data through linear hyperplanes of original predictor space.  Can allow for non-linearity by:

* Augmenting predictors with squared / cubic predictors.  Note: no need to add cross product terms
* Using GIFI approach which bins predictors thought to have a nonlinear approach (based on prior knowledge or characteristics of data.

However: Other predictive modeling technique can more naturally identify nonlinear structures (and are recommended) without having to modify predictor space.  

## Penalised (a.ka.a Shrinkage) Models
OLS coefficients unbiased and of all unbiased linear techniques will have lowest variance.  However may be possible to produce model with smaller SME by sacrificing some bias for a much more substantial drop in variance.  Predictors with large correlations -> large variance, can combat collinearity using biased models.

### Ridge Regression
To create bias, add penalty to SSE if estimates become large.  

\[\text{SSE}_{L_2} = \sum^n_{i=1}(y_i-\hat{y}_i)^2 + \lambda \sum^P_{j=1}\beta_j^2\]

$L_2$: Square (second-order penalty) being used; parameter estimates can only get large if proportional reduction in SSE.  Method shrinks estimates towards 0 as $\lambda$ penalty becomes large. Cross validation is used to optimise the penalty value.  

Model does not perform feature selection; i.e. no parameter estimate will be set to 0

Plots:

1. Penalty vs standardized coefficients; as the penalty increases the standardized coefficients tend towards zero

2. Fraction of full solution vs standardized coefficients.

Fraction of full solution: Amount that the ridge regression coefficient estimates have been shrunken towards zero; small value indicates they have been shrunken very close to zero.
\[||\hat{\beta}_\lambda^R||/||\hat{\beta}||_2\]
where:

* $\hat{\beta}_\lambda^R$ = ridge regression coefficients
* $\hat{\beta}$ =least squares coefficient estimates
* $||\beta||_2 = \sqrt{\sum^P_{j=1}\beta_j^2}$ is the $\ell_2$ norm, measures the distance of $\beta$ from zero.  

As $\lambda$ increases, the $\ell_2$ norm of $\hat{\beta}_\lambda^R$ will decrease and so will the fraction of the full solution.

The fraction of full solution ranges form 1 (when $\lambda = 0$ when ridge regression coefficient = OLS) to 0 (when $\lambda = \infty$)

### Lasso (Least Absolute Shrinkage and Selection Operator Model)

\[\text{SSE}_{L_1} = \sum^n_{i=1}(y_i-\hat{y}_i)^2 + \lambda \sum^P_{j=1}|\beta_j|\]

Lasso Coefficient Path: Trace plot showing the relationship between the penalty and the standardised coefficients; as the penalty  

The lasso has been extended to:

* Linear discriminant analysis
* PLS
* PCA

### Lasso vs Ridge

Ridge                            | Lasso 
---------------------------------| --------------------------------------
L2 Norm                          | L1 Norm
Coefficients are not set to zero | Performs features selection
Shrinks coefficients of correlated predicted towards each other | Tends to pick one of the correlated predictors and ignore the rest

Why Lasso Results in Feature Selection, but Not Ridge:

Optimisation formulations:

Ridge:
\[\matrix{\text{minimize}\\\beta} \left\{ \sum^n_{i=1} \left(y_i - \beta_0 - \sum^p_{j=1}\beta_j x_{ij}\right)^2 \right\} \;\;\;\text{subject to }\sum^p_{j=1}\beta_j^2 \leq s\]

Lasso:
\[\matrix{\text{minimize}\\\beta} \left\{ \sum^n_{i=1} \left(y_i - \beta_0 - \sum^p_{j=1}\beta_j x_{ij}\right)^2 \right\} \;\;\;\text{subject to }\sum^p_{j=1}|\beta_j| \leq s\]

i.e., $\forall \lambda$ there is a value of $s$ that will give the same ridge/lasso coefficient estimates.

When $p = 2$,

* Ridge: smallest RSS out of all points lies within diamond defined by $|\beta_1| + |\beta_2| \leq s$.
* Ridge: smallest RSS out of all points lies within circle defined by $\beta^2_1 + \beta^2_2 \leq s$.

When $s$ is large then constraint region is also large and so coefficient estimates can be large.  

In the following chart:

* $\hat{\beta}$ is the least squared solution
* The blue circle is the ridge regression constraint
* The black diamond is the lasso regression constraint
* Each quadrant relates to a different regression problem
* The ellipses in each quadrant are the contours of the residual sum of squares (RSS); all points on a given ellipse share a common value of the RSS.  As the ellipses expand away from $\hat{\beta}$, the RSS increases.
* The lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region.

Ridge regression constraint is circular and no sharp points and so intersection will not generally occur on an axis.  
The lasso constraint has corners and so will often intersect at the corner (and therefore one of the coefficients will equal zero).  

![Lasso/ Ridge Optimisation](LassoVsRidge.JPG)


Further reasoning as to why Lasso leads to feature selection, as per [Cross Validated, Why does the Lasso provide Variable Selection?](https://stats.stackexchange.com/questions/74542/why-does-the-lasso-provide-variable-selection/74569): 

Consider the model $y = \beta x + e$, with an L1 penalty on $\hat{\beta}$ and a least-squares loss function on $\hat{e}$.  We can expand the expression to be minimized as:

\[\min y^Ty -2 y^Tx\hat{\beta} + \hat{\beta} x^Tx\hat{\beta} + 2\lambda|\hat{\beta}|\]

Assume the least-squares solution is some $\hat{\beta} > 0$, which is equivalent to assuming that $y^Tx > 0$, and see what happens when we add the L1 penalty.  With $\hat{\beta}>0$, $|\hat{\beta}| = \hat{\beta}$, so the penalty term is equal to $2\lambda\beta$.  The derivative of the objective function w.r.t. $\hat{\beta}$ is:

\[-2y^Tx +2x^Tx\hat{\beta} + 2\lambda\]  

which evidently has solution $\hat{\beta} = (y^Tx - \lambda)/(x^Tx)$.  

Obviously by increasing $\lambda$ we can drive $\hat{\beta}$ to zero (at $\lambda = y^Tx$).   However, once $\hat{\beta} = 0$, increasing $\lambda$ won't drive it negative, because, writing loosely, the instant $\hat{\beta}$ becomes negative, the derivative of the objective function changes to:

\[-2y^Tx +2x^Tx\hat{\beta} - 2\lambda\]

where the flip in the sign of $\lambda$ is due to the absolute value nature of the penalty term; with $\hat{\beta}< 0$, $|\hat{\beta}|\neq \hat{\beta}$. As the derivative is w.r.t. to $\hat{\beta}$ the sign flips on that term; not on the $-2y^Tx\hat{\beta}$ term because that term includes $\hat{\beta}$ not $|\hat{\beta}|$. 

So when $\beta$ becomes negative, the penalty term becomes equal to $-2\lambda\beta$, and taking the derivative w.r.t. $\beta$ results in $-2\lambda$.  This leads to the solution $\hat{\beta} = (y^Tx + \lambda)/(x^Tx)$, which is obviously inconsistent with $\hat{\beta} < 0$ (given that the least squares solution $> 0$, which implies $y^Tx > 0$, and $\lambda > 0$).  There is an increase in the L1 penalty AND an increase in the squared error term (as we are moving farther from the least squares solution) when moving $\hat{\beta}$ from $0$ to $ < 0$, so we don't, we just stick at $\hat{\beta}=0$.

It should be intuitively clear the same logic applies, with appropriate sign changes, for a least squares solution with $\hat{\beta} < 0$.  

With the least squares penalty $\lambda\hat{\beta}^2$, however, the derivative becomes:

\[-2y^Tx +2x^Tx\hat{\beta} + 2\lambda\hat{\beta}\]

which evidently has solution $\hat{\beta} = y^Tx/(x^Tx + \lambda)$.  Obviously no increase in $\lambda$ will drive this all the way to zero.  So the L2 penalty can't act as a variable selection tool without some mild ad-hockery such as "set the parameter estimate equal to zero if it is less than $\epsilon$".  

Obviously things can change when you move to multivariate models, for example, moving one parameter estimate around might force another one to change sign, but the general principle is the same: the L2 penalty function can't get you all the way to zero, because, writing very heuristically, it in effect adds to the "denominator" of the expression for $\hat{\beta}$, but the L1 penalty function can, because it in effect adds to the "numerator".  


Source:

* An Introduction to Statistical Learning with Applications in R, Casella, Fienberg and Olkin
* [Regularization in Machine Learning: Connect the dots](https://towardsdatascience.com/regularization-in-machine-learning-connecting-the-dots-c6e030bfaddd)
* [Cross Validated, Why does the Lasso provide Variable Selection?](https://stats.stackexchange.com/questions/74542/why-does-the-lasso-provide-variable-selection)



### Least Angle Regression (LARS)
Broad framework (that envelopes lasso) that can be used to fit lasso models more efficiently

### Elastic Net
Combines ridge and lasso:
\[\text{SSE}_{\text{E}_\text{net}} = \sum^n_{i=1}(y_i-\hat{y}_i)^2 + \lambda_1 \sum^P_{j=1}\beta^2_j + \lambda_2 \sum^P_{j=1}|\beta_j|\]

Advantage:

* Enables regularisation via ridge-type penalty
* Feature selection via lasso penalty

Both penalties require tuning.



## Computing - Case Study: Quantitative Structure-Activity Relationship Modeling

Solubility Data:

* Predictors for test and training test sets in data frames called `solTrainX` and `solTestX`
* Alternative versions that have been Box-Cox transformed are in the data frames `solTrainXTrans` and `solTestXTrans`
* Solubility (response value) for each compound contained in `solTrainY` and `solTestY`

```{r loadSolubilityData, include=FALSE}
data(solubility)
# data objects
# ls(pattern = "^solT")
```

* Each column of data corresponds to a predictor and the rows correspond to compounds.  There are `r ncol(solTrainX)` columns in the data.  Random sample of column names below:
```{r sampleSolubilityPredictors, echo=FALSE, results='asis'}
set.seed(2)
cat(paste('\t*', 
          sample(names(solTrainX), 8)
          ), sep = '\n')
```

```{r solubilityFingers, include=FALSE}
### Find the columns that are not fingerprints (i.e. the continuous
### predictors). grep will return a list of integers corresponding to
### column names that contain the pattern "FP".
Fingerprints <- grep("FP", names(solTrainXtrans))
SolubilityCounts <- grep("Num", names(solTrainXtrans))
names(solTrainXtrans)[Fingerprints]
names(solTrainXtrans)[SolubilityCounts]
names(solTrainXtrans)[-c(Fingerprints, SolubilityCounts)]
```

Predictors:

* `r length(Fingerprints)` binary descriptors
* `r length(SolubilityCounts)` count descriptors
* `r length(solTrainXtrans[-c(Fingerprints, SolubilityCounts)])` continuous descriptors

Sample size: `r scales::comma(nrow(solTrainX) + nrow(solTestX))`

* Data has been split into training and test sets:

    * Training Set (n =  `r scales::comma(nrow(solTrainX))`
        * To tune and estimate models
        * To determine initial estimates of performance using repeated 10-fold cross-validation
    * Test set n =  `r scales::comma(nrow(solTestX))`): For final characterisation of models of interest
    
**Data Exploration**

```{r solubility_correlation}
res <- cor(rbind(solTrainX, solTestX))
res[lower.tri(res, diag = TRUE)] <- NA
res <- as.data.frame(res) %>%
    rownames_to_column(var = "VariableA") %>%
    gather(key = "VariableB", value = "Correlation", -VariableA) %>%
    filter(!is.na(Correlation)) %>%
    mutate(AbsCorrelation = abs(Correlation)) %>%
    arrange(desc(AbsCorrelation))

# sum(res$Correlation > 0.9)

```

On average, descriptors are uncorrelated (average absolute correlation = `r round(mean(res$AbsCorrelation),2)`), however there are `r sum(res$Correlation > 0.9)` pairs of descriptors with a correlation greater than 0.9.   One pair of descriptors (SurfaceArea1 and SurfaceArea2) are identical for `r scales::percent(with( solTrainX, sum(SurfaceArea1 == SurfaceArea2)/(nrow(solTrainX) )))` of compounds, but small differences can contain certain information for prediction.

```{r removeSolTrainBindTest, include=FALSE}
rm(res)
```


The relationship between Solubility and 

a. Molecular Weight; as molecular weight increases, solubility generally decreases
b. `r names(solTrainX)[100]`
```{r solubilityRelationshipEgs, message=FALSE, warning=FALSE, results = 'hold'}

library(lattice)
library(corrplot)

### Some initial plots of the data

p1 <- xyplot(solTrainY ~ solTrainX$MolWeight, type = c("p", "g"),
       ylab = "Solubility (log)",
       main = "(a)",
       xlab = "Molecular Weight")
# xyplot(solTrainY ~ solTrainX$NumRotBonds, type = c("p", "g"),
#        ylab = "Solubility (log)",
#        xlab = "Number of Rotatable Bonds")
p2 <- bwplot(solTrainY ~ ifelse(solTrainX[,100] == 1, 
                          "structure present", 
                          "structure absent"),
       ylab = "Solubility (log)",
       main = "(b)",
       horizontal = FALSE)
gridExtra::grid.arrange(p1, p2, nrow = 1)
rm(p1, p2)
```
Skewness; recall that variables are considered skewed if max/min > 20 or if |skewness statistic| much greater than zero. 

```{r solubilitySkewness}
res <- solTrainX %>%
  select(-starts_with("FP")) %>%
  summarise_all(funs(e1071::skewness, n_distinct, max, min)) %>%
  gather(key="key", value="value") %>%
  extract(key, c("variable", "metric"), "(.*)_(e1071::skewness|n_distinct|max|min)") %>%
    mutate(metric = ifelse(metric == "e1071::skewness", "skewness", metric)) %>%
  spread(metric, value) %>%
  mutate(max_min_ratio = ifelse(min==0, (max+1)/(min+1), max/min)) %>%
  select(variable, n_distinct, skewness, max_min_ratio, min, max) %>%
    mutate(AbsSkewness = abs(skewness)) 
```

The mean skewness of continuous variables in training set was `r round(mean(res$AbsSkewness),1)` (minimum `r round(min(res$AbsSkewness),1)`), maximum `r round(max(res$AbsSkewness),1)`), indicating variables have tendency to be right-skewed.


```{r solubilitySkewnessCleanUp, include=FALSE}
rm(res)
```


Box-Cox transformation were therefore applied to all predictors.  

To check for linear relationships between predictors and solubility, scatter plots with regression line used which suggests both linear, e.g. MolWeight,  and non-linear, e.g., NumChlorine, relationships exist (may want to include some quadratic terms)

```{r solubilityScatters, fig.height=10, message=FALSE, warning=FALSE}
featurePlot(solTrainXtrans[, -Fingerprints],
            solTrainY,
            between = list(x = 1, y = 1),
            type = c("g", "p", "smooth"),
            labels = rep("", 2))
```

To check for significant between-predictor correlations, PCA used on full set of transformed predictors.

<!--Errata: Figure 6.4, not cumulative-->
```{r solubilityPCA}
solPCA <- prcomp(solTrainXtrans, center = TRUE, scale = TRUE)
#pctVar <-  solPCA$sdev^2/sum(solPCA$sdev^2)*100
#head(pctVar)

tab <- summary(solPCA)$importance[2,]
tab <- data.frame(PC = names(tab), Var = tab, row.names=NULL, stringsAsFactors = FALSE) %>% mutate(PC=parse_number(PC))


ggplot(tab, aes(x = PC, y=Var))+ 
         geom_line() + 
    scale_y_continuous(name = "% Variance", labels = scales::percent) +
    xlab("Components")

rm(tab, solPCA)
```


Scree plot shows that amount of variability drops sharply, with no one component accounting for more than 13% of variance.  This indicates that the structure of the data is contained in a much smaller number of dimensions than the number of dimensions in the original space; often due to large number of collinearities in the predictors.  
Collinearity can create problems in developing some models (e.g. linear regression) and will require appropriate pre-processing steps.

The correlation structure is shown for the transformed continuous predictor and shows many strong positive correlations.

```{r solubilityCorrplot}
### Full namespace used to call this function because the pls
### package (also used in this chapter) has a function with the same
### name.
corrplot::corrplot(cor(solTrainXtrans[, -Fingerprints]), 
                   order = "hclust", 
                   tl.cex = .8)
```

### Ordinary Linear Regression 

* Standard function `lm` requires predictors and response variable in same dataframe

```{r fitLmToSolTrain}
# all predictors
trainingData <- solTrainXtrans
trainingData$Solubility <- solTrainY
lmFitAllPredictors <- lm(Solubility ~ ., data = trainingData)
#summary(lmFitAllPredictors)
# RMSE
#sqrt(mean(lmFitAllPredictors$residuals^2))

# R^2
#summary(lmFitAllPredictors)$r.squared

```




Training set performance: 

* RMSE = `r round(sqrt(mean(lmFitAllPredictors$residuals^2)),3)` 
* $R^2$ = `r  round(summary(lmFitAllPredictors)$r.squared,3)`

Can predict new samples using `predict` function:
```{r predictLMToSolTest}
lmPred1 <- predict(lmFitAllPredictors, solTestXtrans)
head(lmPred1)
```

Test set performance estimated using `caret::defaultSummary`:
```{r LMSolPerformance}
lmValues1 <- data.frame(obs = solTestY, pred = lmPred1)
defaultSummary(lmValues1)
```





**Resampling estimate of performance**

* With ~1000 training samples, should be sufficient to use 10-fold cross-validation as opposed to repeated cross-validation in order to obtain reasonable estimates of model performance.  Use the function `trainControl` to specify type of resampling:

```{r lmSolTrainPerf}

### Create a control function that will be used across models. We
### create the fold assignments explicitly instead of relying on the random number seed being set to identical values.
set.seed(100)
indx <- createFolds(solTrainY, returnTrain = TRUE)
ctrl <- trainControl(method = "cv", index = indx)
rm(indx)

#ctrl <- trainControl(method = "cv", number = 10)

set.seed(100)
lmFit1 <- train(x = solTrainXtrans, y = solTrainY, method = "lm", trControl = ctrl)
lmFit1


```

* For models built to explain, it is important to check model assumptions, such as the residual distribution.
* For predictive models, some of the same diagnostic techniques can highlight where model is not predicting well.



Plot of residuals vs predicted values:
```{r plotlmSolTrainResidPred}
## plot the points (type = 'p') and a background grid ('g')
xyplot(resid(lmFit1) ~ predict(lmFit1), type = c("p", "g"), xlab = "Predicted", ylab = "Residuals")
```

* A random cloud of points should provide comfort that no major terms (e.g.. quadratics) missing from model
* `resid` function generates model residuals for training set
* `predict` function without additional dataset generates predicted values for training set


Predicted vs observed values to assess how close predictions are to the actual values:
```{r plotlmSolTrainObsPred}
xyplot(solTrainY ~ predict(lmFit1), type = c("p", "g"), xlab = "Predicted", ylab = "Observed")
```

Basic diagnostic tests do not suggest any bias in prediction; distribution between predictive values and residuals appear to be random about zero.


Can remove highly correlated predictors and build smaller model:

Identify predictors that have high pairwise correlation and remove predictors so that no absolute pairwise correlation is greater than some pre-specified level (e.g. 0.9).  Note: In practice the threshold would need to be be smaller , however in this instance it would also remove important variables.  The section on Feature Selection will further discuss this data and how to investigate how terms fit into the model, interactions and nonlinear transformations.

<!-- # Errata here: SolTrainY in book not solTestY -->
```{r solTrainXCorrelations, results = 'hold'}

### Linear regression model with all of the predictors. This will produce some warnings that a 'rank-deficient fit may be  misleading'. This is related to the predictors being so highly correlated that some of the math has broken down.
# set.seed(100)
# lmTune0 <- train(x = solTrainXtrans, y = solTrainY,
#                 method = "lm",
#                 trControl = ctrl)
# lmTune0 

### Now use a set of predictors reduced by unsupervised filtering; apply a filter to reduce extreme between-predictor correlations and note the lack of warnings.

corThresh <- .9
tooHigh <- findCorrelation(cor(solTrainXtrans), corThresh)
corrPred <- names(solTrainXtrans)[tooHigh]
trainXfiltered <- solTrainXtrans[, -tooHigh]
testXfiltered <- solTestXtrans[, -tooHigh]

set.seed(100)
lmFiltered <- train(trainXfiltered, solTrainY, method = "lm", trControl = ctrl)
lmFiltered

# To obtain training R2
lmValuesF <- data.frame(obs = solTrainY, pred = lmFiltered$finalModel$fitted.values)

rm(tooHigh, trainXfiltered, testXfiltered, corrPred, corThresh)

```


Data | RMSE | $R^2$
-------------|------|-------
Training (All predictors) | `r round(sqrt(mean(lmFitAllPredictors$residuals^2)),3)` | `r  round(summary(lmFitAllPredictors)$r.squared,3)`
Test (All predictors)    | `r round(defaultSummary(lmValues1)[1],3)`    | `r round(defaultSummary(lmValues1)[2],3)`
Train (Filtered)| `r round(sqrt(mean(lmFiltered$finalModel$residuals^2)),3)`    | `r round(defaultSummary(lmValuesF)[2],3)`
Test (Filtered)| `r round(lmFiltered$results[1,"RMSE"],3)`    | `r round(lmFiltered$results[1,"Rsquared"],3)`

```{r cleanupLM, include=FALSE}
rm(lmFitAllPredictors, lmValues1, lmValuesF, lmFiltered, lmFit1, lmPred1)
```


**Robust Linear Regression Model**, `MASS::rlm` uses Huber function by default.  


```{r fitRlmToSolTrain, eval=FALSE}
# converged in 18 iterations
rlmFitAllPredictors <- MASS::rlm(Solubility ~ ., data = trainingData)
rm(rlmFitAllPredictors)
```

However, it is recommended to first ensure that the covariance matrix is not singular, as `rlm` does not allow singularity.  

Note that MASS::rlm solution converges without pre-processing of data, but caret::train with method = rlm does not converge.  **Question**:  Why not? Perhaps because performing 10-fold CV?

 
```{r fitPCRobustRToSolTrain}

set.seed(100)
rlmPCA <- train(solTrainXtrans, solTrainY, method = "rlm", preProcess = "pca", trControl = ctrl, maxit = 30)

rlmPCA
rm(rlmPCA)
```


#### Partial Least Squares

`pls::plsr` function

* Requires model formula
* The first Dayal and MacGregor kernel algorithm used; can specify others using the `method` argument, e.g. "oscorespls", "simpls", or "widekernelpls".
* Use `ncomp` argument  to set number of components, otherwise maximum number of components will be calculated.
* Use `validation` argument for K-fold or leave-one-out CV

Helper functions to extract PLS components:

* `loadings`
* `scores`
* `plot`

Given that many predictors highly correlated and overall information within predictors space contained in smaller number of dimensions, predictor conditions are favourable for applying PLS.

Use cross validation to determine optimal number of PLS components to retain in order to minimise RMSE.

```{r solubilityPls, include=FALSE}
#plsFit <- plsr(Solubility ~ ., data = trainingData)
#predict(plsFit, solTestXtrans[1:5,], ncomp = 1:2)

## The default tuning grid evaluates components 1... tuneLength
set.seed(100)
plsTune <- train(x = solTrainXtrans, 
                 y = solTrainY, 
                 method = "pls", 
                 tuneLength = 20, 
                 # ALT: 
                 #tuneGrid = expand.grid(ncomp = 1:20),
                 trControl = ctrl, 
                 preProc = c("center", "scale"))
#plsTune
predict(plsTune, solTestXtrans)

plsTune$bestTune[,"ncomp"]
plsTune$results$RMSE[plsTune$bestTune[,"ncomp"]]

```

In the following code, PCR is performed using the same cross-validation sets to compare its performance to PLS.

```{r solubilityPCR, include=FALSE}
set.seed(100)
pcrTune <- train(x = solTrainXtrans, y = solTrainY,
                 method = "pcr",
                 tuneGrid = expand.grid(ncomp = 1:35),
                 trControl = ctrl)
pcrTune                  
```
A comparison of PLS to PCR is below; PLS found a minimum RMSE (`r  round(plsTune$results$RMSE[plsTune$bestTune[,"ncomp"]],3)`) with `r plsTune$bestTune[,"ncomp"]` components, whereas PCR found a  minimum RMSE (`r  round(pcrTune$results$RMSE[pcrTune$bestTune[,"ncomp"]],3)`) with `r pcrTune$bestTune[,"ncomp"]` components.

```{r solubilityPlsVsPCR}
plsResamples <- plsTune$results
plsResamples$Model <- "PLS"

pcrResamples <- pcrTune$results
pcrResamples$Model <- "PCR"

plsPlotData <- rbind(plsResamples, pcrResamples)
xyplot(RMSE ~ ncomp,
       data = plsPlotData,
       #aspect = 1,
       xlab = "# Components",
       ylab = "RMSE (Cross-Validation)",
       auto.key = list(columns = 2),
       groups = Model,
       type = c("o", "g"))


```

Although predictive ability is similar, PLS finds a simpler model that uses far fewer components than PCR.  Using the one-standard error method (simpler solution within 1 standard error of numerically optimal value) reduces this to `r  min(which(plsTune$results$RMSE >=  plsTune$results$RMSE[plsTune$bestTune[,"ncomp"]] - plsTune$results$RMSESD[plsTune$bestTune[,"ncomp"]] & plsTune$results$RMSE <=  plsTune$results$RMSE[plsTune$bestTune[,"ncomp"]] + plsTune$results$RMSESD[plsTune$bestTune[,"ncomp"]]))` components for PLS and 
`r min(which(pcrTune$results$RMSE >=  pcrTune$results$RMSE[pcrTune$bestTune[,"ncomp"]] - pcrTune$results$RMSESD[pcrTune$bestTune[,"ncomp"]] & pcrTune$results$RMSE <=  pcrTune$results$RMSE[pcrTune$bestTune[,"ncomp"]] + pcrTune$results$RMSESD[pcrTune$bestTune[,"ncomp"]]))` for PCR

The following figure shows the relationship between the first two components of both PLS and PCR; the correlation is greater for the PLS components and illustrates why PLS is more quickly steered towards underlying relationship with the response.
```{r solubilityScoresVsREsponse}
as.data.frame(unclass(plsTune$finalModel$scores)) %>%
    mutate(Model = "PLS", Solubility = solTrainY) %>%
    gather(Component, Score, -Solubility, -Model) %>%
    rbind(
        as.data.frame(unclass(pcrTune$finalModel$scores)) %>%
            mutate(Model = "PCR", Solubility = solTrainY) %>%
            gather(Component, Score, -Solubility, -Model)) %>%
    filter(Component %in% c("Comp 1", "Comp 2")) %>%
    ggplot(aes(Score, Solubility)) + 
    geom_point() + 
    facet_grid(Model ~ Component)
```

Variable importance for the PLS model shown below:

```{r solubilityPlsVarIMp, results = 'hold'}
plsImp <- varImp(plsTune, scale = FALSE)
plot(plsImp, top = 25, scales = list(y = list(cex = .95)))
rm(plsImp, plsTune, plsPlotData, plsResamples, pcrTune, pcrResamples)
```

#### Penalized Regression Models

Ridge Regression

* MASS::lm.ridge
* elasticnet::enet; 
    * lambda specifies the ridge-regression penalty
    * lasso penalty can be computed efficiently for many values of the penalty, predict.enet generates predictions for one or more values of the lasso penalty simultaneously using the `s` and `mode` argument.  For ridge regression, desire lasso penalty of 0 and so specify the full solution using `s=1` and `mode = "fraction"` 
    
**Notes on using `predict.enet`**: The meaning of `s` depends on the value of `model`.  For example, if `mode` = "norm" and `s` = 2000, then predict.enet will extract the coefficient vector with L1 norm = 2000.  If `mode` = "fraction" and `s`=0.45, then predict.enet will extract the coefficient vector with L1 norm fraction = 0.45.

```{r ridgeRegression}

ridgeModel <- elasticnet::enet(x = as.matrix(solTrainXtrans), y = solTrainY, lambda = 0.001)

ridgePred <- predict(ridgeModel, newx = as.matrix(solTestXtrans), s = 1, mode = "fraction", type = "fit")

rm(ridgeModel)
head(ridgePred$fit)

rm(ridgePred)
```

The following figure shows how the RMSE changes with $\lambda$; 
```{r ridgeTune, results = 'hold'}
# Define the candidate set of values
ridgeGrid <- data.frame(lambda = seq(0, .1, length = 15))
# ridgeGrid <- expand.grid(lambda = seq(0, .1, length = 15))

set.seed(100)
ridgeRegFit <- train(x = solTrainXtrans, 
                     y = solTrainY, 
                     method = "ridge", 
                     tuneGrid = ridgeGrid, 
                     trControl = ctrl, 
                     preProc = c("center", "scale"))

ridgeRegFit
print(update(plot(ridgeRegFit ), xlab = "Penalty"))

rm(ridgeGrid)
```


with no $\lambda$ the error is inflated, when increased, the error drops from `r ridgeRegFit$results$RMSE[1]` to `r ridgeRegFit$results$RMSE[15]`

```{r plotRidgeRegCoeffPath, eval=FALSE, include=FALSE, results = 'hold'}
coeffs <- predict(ridgeRegFit, type = "prob", mode = "fraction")


as.data.frame(unclass(coeffs$coefficients)) %>%
 mutate(Fraction = coeffs$fraction) %>%
 gather(Variable, Coefficient, -Fraction) %>%
 mutate(Col = ifelse(Variable %in% c("NumNonHAtoms", "NumNonHBonds", "NumMultBonds"), Variable, "Other"),
        Col = factor(Col, levels = c("NumNonHAtoms", "NumNonHBonds", "NumMultBonds", "Other"))) %>%
 ggplot(aes(x = Fraction, y = Coefficient, group=Variable, colour = Col)) +
 geom_line() +
     scale_color_manual(name = "", values = c("NumNonHAtoms" = "green", "NumNonHBonds" = "purple", "NumMultBonds" = "orange", "Other" = "gray"), breaks = c("NumNonHAtoms", "NumNonHBonds", "NumMultBonds")) + 
    theme(legend.position = "bottom")

rm(ridgeRegFit)
rm(coeffs)
```


Lasso model

* lars::lars
* elastnet::enet; predictor data must be a matrix object.  Use normalize argument to center and scale prior to modelling.  lambda controls ridge regression penalty, setting to 0 fits the lass model.  Lasso penalty does not need to be specified until the time of prediction.
* glmnet::enet
* biglars (large data sets)
* FLLat (fused lasso)
* grplasso (group lasso)
* penalized
* relazo (relazes lasso)

Predictors should be centered and scaled prior to modelling.  

```{r lasso, results = 'hold'}
enetModel <- elasticnet::enet(x = as.matrix(solTrainXtrans), y = solTrainY, lambda = 0, normalize = TRUE)

enetPred <- predict(enetModel, newx = as.matrix(solTestXtrans), s = .1, mode = "fraction", type = "fit")

head(enetPred$fit)
rm(enetPred)
```


```{r lassoCoef, results = 'hold'}
enetCoef<- predict(enetModel, newx = as.matrix(solTestXtrans), s = .1, mode = "fraction", type = "coefficients")
tail(enetCoef$coefficients)

rm(enetModel, enetCoef)
```

**Question**: How to recreate coefficient path ... 


The following plot shows the performance profiles across three values of the ridge penalty and 20 values of the lasso penalty.  


The pure lasso model (with $\lambda_1$ = 0) has an initial drop in the error and then an increase when the fraction of the full solution (amount that the ridge regression coefficient estimates have been shrunken towards zero; small value indicates they have been shrunken very close to zero) is greater than 0.2.  The two models with nonzero values of the ridge penalty have minimum errors
with a larger model. 

```{r lassoTune, results = 'hold'}

enetGrid <- expand.grid(.lambda = c(0, 0.01, .1), .fraction = seq(.05, 1, length = 20))

set.seed(100)
enetTune <- train(solTrainXtrans, solTrainY,
                  method = "enet",
                  tuneGrid = enetGrid,
                  trControl = ctrl,
                  preProc = c("center", "scale"))

plot(enetTune)

coeffs <- predict.enet(enetTune$finalModel, type = "coefficients", mode = "fraction")

as.data.frame(unclass(coeffs$coefficients)) %>%
 mutate(Fraction = coeffs$fraction) %>%
 gather(Variable, Coefficient, -Fraction) %>%
 mutate(Col = ifelse(Variable %in% c("NumNonHAtoms", "NumNonHBonds", "NumMultBonds"), Variable, "Other"),
        Col = factor(Col, levels = c("NumNonHAtoms", "NumNonHBonds", "NumMultBonds", "Other"))) %>%
 ggplot(aes(x = Fraction, y = Coefficient, group=Variable, colour = Col)) +
 geom_line() +
     scale_color_manual(name = "", values = c("NumNonHAtoms" = "green", "NumNonHBonds" = "purple", "NumMultBonds" = "orange", "Other" = "gray"), breaks = c("NumNonHAtoms", "NumNonHBonds", "NumMultBonds")) + 
    theme(legend.position = "bottom")

rm(coeffs, enetGrid)

```

In the end, the optimal performance was associated with
the lasso model ($\lambda = 0$) with a fraction of `r enetTune$bestTune[1, "fraction"]`, corresponding to `r sum(predict.enet(enetTune$finalModel, s=0.15, type="coef", mode="fraction")$coefficients != 0)`  predictors out of a possible `r ncol(solTrainXtrans)`.

Note: To extract the coefficients used in the final model, use the following code, either specify the optimal fraction (s = `r enetTune$bestTune[1, "fraction"]`) or enter programmatically as follows:
```{r enetCoefs, eval=FALSE}
sum(predict.enet(enetTune$finalModel, s=enetTune$bestTune[1, "fraction"], type="coef", mode="fraction")$coefficients != 0)

rm(enetTune)
```


```{r cleanupch6}
rm(ctrl, solTestX, solTestXtrans, solTrainX, solTrainXtrans, trainingData, solTestY, solTrainY, Fingerprints, SolubilityCounts)
```



# Nonlinear Regression Models

Linear regression models can be adapted to nonlinear trends in data by manually adding model terms but requires knowledge of specific nature of nonlinearity in data.

Advantage of nonlinear models (NN, MARS, SVN, KNN, tree models) is that don't need to know, or specify, form of nonlinearity prior to model training. 

## Neural Networks

* Hidden units:
    * Intermediary set of unobserved variables used in modelling (similar to PLS)
    * Linear combination of original predictors, but typically transformed by a nonlinear function $g(\centerdot)$, e.g. the logistic/sigmoidal function 
    $$
    \begin{aligned}
    h_k(\mathbf{x}) = g\left(\beta_{0k} + \sum^P_{i=1}x_j\beta_{jk}\right), \;\; \text{where}
    \\
    g(u) = \frac{1}{1 + e^{-u}}\end{aligned}
    $$
    
    where $\beta_{jk}$ is the effect of the $j$th predictor on the $k$th hidden unit
    
    * Not estimated in hierarchical fashion (as per PLS)

* Another linear combination connects the hidden units to the outcome:
    \[f(\mathbf{x}) = \gamma_o + \sum^{H}_{k = 1} \gamma_k h_k\]


For $H$ hidden units and $P$ predictors, there are $H(P+1) + (H + 1)$ parameters being estimated.  

Solution:

* Parameters usually optimised to minimize the sum of squared residuals
* Parameters usually initialised to random variables
* Back-propagation algorithm efficient, but solution is not a global solution, meaning no guarantee that resulting set of parameters are uniformly better than any other set

Limitations

* No global optimum
* Tendency to overfit; solutions:
    * Early stopping: Prematurely halt iterative algorithm for solving the regression equations, e.g. stop when error rate starts to increase (instead of using a numerical tolerance to indicate parameter estimates or error rates are stable)... but how to estimate the error rate?  Training error rate often optimistic and further splitting training set can be problematic
    * Use weight decay, penalisation method to regularize the model (similar to ridge regression); add penalty ($\lambda$) so that any large value must have significant effect on model errors to be tolerated; aim would therefore be to minimize
    \[\sum^n_{i=1}(y_i - f_i(x))^2 + \lambda\sum^H_{k=1}\sum^P_{j=0}\beta_{jk}^2 + \lambda\sum^H_{k=0}\gamma_k^2\]
    

Tuning parameters for weight decay NN include:

* Regularization value, usually between 0 and 0.1
* Number of hidden units.
 
Since regression coefficients are being summed, should be centred and scaled prior to modelling (such that on same scale)

Neural network models may differ to  the single-layer feed forward NN represented here; e.g. can have:

* More than one layer of hidden units
* Loops in both directions
* Bayesian approaches

Limitations

* Parameter estimates are locally optimal (unlikely to be a global optimum)
    * To overcome, use **model averaging**, i.e., create several models with different starting values and average the results to produce a more stable prediction
    * Adversely affected by high correlation among predictor variables (since use gradients to optimise the model parameters).  To overcome use the following techniques which have the added advantage of fewer model terms thus improving computational time:
        * Pre-filter predictors to remove predictors that are associated with high correlations.
        * Use a feature extraction technique, i.e., principal component analysis prior to modelling to eliminate correlations.  
        
        
**Self-organisation maps**

* Similar model to NN
* Can be used as unsupervised, exploratory technique or in supervised fashion for prediction


## Multivariate Adaptive Regression Splines (MARS)

* Uses surrogate (instead of original) features
* Surrogate features:
    * Contrasted version of a predictor
    * Usually a function of one or two predictors
* For each original predictor, have left-hand and right-hand surrogate features which are essentially piecewise linear and operate on isolated portion of original data.  The cutpoint / spline is determined by considering each data point as a candidate; that which achieves the minimum error is used.  New features described by a hinge function:
\[h(x) = \begin{cases} x \; x > 0 \\
                        0 \; x \leq 0
                        \end{cases}\]
A **pair** of hinge functions can be written as:

* $h(x-a)$ which is non-zero when $x>a$, and
* $h(a - x)$ which is non-zero when $x < a$

Model Building Strategy:

* Similar to forward stepwise linear regression, but instead of using original inputs, can use hinge functions, so model has form:
    \[f(X) = \beta_0 + \sum^M_{i=1}\beta_m h_m(x)\]
* Given a choice for $h_m$, coefficients $\beta_m$ estimating by minimising RSS (i.e. by standard linear regression), but how to choose $h_m$?

The collection of basis functions is \[C = {(X_j-t)+, (t - X_j)+}, t \in {x_{ij},x_{2j}, \ldots x_{Nj}}, j = 1, 2, \ldots, p. \]  If all input values distinct then there are $2Np$ basis functions.  The basis function from the collection that produces the largest decrease in training error is selected.   The process is continued until model set contains some preset maximum number of terms.  
* Typically a backward deletion procedure is applied.  The term whose removal causes the smallest increase in residual squared error is deleted to produce an estimated best model for each number of terms $\lambda$.

* Iteratively look for next set of features that yield best model fit
* Consider each data point for predictor as candidate cut point by creating linear regression model with candidate features and calculate corresponding model error.

Note that MARS can select a predictor more than once during the iterations.  
see [http://uc-r.github.io/mars](http://uc-r.github.io/mars)

MARS can also build models where features involve multiple predictors at once.

* Second-degree MARS model: Conduct same search of single term and after creating initial pair of features, instigate another search to create new cuts to couple with each of the original features, e.g. to include terms $A$, $A \times B$,  $A \times C$

Tuning parameters:

* Degree of features
* Number of retained terms
    * Can be automatically determined using default pruning procedure (GCV).  Evaluates individual model.  Since it does not reflect uncertainty from feature selection, suffers from selection bias.  
    * Set by user
    * Determined using external resampling technique (e.g. cross-validation).  Exposed to variation in entire model building process, including feature selection.

Advantages:

* Model automatically conducts feature selection therefore potentially things predictor set using same algorithm used to build model.  Feature selection routine has a direct connection to functional performance.
* Interpretability.  Each hinge function responsible for specific region.  
    * When model is additive (one-degree), contribution of each predictor can be isolated without need to consider others.  
    * When model is non-additive, (e.g. 2-degree and 2 predictors, 3 out 4 regions will be zero) can isolate interaction.
* Requires little pre-processing.  Data transformations and filtering of predictors not required;     * a zero variance predictor will never be chosen for a split since it offers no possible predictive information.
    * Correlated predictors do not drastically impact model performance but can complicate model interpretation. 
    
Variable Model Importance:  To understand how predictors affect model 

* Track reduction in RMSE (GCV stat) when adding feature to model (and attribute to original feature)

Interpreting additive model: Can original predictor and predicted response value when all others are held constant at their mean level.  The additive nature of the model allows each predictor to be viewed in isolation.  Changing the values of the other predictor variables will not alter the shape of the profile, only the location on the $y$-axis where the profile starts.


## Support Vector Machines

* Originally developed for classification models
* Used for robust regression; to minimize effect of outliers
* Many types; here $\epsilon$-insensitive regression
* Recall: regression aim to minimize SSE, but can be influenced by just one observation falling far from data trend.
* With influential observations, consider alternative minimisation metrics:
    * Huber function; squared residuals when small and absolute residuals when large
* SVM for regression function $\sim$ Huber function; for given threshold ($\epsilon$)
    * Data points with residuals within threshold do not contribute to regression fit $\rightarrow$ Samples that the model fits well have _no_ effect on regression equation.  With large $\epsilon$ only outliers define regression line
    * Data points with absolute difference > $\epsilon$ contribute linear-scale amount; i.e. squared residuals not used.
    * Aim: to choose regression coefficients that minimise:
\[\text{Cost}\sum^n_{i=1}L_e(y_i - \hat{y}_i) + \sigma^P_{j=1}\beta_j^2,\]
where $L_e(\centerdot)$ is the $\epsilon$-insensitive function and the _cost_ parameter is used to penalise large residuals (unlike weight decay in neural networks it is attached to residuals and not parameters)

Prediction with simple linear regression for new sample $u$:
\[\hat{y} = \beta_0 + \sum^P_{j=1}\beta_ju_j\]

Prediction with $\epsilon$-insensitive regression: Replace $\beta_j$ with $\sum^n_{i=1}\alpha_ix_{ij}$, such that prediction is
$$\begin{aligned}
\hat{y} &= \beta_0 + \sum^P_{j=1}u_j\left(\sum^n_{i=1}\alpha_ix_{ij}\right)\\
        &= \beta_0 + \sum^n_{i=1}\alpha_i \left(\sum^P_{j=1}x_{ij}u_j\right)\\
        &= \beta_0 + \sum^n_{i=1}\alpha_i K(\mathbf{x_i}, \mathbf{u})
\end{aligned}$$

where $K(\mathbf{x_i}, \mathbf{u})$ is referred to as the _kernel function_

Noting that:

* Number $\alpha$ parameters = number data points.  Cost parameter regularizes model to prevent over-parameterisation
* Require _subset of_ training data points (the outliers for which $\alpha \neq 0$) to make new predictions.  Since the regression line is determined using these samples, they are called the __support vectors__.
* Kernel function above was linear; can replace with dot product $\mathbf{x}^\prime \mathbf{u}$.  There are other types of kernel functions 
$$\begin{aligned}
    \text{linear} &= \mathbf{x}^\prime \mathbf{u}\\
    \text{polynomial} &= (\phi(\mathbf{x}^\prime \mathbf{u}) + 1) ^\text{degree}\\
    \text{radial basis function} &= \exp(-\sigma||\mathbf{x}- \mathbf{u}||^2\\
    \text{hyperbolic tangent} &= \tanh(\phi({x}^\prime \mathbf{u}) + 1)\\
\end{aligned}$$
where $\phi$ and $sigma$ are scaling parameters.  Radial basis function shown to be effective but when regression line is truly linear, linear kernel function will be the better choice.

Tuning parameters:

* __Cost__ value; main tool to adjust complexity of model. Large cost $\rightarrow$ flexible model as effect of errors amplified.  Small cost $\rightarrow$  model will stiffen and become less likely to over-fit (but more likely to underfit) as contribution of squared parameters ($\sum^P_{j=1}\beta_j^2$) proportionally large in modified error function
* Size funnel $\epsilon$, but suggestion is to keep if fixed and tune over other parameters.  
* Kernel specific tuning parameters
    * Polynomial kernel function:  \text{degree}
    * Radial basis function: scale parameter $\sigma$; but can calculate computationally using 10th and 90th percentile of $||x - x^\prime||^2$ as a range for $\sigma$ and use the midpoint of the percentiles..
    
Predictors enter model as sum of cross products and so differences in predictor scales can affect model.  Recommendation: centre and scale predictors prior to building SVM model.  

## $K$-Nearest Neighbours

* Predicted response = summary statistic(response of the $K$ nearest neighbours in the training data set).
* For summary statistics can use mean or median
* Nearest neighbours = f(distance definition):
    * Euclidean distance = straight line distance
    \[\left(\sum^P_{j = 1}(x_{aj}-x_{bj})^2\right)^P\]
    where $\mathbf{x_a}$ and $\mathbf{x_b}$ are two individual samples
    * Minkowski distance = generalisation of Euclidean distance 
     \[\left(\sum^P_{j = 1}|x_{aj}-x_{bj}|^2\right)^P\]
        * $q = 1$: Manhattan = city block, common for samples with binary predictors
        * $q = 2$ Euclidean
    * Tanimoto - computational chemistry problems when molecules described using binary fingerprints
    * Hamming
    * Cosine
    
As $K$NN depends on distance, predictor scale has big impact on samples.  Data with predictors on different scales will generate distances weighted towards predictors with the largest scales.  Recommendation: Centre and scale prior to performing $K$NN


**Missing values**:  Makes it impossible to compute distance between samples.  Options:

* Exclude samples or predictors with missing values
* Impute missing data using naive estimators such as mean of the predictor, or nearest neighbour using only predictors with complete information

Steps:

* Pre-process data
* Select distance metric
* Find optimal number of neighbours = tuning parameter
    * Resampling
    
Limitations:

* Computational time; to predict sample distances between sample and all other samples needs to be computed.  Computation time increases with $n$ because training data must be loaded into memory and new sample and all training samples must be computed.  Replace original data with less memory-intensive representation that describes locations, e.g. $k-d$ tree ($k$-dimensional tree), which orthogonally partitions predictor space using tree approach. Once tree grown, distances for new sample only computed for training observations in tree close to sample.  Provides significant computational improvement when $n$ >> $p$.
* Disconnect between local structure and predictive ability (i.e. when irrelevant or noisy predictors are in the neighbourhood of the new sample).  To overcome:
    * Remove irrelevant, noise-laden predictors
    * Weight neighbour's contribution based on distance to new sample


# Chapter 8: Regression Trees and Rule-Based Models

* Tree-based model = $\geq$ 1 nested _if-then_ statements
* Rule-based model = simplified tree-based model in which samples may be covered by multiple rules?

Strengths:

* Highly interpretable
* Easy to implement
* Can handle many types of predictors (sparse, skewed, continuous, categorical,etc) without need to pre-process
* No requirement to specify form of predictor's relationship to response (such as in linear regression)
* Implicitly handles missing data
* Implicitly conducts feature selection

Weaknesses:

* Model instability; slight changes in data can drastically change tree/rule structure and therefore interpretation
* Less-than-optimal predictive performance as models define rectangular regions that may not adequately define the relationship between predictors and the response

To overcome weaknesses, ensembles used which combine many trees (or rule-based) models into one.  

## Basic Regression Trees

partition data into smaller groups that are more homogeneous with respect to response, by determining:

* Predictor to split on and value of split
* Depth /complexity of tree
* Prediction equation in terminal nodes

Techniques for constructing regression trees:

* CART (classification and regression tree)

**CART method**:

* Start with entire data set $S$
* Tree growing step: Search each distinct value of very predictor to find predictor and split value that partitions the data into two groups $S_1$ and $S_2# such that overall sums of squares are minimised:
    \[\text{SSE} =  \sum_{i \in S_1}(y_i - \bar{y}_1)^2 + \sum_{i \in S_2}(y_i - \bar{y}_2)^2\]
where $\bar{y}_i$ is average of training set outcome within group $S_i$.  Recursively partition the $S_1$ and $S_2$ based on this formula, until number of samples in splits falls below some threshold (e.g. 20 samples)
* Pruning step: Fully grown tree may be large and likely over-fit training set.  Pruning methods:
    * Cost-complexity tuning: Penalise error rate using size of tree using a __complexity parameter__ $c_p$.  For given value of $c_p$ find lowest penalised error rate 
    \[\text{SSE}_C_p = \text{SSE} + c_p \times (\text{# Terminal Nodes})\]
    


Splitting continuous predictors: Order data
Splitting binary predictors: Only one possible split point
Splitting categorical predictors: Options:
    * Decompose categorical predictors into set of binary dummy variables (one vs all split)
    * Allow the model to determine how to split the values


## Regression Tree Models
## Rule-Based Models
## Bagged Trees
## Random Forests
## Boosting
## Cubist

# Chapter 11: Measuring Performance in Classification Models

```{r}
library(tidyverse)
library(datasets)
library(caret)
# load the iris dataset
data(iris)
split_index <- createDataPartition(iris$Species, p=0.80, list=FALSE)
# use  80% of data to train model
iris_tr <- iris[split_index,]

# use 20% of the data for validation
iris_te <- iris[-split_index,]

# define training control
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(Species~., data=select(iris_tr, Sepal.Length, Sepal.Width, Species), trControl=train_control, method="rf")
# summarize results
print(model)

predictions <- predict(model, iris_te, type = "prob")
predictions.class <- predictions %>%  mutate('class'=names(.)[apply(., 1, which.max)]) %>% dplyr::select(class) %>%
    mutate(class = as.factor(class)) %>% pull()

confusionMatrix(predictions.class, iris_te$Species)


getMultinomialCM <- function(prediction, target){
    data <- cbind(prediction, target)
    
    cm <- with(dat, table(target, prediction))
    # recall
    recall <- diag(cm / apply(cm, 1, sum))
    # precision
    precision <- diag(cm/apply(cm, 2, sum))
    # accuracy
    accuracy <- sum(diag(cm))/sum(cm)
    
    cm <- cm %>% cbind(recall) %>%
        rbind(precision = c(precision, accuracy))
    cm
}
getMultinomialCM(predictions.class, iris_te$Species)

```

Calibration Plots
```{r}

obs <- model.matrix( ~Species-1, data = iris_te %>% dplyr::select(Species))

i <- 1
cal <- NULL
for (i in 1:ncol(predictions)){
    dat <- data.frame(obs = obs[,i], pred = predictions[,i]) 
    dat$pred_bin <- cut(dat$pred, breaks = c(-0.001, seq(0.1, 1, 0.1)))
    dat$midpoint <- seq(.1/2, 1-.1/2, by = 0.1)[as.numeric(dat$pred_bin)]
    
    dat <- dat %>% group_by(midpoint) %>%
        summarise(n = n(), 
                  events = sum(obs)) %>%
        mutate(event_rate = events / n)
    
    dat$class <- names(predictions)[i]
    cal <- bind_rows(cal, dat)
}

ggplot(cal, aes(midpoint, event_rate, colour = class)) + 
    geom_line() + 
    geom_point()
```
Heatmap
```{r}
cbind(predictions, Target = iris_te$Species) %>%
    gather(Pred_Species, Pred_Probability, -Target) %>%
    ggplot(aes(Pred_Species, Target, fill = Pred_Probability)) + 
           geom_tile(colour = "white") + 
    scale_fill_gradient(low = "white", high = "steelblue")
```


# Chapter 12: Discriminant Analysis and Other Linear Classification Models

# Chapter 13: Nonlinear Classification Models

# Chapter 14: Classification Trees and Rule-Based Models

---------
# Chapter 16: Remedies for Severe Class Imbalance

Overview:

* Impact of class imbalance on performance measures
* Methodologies for post-processing model predictions
* Predictive models that can mitigate issue during training

## Case Study: Predicting Caravan Policy Ownership

TODO: Read PDF.

* Imbalanced; only 6% purchased policies
* 85 Predictors; see [here](https://rdrr.io/rforge/DWD/man/ticdata.html) for more details
    * Customer subtype: 39 unique values; though many comprise < 5% of customers
    * Demographic factors; religion, education level, social class, income + 38 others
    * Product ownership information; number (or contribution)  to policies
* Many categorical predictors had $\geq$ 10 levels
* Count-based predicted fairly sparse (few non-zero values)
* Stratified random sampling (strata $\equiv$ response variable), with rate of customers with caravan policies ~6% in each.
    * Training set ($n = 6877$) to estimate model parameters and tune model
    * Evaluation set ($n = 983$) to develop post-processing techniques, e.g. probability cut-offs
    * Customer set ($n = 1962$) for final model evaluation

## Effect of Class Imbalance

Models:

* Random forest.  1000 trees in forest tuned over 5 values of $m_\text{try}$ parameter with optimal value of $m_\text{try} = 126$ 
* Flexible discriminant analysis (MARS hinge functions) (FDA): Used first-degree features; tuned over 25 values for the number of retained terms.  Resampling process determined that 13 model terms appropriate.
* Logistic regression.  Simple additive model (no interactions or non-linear terms) with reduced predictor set (many near-zero variance predictors removed so that model resulted in a stable solution)

Model tuning:

* 10-fold cross-validation, each holdout sample with ~ 687 customers to provide reasonable estimates of uncertainty

Performance metrics:

* Overall accuracy
* Kappa
* AUROC (used to choose optimal model)
* Sensitivity
* Specificity

Findings:

* Models only predicted 13/59 customer would purchase.  Had good specificity (since almost every customer predicted no insurance), but poor sensitivity
* 82% of customers had predicted probability of $\leq$ 10% of having insurance
* Lift plots: How many individuals would need to be contacted in order to capture a specific percent of those who might purchase the policy, i.e. to find 60% of those with policies, around 30% of the population would need to be sampled
* ROC curves did not differentiate the models
* when classes are balanced, lift and ROC curves are not so similar

Strategies for overcoming class imbalances:
 
* **Model Tuning**: Tune model to maximise accuracy of minority class(es)
* **Aternative Cutoffs**: Examine ROC to select cut-offs to:
    * Achieve targets for sensitivity/specificity (if given)
    * Minimise distance between ROC curve and perfect model (100% Sensitivity and Specificity, i.e. top LH corner)
    * Maximise Youden's $J$ index model, which measures the proportion of correctly predicted samples for both event and non-event groups

Note that it is important, especially for small sample sizes, to use an independent data set to derive the cut-off.  If use:

* Training set  $\rightarrow$ large optimistic bias in the class probabilities $\rightarrow$ inaccurate assessments of the sensitivity and specificity. 
* Test set $\rightarrow$ lose unbiased source to judge model performance.  

Alternative cut-offs does not impact overall predictive effectiveness of the model, but rather impacts the trade-ff between the two types of errors.

* **Adjusting Prior Probabilities**: Priors that reflect natural class imbalance will bias predictions to majority class; better to use balanced priors, or balanced training set (e.g. in naive Bayes and discriminant analysis classifiers).  However, the new class probabilities are unlikely to change the rankings or AUROC; gain this is just impacting the the trade-off between sensitivity and specificity

* **Unequal Case Weights**: Can increase weights, e.g., in logistic regression, boosting and cart tree models, for samples in the minority classes; effectively upsampling (or duplicating data points) within the minority class(es)  (therefore related to sampling methods) 

* **Sampling Methods**: Select training set sample to have roughly equal event rates during initial data collection; so that model doesn't have to deal with imbalance.  Will however require that test set sampled to be more consistent with "real life", to obtain true estimates of future performance.  Post-hoc sampling approaches:
    * Down-sampling: Reduce number of samples to improve class balance.  Methods:
        * Randomly sample majority classes
        * Bootstrap sample  so that classes are balanced in the bootstrap sample.  Advantage is that can be ran many times to obtain an estimate of variation about the down-sampling.
    * Up-sampling: simulates or imputes additional data points. Majority class only appears once; minority class are sampled with replacement.
    * Synthetic minority over-sampling technique (SMOTE): Uses up- and down-sampling.
    * Uses three operational parameters; (1) amount of up=sampling, (2) amount of down-champing, and (3) number of neighbours used to impute new cases.  
    * To up-sample the minority class, SMOTE synthesises new data points based on its KNNs.  
    * Down-sampling based on random sampling

**Warning re: Sampling Methods:** When using modified versions of the training set, resampled estimates of model performance can become biased, i.e., if data up-sampled, resampling procedures likely to have same sample in the cases that are used to build the model as well as the holdout set, leading to optimistic results.  Despite this, resampling methods can be effective at tuning the models.

**Findings re: Sampling Methods**: Sampling approaches have benefit of enabling better trade-offs between sensitivity and specificity.


* **Cost-Sensitivty Training**:  instead of optimising accuracy/impurity, can instead optimise cost or loss function that deferentially weights specific types of errors.  By assigning higher cost to misrepresented true cases (false negatives) may bias model to under-represented class.  This method has the potential to make true improvements to classifier, rather than simply altering the cut-off to make trade-offs between sensitivity and specificity.  

* With SVM, can specify costs for specific classes.  In current implementation will not be able to perform ROC curve, instead use Kappa, sensitivity or specificity.  TODO check: apply cost to class not specific type of error.
* Many classification trees (CART and C5.0) can also incorporate differential costs.
 
 
 
 
 
 TODO: READ CH. 14   
    $\text{Pr(misclassification)} = Pr[2|1]\pi_1 + Pr[1|2]\pi_2$
where
* $pi_i$ = prior probability of sample being in class $i$
* $Pr[j|i] = probability of mistakenly predicting a class $i$ as class $j$.

Use $p_i$ as estimate of $Pr[j|i]$ 

For given sample, classify into class 1 if  \frac{p_1}{p_2} > \frac{\pi}{}

```{r getTicdata, eval=FALSE, include = FALSE}
load("../../static/datasets/ticdata.rda")
head(ticdata)

## Find which columns are regular factors or ordered factors
isOrdered <- unlist(lapply(ticdata, is.ordered))
isFactor <- unlist(lapply(ticdata, is.factor))
convertCols <- names(isOrdered)[isOrdered | isFactor]
convertCols <- convertCols[convertCols != "CARAVAN"]

for(i in convertCols) ticdata[,i] <- factor(gsub(" ", "0",format(as.numeric(ticdata[,i]))))

ticdata$CARAVAN <- factor(as.character(ticdata$CARAVAN),
                          levels = rev(levels(ticdata$CARAVAN)))

### Split the data into three sets: training, test and evaluation. 
library(caret)
set.seed(156)
split1 <- createDataPartition(ticdata$CARAVAN, p = .7)[[1]]
other     <- ticdata[-split1,]
training  <- ticdata[ split1,]

set.seed(934)
split2 <- createDataPartition(other$CARAVAN, p = 1/3)[[1]]
evaluation  <- other[ split2,]
testing     <- other[-split2,]

predictors <- names(training)[names(training) != "CARAVAN"]

testResults <- data.frame(CARAVAN = testing$CARAVAN)
evalResults <- data.frame(CARAVAN = evaluation$CARAVAN)

# Some algorithms cannot have more than a given number of factor levels; so convert to dummy variables
trainingInd <- data.frame(model.matrix(CARAVAN ~ ., data = training))[,-1]
evaluationInd <- data.frame(model.matrix(CARAVAN ~ ., data = evaluation))[,-1]
testingInd <- data.frame(model.matrix(CARAVAN ~ ., data = testing))[,-1]

trainingInd$CARAVAN <- training$CARAVAN
evaluationInd$CARAVAN <- evaluation$CARAVAN
testingInd$CARAVAN <- testing$CARAVAN

isNZV <- nearZeroVar(trainingInd)
noNZVSet <- names(trainingInd)[-isNZV]

testResults <- data.frame(CARAVAN = testing$CARAVAN)
evalResults <- data.frame(CARAVAN = evaluation$CARAVAN)
```


```{r runBaselineTicDataModels, eval=FALSE, include=FALSE}

### These functions are used to measure performance
fiveStats <- function(...) c(twoClassSummary(...), defaultSummary(...))
fourStats <- function (data, lev = levels(data$obs), model = NULL)
{

  accKapp <- postResample(data[, "pred"], data[, "obs"])
  out <- c(accKapp,
           sensitivity(data[, "pred"], data[, "obs"], lev[1]),
           specificity(data[, "pred"], data[, "obs"], lev[2]))
  names(out)[3:4] <- c("Sens", "Spec")
  out
}

ctrl <- trainControl(method = "cv",
                     classProbs = TRUE,
                     summaryFunction = fiveStats)

ctrlNoProb <- ctrl
ctrlNoProb$summaryFunction <- fourStats
ctrlNoProb$classProbs <- FALSE


# Random Forest
set.seed(1410) # 8 minutes
start <- Sys.time()
rfFit <- train(CARAVAN ~ ., data = trainingInd,
               method = "rf",
               trControl = trainControl(method = "none", classProbs = TRUE, summaryFunction = fiveStats), #ctrl,
               ntree = 500, #1500
               tuneLength = 1, #5
               metric = "ROC")
Sys.time() - start
rfFit

start <- Sys.time() # 3.8 minutes
rfModel <- randomForest(trainingInd[,1:ncol(trainingInd)-1], trainingInd[,ncol(trainingInd)])
Sys.time() - start


evalResults$RF <- predict(rfFit, evaluationInd, type = "prob")[,1]
testResults$RF <- predict(rfFit, testingInd, type = "prob")[,1]
rfROC <- roc(evalResults$CARAVAN, evalResults$RF,
             levels = rev(levels(evalResults$CARAVAN)))
rfROC

library(pROC)
rfEvalCM <- confusionMatrix(predict(rfFit, evaluationInd), evalResults$CARAVAN)
rfEvalCM

# Logistic Regression
set.seed(1410)
lrFit <- train(CARAVAN ~ .,
               data = trainingInd[, noNZVSet],
               method = "glm",
               trControl = ctrl,
               metric = "ROC")
lrFit

evalResults$LogReg <- predict(lrFit, evaluationInd[, noNZVSet], type = "prob")[,1]
testResults$LogReg <- predict(lrFit, testingInd[, noNZVSet], type = "prob")[,1]
lrROC <- roc(evalResults$CARAVAN, evalResults$LogReg,
             levels = rev(levels(evalResults$CARAVAN)))
lrROC

lrEvalCM <- confusionMatrix(predict(lrFit, evaluationInd), evalResults$CARAVAN)
lrEvalCM

# Flexible Discriminant Analaysis
set.seed(1401)
fdaFit <- train(CARAVAN ~ ., data = training,
                method = "fda",
                tuneGrid = data.frame(degree = 1, nprune = 1:25),
                metric = "ROC",
                trControl = ctrl)
# fdaFit
# 
# evalResults$FDA <- predict(fdaFit, evaluation[, predictors], type = "prob")[,1]
# testResults$FDA <- predict(fdaFit, testing[, predictors], type = "prob")[,1]
# fdaROC <- roc(evalResults$CARAVAN, evalResults$FDA,
#               levels = rev(levels(evalResults$CARAVAN)))
# fdaROC
# 
# fdaEvalCM <- confusionMatrix(predict(fdaFit, evaluation[, predictors]), evalResults$CARAVAN)
# fdaEvalCM


# Summarise Baseline Models
labs <- c(RF = "Random Forest", LogReg = "Logistic Regression")
          #,          FDA = "FDA (MARS)")
lift1 <- caret::lift(CARAVAN ~ RF + LogReg , data = evalResults, labels = labs)

plotTheme <- caretTheme()

#plot(fdaROC, type = "S", col = plotTheme$superpose.line$col[3], legacy.axes = TRUE)
plot(rfROC, type = "S", col = plotTheme$superpose.line$col[1], legacy.axes = TRUE)
plot(lrROC, type = "S", col = plotTheme$superpose.line$col[2], add = TRUE, legacy.axes = TRUE)
legend(.7, .25,
       c("Random Forest", "Logistic Regression"),
       cex = .85,
       col = plotTheme$superpose.line$col[1:3],
       lwd = rep(2, 3),
       lty = rep(1, 3))

xyplot(lift1,
       ylab = "%Events Found",
       xlab =  "%Customers Evaluated",
       lwd = 2,
       type = "l")



```



# New R commands
`apropos`: search R packages for a given term in currently loaded packages
```{r apropos, eval=FALSE, include=FALSE}
apropos("confusion")
```

`RSiteSearch`: find function in any package
```{r eval=FALSE, include=FALSE}
RSiteSearch("confusion", restrict="functions")
```

