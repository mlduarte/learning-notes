---
title: Neural Networks and Deep Learning
author: ''
date: '2018-10-15'
slug: neural-networks-and-deep-learning
categories: []
tags: []
banner: "img/banners/NNandDL.png"
---

```{r init, message=FALSE, warning=FALSE, echo = FALSE}
require(tidyverse)
require(latex2exp)
require(reticulate)
```


# Introduction {#week1}
## Introduction to Deep Learning
Deep learning $\equiv$ training a neural network.

What is a Neural Network?  Example: Housing price prediction 

* Aim predict price = f(size of house)
*  Linear regression: Fit straight line, but know prices can never be 0, looks like piecewise linear regression, but simplest NN.
\[\text{Size} \rightarrow \bigcirc \rightarrow \text{Price (y)}\],
where the neuron, computes the linear function, takes a max of zero and outputs the price.  This function is called a **ReLU** (Rectified Linear Unit) function.  
* A larger NN is formed by stacking many of these NNs together.  
![NN_eg](/img/DLS_NNIntro.png)

## Supervised Learning with Neural Networks

Input(x)            | Output (y)                    | Application           | Type of NN
--------------------| ----------------------------- | ----------------------|-----------------------------------
Home features       | Price                         | Real Estate           | Standard
Ad and user info    | Click on ad? (0/1)            | Online Advertising    | Standard
Image               | Object ($1, \ldots, 1000$)    | Photo tagging         | Convolution on NN (CNN)
Audio               | Text transcript               | Speech recognition    | Recurrent NN (RNN) (as sequential)
English             | Chinese                       | Machine translation    | RNN (as sequential)
Image, Radar info   | Position of other cars        | Autonomous driving    |(hybrid, incl. CNN)

![NN_types](/img/DLS_NNTypes.png)

Success will depend on selecting the appropriate:

* $x$ and $y$
* Type of NN


Deep learning can be applied to both types of data;

* Structured data:  Essentially databases of data.
* Unstructured data: Audio, images, text.  Features might be pixel values in an image or individual words in a  piece of text.  Deep learning helping to interpret unstructured data as well.


## Why is Deep Learning taking off?

Basic technical ideas have been around for decades.  Main drivers for rise of deep learning:

* Amount of data (computer, sensors, cell phones, etc.)
![NN_eg](/img/DLS_DrivenByScale.png)
Note that $m$ is used to denote the number of training examples.

* Computation.  Faster computation has helped increase the number of iterations through the Idea-Code-Experiment cycle as now can perform an iteration in 10 minutes / 1 day as opposed to a moth.
* Algorithms (to make computation faster). Huge breakthrough: Sigmoid function -> ReLU function.  Gradient descent on the sigmoid function is very slow on the extreme values

Course resources:

* Discussion forum; questions technical questions, etc..  Go to course home page.  Otherwise at feedback@deeplearning.ai

Hero of Deep Learning: Geoffrey Hinton

* 1982: Hinton, Rumelhart: seminal backpropagation algorithm, but it had been developed many times earlier.  
* Example "Mary has mother Victoria", gave features such as nationality, generation, branch of family tree
# Basics of Neural Network programming.  Two views.  Psychologist: Concept = bundle of features.  AI: Concept = formal structuralist view  
* Most proud of work with Terry Segnowski (wake and sleep phases) vs back propagation (forward pass and backward pass) (Boltzmann machines)  Restricted Boltzmann machines were ingredients of the winning entry for the Netflix competition.
* Restricted Boltzmann machine.  One layer of hidden features, can learn one layer.  Then treat those features as data and do it again.  And repeat.  Uy Tay: Treat these as a single model; on top a restricted Boltzmann machine and below it use a Sigmoid belief net.
* Recirculation algorithm: send information around loop so that information doesn't change.  You want to train it without having to do back propagation.  Don't want to change info.  Relates to spike-timing dependent plasticity
* Fast weights
* Capsules




# Neural Network Basics {#week2}

## Logistic Regression as a Neural Network
## Binary Classification

* When implement NN< want to process entire training set without using an explicit for loop
* Usually have forward propagation followed by backward propagation step
* Will use logistic regression to convey ideas
* Example of binary classification: y = {1 = cat; 0 = no cat}
* Representation of image in computer
    * 3 matrices, one each for red, green and blue pixel intensity values
    * If image is 64 pixels  $\times$  64 pixels, then would have 3 $64 \times 64$ matrices 
    * In slide, the pictures are much smaller and so represented by 3 $5\times4$ matrices.
* Pixel intensity values are unrolled into one feature vector $x$, which will have $3 \times 64\times 64$ rows, such that $n = n_x = n$ =`r scales::comma(3 * 64 * 64)`.

Notation:

* training example: $(x,y), \; x \in \mathbb{R}^{n_x}, y \in {0,1}$
* $m = m_\text{train}$ training examples: $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(m)}, y^{(m)})$
* $m_\text{test}$ test examples
* Data $X = \left[\begin{array}{cccc}
\mid        & \mid          &           & \mid \\
{\bf X}^{(1)} & {\bf X}^{(2)}   & \cdots    & {\bf X}^{(m)}\\
\mid        & \mid          &           & \mid \\
\end{array} \right]$, where $X$ has $n_x$ rows (number of features) and $m$ columns (number of training examples), i.e. $X \in \mathbb{R}^{n_x \times m}$.  Python: `X.shape = (n, m)`
* Output (response value) is also stacked in columns, \[Y = [y^{(1)}, y^{(2)}, \ldots, y^{(m)}],\] i.e., $Y \in \mathbb{R}^{1 \times m}$.  Python: `Y.shape = (1, m)`

Refer to _notation guide_ if you forget!

## Logistic Regression

* Given $x$, want $\hat{y} = P(y = 1 | x)$
* Features: $x \in \mathbb{R}^{n_x}$
* Parameters: $w \in \mathbb{R}^{n_x}$, $b \in \mathbb{R}$
* How to generate $\hat{y}$?
    * Linear regression: $\hat{y} = w^Tx + b$.  Not good for binary classification, as want to enforce $0 \leq \hat{y} \leq 1$.
    * Logistic regression: $\hat{y} = \sigma(w^Tx + b)$, where 
        * Let $z = w^Tx + b$
        * $\sigma$ is the sigmoid function, which cross the vertical axis at 0.5. 
        \[\sigma(z) = \frac{1}{1 + e^{-z}}\]
        

        
 ```{r sigmoid_function, fig.height=3, fig.width=6}
data.frame(z = seq(-10, 10, .001)) %>%
    mutate(sigmoid = 1/(1 + exp(-z))) %>%
    ggplot(aes(z, sigmoid)) + 
    geom_line()
```

Recall exponential function will be close to 0 for large negative values and approach infinity for large positive values.


```{r exp_function, fig.height=3, fig.width=6}
data.frame(x = seq(-1, 1, .01)) %>%
    mutate(y = exp(x)) %>%
    ggplot(aes(x, y)) + 
    geom_line() + 
    labs(y = "exp(x)")
```   
Aim of logistic regression: Learn parameters $w$ and $b$ so that $\hat{y}$ is a good estimate of chance of $y$ being 1 or 0.

Note: In this course, treat W and B separately, where B corresponds to an intercept term.


## Logistic Regression Cost Function

First, recall log function:


```{r log_function, fig.height=3, fig.width=6}
data.frame(x = seq(0,1,0.001)) %>%
    mutate(y = log(x)) %>%
    ggplot(aes(x,y)) + 
    geom_line() + 
    labs(y = "y = log(x)")
``` 

Recall logistic regression model: $\hat{y} = \sigma(w^Tx + b)$ where $\sigma(z) = \frac{1}{1 + e^{-z}}$
    
Given ${(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(m)}, y^{(m)})}$, want $\hat{y}^{(i)} = y^{(i)}$.

Noting that $z^{(i)} = w^Tx^{(i)} + b$

**Loss (error) function**: 

* Need to define to measure how good $\hat{y}$ is when true label is $y$.
* Could do $\mathcal{L}(\hat{y}, y) = \frac{1}{2}(\hat{y} - y)^2$ but makes gradient descent not work well.  Optimisation problem becomes non-convex and with lots of local optima.
* Use  $\mathcal{L}(\hat{y}, y) = -\left(y \log \hat{y} + (1-y)\log(1-\hat{y})\right)$

if $y=1$, $\mathcal{L}(\hat{y}, y) = - \log \hat{y}$  to make loss function small, 

* want $\log \hat{y}$ large (because of negative), therefore
* want $\hat{y}$ large
        


```{r cost_term1, fig.height=3, fig.width=6}
data.frame(yhat = seq(0,1,0.001)) %>%
    mutate(cost = -log(yhat)) %>%
    ggplot(aes(yhat,cost)) + 
    geom_line() + 
    xlab(TeX("$\\hat{y}$")) + 
    ylab(TeX("$-\\log \\;\\hat{y}$"))
```   
if $y=0$, $\mathcal{L}(\hat{y}, y) = -\log(1-\hat{y})$

* Want $\log (1-\hat{y})$ (because of negative) large, therefore
* Want $\hat{y}$ small
 
  

```{r cost_term2, fig.height=3, fig.width=6}
data.frame(yhat = seq(0,1,0.001)) %>%
    mutate(cost = -log(1-yhat)) %>%
    ggplot(aes(yhat,cost)) + 
    geom_line() + 
    xlab(TeX("$\\hat{y}$")) + 
    ylab(TeX("$-\\log \\;(1-\\hat{y})$"))
```


**Cost function**:
\[J(w,b) = \frac{1}{m}\sum^m_{i=1}\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) = -\frac{1}{m}\sum^m_{i=1}\left[y^{(i)} \log \hat{y}^{(i)} + (1-y^{(i)})\log(1-\hat{y}^{(i)})\right]\]


Loss function applied to just a single training example.  Cost function is cost of parameters.  In training logistic regression, want to find $W$ and $B$ to minimize overall cost function $J$.

Logistic regression can be seen as a very very small neural network.


## Gradient Descent
Used to learn the parameters $w$ and $b$ on your training algorithm.  

Recap: $\hat{y} = \sigma(w^Tx + b)$, $\sigma(z) = \frac{1}{1 + e^{-z}}$

The cost function, 
$J(w,b) = \frac{1}{m}\sum^m_{i=1}\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) = -\frac{1}{m}\sum^m_{i=1}\left[y^{(i)} \log \hat{y}^{(i)} + (1-y^{(i)})\log(1-\hat{y}^{(i)})\right]$, is convex.

Want to find $w$, $b$ that minimize $J(w,b)$.

For logistic regression, almost any initialisation of $w$ and $b$ works.  Usually set to zero for LR (could do random)

Algorithm for just one parameter $w$:

Repeat { 

* $w := w - \alpha \frac{\partial J(w,b)}{\partial  w}$ 
* $b := w - \alpha \frac{\partial  J(w,b)}{\partial  b}$ 
    
} where $\alpha$ is the learning rate.  
Recall definition of derivative = slope of function (height/width).

![Gradient Descent Algorithm](/img/DLS_GradDesc.png)


## Computation graph

Computations of NN organised in terms of 

* Forward propagation step (compute output of NN) (left to right)
* Back propagation step (compute gradients / derivatives) (right to left)

Computation graph explains why organised this way.

Use computation graph when want to optimise J.

* Forward propagation: Compute value of J
* Back propagation: next slides

**Example**: Find minimum of function $J(a,b,c) = 3(a + bc)$.

Steps: 

1. Initialisation.  Let $a = 5$, $b = 3$, $c = 2$.
2. Forward Propagation.  Compute 

* $u = bc \rightarrow 6$ 
* $V = a + u \rightarrow 11$
* $J = 3v \rightarrow 33$

![Computation Graph](/img/DLS_ComputationGraph.png)

3.  Back Propagation.  Compute

* Compute derivative of $J$ with respect to $v$: \[\frac{dJ}{dv} = \frac{d(3v)}{dv} = 3\]
* Compute derivative of $J$ with respect to $a$; use **chain rule** $\frac{dJ}{da} = \frac{dJ}{dv}\frac{dv}{da}$ such that 
$$\begin{aligned}
    \frac{dJ}{da}   &= \frac{dJ}{dv}\frac{dv}{da} \\
                    &= \frac{d(3v)}{dv} \times \frac{d(a+u)}{da}\\
                    &= 3 \times 1 \\
                    & = 3
\end{aligned}$$
* Compute derivative of $J$ with respect to $u$;
$$\begin{aligned}
    \frac{dJ}{du}   &= \frac{dJ}{dv}\frac{dv}{du} \\
                    &= \frac{d(3v)}{dv} \times \frac{d(a+u)}{du}\\
                    &= 3 \times 1 \\
                    & = 3
\end{aligned}$$
* Compute derivative of $J$ with respect to $b$;
$$\begin{aligned}
    \frac{dJ}{db}   &= \frac{dJ}{du}\frac{du}{db} \\
                    &= 3 \times \frac{d(bc)}{db}\\
                    &= 3 \times c \\
                    & = 6
\end{aligned}$$

* Compute derivative of $J$ with respect to $c$;
$$\begin{aligned}
    \frac{dJ}{dc}   &= \frac{dJ}{du}\frac{du}{dc} \\
                    &= 3 \times \frac{d(bc)}{dc}\\
                    &= 3 \times b \\
                    & = 9
\end{aligned}$$

In code use **dvar** for derivative of final output variable, such as $J$, with respect to various intermediate quantities.


## Logistic Regression Gradient Descent

Recap: 
$$\begin{aligned}
     z &= w^Tx + b \\
     \hat{y} &= a = \sigma(z)  = \frac{1}{1 + e^{-z}}\\
     \mathcal{L}(a,y) &= -\left(y \log (a) + (1-y)\log(1-a)\right)
\end{aligned}$$

Consider logistic regression with two features, $w_1, w_2$,
![Computation Graph](/img/DLS_ComputationGraphLR.png)
Logistic Regression Derivatives:

$$\begin{aligned} 
 \text{"da"} =    \frac{d\mathcal{L}(a,y)}{da} &= \frac{d \left[-\left(y \log (a) + (1-y)\log(1-a)\right)\right]}{da}\\
                    &= -\frac{y}{a} + \frac{1-y}{1-a} \\
   \text{"dz"} =   \frac{d\mathcal{L}(a,y)}{dz} &= \frac{dL}{da}\frac{da}{dz} \\
                        &= \frac{dL}{da} \times \left[\frac{d}{dz} \frac{1}{1 + e^{-z}}\right]\\
                        &= \frac{dL}{da} \times \left[\frac{du}{dz}\frac{da}{du}\right] \;\;\text{where $u = 1 + e^{-z}$}\\
                        & = \frac{dL}{da} \times \left[-e^{-z}(-u^{-2}\right]\\
                        &= \frac{dL}{da} \times \left[e^{-z}(1 + e^{-z})^{-2}\right] \\
                        & = \frac{dL}{da} \times \left[ \frac{e^{-z}}{(1 + e^{-z})^{-2}} \right]\\
                        & =  \frac{dL}{da} \times \left[ a(1-a) \right] \\
                        & =  \left[-\frac{y}{a} + \frac{1-y}{1-a}\right] \times \left[ a(1-a) \right] \\
                        &= \left[-\frac{y(1-a)}{a(1-a)} + \frac{(1-y)a}{a(1-a)}\right] \times \left[ a(1-a) \right] \\
                        &= -y(1-a) + (1-y)a \\
                        &= -y +ya + a - ya \\
                        &= a - y
\end{aligned}$$

Noting that $a = \frac{1}{1 + e^{-z}}$ and $1-a = \frac{1 + e^{-z} - 1}{1 + e^{-z}} = \frac{e^{-z}}{1 + e^{-z}}$


Now compute by how much you have to change $w_1$, $w_2$ and $b$.
$$\begin{aligned} 
    \text{"dw1"} = \frac{d\mathcal{L}(a,y)}{dw_1} &= \frac{dz}{dw_1}\frac{dL}{dz} \\
                        &= \frac{d(w_1x_1 + w_2x_2 + b)}{dw_1}\frac{dL}{dz} \\
                         &= x_1 (a-y) \\
   \text{"dw2"} = \frac{d\mathcal{L}(a,y)}{dw_2} &= \frac{dz}{dw_2}\frac{dL}{dz} \\
                        &= \frac{d(w_1x_1 + w_2x_2 + b)}{dw_2}\frac{dL}{dz} \\
                         &= x_2 (a-y) \\
   \text{"db"} = \frac{d\mathcal{L}(a,y)}{db} &= \frac{dz}{db}\frac{dL}{dz} \\
                        &= \frac{d(w_1x_1 + w_2x_2 + b)}{dw_1}\frac{dL}{dz} \\
                         &=  (a-y) \\
\end{aligned}$$

The update steps are therefore:
$$\begin{aligned}
    w_1 := w_1 - \alpha dw_1 \\
    w_2 := w_2 - \alpha dw_2 \\
    b := b - \alpha db \\
\end{aligned}$$


## Gradient Descent on $m$ Examples
Previously, with 1 training example, used loss function.  Here with $m$ training examples, use cost function 

$$\begin{aligned}
J(w,b)  = \frac{1}{m} \sum^m_{i=1} \mathcal{L}(a^{(i)},y^{(i)}) \\
\rightarrow a^{(i)} = \hat{y}^{(i)} = \sigma(z^{(i)}) = \sigma(w^Tx^{(i)} + b)
\end{aligned}$$

The derivative of the cost function w.r.t. $w_1$ is:
\[\frac{d}{dw_1}J(w,b) = \frac{1}{m} \sum^m_{i=1} \frac{d}{dw_1}\mathcal{L}(a^{(i)},y^{(i)})\]
 
The derivatives $\frac{d}{dw_1}\mathcal{L}(a^{(i)},y^{(i)})$ were computed in previous section, therefore need to take average in order to obtain for cost function (as opposed to loss function)


To recap, one step of gradient descent (assuming just two features, i.e., $n = 2$):

1. Initialise $J=0$, $dw_1 = 0$, $dw_2 = 0$, $db = 0$
2. For $i = 1$ to $m$ (i.e., each training example)
    * $z^{(i)} = w^T x^{(i)} + b$
    * The prediction $a^{(i)} = \sigma(z^{(i)})$
    * $J += -[y^{(i)}\log a^{(i)} + (1-y^{(i)})\log(1-a^{(i)})]$
    * $dz^{(i)} = a^{(i)} - y^{(i)}$
    * $dw_1 += x_1^{(i)} dz^{(i)}$
    * $dw_2 += x_2^{(i)} dz^{(i)}$
    * $db += dz^{(i)}$
3. Need to divide by m, such that
    * $J /=m$
    * $dw_1 /= m$
    * $dw_2 /= m$
    * $db /= m$
Noting that $J$, $dw_1$, $dw_2$, $db$ are accumulators, but $$z^{(i)}$ and $a^{(i)}$ are not accumulators, hence the superscript
4.  Update the parameters: 
$$\begin{aligned}
    w_1 := w_1 - \alpha dw_1 \\
    w_2 := w_2 - \alpha dw_2 \\
    b := b - \alpha db \\
\end{aligned}$$
    
Weakness in calculation as implemented here: Two for-loops; (1) over training example, (2) over features (here only two, but otherwise have $dw_1, dw_1, \ldots dw_n$)

In deep learning, without using explicit for-loops will help you scale.  Better to use vectorisation techniques to remove for-loops (this is very important for deep learning)

This is one step of the gradient descent algorithm.

## Python and Vectorisation
## Vectorisation

Deep learning shines with large training sets (therefore very important for code to run quickly)

Calculation of  $z = w^T x + b$, for $w \in \mathbb{R}^{n_x}$, $x \in \mathbb{R}^{n_x}$:

* Non-vectorised.  Set $z=0$ then for $i$ in range($n_x$), set $z += w{i} \times x[i]$.  At end of for-loop, set $z +=b$.
* Vectorised form of calculating in Python is as follows:  $z$ = np.dot($w$, $x$) + $b$.

Code:
```{python python_vectorisation_timing, eval=TRUE, echo = TRUE, results = 'hold'}
import numpy as np
a =  np.array([1,2,3,4])
print(a)

import time
a = np.random.rand(1000000)
b = np.random.rand(1000000)

tic = time.time()
c = np.dot(a,b)
toc = time.time()

print(c)
print("Vectorised version:" + str(1000*(toc-tic)) + "ms")

c = 0
tic = time.time()
for i in range(1000000):
    c += a[i]*b[i]
toc = time.time()

print(c)
print("For loop:" + str(1000*(toc-tic)) + "ms")
```
 
Non-vectorised version took about 300 times faster

A lot of scaleable deep learning algorithms are don on a GPU (Graphics processing unit).  Within Jupyter on CPU.  Both GPU and CPU have parallelization instructions (SIMD = single instruction multiple data).  Built-in functions such as np.function that don't require a for loop are used, it enables Python to take better advantage of parallelism.  GPU is very good at SIMD calculations, but CPU not bad either.


## More Vectorisation Examples

Whenever possible avoid explicit for-loops.

Example: To calculate $u = Av$, $u_i = \sum_jA_{ij} v_j$

* Non vectorised
$$\begin{aligned}
    & \text{for } i \ldots \\  
   & \;\;\;\; \text{for } j \ldots \\  
   & \;\;\;\; \;\;\;\;    u[i] += A[i][j] * v[j]
\end{aligned}$$
            
* Vectorised:  $u = \text{np.dot}(A,v)$  
    
Example: Given $v =\left[\begin{array}{c}
v_1 \\
\cdots \\
v_n \\
\end{array} \right]$, need to apply exponenetial operation on every element, such that $u = \left[\begin{array}{c}
e^{v_1} \\
\cdots \\
e^{v_n} \\
\end{array} \right]$
 
 
* Non-vectorised:
```{python python_for_loop, eval=FALSE, include=FALSE, echo = TRUE}
u  = np.zeros((n,1))
for i in range(n):
    u[i] = math.exp(v[i])
```
        
* Vectorised
```{python python_vectorisation, eval=FALSE, include=FALSE, echo = TRUE}
import numpy as np
u = np.exp(v)
```

  
Note that the `numpy` library has many vectorised functions, including:

* np.log(v)
* np.abs(v)
* np.maximum(v, 0)
* v**2
* 1/v
    
    
Here remove the for-loop for the features.  Now have just one left.  

1. Initialise $J=0$, $db = 0$, $dw = \text{np.zeros}(n_x, 1)$

2. For $i = 1$ to $m$, 

    * $z^{(i)} = w^T x^{(i)} + b$
    * The prediction $a^{(i)} = \sigma(z^{(i)})$
    * $J += -[y^{(i)}\log a^{(i)} + (1-y^{(i)})\log(1-a^{(i)})]$
    * $dz^{(i)} = a^{(i)} - y^{(i)}$
    * `$dw += x{(i)} dz{(i)}$`
    * $db += dz^{(i)}$

3. Need to divide by m, such that

    * $J /=m$
    * `$dw /= m$`
    * $db /= m$
    
Noting that $J$, $dw_1$, $dw_2$, $db$ are accumulators, but $z^{(i)}$ and $a^{(i)}$ are not accumulators, hence the superscript

4.  Update the parameters: 
$$\begin{aligned}
    w_1 := w_1 - \alpha dw_1 \\
    w_2 := w_2 - \alpha dw_2 \\
    b := b - \alpha db \\
\end{aligned}$$

## Vectorising Logistic Regression Predictions

Vectorisation to compute predictions, $a$: 

Forward Propagation:

$$\begin{aligned}
z^{(1)} = w^Tx^{(1)} + b    && z^{(2)} = w^Tx^{(2)} + b &&&  z^{(3)} = w^Tx^{(3)} + b \\ 
a^{(1)} = \sigma(z^{(1)})   && a^{(2)} = \sigma(z^{(2)}) &&& a^{(3)} = \sigma(z^{(3)}) 
\end{aligned}$$

Let 

 * $Z = [z^{(1)} z^{(2)} \ldots z^{(m)}$, where $z \in \mathbb{R}^{1 \times m}$
 * Training inputs: $X = \left[\begin{array}{cccc}
\mid        & \mid          &           & \mid \\
{\bf X}^{(1)} & {\bf X}^{(2)}   & \cdots    & {\bf X}^{(m)}\\
\mid        & \mid          &           & \mid \\
\end{array} \right]$, where $X$ has $n_x$ rows (number of features) and $m$ columns (number of training examples), i.e. $X \in \mathbb{R}^{n_x \times m}$ 
 * $\mathbf{b} = [b b \ldots b]$, where $b \in \mathbb{R}^{1 \times m}$
 
So that $Z = w^T \mathbf{X} + \mathbf{b}$, achieved in python using `z = np.dot(w.T, x) + b`, but in python $b$ is a raw number, $1 \times 1$.  This operation is called **broadcasting**

Similarly, let:

* $A = [a^{(1)} a^{(2)} \ldots a^{(m)}]$ = $\sigma(Z)$

## Vectorising Logistic Regression Output
How to use vectorisation to perform gradient computations for all $M$ training samples:

Recall for gradient computation, calculated

\[
dz^{(1)} = a^{(1)} - y ^{(1)}\\
dz^{(2)} = a^{(2)} - y ^{(2)}\\
\vdots
\]

If

* $dZ = [dz^{(1)} d^{(2)} \ldots d^{(m)}]$
* $A = [a^{(1)} a^{(2)} \ldots a^{(m)}]$
* $Y = [y^{(1)} y^{(2)} \ldots y^{(m)}]$

Then $dZ = A - Y$

In previous implementation still had for loop for $dw$ and $db$ over the training examples, i.e. for the $dw$
\[
dw = 0 \\
dw += x^{(1)} dz^{(1)}\\
dw += x^{(2)} dz^{(2)}\\
\vdots\\
dw/=m
\]

and for the $db$:
\[
db = 0 \\
db +=  dz^{(1)}\\
db += dz^{(2)}\\
\vdots\\
db/=m
\]

Vectorised version: \[db = \frac{1}{m}  \sum^m_{i=1}dz^{(i)} = \frac{1}{m}  \text{np.sum}(dz)\] and 
$$\begin{aligned}
dw &= \frac{1}{m} \mathbf{X} dz^T  \\
    &= \frac{1}{m} \left[\begin{array}{cccc} \mid & \mid &   & \mid\\
    {\bf X}^{(1)} & {\bf X}^{(2)}   & \cdots    & {\bf X}^{(m)} \\
    \mid        & \mid          &           & \mid
    \end{array}\right]
    
    \left[\begin{array}{c}
    dz^{(1)}\\
    dz^{(2)}\\
    \vdots\\
    dz^{(m)}\\
\end{array} \right]\\
&= \frac{1}{m}[x^{(1)}dz^{(1)} + \ldots + x^{(m)}dz^{(m)}]
  
 \end{aligned}$$

Implementing Vectorised Logistic Regression
1. Initialise $J=0$, $db = 0$, $dw = \text{np.zeros}(n_x, 1)$

2 Forward & back Propagation on all $m$ training examples
    a. $Z = w^T \mathbf{X} + \mathbf{b}$, achieved in python using `z = np.dot(w.T, x) + b`
    b. $A = \sigma(z)$
    c. $dZ = A - Y$
    d. $dw = \frac{1}{m} X dZ^T$
    e. $db = \frac{1}{m} \text{np.sum}(dz)$

3. Gradient descent update
    w := w - \alpha dw
    b := b - \alpha db

If want multiple iterations of gradient descent, then will still need for-loop


## Broadcasting in Python

Technique that python and numpy allows you to use to make code more efficient

Broadcasting example.  Aim to calculate percentage of calories from carbs, proteins and fats, i.e. % in carbs in apples is 56 / (56 + 1.2 + 1.8).  Can this be done without an explicit for-loop?

![Broadcasting Eg](/img/DLS_broadcastingEg.png)

```{python broadcasting_eg, echo=TRUE}
import numpy as np
A = np.array([  [56.0, 0.0, 4.4, 68.0],
                [1.2, 104.0, 52.0, 8.0],
                [1.8, 135.0, 99.0, 0.9]])
print(A)

cal = A.sum(axis = 0) #to sum vertically
print(cal)

percentage = 100 * A/cal.reshape(1,4) # example of python broadcasting.  Dividing 3x4 by 1x4 matrix.
print(percentage)

```

Broadcasting allows you to divide a $3 \times 4$ matrix by a $1 \times 4$ matrix. Python will auto-expand vectors / matrices to allow calculations of vectors and matrices.  

**Examples** 
![Broadcasting Eg](/img/DLS_broadcastingEg2.png)

For more information, see `numpy` documentation.  Equivalent to `bsxfun` in Matlab and Octave.

## A note on python/numpy vectors

Flexibility + broadcasting = strength and weakness

* Strength: can get a lot of done with just one line of code
* Weakness: with flexibility can add subtle bugs.  E.g. Column + row vector may give matrix, rather than perhaps wanted a column or row vector

```{python python_vectors}
import numpy as np
a = np.random.randn(5) # 5 random gaussian
print(a)
print(a.shape) # rank 1 array, neither row nor column vector
print(a.T)     # looks the same as a
print(np.dot(a, a.T)) #would expect matrix, instead get a number

# recommends not using data structures of rank 1 array.
# instead commit to making it a row or column vector
a = np.random.randn(5,1)
print(a)
print(a.T)
print(np.dot(a, a.T))
```


Recommendations:

* Don't use _rank 1 arrays_; instead use row or column vectors.  If do end up with a rank 1 array can use `a = a.reshape((5,1))` to reshape  
* Use `assert(a.shape == (5,1))` type statements to ensure shape


## Quick tour of Jupyter/iPython Notebooks

* Shift + Enter to run (but could be different) 
* Cell > Run Cells
* Markdown cell
* Code is running on a kernel; with excessively large job, the kernel might die.  Just click Kernel > Restart

## Explanation of logistic regression cost function



## Hero of Deep Learning: Pieter Abbeel

Deep reinforcement learning.  Many more questions than in supervised learning.  Where does the data come from?  What actions you took early got you the reward later?  Safety issues.  Still need to solve longer horizon problems.  Wants to run the reinforcement learning in the inner loop to try to improve it.  See also Andrej Karpathy's deep learning course which has videos online.  Berkeley has a deep reinforcement learning course which has all of the lectures online.

Ensure to try things out with frameworks like TensorFlow, Chainer, Theano, PyTorch etc.; it's very easy to get going and get something up and running very quickly. To get to practice yourself, right? 

Discusses article in [Mashable](https://mashable.com/2017/07/28/16-year-old-ai-genius/#0Htj4Vt2IiqO) about a 16-year-old who is one of the leaders on Kaggle competitions. And it just said, he just went out and learned things, found things online, learned everything himself and never actually took any formal course per se. 

At present, deep reinforcement learning always starts from scratch, would be better if it could learn from the past.







# Shallow Neural Network {#week3}
## Neural Networks Overview
* Logistic regression had one hidden layer; which calculates both $z^{[1]}$ and $a^{[1]} = \sigma(z^{[1]})$
* Neural networks can consider of more than one hidden layer
* Square brackets [] used to reference hidden layer
* Round brackets () used to reference training example

![NN Representation](/img/ShallowNN_ComputingOutput1.JPG)

## Neural Networks Representation

Example: Single hidden layer consists ($\equiv$ 2 layer neural network)

* Input layer, $a^{[0]} = \mathbf{X} = {x_1, x_2, x_3}$  within training set.  Use $a$ for activation.  Called layer 0 as not counted as not counted as an "official layer".
* Hidden layer(s) generates activations. The first hidden layer, for example, with 4 nodes, is denoted $a^{[1]} = \{a_1^{[1]}, a_2^{[1]}, a_3^{[1]},  a_{4}^{[1]}\}$.  Called __hidden__ as don't see true values in training set.  First hidden layer will have associated parameters, for example when $X \in \mathbb{R}^{1 \times 3}$ and $a^{[1]} \in \mathbb{R}^{4 \times 1}$ then will have parameters
    * $w^{[1]} \in \mathbb{R}^{4 \times 3}$
    * $b^{[1]} \in \mathbb{R}^{4 \times 1}$ 
* Output layer ($y$ within training set).  With one hidden layer, this is referred to as a the second layer and have  $\hat{y} = a^{[2]}$  In this example of the neural network with one hidden layer of 4 units, the parameters associated with the output layer are:
    * $w^{[2]} \in \mathbb{R}^{1 \times 4}$
    * $b^{[2]} \in \mathbb{R}^{1 \times 1}$ 

## Computing a Neural Network's Output

* Use matrix multiplication to calculate nodes each activation layer to avoid use of for loops

![NN Output Computation](/img/ShallowNN_ComputingOutput2.JPG)

* For a 2 layer neural network will need just 4 equations:

![NN Output Computation](/img/ShallowNN_ComputingOutput3.JPG)


## Vectorizing across multiple examples

* Last example, for one single training example
* This example, for $m$ training examples
* For each training example, $i = 1 \ldots m$
    * $z^{[1](i)} = w^{[1]}x^{(i)} + b^{[1]}$
    * $a^{[1]} = \sigma(z^{[1](i)})$
    * $z^{[2](i)} = w^{[2]}a^{[1](i)} + b^{[2]}$
    * $a^{[2]} = \sigma(z^{[2]})$
* But want to get rid of for loop and so stack $x$,$z$ and $a$ vertically (in columns) such that
    * $Z^{[1]} = W^{[1]}X + b^{[1]}$
    * $A^{[1]} = \sigma(Z^{[1]})$
    * $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$
    * $A^{[2]} = \sigma(Z^{[2]})$


* $X \in \mathbb{R}^{(n_x, m)}$
* Horizontal index = index to training example
* Vertical index = index to node in neural network

## Explanation for Vectorized Implementation

* First training example, compute: $z^{[1](1)} = w^{[1]}x^{(1)} + b^{[1]}$
* Second training example, compute: $z^{[1](2)} = w^{[1]}x^{(2)} + b^{[1]}$
* Third training example, compute: $z^{[1](3)} = w^{[1]}x^{(3)} + b^{[1]}$

Now ignore $b$ for now (i.e. assume 0), such that:

* First training example, compute: $z^{[1](1)} = w^{[1]}x^{(1)}$
* Second training example, compute: $z^{[1](2)} = w^{[1]}x^{(2)}$
* Third training example, compute: $z^{[1](3)} = w^{[1]}x^{(3)}$


![NN Output Computation](/img/ShallowNN_Vectorized_Implementation.JPG)

Note that $W$ is a matrix

* $w^{[1]} \in \mathbb{R}^{4 \times 3}$
 * $w^{[2]} \in \mathbb{R}^{1 \times 4}$
  
Makes use of Python broadcasting.

## Activation Functions

So far been using sigmoid function as activation function \[a = \frac{1}{1 + e^{-z}}\], but this is not the best choice; there are other options available.

In general case, replace $\sigma(z^{[k]})$ by $g(z^{[1]})$.

Other options:

##### 1. Hyperbolic Tangent function \[a = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\]
It is a shifted sigmoid function that goes from -1 to 1 instead of from 0 to 1.  Crosses origin.  Almost always works better. As means closer to zero (similar to centring data)

Pretty much never uses sigmoid function; with _exception_ for output layer in binary classification.  as output $y \in {0,1}$, i.e. $0 \leq \hat{y} \leq 1$.  So would use tanh function in hidden layer and sigmoid function activation in output layer.

Downside: if $z$ is very small or very large then derivative becomes very small and so can slow down gradient descent (similar to sigmoid function)

##### 2. ReLU (rectified linear unit) function \[a = \max(0, z)\]
    * If $z > 0$ then derivative is positive
    * If $z < 0$ then derivative is 0
    * If $z = 0$, then derivative not defined, but odds very small.  In practice, can pretend that either 0 or 1.
    
This is becoming increasingly the default choice for activation function.     

Advantage: Derivative is different from zero when $z > 0$ and so does not slow down loading

Disadvantage:  Derivative is zero when $z < 0$

##### 3. Leaky ReLU \[a = \max(0.01z, z)\]

* Usually works better but not used as much in practice
Andrew Ng usually uses 
* Can use a training parameter instead of 0.01


## Why You Need Non-Linear Activation Functions
If you use a linear activation function (identity), then all you calculate is 
$$\begin{aligned}
    a^{[2]}  = z^{[2]} & = w^{[2]}a^{[1]}x + b^{[2]} \\
                        & = w^{[2]}(w^{[1]}a^{[0]} + b^{[1]})x + b^{[2]} \\
                        & = (w^{[2]}w^{[1]})x + (w^{[2]} b^{[1]} + b^{[2]} ) \\ 
                        &= w^\prime x + b^\prime
\end{aligned}$$

So no matter how many layers, will just be computing a linear activation function, so may as well not have any hidden layers.

Linear hidden layer useless.  __Exception__ for output unit only of regression problem, i.e. when $y \in \mathbb{R}$.  But even then, if $y > 0$ then probably better to use a ReLU function.

## Derivatives of Activation Functions
Required for back-propagation of gradient descent.

Sigmoid activation function:
$$\begin{aligned}
    g(z) &= \frac{1}{1 + e^{-z}} \\
    \frac{d}{dz}g(z) &= \frac{1}{1 + e^{-z}}(1 - \frac{1}{1 + e^{-z}}) \\
    &= g(z)\left(1 - g(z)\right) \\
    &= a\left(1 - a\right)
\end{aligned}$$

Values:
* If $z = 10, g(z) \approx 1. g^\prime(z) \approx 1(1-1) \approx 0$
* If $z = -10, g(z) \approx 0. g^\prime(z) \approx 0(1-0) \approx 0$
* If $z = 0, g(z) = 0.5. g^\prime(z) = 0.5(1-0.5) = 0.25$

Tanh activation function
$$\begin{aligned}
    g(z) &= \tanh(z) \\
    \frac{d}{dz}g(z) &= \frac{e^z - e^{-z}}{e^z + e^{-z}}\\
    &= 1 - \left(\tanh(z)\right)^2 \\
    &= 1 - a^2
\end{aligned}$$

Values:

* If $z = 10, g(z) \approx 1. g^\prime(z) \approx 1-1^2 \approx 0$
* If $z = -10, g(z) \approx -1. g^\prime(z) \approx 1-(-1)^2 \approx 0$
* If $z = 0, g(z) = 0. g^\prime(z) = 1-0 = 1$

ReLU
$$\begin{aligned}
    g(z) &= \max(0, z) \\
    g^\prime(z) &= \begin{cases} 0 &\text{if $z < 0$}\\
                                1 & \text{if $z > 0$}\end{cases}
\end{aligned}$$
Gradient not technically defined at $z = 0$ but can set to 1. 

Leaky ReLU
$$\begin{aligned}
    g(z) &= \max(0.01z, z) \\
    g^\prime(z) &= \begin{cases} 0.01 &\text{if $z < 0$}\\
                                1 & \text{if $z \geq 0$}\end{cases}
\end{aligned}$$
Gradient not technically defined at $z = 0$ but can set to 1. 

## Gradient descent for Neural Networks

Parameters for single hidden layer:

* $w^{[1]} \in \mathbb{R}^{(n^{[1]},n^{[0]})}$
* $b^{[1]} \in \mathbb{R}^{(n^{[1]},1)}$
* $w^{[2]} \in \mathbb{R}^{((n^{[2]},n^{[1]}))}$
* $b^{[2]} \in \mathbb{R}^{(n^{[2]}, 1)}$

where

* $n_x = n^{[0]}$ is the number of input parameters
* $n^{[1]}$ is the number of units in the single hidden layer
* $n^{[2]} = 1$ is the output unit


Assuming binary classification, loss function is\[J(w^{[1]}, b^{[1]}, w^{[2]}, b^{[2]}) = \frac{1}{m}\sum^n_{i=1}\mathcal{L}(\hat{y}, y)\]

To train parameters, need gradient descent.  After initialising parameters _randomly_ (important not to set all to zero),

Repeat {

* Compute prediction ($\hat{y}^{(i)}, i = 1, \ldots, m$)
* Compute derivative $dW^{[1]} = \frac{dJ}{dw^{[1]}}$, $db^{[1]} = \frac{dJ}{db^{[1]}}$, $\ldots$
* Update phase 

$$\begin{aligned}
        w^{[1]} &:= W^{[1]} - \alpha dW^{[1]}\\
        b^{[1]} &:= b^{[1]} - \alpha db^{[1]}\\
        \vdots\\
\end{aligned}$$  
}

Formulas for computing partial derivatives.

Forward propagation:
$$\begin{aligned}
    Z^{[1]} &= W^{[1]}X + b^{[1]}\\
    A^{[1]} &= g^{[1]}(Z^{[1]})\\
    Z^{[2]} &= W^{[2]}A^{[1]} + b^{[2]}\\
    A^{[2]} &= g^{[2]}(Z^{[2]})
\end{aligned}$$
    
Back propagation
$$\begin{aligned}
    dZ^{[2]} &= A^{[2]} - Y \\
    dW^{[2]} &= \frac{1}{m} dZ^{[2]}A^{[1]T}\\
    db^{[2]} &= \frac{1}{m}\text{np.sum($dZ^{[2]}$, axis = 1, keepdens = True}\\
    dZ^{[1]} &=  W^{[2]T}dZ^{[2]} *  g^{[1]\prime}(Z^{[1]})
\end{aligned}$$
    
    
## Backpropagation Intuition
![Computation Graph](/img/DLS_ComputationGraphLR.png)



Logistic Regression
$$\begin{aligned} 
 \text{da} =    \frac{d\mathcal{L}(a,y)}{da} &= \frac{d \left[-\left(y \log (a) + (1-y)\log(1-a)\right)\right]}{da}\\
                    &= -\frac{y}{a} + \frac{1-y}{1-a} \\
   \text{dz} =   \frac{d\mathcal{L}(a,y)}{dz} &= \frac{d\mathcal{L}}{da}\frac{da}{dz} \\
                    &= \text{da} \frac{d}{dz}g(z) \\
                    &= \text{da} \; g^\prime(z)\\
 \text{dw} = \frac{d\mathcal{L}(a,y)}{dw} 
                        &=\frac{d\mathcal{L}(a,y)}{dz} \frac{dz}{dw}\\
                        &= \text{dz} \; \frac{d}{dw}(w*x + b)\\
                        &= \text{dz} \;  x\\
            \text{db}  \frac{d\mathcal{L}(a,y)}{db} &= \frac{d\mathcal{L}(a,y)}{dz} \frac{dz}{db}\\
            &= \text{dz} \frac{d}{db}(w*x + b) \\
            &= \text{dz}
\end{aligned}$$



Neural Network
\begin{aligned} 
    \text{dz}^{[2]} & =   a^{[2]} - y\\
    \text{dw}^{[2]} &=\frac{d\mathcal{L}}{dz^{[2]}} \frac{dz^{[2]}}{dw^{[2]}}\\
                    &= \text{dz}^{[2]} \; \frac{d}{dw^{[2]}}(W^{[2]}*a^{[1]} + b^{[2]})\\
                    &= \text{dz}^{[2]} \;  a^{[1]}\\
    \text{db}^{[2]} &= \frac{d\mathcal{L}}{dz^{[2]}} \frac{dz^{[2]}}{db^{[2]}}\\
                    &= \text{dz}^{[2]} \frac{d}{db^{[2]}}(W^{[2]}*x + b^{[2]}) \\
                    &= \text{dz}^{[2]} \\
            
    \text{dz}^{[1]}  &= \frac{d\mathcal{L}}{dz^{[2]}}\frac{dz^{[2]}}{dz^{[1]}} \\
                    &= \text{dz}^{[2]} \; \frac{d}{dz^{[1]}}\left(W^{[2]}a^{[1]} + b^{[2]}\right)\\
                    &= \text{dz}^{[2]} \; \frac{d}{dz^{[1]}}\left(W^{[2]}g^{[1]}(z^{[1]}) + b^{[2]}\right)\\
                    &= \text{dz}^{[2]} \; \left(W^{[2]}g^{[1]\prime}(z^{[1]})\right)\\
                    &= w^{[2]T} dz^{[2]} * g^{[1]\prime}(z^{[1]})\\
    \text{dw}^{[1]} &=\frac{d\mathcal{L}}{dz^{[1]}} \frac{dz^{[1]}}{dw^{[1]}}\\
                    &= \text{dz}^{[1]} \; \frac{d}{dw^{[1]}}(w^{[1]}*x + b^{[1]})\\
                    &= \text{dz}^{[1]} \;  x^T\\
    \text{db}^{[1]}  &= \frac{d\mathcal{L}}{dz^{[1]}} \frac{dz^{[1]}}{db^{[1]}}\\
                    &= \text{dz}^{[1]} \frac{d}{db^{[1]}}(w^{[1]}*x + b^{[1]}) \\
            &= \text{dz}^{[1]}
\end{aligned}

$$\begin{aligned}
    dZ^{[2]} &= A^{[2]} - Y \\
    dW^{[2]} &= \frac{1}{m} dZ^{[2]}A^{[1]T}\\
    db^{[2]} &= \frac{1}{m}\text{np.sum($dZ^{[2]}$, axis = 1, keepdens = True}\\
    dZ^{[1]} &=  W^{[2]T}dZ^{[2]} *  g^{[1]\prime}(Z^{[1]})\\
    dW^{[1]} &= \frac{1}{m} dZ^{[1]}X^{T}\\
    db^{[1]} &= \frac{1}{m}\text{np.sum($dZ^{[1]}$, axis = 1, keepdens = True}
\end{aligned}$$

See alternative explanations of backpropagation refer to:

* [https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/](https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/)
* [http://colah.github.io/posts/2015-08-Backprop/](http://colah.github.io/posts/2015-08-Backprop/)

## Random Initialisation
* Logistic regression can initialise weights to zero
* Neural network, if initialise weights to zero, gradient descent will not work.  Initialising bias ($b$) to zero is okay.
* If weights and bias are zero, then 
    * $a_1^{[1]} = a_2^{[1]}$
    * $dz_1^{[1]} = dz_2^{[1]}$
    * The first row will always equal the second row.  This is called the symmetry breaking problem.
    * Then may as well just have one unit
    * Proof can be performed via induction
* Solution, e.g. for 1-layer network with 2 units in hidden layer: 
    * Set $w^{[1]}$ = np.random.randn((2,2)) * 0.01
    * Set $b^{[1]}$ = np.zero((2,1))
    * Set $w^{[1]}$ = np.random.randn((1,2)) * 0.01
    * Set $b^{[2]}$ = 0
    
Use 0.01 as want to avoid large values of z (which will happen if have large w)
$$\begin{aligned}
    z^{[1]} &= W^{[1]}x + b^{[1]}\\
    a^{[1]} &= g^{[1]}(z^{[1]})
\end{aligned}$$   
Less of issue if not using sigmoid or tanh activation functions    

For deep neural networks may be a better option than 0.01.

## Hero of Deep Learning: Ian Goodfellow

* Wrote first textbook on modern version of deep learning
* GAN = generative algorithm


# Deep Neural Network {#week4}



## Deep L-layer neural network
* To assess quality of class probabilities
* Shows observed probability vs predicted class probability
* Method:
    * Obtain scores for test set using classification model
    * Bin data into, e.g. 10, groups based on class probabilities 
    * For each bin, determine observed event rate
    * Plot bin midpoint on x-axis and observed event rate on y-axis
    * If points fall on 45$^{\circ}$ line then well-callibrated
    
    
* Can set layer as hyperparameter for tuning    
* $L$: Number of layers
* $n^{[\ell]}$: Number of units in layer $\ell$. Recall that input layer is layer 0, but is not counted in number of layers.
* $a^{[\ell]} = g^{[\ell]}(z^{[\ell]})$: Activations in layer $\ell$
* $w^{[\ell]}$: Weights for computing values $z^{[\ell]}$
* $b^{[\ell]}$: Bias vector for layer $\ell$ 
* $X = a^z{[\ell]}$ = input layer
* $a^{[L]} = \hat{y}$ = output layer 

## Forward Propagation

![Deep NN Forward Propagation](/img/DeepNN_ForwardPropagation.JPG)

## Getting your matrix dimensions right

* Recall: $z^{[1]} =  a^{[0]} + b^{[1]}$ of the first hidden layer  has dimension ($n^{[1]}$,1)

Dimensions:

* $w^{[\ell]}: (n^{[\ell]}, n^{[\ell-1]})$.  Same dimension for $dw^{[\ell]}$ in backpropagation.
* $b^{[\ell]}: (n^{[\ell]}, 1)$.  Same dimension for $db^{[\ell]}$ in backpropagation.
* The activation $a^{[\ell]} = g^{[\ell]}(z^{[\ell]})$ has the same dimension as $z^{[\ell]}$ 

For vectorisation

* Stack $z$ horizontally for each training example, so $Z^{[\ell]}: (n^{[\ell]},m)$
* Stack $A$ horizontally for each training example, so $A^{[\ell-1]}: (n^{[\ell-1]},m)$.  
* $b$ remains $(n^{[\ell]},1)$ but with python broadcasting this becomes $(n^{[\ell]},m)$

In summary:

* $Z^{[\ell]}, A^{[\ell]}: (n^{[\ell]},m)$, but with $\ell = 0, A^{[0]} = X: (n^{[0]},m)$
* Similarly for backpropagation,  $dZ^{[\ell]}, dA^{[\ell]}: (n^{[\ell]},m)$

 
## Why deep representations

Deep network 

* Face recognition/detection (visualisation will make more sense with convolutional neural network (CNN))
    * First layer - where are the edges? (small edges of image)
    * Second layer - detect eye, nose
    * Later layer - detect (larger areas of image)
* Speech recognition
    * First layer - low level audio-waveform
    * Second layer - basic units of sound, phoneme
    * Third layer - Words
    * Last layer - Sentences/  Phases
* Could be dangerous to make analogy between deep learning and how the human brain works

Circuit theory and deep learning

> Informally, there are functions you can compute with a small L-layer deep nueral network that shallower networks  require exponentially more hidden units to compute.

Re-branding: Neural networks with many layers re-branded as "Deep Learning".

Usually starts with logistic regression and then tries 1-2 hidden layers and considers the number of hidden layers as a tuning parameters.

## Building blocks of deep neural networks

* Usually cache W and B for convenience.

![Deep NN Forward Propagation](/img/DeepNN_BuildingBlockFlow.JPG)    

## Forward and Backward Propagation
![Forward Propagation](/img/DeepNN_FandBPropagation1.JPG)
![Back Propagation](/img/DeepNN_FandBPropagation2.JPG)
![Forward and Back Propagation](/img/DeepNN_FandBPropagation3.JPG)

## Parameters and Hyperparameters

Parameters: $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}, \ldots$

Hyperparameters: Control parameters (i.e. determine their values)

* Learning rate $\alpha$ (previously may have been sloppy and referred to as parameter)
* Number of iterations
* Number of hidden layers $L$
* Number of hidden units $n^{[1]}, n^{[2]}, \dots$
* Choice of activation function
* $\vdots$
* Momentum
* Mini-batch size
* Regularization

Empirical process: Idea $\rightarrow$ Code $\rightarrow$ Experiment $\rightarrow$ Idea $\rightarrow$ $\ldots$.  i.e. need to try a lot and see how it works

Very difficult to know in advance the hyperparameters.  Need to go around cycle and iterate.  But in next course will discuss systematic way for trying out a range of values.  Note that even if tune to best hyper-parameter today may change in a year or so for the same problem (could be CPU..)

## What does it have to do with the human brain?

Not a whole lot... but analogy is made because of:
![Brain Analogy](/img/DeepNN_BrainAnalogy.JPG)
