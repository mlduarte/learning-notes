---
title: 'Book: Using Multivariate Statistics'
author: Marie
date: '2018-06-11'
slug: book-using-multivariate-statistics
banner: "img/banners/tabachnick.png"
categories:
  - Study-Notes
tags:
  - Book
  - Study-Notes
---

```{r load_packages, message=FALSE, warning=FALSE, include=FALSE}
if (!requireNamespace("tidyverse")) install.packages("tidyverse")
require("tidyverse")

if (!requireNamespace("knitr")) install.packages("knitr")
require("knitr")

if (!requireNamespace("psych")) install.packages("psych")
require("psych")

if (!requireNamespace("GPArotation")) install.packages("GPArotation")
require("GPArotation")

if (!requireNamespace("DiagrammeR")) install.packages("DiagrammeR")

require(DiagrammeR)
library(DiagrammeRsvg)
library(magrittr)
library(rsvg)
library(png)


```

# Principal Components and Factor Analysis

## General Purpose 

* To find subsets of variables (factors/components/latent variables) that are independent, where variables in a subset are correlated
* To summarise patterns of correlations among variables
* To decrease number of variables to a smaller number of factors/components, by using linear combinations of observed variables (more parsimonious and because factors/components are uncorrelated, more reliable)
* To understand underlying processes
* To describe (and maybe understand) relationship between variables
* To test theory about underlying processes

## Steps:

* Hypothesise factors underlying domain of interest
* Select and measure variables
* Prepare correlation matrix
* Extract factors/components
* Determine number of factors/components
* Rotate to increase interpretability
* Interpret results
  * Should make sense
  * Easier if variables within one factor do not correlate with variables in another factor
* Verify factor structure by establishing validity, for example, confirm that factor scores change with experimental conditions as predicted

For factor analysis

* Hypothesise factors underlying domain of interest
* Select 5-6 variables thought to be a pure measure of the factor (marker variables); consider variable complexity (i.e the number of factors for which a variable may correlate)
* Choose a sample across variables/factors with spread of scores
* Be wary to pool results of several samples; samples differing, for example, in SES may also have different factors.  Only pool if different samples produce the same factors.

  
## Types of Factor Analysis

Exploratory | Confirmatory
-------------------------------------------- | ----------------------------------
Early stages of research  |  Advanced stages of research
To summarise data by grouping together correlated variables | To test theory about latent processes
Variables may not have been chosen with underlying processes in mind | Variables chosen to reveal underlying processes
Factor Analysis / PCA | Usually via Structural Equation Modelling (SEM)


## Research Questions
* How many factors
* What do the factors mean
* How much variability is accounted for by each/all factors
* How well does the factor solution fit that expected
* Had factors been measured directly, what scored would subjects have received?

## Limitations

  * No ready criteria for verification
  * Infinite rotations; final choice is based on subjective assessment of interpretability
  * Often used as a last resort to make order from chaos (i.e. suffers from a poor reputation)
  * Exploratory
  * Subjective
  * Sensitive to outliers, missing data, poorly distributed samples, small samples

### Outliers (among cases)


* Univariate and multivariate (combination of variables) outliers will have greater influence on factor solution 
* TODO chapter 4 and 13.7.4.14 to detect and reduce influence

### Outliers among variables (FA only)
If there is a factor with $\leq$ 2 variables and significant variable accounted for:

* Ignore, or treat with caution
* The factor should be researched further with structural equation modelling (SEM)

Factors with few variables and small variance accounted for are unreliable.

### Missing data
Estimate if 

* Missing distribution of values not random
* Sample size will be too small if delete cases; 

Otherwise, delete cases

### Poorly distributed variables (Non-Normality)
* If PCA/FA used descriptively, i.e. to summarise relationships, normality assumptions not in force, but solution is enhanced with normality
* If statistical inference used to determine number of factors, single variable and multivariate normality (linear combinations of variables are normally distributed) is assumed
* If variable has substantial skewness and kurtosis, consider variable transformation
* Multivariate normality tests are sensitive
* Note that some SEMS permit PCA/FA with non-normal variables

### Non-Linearity
* Correlations measures linear relationships therefore analysis is degraded by non-linear relationship (as not captured)
* Assessed visually using scatterplots

### Small samples

Sample size required will depend on population correlations and the number of factors
    
Characteristics | Recommended Sample Size
----------------|-------------------------
Communalities > 0.6 | < 100
Communalities ~ 0.5, loadings > 0.8 | 100-200
Low communality; small # factors,3-4 indicators for each factor  | > 300
Low communality, large # weakly determined factors | > 500
  
### Multicollinearity  and Singularity (FA only)

Although in FA/PCA interest is in finding correlated variables, if correlation is too high, matrix inversion becomes unstable (multicollinearity) or impossible (singularity).  If determinant of correlation matrix ($R$) is close to zero, then multicollinearity or singularity may be present

To detect, look at Squared Multiple Correlation (**SMC**), if SMC is high (> 0.9 multicollinearity, 1=singularity) then delete that variable

### Lack of Factorability within Correlation Matrix (R)
FA assumes relationships between variables; ; a factorable matrix should have several sizeable correlations. Note however that high bivariate correlations are not a guarantee of factors; they may merely be two similar variables and not reflect an underlying process simultaneously affecting them.

If factors are present, then 

* Look at correlation matrix (**R**): Should expect many sizeable correlations.  if no correlation within the correlation matrix (**R**) with exceeds 0.3 then there is probably nothing to factor analyse.  
* the anti-image correlations matrix (pairwise correlations adjusted for the effects of all other variables) will have mostly small values among the off-diagonal elements. 
* Kaiser's measure of sampling adequacy, $\sum(\text{squared correlations})/(\sum(\text{squared correlations}) + \sum(\text{squared partial correlations}))$ > 0.6

## Fundamental Equations for Factor Analysis


Matrices 

Label | Matrix Name | Description | Orthogonal | Oblique
------|---------------|-----------------------------------------|---------|-------
$R$ | Correlation               |(Observed) Between-variable correlation | <i class="fa fa-check-square-o"></i> |<i class="fa fa-check-square-o"></i>
$Z$ | Variable                  |Standardized observed variable values(scores) | <i class="fa fa-check-square-o"></i> |<i class="fa fa-check-square-o"></i>
$F$ | Factor-score              |Standardized factor scores | <i class="fa fa-check-square-o"></i> |<i class="fa fa-check-square-o"></i>
$A$ | Factor loading            |Correlations between factor & variable | <i class="fa fa-check-square-o"></i> |<i class="fa fa-square-o"></i>
$A$ | Pattern                   |Contribution of factor to variance | <i class="fa fa-square-o"></i> |<i class="fa fa-check-square-o"></i>
$C$ | Structure                 | Correlations between variables and correlated factors | <i class="fa fa-square-o"></i> |<i class="fa fa-check-square-o"></i>
$B$ | Factor-Score Coefficients | Coefficients to generate factor scores from variables | <i class="fa fa-check-square-o"></i> |<i class="fa fa-check-square-o"></i>
$\Phi$                          | Factor correlation|Correlations among factors | <i class="fa fa-square-o"></i> |<i class="fa fa-check-square-o"></i>
$L$ | Eigenvalue                |Diagonal matrix of eigenvalues, one per factor | <i class="fa fa-check-square-o"></i> |<i class="fa fa-check-square-o"></i>
$V$ | Eigenvector               |Eigenvectors, one per eigenvalue | <i class="fa fa-check-square-o"></i> |<i class="fa fa-check-square-o"></i>


<br><br>
Equations: 

* **Reproduced Correlation Matrix ($\bar{R}$)**: Factor correlation, $\bar{R} = A A'$
* **Residual Correlation Matrix ($R_\text{res}$)**: Observed - Reproduced (small for good factor analysis), $R_\text{res} = R - \bar{R}$
* **Factor Score Coefficients Matrix ($B$)**: Coefficients from linear combinations of variables, $B = R^{-1} A$.
* **Factor Scores**: $F = ZB$
* **Predicted Scores**: $Z = FA'$

    
### Fundamental Equations for Factor Analysis: Extraction
The correlation matrix ($R$), which by nature is symmetric, can be diagonalised by the matrix $V$ (whose columns are the eigenvectors), such that 
\[L = V'RV\]
where $L$ is a diagonal matrix with values eigenvalues in the diagonal

**Example**
```{r load_data_ski, echo=FALSE, message=FALSE, warning=FALSE}
dat.ski <- data.frame(skiers = paste0("S", c(1:5), sep=""), cost = c(32, 61, 59, 36, 62), lift=c(64, 37, 40, 62, 46) , depth = c(65, 62, 45, 34, 43), powder = c(67, 65, 43, 35, 40)) 
dat.ski %>% kable()
```
<br><br>

Correlation Matrix
```{r ski_corr}
cor.ski <- cor(dplyr::select(dat.ski, -skiers)) 
cor.ski
```

Strong correlations between:

* importance placed on _cost_ of ski ticket and speed of ski _lift_
* importance placed on snow _depth_ and mositure (_powder_)


Eigenvalues and Eigenvectors:
```{r ski_eigenvalues}
eig.ski <- eigen(cor.ski)
eig.ski
```

Note that $L = V'RV'$
```{r ski_diag}
zapsmall(t(eig.ski$vectors) %*% cor.ski %*% eig.ski$vectors)
diag(eig.ski$values)
```
and that pre-and post-multplying the corelation matrix by eigenvectors does not change it so much as repackage it:
\[V'V = I\]

```{r ski_identity}
zapsmall(crossprod(eig.ski$vectors))
```

Rearranging the equation $L=V'RV$,

\[\begin{align}
  R &= VLV'\\
    &= V\sqrt{L}\sqrt{L} V'\\
    &=(V\sqrt{L})(\sqrt{L}V') \\
    &= A A'
    \end{align}\]
where $A=V\sqrt{L}$ is referred to as the factor loading matrix; the correlation matrix is a product of hte factor laoding matrix $A$ and its transpose.

In this case, the (unrotated) factor loading matrix is:
```{r ski_loading}
eig.ski$vectors %*% sqrt(diag(eig.ski$values))
```


  The correlation matrix can herefore be considered a product of two matrices - each a combination of eigenvectors and the square root of eigenvalues  
  
* Because there are 4 variables, there are 4 eigenvalues 
* Each eigenvalue corresponds to a different potential factor; usually only factors with large eigenvalues are retrained
* In good FA, a few factors will almost duplicate correlation matrix
* In this example, only first 2 factors, with eigenvalues > 1 are large enough

Starting with the correlation matrix $R$ and assuming $k$ factors are used, the steps of factor analysis (principal factor solution method) are as follows:

  1. Get eigenvalues $L$ and eigenvectors $V$ of correlation matrix $r$
  2. Calculate $C = \sum \text{diag}(R)$
  3. Calculate the loadings, $A = V[,1:k]\sqrt{L[1:k]}$,  (eq 13.6 of text)
  4. Set $R^* = AA'$  (eq 13.5 of text, R=AA')
  5. Set $C^* = \sum \text{diag} (R^*)$
  6. Update $\text{diag}(R) = \text{diag}(R^*)$
Repeat above steps until max iterations reached, or until $e = |C-C^*)|$ is smaller than some threshold


```{r ski_fa_iter}
# This is taken from the function fac and is replicated using 
#fit <- fa(cor.ski, nfactors=2, fm="pa")
#fit # print results

# Initialisation
r <- cor(dplyr::select(dat.ski, -skiers)) 
n <- dim(r)[2]
r.mat <- r
colnames(r.mat) <- rownames(r.mat) <- colnames(r)
nfactors <- 2
max.iter <- 50
min.err <- 0.001

orig <- diag(r)
comm <- sum(diag(r.mat))
err <- comm
i <- 1
comm.list <- list()

e.values <- eigen(r)$values

# Loop
while (err > min.err) {
  eigens <- eigen(r.mat, symmetric = TRUE)
  if (nfactors > 1) {
    loadings <- eigens$vectors[, 1:nfactors] %*% 
      diag(sqrt(eigens$values[1:nfactors]))
  }
  else {
    loadings <- eigens$vectors[, 1] * sqrt(eigens$values[1]) #A in book
  }
  model <- loadings %*% t(loadings) # eqn 13.5: R = AA' (if r is n x n this will always be an n x n matrix, because n x k %*% k x n gives n x n matrix)
  new <- diag(model)
  comm1 <- sum(new)
  diag(r.mat) <- new                # update diagonals of correlation matrix
  err <- abs(comm - comm1)
  if (is.na(err)) {
    warning("imaginary eigen value condition encountered in fa\n Try again with SMC=FALSE \n exiting fa")
    break
  }
  comm <- comm1
  comm.list[[i]] <- comm1
  i <- i + 1
  if (i > max.iter) {
    if (warnings) {
      message("maximum iteration exceeded")
    }
    err <- 0
  }
}

# Clean-up
eigenv <- eigens$vectors
eigens <- eigens$values
if (nfactors > 1) {
  sign.tot <- vector(mode = "numeric", length = nfactors)
  sign.tot <- sign(colSums(loadings))
  sign.tot[sign.tot == 0] <- 1
  loadings <- loadings %*% diag(sign.tot)
} else {
  if (sum(loadings) < 0) {
    loadings <- -as.matrix(loadings)
  }
  else {
    loadings <- as.matrix(loadings)
  }
  colnames(loadings) <- "MR1"
}
colnames(loadings) <- paste("PA", 1:nfactors, sep = "")
rownames(loadings) <- rownames(r)
loadings[loadings == 0] <- 10^-15
model <- loadings %*% t(loadings)
f.loadings <- loadings

```


After iterating through the Factor Analysis `r i` times we get the following eigenvectors:
```{r ski_fin_eigenvectors, echo=FALSE}
as.data.frame(round(eigenv[, 1:nfactors],3), rownames=FALSE)
```

and eigenvalues
```{r ski_fin_eigenvalues, echo=FALSE}
round(eigens[1:nfactors],3)
```


Note that the property,  $VV' = I$, is maintained.
```{r demo_VVtEI}
crossprod(eigenv[, 1:nfactors])
```

The factor loading matrix, $A = V\sqrt{L}$, is
```{r ski_loadings, echo=FALSE}
#round(eigenv[, 1:nfactors]*sqrt(eigens[1:nfactors]),3)
round(f.loadings,3)
```


The factor loading matrix lists the correlations between factors and variables:

  * Factor 1 reflects snow conditions
  * Factor 2 reflects resort conditions; people who score high on this factor place a lot of importance on the cost of a ski ticket without concern for the lift speeds whereas those that score low on this factor value lift speeds more than ski ticket costs.


## Orthogonal Rotation
When factors are uncorrelated; produces a loading matrix (correlations between factors and variables)

Rotation used to 

  * Maximise high correlations between factors and variables
  * Minimise low correlations between factors and variables
  
\[\begin{align}
  \text{Rotated Loading Matrix} &= (\text{Unrotated Loading Matrix})(\text{Transformation Matrix})\\
    A_\text{Rotated} &= A_\text{Unrotated}\Lambda
    \end{align}\]  

The transformation matrix is a matrix of sines and cosines of an angle $\psi$.  In this example an angle of $\psi = 19^\circ$ is used, with \[\Lambda = \begin{bmatrix}\cos \psi & -\sin \psi\\ \sin \psi & \cos \psi\end{bmatrix}\]

```{r ski_orthog}
rotated <- stats::varimax(loadings)
loadings_rotated <- rotated$loadings
rot.mat <- rotated$rotmat
rot.mat  # transformation matrix
```

The communalities, variance and covariance are shown in the following table:
```{r ski_orthog_res}
fit <- fa(cor.ski, nfactors=2, fm="pa", rotate="varimax")
loadings_sol <- unclass(fit$loadings)
ssls <- apply(loadings_sol, 2, function(x) sum(x^2)) # row-total
comms <- apply(loadings_sol, 1, function(x) sum(x^2)) # column-total
prop_var <- ssls/nrow(loadings_sol)
prop_cov <- ssls/sum(comms)

res <- rbind(loadings_sol, ssls, prop_var, prop_cov)
res <- data.frame(var = rownames(res), res) %>% mutate(communalities = ifelse(var %in% c('cost', 'lift', 'depth', 'powder'), PA1^2 + PA2^2, PA1 + PA2))
res
```


The **reproduced correlation matrix** is $\bar{R} = AA'$:
```{r ski_ortho_Rbar}
round(loadings_sol %*% t(loadings_sol),3)
```

The **residual correlation matrix** is $R_\text{res} = R - \bar{R}$
```{r ski_orthog_RRes}
rres <- cor.ski - loadings_sol %*% t(loadings_sol)
diag(rres) <- 0 # could otherwise replace with communalities
round(rres,3)

```
The FA is good if numbers in residual correlation matrix are small.

Factor score coefficients (for estimating factor scores) are found by multiplying the inverse correlation matrix and the factor loading matrix,  $B = R^{-1}A$
```{r ski_orthog_factor_score_coefficients}
B <- solve(r) %*% loadings_sol
B
```

Factor scores are a product of standardized variable scores and factor score coefficients, $F = ZB$:
```{r ski_orthog_factor_scores}
F <- as.matrix(select(dat.ski, -skiers) %>% mutate_all(scale)) %*% B
F
```
Here, for example, the first subject scores strongly on both snow (strong importance on both depth and powder) and resort factor (strong importance on lift over cost).

Predicting standardized scores on variables from scores on factors is also possible, using a product of scores on factors wiethged by factor laodings, $Z = FA'$:
```{r ski_orthog_score_predict}
Z <- F %*% t(loadings_sol)
round(Z, 3)
```
In algebraic form, the formula is:
\[\begin{align}
  z_\text{COST} &= a_{11}F_1 + a_{12}F_2 \\
  z_\text{LFIT} &= a_{21}F_1 + a_{22}F_2 \\
  z_\text{DEPTH} &= a_{31}F_1 + a_{312}F_2 \\
  z_\text{POWDER} &= a_{41}F_1 + a_{42}F_2
  \end{align}
  \]
  
  The assumption is that each subject has the same latent structure but different scores on the factors themselves.
  

 
Tying it all together, the folloiwng is the computer output from the `psych::fa`
```{r ski_orthog_tied}
invisible(fit <- fa(cor.ski, nfactors=2, fm="pa", rotate="varimax"))
fit
```


* **Factor Loadings (PA1, PA2)**: The correlations between factors and variables.
* **Communality (h2)**: The proportion of variance in a variable that is explained for by the factors, equal to the sum of the squared loadings (SSL).  E.g., `r scales::percent(fit$communality["cost"])` of the variance in COST is accounted for by the two factors.
* **Uniqueness (u2)**: Approximately, $R = FF' + U^2$.   Because unique and eror variances are omitted, a linear combination of factors approximates but does not duplicate the observed correlation matrix and scores on observed variables.  
* **Complexity (com)**: Hoffman's index of complexity, $\frac{(\sum a_I^2)^2}{\sum a_i^4}$
* **SS Loadings**: Sum of squared loadings
* **Proportion Var**: Proportion of variance accounted for by each factor;  `r scales::percent(fit$Vaccounted["Proportion Var", "PA1"])` of the variance in the variables is accounted for by the first factor and `r scales::percent(fit$Vaccounted["Proportion Var", "PA1"])` of the variance in the variables is accounted for by th esecond factor.  
* **Cumulative Var**: Cumulative proprtion of variance explained (across factors). Because rotation is orthogonal, the two factors together account for `r scales::percent(sum(fit$Vaccounted["Proportion Var", ]))` of the variance in the variables
* **Proportion Explained**: The proportion of variance in the solution accounted for by a factor; the two factors account for `r scales::percent(fit$Vaccounted["Proportion Explained", "PA1"])` and `r scales::percent(fit$Vaccounted["Proportion Explained", "PA2"])` of the variance in the solution, respectively.  


  
## Oblique Rotation

When factors are correlated; the loading matrix (correlations between variables and correlated factors) is decomposed into the following:

* Pattern matrix ($A$); when squared, contribution of each factor to variance of variable, without segments o variance that come from overlap betweeen correlated factors
* Structure matrix ($C$); correlations between variables and factors.  
\[C = A\Phi\]

The factor correlations matrix $\Phi$ is also produced.

To determine the structure matrix, the following steps are required:
1. Determine factor score coefficients, $B = R^{-1}A$
2. Determine factor scores, which are a product of the standardized variable scores and factor score coefficients, $F = ZB$
3. Determine correlations among factors 


```{r ski_oblimin_res}

invisible(fit <- fa(dat.ski[,-1], nfactors=2, fm="pa", rotate="oblimin", scores="Thurstone"))

# Pattern matrix
oblimin_A <- unclass(fit$loadings)

# Factor-Score Coefficients
oblimin_B <- solve(r) %*% oblimin_A %*% fit$Phi
oblimin_B
fit$weights 

# Factor Scores
Z <- as.matrix(select(dat.ski, -skiers) %>% mutate_all(scale)) 
oblimin_F <- Z %*% oblimin_B 
oblimin_F

# Factor Correlation; cross product of standardized factor scores / (number of cases - 1)
oblimin_Phi <- (1/(nrow(oblimin_F)-1)) * t(oblimin_F) %*% oblimin_F
diag(oblimin_Phi) <- 1
oblimin_Phi
fit$Phi

# Structure
oblimin_C <- oblimin_A %*% fit$Phi
oblimin_C
unclass(fit$Structure)

```

From Phi, see that correlation between first factor and sezond facotr is quite low.  In this case, one would use orthogonal rotation.




## Major Types of Factor Analyses

* Factor Extraction Techniques:
  * Principal components
  * Principal factors
  * Maximum likelihood factoring
  * Image factoring
  * Alpha factoring
  * Generalized (weighted) least squares factoring
  
### Comparison of PCA to FA


```{r store_charts}

export_svg(DiagrammeR::grViz("../../static/graphs/fa.dot", height=200, width=200)) %>%
  charToRaw %>% rsvg %>% png::writePNG('../../static/graphs/fa.png')

export_svg(DiagrammeR::grViz("../../static/graphs/pca.dot", height=200, width=200)) %>%
  charToRaw %>% rsvg %>% png::writePNG('../../static/graphs/pca.png')
```

In both, the variance that is analysed is the sum of the values in the positive diagonal.

<table>
<tr>
<th>Factor Analysis</th>
<th>Principal Components Analysis</th>
</tr>
<tr>
<td>Only the variance that each observed variable shares with other observed variables is available for analysis (error and unique variance excluded); shared variance is estimated by communalities that are inserted in the positive diagonal of the correlation matrix.  The solution concentrates on variables with high communality values.  The sum of the communalities (sum of the SSLS) is the variance that is distributed among factors.</td>
<td>Ones are in the diagonal; each variables contributes a unit of variance by contributing a 1 to the positive diagonal of the correlation matrix.  All the variance is distributed to components, including error and unique variance for each observed variables.</td>
</tr>
<tr>
<td>Because unique and error variances are omitted, a linear combination of factors approximates but does not duplicate the observed correlation matrix and scores on observed variables.  </td>
<td>If all components are retained, PCA duplicates exactly the observed correlation matrix.</td>
</tr>
<tr>
  <td>Analyses covariance (communalities)</td>
  <td>Analyses variance</td>
</tr>
<tr>
  <td>Goal: Reproduce correlation matrix with few orthogonal factors</td>
  <td>Goal: Reproduce correalation matrix with few orthogonal factors</td>
</tr>
<tr>
  <td>Non-unique solution</td>
  <td>Unique solution</td>
</tr>
<tr>
  <td>Factors are thought to cause variables, _what are the underlying processes that could have produced correlations among these variables?_; _are the correlations among variables consistent with a hypothesised factor structure_?</td>
  <td>Variables cause the component; no underlying theory about which variables should be associated with which factors</td>
</tr>
<tr>
  <td>Best if want to understand underlying structure</td>
  <td>Best when after an empirical summary of the data set</td>
</tr>
<tr>
  <td>![](/graphs/fa.png)</td>
  <td>![](/graphs/pca.png)</td>
</tr>
</table>


  

```{r load_data_factor, message=FALSE, warning=FALSE, include=FALSE}
dat.factor <- readr::read_tsv("../../static/datasets/Tabachnick/factor.dat") 
```  
