---
title: 'Book: Using Multivariate Statistics'
author: Marie
date: '2018-06-11'
slug: book-using-multivariate-statistics
banner: "img/banners/tabachnick.png"
categories:
  - Study-Notes
tags:
  - Book
  - Study-Notes
---

```{r load_packages, message=FALSE, warning=FALSE, include=FALSE}
if (!requireNamespace("tidyverse")) install.packages("tidyverse")
require("tidyverse")

if (!requireNamespace("knitr")) install.packages("knitr")
require("knitr")

if (!requireNamespace("psych")) install.packages("psych")
require("psych")

if (!requireNamespace("GPArotation")) install.packages("GPArotation")
require("GPArotation")

if (!requireNamespace("DiagrammeR")) install.packages("DiagrammeR")

require(DiagrammeR)
library(DiagrammeRsvg)
library(magrittr)
library(rsvg)
library(png)


```

# Principal Components and Factor Analysis

## General Purpose 

* To find subsets of variables (factors/components/latent variables) that are independent, where variables in a subset are correlated
* To summarise patterns of correlations among variables
* To decrease number of variables to a smaller number of factors/components, by using linear combinations of observed variables (more parsimonious and because factors/components are uncorrelated, more reliable)
* To understand underlying processes
* To describe (and maybe understand) relationship between variables
* To test theory about underlying processes

## Steps:

* Hypothesise factors underlying domain of interest
* Select and measure variables
* Prepare correlation matrix
* Extract factors/components
* Determine number of factors/components
* Rotate to increase interpretability
* Interpret results
  * Should make sense
  * Easier if variables within one factor do not correlate with variables in another factor
* Verify factor structure by establishing validity, for example, confirm that factor scores change with experimental conditions as predicted

For factor analysis

* Hypothesise factors underlying domain of interest
* Select 5-6 variables thought to be a pure measure of the factor (marker variables); consider variable complexity (i.e the number of factors for which a variable may correlate)
* Choose a sample across variables/factors with spread of scores
* Be wary to pool results of several samples; samples differing, for example, in SES may also have different factors.  Only pool if different samples produce the same factors.

  
## Types of Factor Analysis

Exploratory | Confirmatory
-------------------------------------------- | ----------------------------------
Early stages of research  |  Advanced stages of research
To summarise data by grouping together correlated variables | To test theory about latent processes
Variables may not have been chosen with underlying processes in mind | Variables chosen to reveal underlying processes
Factor Analysis / PCA | Usually via Structural Equation Modelling (SEM)


## Research Questions
* How many factors
* What do the factors mean
* How much variability is accounted for by each/all factors
* How well does the factor solution fit that expected
* Had factors been measured directly, what scored would subjects have received?

## Limitations

  * No ready criteria for verification
  * Infinite rotations; final choice is based on subjective assessment of interpretability
  * Often used as a last resort to make order from chaos (i.e. suffers from a poor reputation)
  * Exploratory
  * Subjective
  * Sensitive to outliers, missing data, poorly distributed samples, small samples

### Outliers (among cases)


* Univariate and multivariate (combination of variables) outliers will have greater influence on factor solution 
* TODO chapter 4 and 13.7.4.14 to detect and reduce influence

### Outliers among variables (FA only)
If there is a factor with $\leq$ 2 variables and significant variable accounted for:

* Ignore, or treat with caution
* The factor should be researched further with structural equation modelling (SEM)

Factors with few variables and small variance accounted for are unreliable.

### Missing data
Estimate if 

* Missing distribution of values not random
* Sample size will be too small if delete cases; 

Otherwise, delete cases

### Poorly distributed variables (Non-Normality)
* If PCA/FA used descriptively, i.e. to summarise relationships, normality assumptions not in force, but solution is enhanced with normality
* If statistical inference used to determine number of factors, single variable and multivariate normality (linear combinations of variables are normally distributed) is assumed
* If variable has substantial skewness and kurtosis, consider variable transformation
* Multivariate normality tests are sensitive
* Note that some SEMS permit PCA/FA with non-normal variables

### Non-Linearity
* Correlations measures linear relationships therefore analysis is degraded by non-linear relationship (as not captured)
* Assessed visually using scatterplots

### Small samples

Sample size required will depend on population correlations and the number of factors
    
Characteristics | Recommended Sample Size
----------------|-------------------------
Communalities > 0.6 | < 100
Communalities ~ 0.5, loadings > 0.8 | 100-200
Low communality; small # factors,3-4 indicators for each factor  | > 300
Low communality, large # weakly determined factors | > 500
  
### Multicollinearity  and Singularity (FA only)

Although in FA/PCA interest is in finding correlated variables, if correlation is too high, matrix inversion becomes unstable (multicollinearity) or impossible (singularity).  If determinant of correlation matrix ($R$) is close to zero, then multicollinearity or singularity may be present

To detect, look at Squared Multiple Correlation (**SMC**), if SMC is high (> 0.9 multicollinearity, 1=singularity) then delete that variable

### Lack of Factorability within Correlation Matrix (R)
FA assumes relationships between variables; ; a factorable matrix should have several sizeable correlations. Note however that high bivariate correlations are not a guarantee of factors; they may merely be two similar variables and not reflect an underlying process simultaneously affecting them.

If factors are present, then 

* Look at correlation matrix (**R**): Should expect many sizeable correlations.  if no correlation within the correlation matrix (**R**) with exceeds 0.3 then there is probably nothing to factor analyse.  
* the anti-image correlations matrix (pairwise correlations adjusted for the effects of all other variables) will have mostly small values among the off-diagonal elements. 
* Kaiser's measure of sampling adequacy, $\sum(\text{squared correlations})/(\sum(\text{squared correlations}) + \sum(\text{squared partial correlations}))$ > 0.6

## Fundamental Equations for Factor Analysis


Matrices 

Label | Matrix Name | Description | Orthogonal | Oblique
------|---------------|-----------------------------------------|---------|-------
$R$ | Correlation               |(Observed) Between-variable correlation | <i class="fa fa-check-square-o"></i> |<i class="fa fa-check-square-o"></i>
$Z$ | Variable                  |Standardized observed variable values(scores) | <i class="fa fa-check-square-o"></i> |<i class="fa fa-check-square-o"></i>
$F$ | Factor-score              |Standardized factor scores | <i class="fa fa-check-square-o"></i> |<i class="fa fa-check-square-o"></i>
$A$ | Factor loading            |Correlations between factor & variable | <i class="fa fa-check-square-o"></i> |<i class="fa fa-square-o"></i>
$A$ | Pattern                   |Contribution of factor to variance | <i class="fa fa-square-o"></i> |<i class="fa fa-check-square-o"></i>
$C$ | Structure                 | Correlations between variables and correlated factors | <i class="fa fa-square-o"></i> |<i class="fa fa-check-square-o"></i>
$B$ | Factor-Score Coefficients | Coefficients to generate factor scores from variables | <i class="fa fa-check-square-o"></i> |<i class="fa fa-check-square-o"></i>
$\Phi$                          | Factor correlation|Correlations among factors | <i class="fa fa-square-o"></i> |<i class="fa fa-check-square-o"></i>
$L$ | Eigenvalue                |Diagonal matrix of eigenvalues, one per factor | <i class="fa fa-check-square-o"></i> |<i class="fa fa-check-square-o"></i>
$V$ | Eigenvector               |Eigenvectors, one per eigenvalue | <i class="fa fa-check-square-o"></i> |<i class="fa fa-check-square-o"></i>


<br><br>
Equations: 

* **Reproduced Correlation Matrix ($\bar{R}$)**: Factor correlation, $\bar{R} = A A'$
* **Residual Correlation Matrix ($R_\text{res}$)**: Observed - Reproduced (small for good factor analysis), $R_\text{res} = R - \bar{R}$
* **Factor Score Coefficients Matrix ($B$)**: Coefficients from linear combinations of variables, $B = R^{-1} A$.
* **Factor Scores**: $F = ZB$
* **Predicted Scores**: $Z = FA'$

    
### Fundamental Equations for Factor Analysis: Extraction
The correlation matrix ($R$), which by nature is symmetric, can be diagonalised by the matrix $V$ (whose columns are the eigenvectors), such that 
\[L = V'RV\]
where $L$ is a diagonal matrix with values eigenvalues in the diagonal

**Example**
```{r load_data_ski, echo=FALSE, message=FALSE, warning=FALSE}
dat.ski <- data.frame(skiers = paste0("S", c(1:5), sep=""), cost = c(32, 61, 59, 36, 62), lift=c(64, 37, 40, 62, 46) , depth = c(65, 62, 45, 34, 43), powder = c(67, 65, 43, 35, 40)) 
dat.ski %>% kable()
```
<br><br>

Correlation Matrix
```{r ski_corr}
cor.ski <- cor(dplyr::select(dat.ski, -skiers)) 
cor.ski
```

Strong correlations between:

* importance placed on _cost_ of ski ticket and speed of ski _lift_
* importance placed on snow _depth_ and moisture (_powder_)


Eigenvalues and Eigenvectors:
```{r ski_eigenvalues}
eig.ski <- eigen(cor.ski)
eig.ski
```

Note that $L = V'RV'$
```{r ski_diag}
zapsmall(t(eig.ski$vectors) %*% cor.ski %*% eig.ski$vectors)
diag(eig.ski$values)
```
and that pre-and post-multiplying the correlation matrix by eigenvectors does not change it so much as repackage it:
\[V'V = I\]

```{r ski_identity}
zapsmall(crossprod(eig.ski$vectors))
```

Rearranging the equation $L=V'RV$,

\[\begin{align}
  R &= VLV'\\
    &= V\sqrt{L}\sqrt{L} V'\\
    &=(V\sqrt{L})(\sqrt{L}V') \\
    &= A A'
    \end{align}\]
where $A=V\sqrt{L}$ is referred to as the factor loading matrix; the correlation matrix is a product of the factor loading matrix $A$ and its transpose.

In this case, the (unrotated) factor loading matrix is:
```{r ski_loading}
eig.ski$vectors %*% sqrt(diag(eig.ski$values))
```


  The correlation matrix can therefore be considered a product of two matrices - each a combination of eigenvectors and the square root of eigenvalues  
  
* Because there are 4 variables, there are 4 eigenvalues 
* Each eigenvalue corresponds to a different potential factor; usually only factors with large eigenvalues are retrained
* In good FA, a few factors will almost duplicate correlation matrix
* In this example, only first 2 factors, with eigenvalues > 1 are large enough

Starting with the correlation matrix $R$ and assuming $k$ factors are used, the steps of factor analysis (principal factor solution method) are as follows:

  1. Get eigenvalues $L$ and eigenvectors $V$ of correlation matrix $r$
  2. Calculate $C = \sum \text{diag}(R)$
  3. Calculate the loadings, $A = V[,1:k]\sqrt{L[1:k]}$,  (eq 13.6 of text)
  4. Set $R^* = AA'$  (eq 13.5 of text, R=AA')
  5. Set $C^* = \sum \text{diag} (R^*)$
  6. Update $\text{diag}(R) = \text{diag}(R^*)$
Repeat above steps until max iterations reached, or until $e = |C-C^*)|$ is smaller than some threshold


```{r ski_fa_iter}
# This is taken from the function fac and is replicated using 
#fit <- fa(cor.ski, nfactors=2, fm="pa")
#fit # print results

# Initialisation
r <- cor(dplyr::select(dat.ski, -skiers)) 
n <- dim(r)[2]
r.mat <- r
colnames(r.mat) <- rownames(r.mat) <- colnames(r)
nfactors <- 2
max.iter <- 50
min.err <- 0.001

orig <- diag(r)
comm <- sum(diag(r.mat))
err <- comm
i <- 1
comm.list <- list()

e.values <- eigen(r)$values

# Loop
while (err > min.err) {
  eigens <- eigen(r.mat, symmetric = TRUE)
  if (nfactors > 1) {
    loadings <- eigens$vectors[, 1:nfactors] %*% 
      diag(sqrt(eigens$values[1:nfactors]))
  }
  else {
    loadings <- eigens$vectors[, 1] * sqrt(eigens$values[1]) #A in book
  }
  model <- loadings %*% t(loadings) # eqn 13.5: R = AA' (if r is n x n this will always be an n x n matrix, because n x k %*% k x n gives n x n matrix)
  new <- diag(model)
  comm1 <- sum(new)
  diag(r.mat) <- new                # update diagonals of correlation matrix
  err <- abs(comm - comm1)
  if (is.na(err)) {
    warning("imaginary eigen value condition encountered in fa\n Try again with SMC=FALSE \n exiting fa")
    break
  }
  comm <- comm1
  comm.list[[i]] <- comm1
  i <- i + 1
  if (i > max.iter) {
    if (warnings) {
      message("maximum iteration exceeded")
    }
    err <- 0
  }
}

# Clean-up
eigenv <- eigens$vectors
eigens <- eigens$values
if (nfactors > 1) {
  sign.tot <- vector(mode = "numeric", length = nfactors)
  sign.tot <- sign(colSums(loadings))
  sign.tot[sign.tot == 0] <- 1
  loadings <- loadings %*% diag(sign.tot)
} else {
  if (sum(loadings) < 0) {
    loadings <- -as.matrix(loadings)
  }
  else {
    loadings <- as.matrix(loadings)
  }
  colnames(loadings) <- "MR1"
}
colnames(loadings) <- paste("PA", 1:nfactors, sep = "")
rownames(loadings) <- rownames(r)
loadings[loadings == 0] <- 10^-15
model <- loadings %*% t(loadings)
f.loadings <- loadings

```


After iterating through the Factor Analysis `r i` times we get the following eigenvectors:
```{r ski_fin_eigenvectors, echo=FALSE}
as.data.frame(round(eigenv[, 1:nfactors],3), rownames=FALSE)
```

and eigenvalues
```{r ski_fin_eigenvalues, echo=FALSE}
round(eigens[1:nfactors],3)
```


Note that the property,  $VV' = I$, is maintained.
```{r demo_VVtEI}
crossprod(eigenv[, 1:nfactors])
```

The factor loading matrix, $A = V\sqrt{L}$, is
```{r ski_loadings, echo=FALSE}
#round(eigenv[, 1:nfactors]*sqrt(eigens[1:nfactors]),3)
round(f.loadings,3)
```


The factor loading matrix lists the correlations between factors and variables:

  * Factor 1 reflects snow conditions
  * Factor 2 reflects resort conditions; people who score high on this factor place a lot of importance on the cost of a ski ticket without concern for the lift speeds whereas those that score low on this factor value lift speeds more than ski ticket costs.


## Orthogonal Rotation
When factors are uncorrelated; produces a loading matrix (correlations between factors and variables)

Rotation used to 

  * Maximise high correlations between factors and variables
  * Minimise low correlations between factors and variables
  
\[\begin{align}
  \text{Rotated Loading Matrix} &= (\text{Unrotated Loading Matrix})(\text{Transformation Matrix})\\
    A_\text{Rotated} &= A_\text{Unrotated}\Lambda
    \end{align}\]  

The transformation matrix is a matrix of sines and cosines of an angle $\psi$.  In this example an angle of $\psi = 19^\circ$ is used, with \[\Lambda = \begin{bmatrix}\cos \psi & -\sin \psi\\ \sin \psi & \cos \psi\end{bmatrix}\]

```{r ski_orthog}
rotated <- stats::varimax(loadings)
loadings_rotated <- rotated$loadings
rot.mat <- rotated$rotmat
rot.mat  # transformation matrix
```

The communalities, variance and covariance are shown in the following table:
```{r ski_orthog_res}
fit <- fa(cor.ski, nfactors=2, fm="pa", rotate="varimax")
loadings_sol <- unclass(fit$loadings)
ssls <- apply(loadings_sol, 2, function(x) sum(x^2)) # row-total
comms <- apply(loadings_sol, 1, function(x) sum(x^2)) # column-total
prop_var <- ssls/nrow(loadings_sol)
prop_cov <- ssls/sum(comms)

res <- rbind(loadings_sol, ssls, prop_var, prop_cov)
res <- data.frame(var = rownames(res), res) %>% mutate(communalities = ifelse(var %in% c('cost', 'lift', 'depth', 'powder'), PA1^2 + PA2^2, PA1 + PA2))
res
```


The **reproduced correlation matrix** is $\bar{R} = AA'$:
```{r ski_ortho_Rbar}
round(loadings_sol %*% t(loadings_sol),3)
```

The **residual correlation matrix** is $R_\text{res} = R - \bar{R}$
```{r ski_orthog_RRes}
rres <- cor.ski - loadings_sol %*% t(loadings_sol)
diag(rres) <- 0 # could otherwise replace with communalities
round(rres,3)

```
The FA is good if numbers in residual correlation matrix are small.

Factor score coefficients (for estimating factor scores) are found by multiplying the inverse correlation matrix and the factor loading matrix,  $B = R^{-1}A$
```{r ski_orthog_factor_score_coefficients}
B <- solve(r) %*% loadings_sol
B
```

Factor scores are a product of standardized variable scores and factor score coefficients, $F = ZB$:
```{r ski_orthog_factor_scores}
F <- as.matrix(select(dat.ski, -skiers) %>% mutate_all(scale)) %*% B
F
```
Here, for example, the first subject scores strongly on both snow (strong importance on both depth and powder) and resort factor (strong importance on lift over cost).

Predicting standardized scores on variables from scores on factors is also possible, using a product of scores on factors weighted by factor loadings, $Z = FA'$:
```{r ski_orthog_score_predict}
Z <- F %*% t(loadings_sol)
round(Z, 3)
```
In algebraic form, the formula is:
\[\begin{align}
  z_\text{COST} &= a_{11}F_1 + a_{12}F_2 \\
  z_\text{LFIT} &= a_{21}F_1 + a_{22}F_2 \\
  z_\text{DEPTH} &= a_{31}F_1 + a_{312}F_2 \\
  z_\text{POWDER} &= a_{41}F_1 + a_{42}F_2
  \end{align}
  \]
  
  The assumption is that each subject has the same latent structure but different scores on the factors themselves.
  

 
Tying it all together, the following is the computer output from the `psych::fa`
```{r ski_orthog_tied}
invisible(fit <- fa(cor.ski, nfactors=2, fm="pa", rotate="varimax"))
fit
```


* **Factor Loadings (PA1, PA2)**: The correlations between factors and variables.
* **Communality (h2)**: The proportion of variance in a variable that is explained for by the factors, equal to the sum of the squared loadings (SSL).  E.g., `r scales::percent(fit$communality["cost"])` of the variance in COST is accounted for by the two factors.
* **Uniqueness (u2)**: Approximately, $R = FF' + U^2$.   Because unique and error variances are omitted, a linear combination of factors approximates but does not duplicate the observed correlation matrix and scores on observed variables.  
* **Complexity (com)**: Hoffman's index of complexity, $\frac{(\sum a_I^2)^2}{\sum a_i^4}$
* **SS Loadings**: Sum of squared loadings
* **Proportion Var**: Proportion of variance accounted for by each factor;  `r scales::percent(fit$Vaccounted["Proportion Var", "PA1"])` of the variance in the variables is accounted for by the first factor and `r scales::percent(fit$Vaccounted["Proportion Var", "PA1"])` of the variance in the variables is accounted for by the second factor.  
* **Cumulative Var**: Cumulative proportion of variance explained (across factors). Because rotation is orthogonal, the two factors together account for `r scales::percent(sum(fit$Vaccounted["Proportion Var", ]))` of the variance in the variables
* **Proportion Explained**: The proportion of variance in the solution accounted for by a factor; the two factors account for `r scales::percent(fit$Vaccounted["Proportion Explained", "PA1"])` and `r scales::percent(fit$Vaccounted["Proportion Explained", "PA2"])` of the variance in the solution, respectively.  


  
## Oblique Rotation

When factors are correlated; the loading matrix (correlations between variables and correlated factors) is decomposed into the following:

* Pattern matrix ($A$); when squared, contribution of each factor to variance of variable, without segments o variance that come from overlap between correlated factors
* Structure matrix ($C$); correlations between variables and factors.  
\[C = A\Phi\]

The factor correlations matrix $\Phi$ is also produced.

To determine the structure matrix, the following steps are required:
1. Determine factor score coefficients, $B = R^{-1}A$
2. Determine factor scores, which are a product of the standardized variable scores and factor score coefficients, $F = ZB$
3. Determine correlations among factors 


```{r ski_oblimin_res}

invisible(fit <- fa(dat.ski[,-1], nfactors=2, fm="pa", rotate="oblimin", scores="Thurstone"))

# Pattern matrix
oblimin_A <- unclass(fit$loadings)

# Factor-Score Coefficients
oblimin_B <- solve(r) %*% oblimin_A %*% fit$Phi
oblimin_B
fit$weights 

# Factor Scores
Z <- as.matrix(select(dat.ski, -skiers) %>% mutate_all(scale)) 
oblimin_F <- Z %*% oblimin_B 
oblimin_F

# Factor Correlation; cross product of standardized factor scores / (number of cases - 1)
oblimin_Phi <- (1/(nrow(oblimin_F)-1)) * t(oblimin_F) %*% oblimin_F
diag(oblimin_Phi) <- 1
oblimin_Phi
fit$Phi

# Structure
oblimin_C <- oblimin_A %*% fit$Phi
oblimin_C
unclass(fit$Structure)

```

From Phi, see that correlation between first factor and second factor is quite low.  In this case, one would use orthogonal rotation.






## Factor Extraction Techniques:

  * Principal components; to determine components with maximum variance.  Firs PC is the linear combination of observed variables that maximally separated subjects by maximising the variance of their component scores.  Subsequent components are linear combinations of observed variables that extracts maximum variability from residual correlations that are orthogonal to previous extracted components.  Choose solution to reduce large number of variables to smaller number of components.
  * Principal factors: Estimates communality in the positive diagonal of the observed correlation matrix through an iterative procedure with the squared multiple correlations (SMCs) of each variables with all other variables used as the starting values in the iteration.  
  * Maximum likelihood factoring: Factor loadings calculated by maximising the probability of sampling the observed correlation matrix  from a population
  * Image factoring: distributes among factors the variance of a n observed variable that is reflected by the other variables; variable image scores are produced by multiple regression with variable as independent variable and other variables as dependent variables.  A covariance matrix is calculated from the image (predicted) scores
  * Unweighted least squares factoring: Aims to minimises squared differences between the observed and reproduced correlation matrices.  Only considers=s off-diagonal differences.
  * Generalized (weighted) least squares factoring: Also aims to minimize (off-diagonal) squared differences between observed and reproduced correlation matrices, but applies weights to variables.  Variables with substantial shared variance with other variables are weighted more heavily (i.e. considered more important).  
    * Alpha factoring: Used when there is interest in discovering common factors with repeated samples of variables taken from a population of variables.  
  
### Comparison of PCA to FA


```{r store_charts, include=FALSE}

export_svg(DiagrammeR::grViz("../../static/graphs/fa.dot", height=200, width=200)) %>%
  charToRaw %>% rsvg %>% png::writePNG('../../static/graphs/fa.png')

export_svg(DiagrammeR::grViz("../../static/graphs/pca.dot", height=200, width=200)) %>%
  charToRaw %>% rsvg %>% png::writePNG('../../static/graphs/pca.png')
```

In both, the variance that is analysed is the sum of the values in the positive diagonal.

<table>
<tr>
<th>Factor Analysis</th>
<th>Principal Components Analysis</th>
</tr>
<tr>
<td>Only the variance that each observed variable shares with other observed variables is available for analysis (error and unique variance excluded); shared variance is estimated by communalities that are inserted in the positive diagonal of the correlation matrix.  The solution concentrates on variables with high communality values.  The sum of the communalities (sum of the SSLS) is the variance that is distributed among factors.</td>
<td>Ones are in the diagonal; each variables contributes a unit of variance by contributing a 1 to the positive diagonal of the correlation matrix.  All the variance is distributed to components, including error and unique variance for each observed variables.</td>
</tr>
<tr>
<td>Because unique and error variances are omitted, a linear combination of factors approximates but does not duplicate the observed correlation matrix and scores on observed variables.  </td>
<td>If all components are retained, PCA duplicates exactly the observed correlation matrix.</td>
</tr>
<tr>
  <td>Analyses covariance (communalities)</td>
  <td>Analyses variance</td>
</tr>
<tr>
  <td>Goal: Reproduce correlation matrix with few orthogonal factors</td>
  <td>Goal: Reproduce correlation matrix with few orthogonal factors</td>
</tr>
<tr>
  <td>Non-unique solution</td>
  <td>Unique solution</td>
</tr>
<tr>
  <td>Factors are thought to cause variables, _what are the underlying processes that could have produced correlations among these variables?_; _are the correlations among variables consistent with a hypothesised factor structure_?</td>
  <td>Variables cause the component; no underlying theory about which variables should be associated with which factors</td>
</tr>
<tr>
  <td>Best if want to understand underlying structure</td>
  <td>Best when after an empirical summary of the data set</td>
</tr>
<tr>
  <td>![](/graphs/fa.png)</td>
  <td>![](/graphs/pca.png)</td>
</tr>
</table>

## Rotation Techniques
 Rotation is used to improve interpretability of solution, not the quality of the mathematical fit between the observed and reproduced correlation matrices (which is the same before and after rotation).  If underlying processes are correlated, oblique rotation is used, otherwise orthogonal rotation is used (with the advantage of ease in reporting, describing and interpreting results)
 
Orthogonal Rotation Techniques:

* Varimax: maximised variance of loadings on each factor
* Quartimax: Maximise variance of loadings on each variable
* Equimax: Compromise between quartimax and varimax; goal is to simplify both variables and factors

Oblique Rotation Techniques; allow for range of correlation between factors by altering $\delta$ (-4=orthogonal, 0=fairly highly correlation, 1=highly correlated).  

* Oblimin ($\delta=0$): simplify factors by minimising cross-products of loadings

* Equamax
  

```{r load_data_factor, message=FALSE, warning=FALSE, include=FALSE}
dat.factor <- readr::read_tsv("../../static/datasets/Tabachnick/factor.dat") 
```  


# Time-Series Analysis


    
## General Purpose and Description
* Observations over $\geq$ 50  time periods
* Observations may be aggregates (e.g. monthly or daily traffic tickets)
* Goals:
    * Identify patterns in obsevations correlated with themselves, but offset in time
    * Test impact of intervention(s)
    * Forecast 
* Method: Decompose score into:
    * shocks (random process)
    * Trend 
        * Linear: increasing or decreasing mean
        * Quadratic: Mean increases or decreases
    * Effect of earlier scores 
    * Effect of earlier shocks 
    * Seaonality/periodocity/cyclic
* Models:
    * ARIMA($p, d, q$): Auto-regressive integrated moving average:
        * $p$: **Auto-regressive**; effect of earlier scores.  A model with two auto-regressive terms ($p = 2$) depends on two previous observations
        * $d$: **Trend**; the terms needed to make a nonstationary time series stationary.  With two trend terms ($d = 2), teh model has to be differenced twice to make it stationary.  The first difference removes linear trendes and the second quadratic, etc.
        * $q$: **Moving average** (of earlier shocks); with two moving average terms, an observation depends on two preceding random shocks
        
* Steps in analysis:
    1. Identification; of autocorrelation functions (ACFs) and partial autocorrelation functions (PACFs)
        * **Autocorrelation**: self-scorrelation at different lags
        * **Partial Autocorrelation**: self correlation  at different lags with intermediate correlations parialled out
    2. Estimation
    3. Diagnosis
    
    
Recommended References:

* [Time Series Analysis: Forecasting and Control, 5th Edition
George E. P. Box, Gwilym M. Jenkins, Gregory C. Reinsel, Greta M. Ljung](https://www.wiley.com/en-au/Time+Series+Analysis%3A+Forecasting+and+Control%2C+5th+Edition-p-9781118675021)

* Hershberger,S. L., Molenaar, P.C. M., & Corneal, S.E. (1996). A hierarchy of univariate andmultivariatestructural time series models. In G. A. Marcoulides & R. E. Shumacker (Eds.), Advanced structural equation modeling: Issues and Techniques (pp. 159–194).Mahwah, NJ: Lawrence Erlbaum Associates, Inc 


## Kinds of Research Questions

Determine:

1. Patterns in series 
    a. Autocorrelation: Pattern may be of interest to intself or in prepration for forecasing or testing the effects of interventions.    Types of questions:
        * Are there linear or quadratic trends in the data?
        * How quickly to random shocks linger?
    b. Seasonal cycles and trends    
2. Predicted values in future (forecasting)
3. Effect of intervention
4. Relationships between time series (comparing time series).  AKA multivariate time eries, cross-correlation functions, transfer function models, models with input series, dynamic regression.
5. Relationship to covariates (dv)
6. Variability due to the chosen model (effect size and power)


## Assumptions of Time-Series Analysis

### Theoretical Issues
To determine causality require true experient, i.e.

* Random assignment to ttreatment conditions
* Control of extraneious variables
* Manipulation of intervention(s)
    
### Practical Issues Around Assumptions

* Normality of Distributions of Residuals, $e ~ N(0, \sigma^2)$.  Examine normalised plot of residuals before evaluating an intervantion.  Transform DV if residuals are nonnormal (e.g square root, log, inverse)
    
* Homogeneity of Variance and Zero Mean of Residuals: Examine plots of standardized residuals against predicted values.  Transform DV if width of plot varies over time (e.g. use log)
    
* Independence of Residuals.  Obviously inherent within time series due to autocorrelation, but after identification of autocorrelation and differencing there should be no remaining autocreelations or partial autocorellations at various lags in the ACFs and PACFs (if so, examine ACFs and PACFs and adjust model accordingly)
    
* Absence of Outliers.  Outliers sometimes show up in original plot of DV against time, but often more noticeable after initial modelling.  Can greatly affect results and must be dealt with (by deletion and then possibly imputing).  Examine time series plot before and after adjusting for autocorelation and seasonality to identify outlier patterns.

## Fundamental Equations for Time Series: ARIMA models

### Identification of ARIMA ($p, d, q$) Models
* Requires finding values for $p$, $d$ and $q$
* If a value is 0, then term is not required
* First step is to identify the trend ($d$); if it is not stationary, ie. if $d \neq 0$, then the series needs to be made stationary before identifying $p$ and $q$.  A stationary process has both constant mean and constant variance of the time period of the study

#### Trend Components, $d:$ Making the Process Stationary
* First step: Plot time series
    * Is mean shifting? If so, apply differening one or more times, e.g. calculate $t_2 - t_1$
    * Is dispersion increasing or decreasing over time?  If so, apply a logarithmic transformation
    
```{r ts_differenging}
df <- data.frame(week = 1:20, quality = c(19, 21, 17, 19, 20, 21, 27, 28, 20, 24, 31, 20, 29, 21, 28, 28, 29, 31, 23, 34)) 
df$q1 <- c(NA, df$quality[-1] - df$quality[-nrow(df)])
df$q2 <- c(NA, df$q1[-c(1)] - df$q1[-nrow(df)])
df$logq <- log(df$quality)
df$logq_1 = c(NA, df$logq[-1] - df$logq[-nrow(df)])

tab <- df %>%
    mutate_at(vars(logq, logq_1), round, 2) %>%
    mutate_all(as.character) %>%
    bind_rows(c(week = "", quality = "", q1 = paste0("mean=", round(mean(df$q1, na.rm = TRUE),2)), q2 = "", logq = "", logq_1 = "") )
kable(tab, digits = 2)
```

```{r ts_plot}
ggplot(df, aes(week, quality)) + 
    geom_line() + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE)
```
    
Models: 

1. Observation = f(random shock) only: $Y_t = a_t$ where $a_t$ is random shock with constant mean and variance.  This means that observations also have constant mean and variance.
2. Observation = f(previous observation + random shock): $Y_t = \theta_0(Y_{t-1}) + a_t$.    Here, the slope is defined by the mean of the first difference, `r round(mean(df$q1, na.rm = TRUE),2)`.  The following plot shows that differerencing has removed the trend.
```{r ts_plot_diff1, message=FALSE, warning=FALSE}
ggplot(df, aes(week, q1)) + 
    geom_line() + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE)
```



However, after taking the log, the variability is the same and so untransformed difference is used in future analyses
```{r ts_plot_diff1log, message=FALSE, warning=FALSE}
ggplot(df, aes(week, logq_1)) + 
    geom_line() + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE)
```



#### Auto-Regressive Components ($p$)

Memory of process for preceding observations, equal to 0 if no relationship between adjacent observations.  Example of $p = 2$ model is ARIMA (2, 0, 0): \[Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + a_t\]
where $\phi_l$ is the correlation coefficient of the observations with those at lag $l$.
 
#### Moving Average Components ($q$)

Memory of process for preceding random shocks.  For example, when $q = 2$ there is a relationship betweent he current score and the random shock at lag 2 and the model is ARIMA (0, 0, 2): \[Y_t = a_t -\theta_1 a_{t-1} - \theta_2 a_{t-2}\]

#### Mixed Models
When the series has both auto-regressive and moving average components so that both types of correlations are required, e.g. ARIMA(1, 0, 1):\[Y_t = \phi_1 Y_{t-1} - \theta_1 a_{t-1} + a_t\]

#### ACFs and PACFs

### Estimating Model Parameters 

### Diagnose a Model

### Computer Analysis of Small-Sample Time-Series Example

## Types of Time-Series ANAlyses

### Models With Seasonal Components

### Models With Interventions

#### Abrupt, Permanent Effects

#### Abrupt, Temporary Effects

#### Gradual, Permanent Effects

#### Models With Multiple Interventions

### Adding Continuous Variables

## Some Important Issues

### Patterns of ACFs and PACFs

### Effect Size

### Forecasting

### Statistical Methods for Comparing Two Models

## Complete Example of a Time-Series Analysis

### Evaluation of Assumptions

#### Normality ozf Sampling Distributions

#### Homogeneity of Variance

#### Outliers


### Baseline Model Identification and Estimation

### Baseline Model Diagnosis

### Intervention Analysis

#### Model Diagnosis

#### Model Interpretation










