---
title: 'Book: The R Book'
author: Marie
date: '2018-07-20T13:39:46+02:00'
slug: book-the-r-book
banner: "img/banners/crawley.png"
draft: true
categories:
  - Study-Notes
tags:
- R
- Study-Notes
- Book
---


# 22: Time Series Analysis

Concepts: 

* Trend: Often need to detrend (e.g. by differencing) prior to analysis
* Serial dependence: Correlations between adjacent time points
    * Autocorrelation: Relationship between $t$ and $t-1$
    * Partial autocorrelation: Relationship between $t$ and the population at lag $k$, once controlled for correlations at lesser lags.
* Stationary: Time series has the same propoerties regardless of the starting point (i.e white noise has mean zero and constant variance)

```{r ts_load_flies}
blowfly <- read.table("../../static/datasets/therbook/blowfly.txt", header = TRUE) 
attach(blowfly)
names(blowfly)

# convert flies to ts object
flies <- ts(flies)

# plot ts object
plot(flies)
```
Time series features:

* First 200 weaks have regular cycles
* After 200 weeks, upward trend and less regular cycles

Plot of flies at lags 1 - 4:
```{r ts_flies_lag}
par(mfrow=c(2,2))
invisible(sapply(1:4, function(x) plot(flies[-c(361: (361-x+1))], flies[-c(1:x)] ) ))
```

Autocorrelation function $\rho(k)$ at lag $k$:
\[\rho(k) = \frac{\gamma(k)}{\gamma(0)},\] where $\lambda(k)$ is the autocovariance function at lag $k$ of a stationary random function {$Y(t)$} given by \[\gamma(k) = \text{cov}{Y(t),Y(t-k)}\]

Autocorrelation coefficient properties:
* symmetric, $\rho(k) = -\rho(k)$
* $-1 \leq \rho(k) \leq 1$
* If $Y(t)$ and $Y(t-k)$ are independent then $\rho(k)$ = 0, but the converse if not true.

First order autogressive process:
\[Y_t = \alpha Y_{t-1} + Z_t\]
where Z is white noise; serially independent with mean zero and constant variance $\sigma^2$.

In a stationary time series -1 < $\alpha$ < 1, the autocorrelation function of {Y(t)} is $\rho_k = \alpha^k$. 

Partial autocorrelation: Correlation between $Y(t)$ and $Y(t + k)$ after regression of $Y(t)$ on $Y(t+1), Y(t+2), \ldots, Y(t+k-1)$ and is obtained by solving the Yule-Walker equation
\[\rho_k = \sum^p_1\alpha_i\rho_{k-i},   \quad  k >0\]
<!--https://tex.stackexchange.com/questions/374094/reduce-space-between-condition-and-statement-in-latex-case-equation-->

See:
https://datascienceplus.com/author/troywalt/
https://stackoverflow.com/questions/20222124/aggregate-time-series-from-weeks-to-month

# 23: Multivariate Statistics
## Principal components analysis
## Factor analysis
## Cluster analysis

Aim: Look for groups in the data
Allocation methods:

* **Partitioning**.  Partition into the number of clusters specified by the user; `kmeans`
* **Hierarchical**, start with each individual as a separate entity and aggregate; `hclust`
* **Divisive**, start with single aggregate and split into clusters 

### Partitioning

* User specifies the number of clusters
* Calculate centroid (multidimensional equivalent of the mean) of each group
* Assign individual to the group with nearest centroid
* Final clusters will minimise within-cluster sum of squares based on Euclidean distance


Example:

```{r kmeans_data}
require(ggplot2)
require(tidyverse)
require(gridExtra)

kmd <- read.table("../../static/datasets/therbook/kmeansdata.txt", header = TRUE) 
attach(kmd)
names(kmd)

set.seed(2)
kmd$km4 <- kmeans(select(kmd, x, y), 4)[[1]]
kmd$km6 <- kmeans(select(kmd, x, y), 6)[[1]]
p1 <- ggplot(kmd) + geom_point(aes(x, y))
p2 <- ggplot(kmd) + geom_point(aes(x, y, color = as.factor(group)))
p3 <- ggplot(kmd) + geom_point(aes(x, y, color = as.factor(km4)))
p4 <- ggplot(kmd) + geom_point(aes(x, y, color = as.factor(km6)))
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

```{r kmeans_misclassification}
kmd <- kmd %>%
  mutate(km6 = as.factor(km6), 
         km6 = fct_relevel(km6, "5", "6", "3", "1", "2", "4"))

with(kmd, table(group, km6))
```

#### Taxonomic use of `kmeans`

Example:  For 120 plants, which of the 7 variables (fruit size, bract length, internode length, petal width, sepal length, petiole length, leave width) are the most useful for taxonomic purposes?

```{r kmeans_taxa_data, message=FALSE, warning=TRUE}
taxa <- read.table("../../static/datasets/therbook/taxa.txt", header = T)
GGally::ggpairs(taxa) + theme(strip.text = element_text(size = 8))
```


The scatterplot matrix shows:

* Excellent data separation on Sepal
* Reasonable separaterion on Leaf 

Generally the number of groups is unknown, but here it is known that there are four taxa (which appear in order, rows of 30 each in the data)

```{r kmeans_taxa_model}
set.seed(282)
taxa$km4 <- kmeans(taxa, 4)[[1]]
taxa$act <- rep(1:4, each = 30)
with(taxa, table(act, km4))

taxa <- taxa %>%
  mutate(km4 = as.factor(km4), 
         km4 = fct_relevel(km4, "3", "1", "4", "2"))
with(taxa, table(act, km4))
```

As in this case, the group is known, can use a supervised method, e.g. classivication tree model, to determine the key that ascribes taxa to islands better.

### Hierarchical cluster analysis

* Groups samples most similar to each in same limb of tree
* Similarity based on distance, e.g. Euclidean distance
* Distance between all samples placed in a matrix produced by the `dist` function
* Initially each sample is its own cluster, the `hclust` alogrithm proceeds iteratively, at each stage joining the two most similar clusters, until there is just one cluster.

Example: 54 plant species over 89 plots receiving different experimental treatments.  Aim: 

* Which plots are most similar in their botanical composition 
* Are there reasonable homogenous groups of plots?

```{r hclust_data}
pgdata<-read.table("../../static/datasets/therbook/pgfull.txt",header=T)
attach(pgdata)
labels <- paste(plot, letters[lime], sep = "")
```

Steps:

1. Create dissimilarity matrix
2. Carry out hierarchical cluster analysis
```{r hclust_model}

# Dissimilarity matrix
dm <- dist(pgdata[, 1:54])
# Hclust
hpg <- hclust(dm)
plot(hpg, labels = labels, main = "")
```



## Discriminant analysis
## Neural networks

