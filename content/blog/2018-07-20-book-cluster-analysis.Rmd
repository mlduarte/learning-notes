---
title: 'Book: Cluster Analysis'
author: Marie
date: '2018-07-20'
slug: book-cluster-analysis
banner: "img/banners/everitt.png"
categories:
  - Study-Notes
tags:
- R
- Study-Notes
- Book
- Clustering
---

```{r load_data_pckg}
require(cluster.datasets)
```

# Measurement of proximity

## Introduction

Dissimilarity = distance = simalirty = proximity.  Small dissimilarity/distance.  Large similarity

Proximity:

* Direct
* Indirect

## Similarity measures for cateogrical data

* Similarity measures most commonly used
* Generally scaled in [0,1]
* Two individuals have similarity coefficient equal to one if they have identical values for all variables.


### Similarity measures for binary data

* Difficulty arises with zero; attributing a large degree of similarity to a pair of individuals because they lack a number of attributes may not be sensible
* Similarity measures include
    * Matching coefficient: Use when co-abscence important
    * Jaccard coefficient
    * Rogers and Tanimoto: Also treat positive matches and negative matches equally.
    * Sneath and Sokal
    * Gower and Leendre

### Similarity measures for categorical data with more than two levels
* Can consider each level as a single binary vairable, however there will be a large number of negative matches.
* Better to allocate a score of zero or one to each variable, depending on whether the two individuals are the same on that variable.  These scores are then averaged over all p variables to give similarity coefficient \[S_{ij} \frac{1}{p}\sum^p_{k = 1} s_{ijk}\]
* Transformed genetic dissimilarity measure discussed
* Linguists have assigned 1 to pairs if the variable value is in the same group (e.g. of words with a similar meaning)

## Dissimilarity and distance measures for continuous data

Distance measure types;

* Distance measures
    * Euclidean distance (l2 norm)\[d_{ij} = \left[\sum^p_{k = 1}w^2_k(x_{ik} - x_{jk})^2\right]\]
    * City block distance (taxicab, rectilinear, Manhattan) (l1 norm) \[d_{ij} = \left[\sum^p_{k = 1}w^2_k|x_{ik} - x_{jk}|\right]\]
    * Minkowski distance ($l_r$ norm)
    * Canberra distance; very sensitive to small changes near $x_{ik} = x_{jk} = 0$
    * Pearson correlation
    * Angular separation (cross product index())
* Correlation-type measures

## Similarity measures for data containing both continuous and categorical variables
## Proximity measures fro structured data
## Inter-group proximity measures
### Inter-group proximity derived from teh proximity matrix
### Inter-group proximity based on group summaries for contiuous data
### Inter-group proximity based on gorup summaries for categorical data

## Weighting variables
## Standardization
## Choise of proximity measure

# Miscellaneous clustering methods

## Density search clustering techniques
## Density-based spatial clustering of applications with noise
## Techniques which allow overlapping clusters
## Simultaneous clustering of objects and variables
## Clustering with constraints
## Fuzzy clustering

## Clustering and artificial neural networks

### Components of a neural network

Features of a neural network:

1. Neurons 
2. Connections between units
3. Training algorithms 

* From a set of predictors (input) $x_1, x_2, \ldots, x_p$ and weights $w_1, w_2, \ldots, w_p$, the neuron provides a response (output) $y$ 
\[ y = \text{sign} \left(w_0 + \sum^p_{i = 1} w_i x_i     \right) \]
* The neuron will only fire if the summation is positive (sign = 1)



```{r store_charts, include=FALSE}
# http://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html
require(png); require(rsvg); require(tidyverse)
DiagrammeRsvg::export_svg(DiagrammeR::grViz("../../static/graphs/nn.dot", height=200, width=200)) %>%
  charToRaw %>% rsvg %>% png::writePNG('../../static/graphs/nn.png')

```

In the following chart, X is the input layer, Z the hidden layer and Y the output layer.
![Neural Network](/graphs/nn.png)


The model is typically trainined by minimising the RSS (residual sum of squares).




### The Kohonen self-organising map

* NN usually applied for supervised problems
* Kohonen self-organising map is unsupervised
* Network contains two layers

* Input layer consisting of $p$-dimensional observations $\mathbf{x}$;
* Output layer (grid) consisting of $k$ nodes for the $k$ clusters, each of which is associated with a $p$-dimensional weight $\mathbf{w}$.
  
![Figure 8.17, Everitt](/img/everitt_som.png)

Algorithm: 
 
 * Unlike in $k$-means, the $p$-dimensional weight vectors associated with the $k$ output nodes are initially assigned a random value between (0, 1)
 * Each of the $p$-dimensional observations ($\mathbf{x}$) are also scaled in (0, 1)
 * The Euclidean distance (or other distance) is calculated between the observation and each of the $k$ $p$-dimensional weight vectors (neurons).  
 * The neuron with the smallest distance (the winner) is updated, as are a small neighbourhood of neurons around the winner; its weight vector $\mathbf{w}_\text{old}$ is brought closer to the input patterns $\mathbf{x}$ as follwos:
 \[\mathbf{w}_\text{new} = \mathbf{w}_\text{old} + \alpha(\mathbf{x} - \mathbf{w}_\text{old})\]
 The value of $\alpha$ is a small fraction, which decreases as learning takes place, as does the size of the neighbourhood.  The excited neurons in the neighbourhood ofthe winner are udpated in a similar manner but with a smaller $\alpha$.
 * As the network learns, the weights are modified and the input observations are provisionally assigned to clusters.
 

  
### Application of neural nets to brainstorming sessions
