---
title: Sequence Models
author: ''
date: '2019-03-04'
slug: sequence-models
categories: [Study-Notes]
tags: 
- Coursera
- Study-Notes
banner: img/banners/sequenceNN.png
---



<div id="week1" class="section level1">
<h1>Recurrent Neural Networks</h1>
<div id="why-sequence-models" class="section level2">
<h2>Why sequence models</h2>
<ul>
<li>Speech recognition
<ul>
<li>Input: Audio clip (sequence)</li>
<li>Output: Sequence of words (sequence)</li>
</ul></li>
<li>Music generation
<ul>
<li>Input: e.g. Genre to generate</li>
<li>Output: Music (sequence)</li>
</ul></li>
<li>Sentiment classification
<ul>
<li>Input: Phrase (sequence)</li>
<li>Output: Stars</li>
</ul></li>
<li>DNA sequence analysis
<ul>
<li>Input: DNA sequence</li>
<li>Output: Which part of sequence corresponds to a protein</li>
</ul></li>
<li>Machine translation
<ul>
<li>Input: sentence, e.g. in french</li>
<li>Output: same sentence in different language</li>
</ul></li>
<li>Video activity recognition
<ul>
<li>Input: Sequence of video frames</li>
<li>Output: Activity</li>
</ul></li>
<li>Name entity recognition, which can be used to find people’s names, companies names, times, locations, countries, currency names, etc in different types of text
<ul>
<li>Input: Sentence</li>
<li>Output: List of names in sentence</li>
</ul></li>
<li>Supervised learning examples</li>
<li>Can have both X and Y as sequences (which may or may not have the same length), or either X or either Y as sequence only.</li>
</ul>
</div>
<div id="notation" class="section level2">
<h2>Notation</h2>
<p>Motivating example: Named entity recognition,</p>
<ul>
<li><span class="math inline">\(x\)</span>: Harry Potter and Hermoine Granger invented a new spell</li>
<li><p><span class="math inline">\(y\)</span>: For each part of input word, is it part of a person’s name? (more sophisticated alternatives: Start/stop for names)</p></li>
<li><span class="math inline">\(x^{&lt;t&gt;}\)</span>: index to position into input sequence, e.g. <span class="math inline">\(x^{&lt;2&gt;}\)</span> in above example, would be Potter as it is the second word in the sequence.</li>
<li><span class="math inline">\(x^{(i)&lt;t&gt;}\)</span>: input sequence position <span class="math inline">\(t\)</span> of <span class="math inline">\(i^{\text{th}}\)</span> training example</li>
<li><span class="math inline">\(y^{&lt;t&gt;}\)</span>: index to position into output sequence, e.g. <span class="math inline">\(y^{&lt;2&gt;}\)</span> in above example, would be 1 as Potter is part of a name</li>
<li><span class="math inline">\(y^{(i)&lt;t&gt;}\)</span>: output sequence position <span class="math inline">\(t\)</span> of <span class="math inline">\(i^{\text{th}}\)</span> training example</li>
<li><span class="math inline">\(T_x\)</span>: length of input sequence, e.g. 9 in above example</li>
<li><span class="math inline">\(T_y\)</span>: length of output sequence, e.g. 9 in above example</li>
<li><span class="math inline">\(T_x^{(i)}\)</span>: length of input sequence in <span class="math inline">\(i^{\text{th}}\)</span> training example</li>
<li><p><span class="math inline">\(T_y^{(i)}\)</span>: length of output sequence in <span class="math inline">\(i^{\text{th}}\)</span> training example</p></li>
</ul>
<p>How to represent individual words in a sentence?</p>
<ul>
<li>Start with a vocabulary / dictionary</li>
</ul>
<p><span class="math inline">\(\begin{bmatrix} \text{a} \\ \vdots \\ \text{and} \\ \text{harry} \\ \vdots \\ \text{zulu} \end{bmatrix}\)</span></p>
<ul>
<li>Number each word in vobulary sequentially, e.g. with 10,000 words in dictionary (quite small); 30-50K more common. 100K not uncommon. Internet companies: 1Million</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Word</th>
<th>Index</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>a</td>
<td>1</td>
</tr>
<tr class="even">
<td>aaron</td>
<td>2</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="even">
<td>and</td>
<td>367</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="even">
<td>harry</td>
<td>4,075</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="even">
<td>potter</td>
<td>6,830</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="even">
<td>zulu</td>
<td>10,000</td>
</tr>
</tbody>
</table>
<p>Use one-hot representations:</p>
<ul>
<li><span class="math inline">\(x^{1}\)</span> would be a 10,000 length vector of zeros, with exception of the value 1 at index 4075 (<em>Harry</em>)</li>
<li><span class="math inline">\(x^{2}\)</span> would be a 10,000 length vector of zeros, with exception of the value 1 at index 6830 (<em>Potter</em>)</li>
<li><span class="math inline">\(x^{7}\)</span> would be a 10,000 length vector of zeros, with exception of the value 1 at index 1 (<em>a</em>)</li>
</ul>
<p>So each <span class="math inline">\(x^{&lt;t&gt;}\)</span> is a 10,000 (vocabulary size) one-hot vector (one-hot = only one is on, and zero everywhere else) Use <UNK> if come across a word not in vocabulary</p>
</div>
<div id="recurrent-neural-network-model" class="section level2">
<h2>Recurrent Neural Network Model</h2>
<p>Why couldn’t you use a standard neural network?</p>
<p>Problems:</p>
<ul>
<li>Inputs and and output can be different lengths in different examples. (not a good idea to pad, zero pad inputs to consider inputs up to a maximum length)</li>
<li>Doesn’t share features learned across different positions of text, so that Harry learned as a name in position 1 would generalise to other parts</li>
<li>As standard model, would need weights for 100,000 <span class="math inline">\(\times\)</span> maximum number of words</li>
</ul>
<p>Recurrent Neural Networks</p>
<ul>
<li>Assume read from left to right</li>
<li>When reads second word, it takes information from what computed for first word, and so on <img src="/img/recurrentNN/RNN.png" alt="Recurrent Neural Network" /><br />
</li>
<li>Alternative representation, where square is one-step <img src="/img/recurrentNN/RNN_derowed.png" alt="Recurrent Neural Network (De-Rowed)" /><br />
</li>
<li>Parameters, <span class="math inline">\(W_{ax}\)</span>, <span class="math inline">\(W_{ay}\)</span> and <span class="math inline">\(W_{aa}\)</span>, for each time step are shared <img src="/img/recurrentNN/RNN_with_params.png" alt="RNN Parameters" /><br />
</li>
<li>Weakness: Only uses information that is earlier in the sequences, e.g. cannot determine “Teddy” is a name with only first word in the following sentences; need the later words: “Teddy Roosevelt was a great President” and “Teddy bears are on sale!”. Will look at bidrectional RNN (BRNN) later in course.</li>
</ul>
<p><img src="/img/recurrentNN/RNN.png" alt="Recurrent Neural Network" /><br />
Forward propagation (unidirectional RNN)</p>
<ul>
<li>Start with initial activations, <span class="math inline">\(a^{&lt;0&gt;}\)</span> of zeros</li>
<li>Activation at time step 1: generally use tanh (most common) or ReLU activation. Other ways of preventing vanishing gradient problem <span class="math display">\[a^{&lt;1&gt;} = g(W_{aa} a^{&lt;0&gt;} + W_{ax} x^{&lt;1&gt;} + b_a)\]</span></li>
<li>Prediction at time set 1;
<ul>
<li>binary classification: sigmoid activation function</li>
<li>k-way: softmax <span class="math display">\[\hat{y}^{&lt;i&gt;} =  g(W_{ya} a^{&lt;1&gt;} + b_y)\]</span></li>
</ul></li>
<li>More generally, at time <span class="math inline">\(t\)</span>: <span class="math display">\[\begin{aligned}
a^{&lt;t&gt;}         &amp;= g(W_{aa} a^{&lt;t-1&gt;} + W_{ax} x^{&lt;t&gt;} + b_a) \\
\hat{y}^{&lt;t&gt;}   &amp;=  g(W_{ya} a^{&lt;t&gt;} + b_y)
\end{aligned}\]</span></li>
</ul>
<p>Can simplify as: <span class="math display">\[\begin{aligned}
    a^{&lt;t&gt;}         &amp;= g(W_{a} [a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_a) \\
    \hat{y}^{&lt;t&gt;}   &amp;=  g(W_{ya} a^{&lt;t&gt;} + b_y)
\end{aligned}\]</span></p>
<p>where <span class="math display">\[\begin{aligned}
    W_a &amp;= [W_{aa} | W_{ax}] \\
    [a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] &amp;= \begin{bmatrix} a^{&lt;t-1&gt;} \\ x^{&lt;t&gt;} \end{bmatrix} 
\end{aligned}\]</span></p>
<p>Example: If <span class="math inline">\(a\)</span> 100-dimensional and <span class="math inline">\(x\)</span> 10,000-dimensional, then</p>
<ul>
<li><span class="math inline">\(W_{aa}\)</span> would be 100 <span class="math inline">\(\times\)</span> 100</li>
<li><span class="math inline">\(W_{ax}\)</span> would be 100 <span class="math inline">\(\times\)</span> 10,000</li>
<li><span class="math inline">\(W_a\)</span> would be 100 <span class="math inline">\(\times\)</span> 10,100</li>
<li><span class="math inline">\(a^{&lt;t-1&gt;}\)</span> would be 100 <span class="math inline">\(\times\)</span> 1</li>
<li><span class="math inline">\(x^{&lt;t&gt;}\)</span> would be $10,000 <span class="math inline">\(\times\)</span> 1</li>
</ul>
<p>Note:</p>
<ul>
<li><span class="math inline">\(b_a\)</span>: bias</li>
<li><span class="math inline">\(W_\text{xy}\)</span> Multiply by some <span class="math inline">\(y\)</span>-like quantity to compute some <span class="math inline">\(x\)</span>-like quantity</li>
</ul>
</div>
<div id="backpropagation-through-time" class="section level2">
<h2>Backpropagation through time</h2>
<ul>
<li>Programming framework would automatically take care of backpropagation</li>
<li>Given inputs <span class="math inline">\(x^{&lt;1&gt;}, x^{&lt;2&gt;}, \ldots, x^{&lt;T_x&gt;}\)</span>, initial activation <span class="math inline">\(a_0\)</span> and parameters <span class="math inline">\(W_a\)</span> and <span class="math inline">\(b_a\)</span>, calculate activations <span class="math inline">\(a^{&lt;1&gt;}, a^{&lt;2&gt;}, \ldots, a^{&lt;T_x&gt;}\)</span></li>
<li>Given parameters <span class="math inline">\(W_y\)</span>, <span class="math inline">\(b_y\)</span> and activations, can calculate predictions <span class="math inline">\(\hat{y}^{&lt;1&gt;}, \hat{y}^{&lt;2&gt;}, \ldots, \hat{y}^{&lt;T_x&gt;}\)</span></li>
<li>To perform back-propagation, require loss function, standard logistic regression loss function (aka cross-entropy loss) <span class="math display">\[\mathcal{L}(\hat{y}^{&lt;t&gt;}, y^{&lt;t&gt;}) = -y^{&lt;t&gt;} \log \hat{y}^{&lt;t&gt;}  - (1 - y^{&lt;t&gt;}) \log (1 - \hat{y}^{&lt;t&gt;})\]</span> <img src="/img/recurrentNN/BackpropThruTime.png" alt="Backpropagation Through Time" /></li>
</ul>
</div>
<div id="different-types-of-rnns" class="section level2">
<h2>Different types of RNNs</h2>
<p>Wider range of RNN architectures, e.g. when output <span class="math inline">\(T_x \neq T_y\)</span> input sequence length</p>
<ul>
<li>Many-to-many, <span class="math inline">\(T_x, T_y &gt; 1\)</span>.
<ul>
<li><span class="math inline">\(T_x = T_y\)</span>; as per motivating example above </br> <img src="/img/recurrentNN/architecture_many_to_many_same.png" alt="Many to many, same" /><br />
</li>
<li><span class="math inline">\(T_x \neq T_y\)</span>, e.g machine translation, “como estas” <span class="math inline">\(\rightarrow\)</span> “how are you”</br> <img src="/img/recurrentNN/architecture_many_to_many_diff.png" alt="Many to many, same" /><br />
</li>
</ul></li>
<li>Many-to-one, e.g. output 0 or 1 (or a rating from 1 to 5) to indicate sentiment from sentence</br> <img src="/img/recurrentNN/architecture_many_to_one.png" alt="Many to one" /><br />
</li>
<li>One-to-many, e.g. music generation where goal is to output set of notes from input such as genre</br> <img src="/img/recurrentNN/architecture_one_to_many.png" alt="One to Many" /><br />
</li>
<li>One-to-one (standard)</br> <img src="/img/recurrentNN/architecture_one_to_one.png" alt="One to One" /><br />
</li>
<li>attention-based (to discuss later)</li>
</ul>
</div>
<div id="language-model-and-sequence-generation" class="section level2">
<h2>Language model and sequence generation</h2>
<ul>
<li>A language model takes as input a sentence (a sequence, but use notation <span class="math inline">\(y\)</span> rather than <span class="math inline">\(x\)</span>) and determines the probability</li>
<li>To build a language model,
<ol style="list-style-type: decimal">
<li>Obtain training set: large corpus (body) of english text</li>
<li>Require vocabulary (dictionary)</li>
<li>Map each word to the dictionary tokens using one-hot vectors
<ul>
<li>Each word in sentence is represented as <span class="math inline">\(y^{&lt;1&gt;}, y^{&lt;2&gt;}, \ldots\)</span>$</li>
<li>May use <EOS> token for end of sentence (period)</li>
<li>Use <UNK> for words not in vocabulary</li>
</ul></li>
<li>What is the probability of each word in the sentence
<ul>
<li>First word: What is the probability of the first word being any word in the dictionary? With 10,000-word, each output is a 10,000-way soft max output (incl. EOS and UNK). As input, pass zero-vector</li>
<li>Second word: What is probability of second word being each word in dictionary, given first word in sentence</li>
<li><span class="math inline">\(\ldots\)</span></li>
<li>Each step looks at preceding words, RNN learns to predict one word a time going from left to right Note that 15 was considered a word <img src="/img/recurrentNN/example_model_cats_sleep.png" alt="Cats average 15 hours of sleep a day" /></li>
</ul></li>
</ol></li>
<li>Cost function: at time t with tru word <span class="math inline">\(y^{&lt;t&gt;}\)</span> and prediction <span class="math inline">\(\hat{y}^{&lt;t&gt;}\)</span> then loss function is <span class="math display">\[\begin{aligned}
\mathcal{L}(\hat{y}^{&lt;t&gt;}, y^{&lt;t&gt;}) &amp;= -\sum_i y^{&lt;t&gt;}_i \log \hat{y}^{&lt;t&gt;}_i \\
\mathcal{L} &amp;= \sum_t \mathcal{L}^{&lt;t&gt;}(\hat{y}^{&lt;t&gt;}, y^{&lt;t&gt;}) 
\end{aligned}\]</span></li>
</ul>
<p>Note conditional probabilities used to determine the probability of a three-word sentence: <span class="math inline">\(P(y^{&lt;1&gt;}, y^{&lt;2&gt;}, y^{&lt;3&gt;}) = P(y^{&lt;1&gt;})P(y^{&lt;2&gt;}|y^{&lt;1&gt;})P(y^{&lt;2&gt;})P(y^{&lt;1&gt;},y^{&lt;2&gt;})\)</span></p>
</div>
<div id="sampling-novel-sequences" class="section level2">
<h2>Sampling novel sequences</h2>
<p>To get a sense of what is learned</p>
<ul>
<li>Sample first word; randomly sample soft max distribution use<code>np.random.choice</code></li>
<li>Take <span class="math inline">\(x_2 = \hat{y}^{&lt;1&gt;}\)</span> to predict <span class="math inline">\(\hat{y}^{&lt;2&gt;}\)</span></li>
<li>Sample until EOS token (but if no EOS, choose sentence length beforehand)</li>
<li>You may get an UNK token; can reject and resample until not UNK, or leave in output</li>
</ul>
<p>Character-level vocabulary</p>
<ul>
<li>Have been using a word-level vocabulary; alternative is to use a character-level vocabulary</li>
<li>Example vocabulary: [a, b,c, <span class="math inline">\(\ldots\)</span>, z, space, ., ;, 0, <span class="math inline">\(\ldots\)</span> 9, A, Z]</li>
<li>Can use corpus to determine complete vocabulary</li>
<li>Advantage: Don’t have to worry about unknown tokens</li>
<li>Disadvantages:
<ul>
<li>Much longer sequences, 10-20 words = 100+ characters.<br />
</li>
<li>Harder to capture longer-range dependencies.<br />
</li>
<li>More computationally expensive</li>
</ul></li>
<li>As computers are getting more efficient, people are starting to use character-level vocabularies, but practice not widely spread</li>
</ul>
<p>Sampling novel sentences from a model trained using newspaper corpuses gives very different results to those trained using a shakespear corpus.</p>
</div>
<div id="vanishing-gradients-with-rnns" class="section level2">
<h2>Vanishing gradients with RNNs</h2>
<ul>
<li>RNNs tend to run in to vanishing gradient problems</li>
<li>Consider:
<ul>
<li>The <strong>cat</strong>, which already ate <span class="math inline">\(\ldots\)</span>, <strong>was</strong> full</li>
<li>The <strong>cats</strong>, which already ate <span class="math inline">\(\ldots\)</span>, <strong>were</strong> full</li>
</ul></li>
<li>Basic RNN not good at picking up very long-range dependency</li>
<li>Recall concept of vanishing gradients with very deep neural networks (e.g. 100 layers); gradient at last layer will have hard time impacting gradient at earlier level. Similar occurs with basic RNN.<br />
</li>
<li>Basic RNN has mainly local influences, i.e. preceding 1-3 words have much more influence than words preceding the current word by, e.g. 10 places</li>
<li>Exploding gradients
<ul>
<li>Not as common as Vanishing</li>
<li>But when exists, can be catestrophic</li>
<li>If you see “NaN” in gradient vectors, can apply gradient clipping; i.e. set gradient = max(X, gradient)</li>
</ul></li>
</ul>
<p>As vanishing gradient harder to deal with, will be discussed in next few sections.</p>
<p>See <a href="/blog/2018/12/07/2018-12-07-improving-deep-learning-networks#vanishing">Improving Deep Learning Networks</a></p>
</div>
<div id="gated-recurrent-unit-gru" class="section level2">
<h2>Gated Recurrent Unit (GRU)</h2>
<ul>
<li>Better than basic RNN at capturing long range connections and helps with vanishing gradient problems</li>
<li>Recall formula for activation at time <span class="math inline">\(t\)</span>, <span class="math display">\[a^{&lt;t&gt;}  = g(W_{aa} a^{&lt;t-1&gt;} + W_{ax} x^{&lt;t&gt;} + b_a)\]</span>
<ul>
<li>activation from last time step</li>
<li>Input token</li>
<li>g could be tahn activation function <img src="/img/recurrentNN/basic_rnn_activation.png" alt="Basic RNN activation function" /></li>
</ul></li>
<li>Motivating example
<ul>
<li>The <strong>cat</strong>, which already ate <span class="math inline">\(\ldots\)</span>, <strong>was</strong> full</li>
<li>The <strong>cats</strong>, which already ate <span class="math inline">\(\ldots\)</span>, <strong>were</strong> full</li>
</ul></li>
<li>Formulas:</li>
</ul>
<p><span class="math display">\[\begin{aligned}
    \text{Candidate Memory Cell:} &amp;   \tilde{c}^{&lt;t&gt;} &amp;= \tanh(w_c[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_c) \\
    \text{Gate:}                    &amp; \Gamma_u &amp;= \sigma(w_u[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_u\\
    \text{Memory Cell:} &amp; c^{&lt;t&gt;} &amp;= \Gamma_u \tilde{c}^{&lt;t&gt;} +  (1-\Gamma_u) c^{&lt;t-1&gt;}\\
\end{aligned}\]</span></p>
<ul>
<li>Use <span class="math inline">\(c\)</span> for memory _c_ell</li>
<li>The GRU will output an activation that is the same as the memory cell, but this notation is used as these will differ for other models, e.g., LSTM <span class="math display">\[c^{&lt;t&gt;} = a^{&lt;t&gt;}\]</span></li>
<li>At each time step, consider overriding memory cell with the candidate memory cell based on the value of the gate
<ul>
<li>As the gate is a sigmoid function it will usually be practically zero or practically one</li>
<li>If the gate function is 0, the memory cell <span class="math inline">\(c^{&lt;t&gt;}\)</span> remains the same as at <span class="math inline">\(t-1\)</span>, i.e., <span class="math inline">\(c^{&lt;t&gt;} = c^{&lt;t-1&gt;}\)</span>. If the gate function is 1, the memory cell is updated to the candidate value</li>
</ul></li>
<li>For example, if the cell <span class="math inline">\(c\)</span> captures concept of plural/singular, the gate would have a value of 1 at the <em>cat</em> token remain at zero until after a new noun captures after the word <em>was</em>.</li>
<li>The dimensions of the memory cell <span class="math inline">\(c^{&lt;t&gt;}\)</span>, candidate memory cell <span class="math inline">\(\tilde{c}^{&lt;t&gt;}\)</span> and gate <span class="math inline">\(\Gamma_u\)</span> are the same.
<ul>
<li>So if all are 100-dimensional vector; most are zeros or ones and tells which are the bits you want to update. (in practice, may be in-between zero)</li>
<li>The asterix is element-wise in the memory cell formula</li>
<li>Different elements (bits) may be used for different concepts; i.e. one for singular/plural, another for the concept of food, etc.</li>
</ul></li>
</ul>
<div class="figure">
<img src="/img/recurrentNN/simplified_gru.png" alt="Simplified GRU" />
<p class="caption">Simplified GRU</p>
</div>
<p>Full GRU</p>
<ul>
<li>Uses <span class="math inline">\(r\)</span> to measure the relevance of the current memory cell <span class="math inline">\(c^{&lt;t-1&gt;}\)</span> when calculating the candidate value <span class="math inline">\(\tilde{c}^{&lt;t&gt;}\)</span></li>
</ul>
<p><span class="math display">\[\begin{aligned}
    \text{Candidate Memory Cell:}\;\;\; &amp;   \tilde{c}^{&lt;t&gt;} &amp;&amp;= \tanh(\color{blue}{\Gamma_r} * W_c[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_c) \\
    \text{Gate:}\;\;\;                &amp; \Gamma_u &amp;&amp;= \sigma(W_u[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_u\\
    \color{blue}{\text{relevance:}}\;\;\;                &amp; \color{blue}{\Gamma_r} &amp;&amp;= \color{blue}{\sigma(W_r[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_r}\\
    \text{Memory Cell:}\;\;\; &amp; c^{&lt;t&gt;} &amp;&amp;= \Gamma_u * \tilde{c}^{&lt;t&gt;} +  (1-\Gamma_u) c^{&lt;t-1&gt;}\\
\end{aligned}\]</span></p>
<ul>
<li>Academic literature may use different notation, here used notation to be similar for GRU and LSTM
<ul>
<li>candidate memory cell <span class="math inline">\(\tilde{h}\)</span></li>
<li>Gate <span class="math inline">\(u\)</span></li>
<li>Relevance <span class="math inline">\(r\)</span></li>
<li>Memory cell <span class="math inline">\(h\)</span></li>
</ul></li>
</ul>
<p>LSTM</p>
<ul>
<li>another instantiations of the same idea</li>
</ul>
</div>
<div id="long-short-term-memory-lstm" class="section level2">
<h2>Long Short Term Memory (LSTM)</h2>
<ul>
<li>More powerful than GRU</li>
<li>Recall GRU equations</li>
</ul>
<p><span class="math display">\[\begin{aligned}
    \text{Candidate Memory Cell:}\;\;\; &amp;   \tilde{c}^{&lt;t&gt;} &amp;&amp;= \tanh(\Gamma_r * W_c[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_c) \\
    \text{Update Gate:}\;\;\;                &amp; \Gamma_u &amp;&amp;= \sigma(W_u[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_u\\
    \text{Relevance Gate:}\;\;\;                &amp; \Gamma_r &amp;&amp;= \sigma(W_r[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_r\\
    \text{Memory Cell:}\;\;\; &amp; c^{&lt;t&gt;} &amp;&amp;= \Gamma_u * \tilde{c}^{&lt;t&gt;} +  (1-\Gamma_u) c^{&lt;t-1&gt;}\\
    \text{Activation Value:}\;\;\; &amp; a^{&lt;t&gt;} &amp;&amp;= c^{&lt;t&gt;}
\end{aligned}\]</span></p>
<p>LSTM</p>
<ul>
<li>As, unlike for GRU, <span class="math inline">\(a^{&lt;t&gt;} \neq c^{&lt;t&gt;}\)</span>, and so memory cell explicitly references the activation value, rather than the memory cell</li>
<li>Introduces a forget gate, which gives option of keeping old memory cell and adding to it.</li>
<li>Introduces output gate</li>
<li>So has three gates instead of two</li>
</ul>
<p><span class="math display">\[\begin{aligned}
    \text{Candidate Memory Cell:}\;\;\; &amp;   \tilde{c}^{&lt;t&gt;} &amp;&amp;= \tanh(W_c[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_c) \\
    \text{Update Gate:}\;\;\;                &amp; \Gamma_u &amp;&amp;= \sigma(W_u[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_u\\
    \text{Forget Gate:}\;\;\;                &amp; \Gamma_f &amp;&amp;= \sigma(W_f[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_f\\
    \text{Output Gate:}\;\;\;                &amp; \Gamma_o &amp;&amp;= \sigma(W_o[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_o\\
    \text{Memory Cell:}\;\;\; &amp; c^{&lt;t&gt;} &amp;&amp;= \Gamma_u * \tilde{c}^{&lt;t&gt;} +  \Gamma_f c^{&lt;t-1&gt;}\\
    \text{Activation Value:}\;\;\; &amp; a^{&lt;t&gt;} &amp;&amp;= \Gamma_o * \tanh c^{&lt;t&gt;}
\end{aligned}\]</span></p>
<div class="figure">
<img src="/img/recurrentNN/LSTM.png" alt="LSTM" />
<p class="caption">LSTM</p>
</div>
<ul>
<li><span class="math inline">\(a^{&lt;t-1&gt;}\)</span> and <span class="math inline">\(x^{&lt;t&gt;}\)</span> used to calculate all the gates (forget, update, output) and also go through the tanh to calculate <span class="math inline">\(\tilde{c}^{&lt;t&gt;}\)</span></li>
</ul>
<div class="figure">
<img src="/img/recurrentNN/LSTM_network.png" alt="LSTM Network" />
<p class="caption">LSTM Network</p>
</div>
<ul>
<li>So long as you set forget and update gates appropriately, it is relatively easy for LSTM to have some value <span class="math inline">\(c^{&lt;0&gt;}\)</span> and have it passed all the way to the right so that <span class="math inline">\(c^{&lt;3&gt;} = c^{&lt;0&gt;}\)</span></li>
</ul>
<p>Variations</p>
<ul>
<li>Have gate values also dependent on <span class="math inline">\(c^{&lt;t-1&gt;}\)</span>, known as a <em>peephole connection</em>, e.g., for output gate: <span class="math display">\[\Gamma_o = \sigma(W_o[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}, c^{&lt;t-1&gt;}] + b_o\]</span></li>
</ul>
<p>When to use GRU vs LSTM?</p>
<ul>
<li>LSTM became earlier than GRU (simplified version)</li>
<li>Different problems, different algorithm will win</li>
<li>Advantage of GRU is simpler and so easier to build bigger network (computationally faster and scales to building bigger models)</li>
<li>LSTM more flexible/powerful (historically more proven choice, although GRU usage is gaining momentum)</li>
</ul>
</div>
<div id="bidirectional-rnn-brnn" class="section level2">
<h2>Bidirectional RNN (BRNN)</h2>
<ul>
<li>To take information earlier and later in sequence</li>
<li>Inputs <span class="math inline">\(x^{&lt;1&gt;}, \ldots, x^{&lt;4&gt;}\)</span></li>
<li>Forward recurrent components <span class="math inline">\(\overrightarrow{a}^{&lt;1&gt;}, \overrightarrow{a}^{&lt;2&gt;}, \overrightarrow{a}^{&lt;3&gt;}, \overrightarrow{a}^{&lt;4&gt;}\)</span></li>
<li>Backward recurrent components <span class="math inline">\(\overleftarrow{a}^{&lt;4&gt;}, \overleftarrow{a}^{&lt;3&gt;}, \overleftarrow{a}^{&lt;2&gt;}, \overleftarrow{a}^{&lt;1&gt;}\)</span>. This is still forward prop.</li>
<li>Predictions <span class="math inline">\(\hat{y}^{&lt;1&gt;}, \ldots, \hat{y}^{&lt;4&gt;}\)</span> where <span class="math display">\[\hat{y}^{&lt;t&gt;} = g(W_y[\overrightarrow{a}^{&lt;t&gt;}, \overleftarrow{a}^{&lt;t&gt;}] + b_y)\]</span></li>
<li>Acyclic graph</li>
<li>BRNN can be with LSTM blocks both forward and backwards. Similarly with GRU.</li>
<li>Disadvantage: Need entire sequence of data before can make a prediction. Need to wait for person to stop talking, i.e. entire utterance. Where can get entire sentence at a time this standard BRNN is suitable.</li>
</ul>
</div>
<div id="deep-rnns" class="section level2">
<h2>Deep RNNs</h2>
<div class="figure">
<img src="/img/recurrentNN/deep_rnn.png" alt="Deep RNN" />
<p class="caption">Deep RNN</p>
</div>
<ul>
<li>May need to stack multiple layers of RNNs to build deeper RNNs</li>
<li>Use <span class="math inline">\(a^{[\text{layer}]&lt;\text{time}&gt;}\)</span></li>
<li>$a^{[2]&lt;3&gt;} = g(W_a^{[2]}[a^{[2]&lt;2&gt;}, a^{[1]&lt;3&gt;}] + b_a^{2}) $</li>
<li>First layer has parameters <span class="math inline">\(W_a^{[1]}, b_a^{1}\)</span>; second layer has parameters <span class="math inline">\(W_a^{[2]}, b_a^{2}\)</span>, and so on</li>
<li>Generally only have 3 recurrent layers (that is already pretty deep)</li>
<li>The recurrent blocks amy be RNN, GRU, LSTM</li>
<li>Sometimes have 3 recurrent layers, with additional non-connected layers on top</li>
</ul>
<div class="figure">
<img src="/img/recurrentNN/deep_RNN_On_Steroids.png" alt="Deep RNN" />
<p class="caption">Deep RNN</p>
</div>
</div>
</div>
<div id="week2" class="section level1">
<h1>Natural Language Processing and Word Embeddings</h1>
<div id="word-representation" class="section level2">
<h2>Word Representation</h2>
</div>
<div id="using-word-embeddings" class="section level2">
<h2>Using word embeddings</h2>
</div>
<div id="properties-of-word-embeddings" class="section level2">
<h2>Properties of word embeddings</h2>
</div>
<div id="embedding-matrix" class="section level2">
<h2>Embedding matrix</h2>
</div>
<div id="learning-word-embeddings" class="section level2">
<h2>Learning word embeddings</h2>
</div>
<div id="word2vec" class="section level2">
<h2>Word2Vec</h2>
</div>
<div id="negative-sampling" class="section level2">
<h2>Negative Sampling</h2>
</div>
<div id="glove-word-vectors" class="section level2">
<h2>GloVe word vectors</h2>
</div>
<div id="sentiment-classification" class="section level2">
<h2>Sentiment Classification</h2>
</div>
<div id="debiasing-word-embeddings" class="section level2">
<h2>Debiasing word embeddings</h2>
</div>
</div>
<div id="week3" class="section level1">
<h1>Sequence Models &amp; Attention Mechanism</h1>
<div id="basic-models" class="section level2">
<h2>Basic Models</h2>
</div>
<div id="picking-the-most-likely-sentence" class="section level2">
<h2>Picking the most likely sentence</h2>
</div>
<div id="beam-search" class="section level2">
<h2>Beam Search</h2>
</div>
<div id="refinements-to-beam-search" class="section level2">
<h2>Refinements to Beam Search</h2>
</div>
<div id="error-analysis-in-beam-search" class="section level2">
<h2>Error analysis in beam search</h2>
</div>
<div id="bleu-score-optional" class="section level2">
<h2>Bleu Score (optional)</h2>
</div>
<div id="attention-model-intuition" class="section level2">
<h2>Attention Model Intuition</h2>
</div>
<div id="attention-model" class="section level2">
<h2>Attention Model</h2>
</div>
<div id="speech-recognition" class="section level2">
<h2>Speech recognition</h2>
</div>
<div id="trigger-word-detection" class="section level2">
<h2>Trigger Word Detection</h2>
</div>
<div id="conclusion-and-thank-you" class="section level2">
<h2>Conclusion and thank you</h2>
</div>
</div>
