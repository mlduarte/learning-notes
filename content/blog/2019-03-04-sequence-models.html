---
title: Sequence Models
author: ''
date: 2019-03-04T13:39:46+02:00
slug: sequence-models
categories: [Study-Notes]
tags: 
- Coursera
- Study-Notes
banner: img/banners/sequenceNN.png
---



<div id="week1" class="section level1">
<h1>Recurrent Neural Networks</h1>
<div id="why-sequence-models" class="section level2">
<h2>Why sequence models</h2>
<ul>
<li>Speech recognition
<ul>
<li>Input: Audio clip (sequence)</li>
<li>Output: Sequence of words (sequence)</li>
</ul></li>
<li>Music generation
<ul>
<li>Input: e.g. Genre to generate</li>
<li>Output: Music (sequence)</li>
</ul></li>
<li>Sentiment classification
<ul>
<li>Input: Phrase (sequence)</li>
<li>Output: Stars</li>
</ul></li>
<li>DNA sequence analysis
<ul>
<li>Input: DNA sequence</li>
<li>Output: Which part of sequence corresponds to a protein</li>
</ul></li>
<li>Machine translation
<ul>
<li>Input: sentence, e.g. in french</li>
<li>Output: same sentence in different language</li>
</ul></li>
<li>Video activity recognition
<ul>
<li>Input: Sequence of video frames</li>
<li>Output: Activity</li>
</ul></li>
<li>Name entity recognition, which can be used to find people’s names, companies names, times, locations, countries, currency names, etc in different types of text
<ul>
<li>Input: Sentence</li>
<li>Output: List of names in sentence</li>
</ul></li>
<li>Supervised learning examples</li>
<li>Can have both X and Y as sequences (which may or may not have the same length), or either X or either Y as sequence only.</li>
</ul>
</div>
<div id="notation" class="section level2">
<h2>Notation</h2>
<p>Motivating example: Named entity recognition,</p>
<ul>
<li><span class="math inline">\(x\)</span>: Harry Potter and Hermoine Granger invented a new spell</li>
<li><p><span class="math inline">\(y\)</span>: For each part of input word, is it part of a person’s name? (more sophisticated alternatives: Start/stop for names)</p></li>
<li><span class="math inline">\(x^{&lt;t&gt;}\)</span>: index to position into input sequence, e.g. <span class="math inline">\(x^{&lt;2&gt;}\)</span> in above example, would be Potter as it is the second word in the sequence.</li>
<li><span class="math inline">\(x^{(i)&lt;t&gt;}\)</span>: input sequence position <span class="math inline">\(t\)</span> of <span class="math inline">\(i^{\text{th}}\)</span> training example</li>
<li><span class="math inline">\(y^{&lt;t&gt;}\)</span>: index to position into output sequence, e.g. <span class="math inline">\(y^{&lt;2&gt;}\)</span> in above example, would be 1 as Potter is part of a name</li>
<li><span class="math inline">\(y^{(i)&lt;t&gt;}\)</span>: output sequence position <span class="math inline">\(t\)</span> of <span class="math inline">\(i^{\text{th}}\)</span> training example</li>
<li><span class="math inline">\(T_x\)</span>: length of input sequence, e.g. 9 in above example</li>
<li><span class="math inline">\(T_y\)</span>: length of output sequence, e.g. 9 in above example</li>
<li><span class="math inline">\(T_x^{(i)}\)</span>: length of input sequence in <span class="math inline">\(i^{\text{th}}\)</span> training example</li>
<li><p><span class="math inline">\(T_y^{(i)}\)</span>: length of output sequence in <span class="math inline">\(i^{\text{th}}\)</span> training example</p></li>
</ul>
<p>How to represent individual words in a sentence?</p>
<ul>
<li>Start with a vocabulary / dictionary</li>
</ul>
<p><span class="math inline">\(\begin{bmatrix} \text{a} \\ \vdots \\ \text{and} \\ \text{harry} \\ \vdots \\ \text{zulu} \end{bmatrix}\)</span></p>
<ul>
<li>Number each word in vobulary sequentially, e.g. with 10,000 words in dictionary (quite small); 30-50K more common. 100K not uncommon. Internet companies: 1Million</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Word</th>
<th>Index</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>a</td>
<td>1</td>
</tr>
<tr class="even">
<td>aaron</td>
<td>2</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="even">
<td>and</td>
<td>367</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="even">
<td>harry</td>
<td>4,075</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="even">
<td>potter</td>
<td>6,830</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="even">
<td>zulu</td>
<td>10,000</td>
</tr>
</tbody>
</table>
<p>Use one-hot representations:</p>
<ul>
<li><span class="math inline">\(x^{1}\)</span> would be a 10,000 length vector of zeros, with exception of the value 1 at index 4075 (<em>Harry</em>)</li>
<li><span class="math inline">\(x^{2}\)</span> would be a 10,000 length vector of zeros, with exception of the value 1 at index 6830 (<em>Potter</em>)</li>
<li><span class="math inline">\(x^{7}\)</span> would be a 10,000 length vector of zeros, with exception of the value 1 at index 1 (<em>a</em>)</li>
</ul>
<p>So each <span class="math inline">\(x^{&lt;t&gt;}\)</span> is a 10,000 (vocabulary size) one-hot vector (one-hot = only one is on, and zero everywhere else) Use <UNK> if come across a word not in vocabulary</p>
</div>
<div id="recurrent-neural-network-model" class="section level2">
<h2>Recurrent Neural Network Model</h2>
<p>Why couldn’t you use a standard neural network?</p>
<p>Problems:</p>
<ul>
<li>Inputs and and output can be different lengths in different examples. (not a good idea to pad, zero pad inputs to consider inputs up to a maximum length)</li>
<li>Doesn’t share features learned across different positions of text, so that Harry learned as a name in position 1 would generalise to other parts</li>
<li>As standard model, would need weights for 100,000 <span class="math inline">\(\times\)</span> maximum number of words</li>
</ul>
<p>Recurrent Neural Networks</p>
<ul>
<li>Assume read from left to right</li>
<li>When reads second word, it takes information from what computed for first word, and so on <img src="/img/recurrentNN/RNN.png" alt="Recurrent Neural Network" /><br />
</li>
<li>Alternative representation, where square is one-step <img src="/img/recurrentNN/RNN_derowed.png" alt="Recurrent Neural Network (De-Rowed)" /><br />
</li>
<li>Parameters, <span class="math inline">\(W_{ax}\)</span>, <span class="math inline">\(W_{ay}\)</span> and <span class="math inline">\(W_{aa}\)</span>, for each time step are shared <img src="/img/recurrentNN/RNN_with_params.png" alt="RNN Parameters" /><br />
</li>
<li>Weakness: Only uses information that is earlier in the sequences, e.g. cannot determine “Teddy” is a name with only first word in the following sentences; need the later words: “Teddy Roosevelt was a great President” and “Teddy bears are on sale!”. Will look at bidrectional RNN (BRNN) later in course.</li>
</ul>
<p><img src="/img/recurrentNN/RNN.png" alt="Recurrent Neural Network" /><br />
Forward propagation (unidirectional RNN)</p>
<ul>
<li>Start with initial activations, <span class="math inline">\(a^{&lt;0&gt;}\)</span> of zeros</li>
<li>Activation at time step 1: generally use tanh (most common) or ReLU activation. Other ways of preventing vanishing gradient problem <span class="math display">\[a^{&lt;1&gt;} = g(W_{aa} a^{&lt;0&gt;} + W_{ax} x^{&lt;1&gt;} + b_a)\]</span></li>
<li>Prediction at time set 1;
<ul>
<li>binary classification: sigmoid activation function</li>
<li>k-way: softmax <span class="math display">\[\hat{y}^{&lt;i&gt;} =  g(W_{ya} a^{&lt;1&gt;} + b_y)\]</span></li>
</ul></li>
<li>More generally, at time <span class="math inline">\(t\)</span>: <span class="math display">\[\begin{aligned}
a^{&lt;t&gt;}         &amp;= g(W_{aa} a^{&lt;t-1&gt;} + W_{ax} x^{&lt;t&gt;} + b_a) \\
\hat{y}^{&lt;t&gt;}   &amp;=  g(W_{ya} a^{&lt;t&gt;} + b_y)
\end{aligned}\]</span></li>
</ul>
<p>Can simplify as: <span class="math display">\[\begin{aligned}
    a^{&lt;t&gt;}         &amp;= g(W_{a} [a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_a) \\
    \hat{y}^{&lt;t&gt;}   &amp;=  g(W_{ya} a^{&lt;t&gt;} + b_y)
\end{aligned}\]</span></p>
<p>where <span class="math display">\[\begin{aligned}
    W_a &amp;= [W_{aa} | W_{ax}] \\
    [a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] &amp;= \begin{bmatrix} a^{&lt;t-1&gt;} \\ x^{&lt;t&gt;} \end{bmatrix} 
\end{aligned}\]</span></p>
<p>Example: If <span class="math inline">\(a\)</span> 100-dimensional and <span class="math inline">\(x\)</span> 10,000-dimensional, then</p>
<ul>
<li><span class="math inline">\(W_{aa}\)</span> would be 100 <span class="math inline">\(\times\)</span> 100</li>
<li><span class="math inline">\(W_{ax}\)</span> would be 100 <span class="math inline">\(\times\)</span> 10,000</li>
<li><span class="math inline">\(W_a\)</span> would be 100 <span class="math inline">\(\times\)</span> 10,100</li>
<li><span class="math inline">\(a^{&lt;t-1&gt;}\)</span> would be 100 <span class="math inline">\(\times\)</span> 1</li>
<li><span class="math inline">\(x^{&lt;t&gt;}\)</span> would be $10,000 <span class="math inline">\(\times\)</span> 1</li>
</ul>
<p>Note:</p>
<ul>
<li><span class="math inline">\(b_a\)</span>: bias</li>
<li><span class="math inline">\(W_\text{xy}\)</span> Multiply by some <span class="math inline">\(y\)</span>-like quantity to compute some <span class="math inline">\(x\)</span>-like quantity</li>
</ul>
</div>
<div id="backpropagation-through-time" class="section level2">
<h2>Backpropagation through time</h2>
<ul>
<li>Programming framework would automatically take care of backpropagation</li>
<li>Given inputs <span class="math inline">\(x^{&lt;1&gt;}, x^{&lt;2&gt;}, \ldots, x^{&lt;T_x&gt;}\)</span>, initial activation <span class="math inline">\(a_0\)</span> and parameters <span class="math inline">\(W_a\)</span> and <span class="math inline">\(b_a\)</span>, calculate activations <span class="math inline">\(a^{&lt;1&gt;}, a^{&lt;2&gt;}, \ldots, a^{&lt;T_x&gt;}\)</span></li>
<li>Given parameters <span class="math inline">\(W_y\)</span>, <span class="math inline">\(b_y\)</span> and activations, can calculate predictions <span class="math inline">\(\hat{y}^{&lt;1&gt;}, \hat{y}^{&lt;2&gt;}, \ldots, \hat{y}^{&lt;T_x&gt;}\)</span></li>
<li>To perform back-propagation, require loss function, standard logistic regression loss function (aka cross-entropy loss) <span class="math display">\[\mathcal{L}(\hat{y}^{&lt;t&gt;}, y^{&lt;t&gt;}) = -y^{&lt;t&gt;} \log \hat{y}^{&lt;t&gt;}  - (1 - y^{&lt;t&gt;}) \log (1 - \hat{y}^{&lt;t&gt;})\]</span> <img src="/img/recurrentNN/BackpropThruTime.png" alt="Backpropagation Through Time" /></li>
</ul>
</div>
<div id="different-types-of-rnns" class="section level2">
<h2>Different types of RNNs</h2>
<p>Wider range of RNN architectures, e.g. when output <span class="math inline">\(T_x \neq T_y\)</span> input sequence length</p>
<ul>
<li>Many-to-many, <span class="math inline">\(T_x, T_y &gt; 1\)</span>.
<ul>
<li><span class="math inline">\(T_x = T_y\)</span>; as per motivating example above </br> <img src="/img/recurrentNN/architecture_many_to_many_same.png" alt="Many to many, same" /><br />
</li>
<li><span class="math inline">\(T_x \neq T_y\)</span>, e.g machine translation, “como estas” <span class="math inline">\(\rightarrow\)</span> “how are you”</br> <img src="/img/recurrentNN/architecture_many_to_many_diff.png" alt="Many to many, same" /><br />
</li>
</ul></li>
<li>Many-to-one, e.g. output 0 or 1 (or a rating from 1 to 5) to indicate sentiment from sentence</br> <img src="/img/recurrentNN/architecture_many_to_one.png" alt="Many to one" /><br />
</li>
<li>One-to-many, e.g. music generation where goal is to output set of notes from input such as genre</br> <img src="/img/recurrentNN/architecture_one_to_many.png" alt="One to Many" /><br />
</li>
<li>One-to-one (standard)</br> <img src="/img/recurrentNN/architecture_one_to_one.png" alt="One to One" /><br />
</li>
<li>attention-based (to discuss later)</li>
</ul>
</div>
<div id="language-model-and-sequence-generation" class="section level2">
<h2>Language model and sequence generation</h2>
<ul>
<li>A language model takes as input a sentence (a sequence, but use notation <span class="math inline">\(y\)</span> rather than <span class="math inline">\(x\)</span>) and determines the probability</li>
<li>To build a language model,
<ol style="list-style-type: decimal">
<li>Obtain training set: large corpus (body) of english text</li>
<li>Require vocabulary (dictionary)</li>
<li>Map each word to the dictionary tokens using one-hot vectors
<ul>
<li>Each word in sentence is represented as <span class="math inline">\(y^{&lt;1&gt;}, y^{&lt;2&gt;}, \ldots\)</span>$</li>
<li>May use <EOS> token for end of sentence (period)</li>
<li>Use <UNK> for words not in vocabulary</li>
</ul></li>
<li>What is the probability of each word in the sentence
<ul>
<li>First word: What is the probability of the first word being any word in the dictionary? With 10,000-word, each output is a 10,000-way soft max output (incl. EOS and UNK). As input, pass zero-vector</li>
<li>Second word: What is probability of second word being each word in dictionary, given first word in sentence</li>
<li><span class="math inline">\(\ldots\)</span></li>
<li>Each step looks at preceding words, RNN learns to predict one word a time going from left to right Note that 15 was considered a word <img src="/img/recurrentNN/example_model_cats_sleep.png" alt="Cats average 15 hours of sleep a day" /></li>
</ul></li>
</ol></li>
<li>Cost function: at time t with tru word <span class="math inline">\(y^{&lt;t&gt;}\)</span> and prediction <span class="math inline">\(\hat{y}^{&lt;t&gt;}\)</span> then loss function is <span class="math display">\[\begin{aligned}
\mathcal{L}(\hat{y}^{&lt;t&gt;}, y^{&lt;t&gt;}) &amp;= -\sum_i y^{&lt;t&gt;}_i \log \hat{y}^{&lt;t&gt;}_i \\
\mathcal{L} &amp;= \sum_t \mathcal{L}^{&lt;t&gt;}(\hat{y}^{&lt;t&gt;}, y^{&lt;t&gt;}) 
\end{aligned}\]</span></li>
</ul>
<p>Note conditional probabilities used to determine the probability of a three-word sentence: <span class="math inline">\(P(y^{&lt;1&gt;}, y^{&lt;2&gt;}, y^{&lt;3&gt;}) = P(y^{&lt;1&gt;})P(y^{&lt;2&gt;}|y^{&lt;1&gt;})P(y^{&lt;2&gt;})P(y^{&lt;1&gt;},y^{&lt;2&gt;})\)</span></p>
</div>
<div id="sampling-novel-sequences" class="section level2">
<h2>Sampling novel sequences</h2>
<p>To get a sense of what is learned</p>
<ul>
<li>Sample first word; randomly sample soft max distribution use<code>np.random.choice</code></li>
<li>Take <span class="math inline">\(x_2 = \hat{y}^{&lt;1&gt;}\)</span> to predict <span class="math inline">\(\hat{y}^{&lt;2&gt;}\)</span></li>
<li>Sample until EOS token (but if no EOS, choose sentence length beforehand)</li>
<li>You may get an UNK token; can reject and resample until not UNK, or leave in output</li>
</ul>
<p>Character-level vocabulary</p>
<ul>
<li>Have been using a word-level vocabulary; alternative is to use a character-level vocabulary</li>
<li>Example vocabulary: [a, b,c, <span class="math inline">\(\ldots\)</span>, z, space, ., ;, 0, <span class="math inline">\(\ldots\)</span> 9, A, Z]</li>
<li>Can use corpus to determine complete vocabulary</li>
<li>Advantage: Don’t have to worry about unknown tokens</li>
<li>Disadvantages:
<ul>
<li>Much longer sequences, 10-20 words = 100+ characters.<br />
</li>
<li>Harder to capture longer-range dependencies.<br />
</li>
<li>More computationally expensive</li>
</ul></li>
<li>As computers are getting more efficient, people are starting to use character-level vocabularies, but practice not widely spread</li>
</ul>
<p>Sampling novel sentences from a model trained using newspaper corpuses gives very different results to those trained using a shakespear corpus.</p>
</div>
<div id="vanishing-gradients-with-rnns" class="section level2">
<h2>Vanishing gradients with RNNs</h2>
<ul>
<li>RNNs tend to run in to vanishing gradient problems</li>
<li>Consider:
<ul>
<li>The <strong>cat</strong>, which already ate <span class="math inline">\(\ldots\)</span>, <strong>was</strong> full</li>
<li>The <strong>cats</strong>, which already ate <span class="math inline">\(\ldots\)</span>, <strong>were</strong> full</li>
</ul></li>
<li>Basic RNN not good at picking up very long-range dependency</li>
<li>Recall concept of vanishing gradients with very deep neural networks (e.g. 100 layers); gradient at last layer will have hard time impacting gradient at earlier level. Similar occurs with basic RNN.<br />
</li>
<li>Basic RNN has mainly local influences, i.e. preceding 1-3 words have much more influence than words preceding the current word by, e.g. 10 places</li>
<li>Exploding gradients
<ul>
<li>Not as common as Vanishing</li>
<li>But when exists, can be catestrophic</li>
<li>If you see “NaN” in gradient vectors, can apply gradient clipping; i.e. set gradient = max(X, gradient)</li>
</ul></li>
</ul>
<p>As vanishing gradient harder to deal with, will be discussed in next few sections.</p>
<p>See <a href="/blog/2018/12/07/2018-12-07-improving-deep-learning-networks#vanishing">Improving Deep Learning Networks</a></p>
</div>
<div id="gated-recurrent-unit-gru" class="section level2">
<h2>Gated Recurrent Unit (GRU)</h2>
<ul>
<li>Better than basic RNN at capturing long range connections and helps with vanishing gradient problems</li>
<li>Recall formula for activation at time <span class="math inline">\(t\)</span>, <span class="math display">\[a^{&lt;t&gt;}  = g(W_{aa} a^{&lt;t-1&gt;} + W_{ax} x^{&lt;t&gt;} + b_a)\]</span>
<ul>
<li>activation from last time step</li>
<li>Input token</li>
<li>g could be tahn activation function <img src="/img/recurrentNN/basic_rnn_activation.png" alt="Basic RNN activation function" /></li>
</ul></li>
<li>Motivating example
<ul>
<li>The <strong>cat</strong>, which already ate <span class="math inline">\(\ldots\)</span>, <strong>was</strong> full</li>
<li>The <strong>cats</strong>, which already ate <span class="math inline">\(\ldots\)</span>, <strong>were</strong> full</li>
</ul></li>
<li>Formulas:</li>
</ul>
<p><span class="math display">\[\begin{aligned}
    \text{Candidate Memory Cell:} &amp;   \tilde{c}^{&lt;t&gt;} &amp;= \tanh(w_c[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_c) \\
    \text{Gate:}                    &amp; \Gamma_u &amp;= \sigma(w_u[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_u\\
    \text{Memory Cell:} &amp; c^{&lt;t&gt;} &amp;= \Gamma_u \tilde{c}^{&lt;t&gt;} +  (1-\Gamma_u) c^{&lt;t-1&gt;}\\
\end{aligned}\]</span></p>
<ul>
<li>Use <span class="math inline">\(c\)</span> for memory _c_ell</li>
<li>The GRU will output an activation that is the same as the memory cell, but this notation is used as these will differ for other models, e.g., LSTM <span class="math display">\[c^{&lt;t&gt;} = a^{&lt;t&gt;}\]</span></li>
<li>At each time step, consider overriding memory cell with the candidate memory cell based on the value of the gate
<ul>
<li>As the gate is a sigmoid function it will usually be practically zero or practically one</li>
<li>If the gate function is 0, the memory cell <span class="math inline">\(c^{&lt;t&gt;}\)</span> remains the same as at <span class="math inline">\(t-1\)</span>, i.e., <span class="math inline">\(c^{&lt;t&gt;} = c^{&lt;t-1&gt;}\)</span>. If the gate function is 1, the memory cell is updated to the candidate value</li>
</ul></li>
<li>For example, if the cell <span class="math inline">\(c\)</span> captures concept of plural/singular, the gate would have a value of 1 at the <em>cat</em> token remain at zero until after a new noun captures after the word <em>was</em>.</li>
<li>The dimensions of the memory cell <span class="math inline">\(c^{&lt;t&gt;}\)</span>, candidate memory cell <span class="math inline">\(\tilde{c}^{&lt;t&gt;}\)</span> and gate <span class="math inline">\(\Gamma_u\)</span> are the same.
<ul>
<li>So if all are 100-dimensional vector; most are zeros or ones and tells which are the bits you want to update. (in practice, may be in-between zero)</li>
<li>The asterix is element-wise in the memory cell formula</li>
<li>Different elements (bits) may be used for different concepts; i.e. one for singular/plural, another for the concept of food, etc.</li>
</ul></li>
</ul>
<div class="figure">
<img src="/img/recurrentNN/simplified_gru.png" alt="Simplified GRU" />
<p class="caption">Simplified GRU</p>
</div>
<p>Full GRU</p>
<ul>
<li>Uses <span class="math inline">\(r\)</span> to measure the relevance of the current memory cell <span class="math inline">\(c^{&lt;t-1&gt;}\)</span> when calculating the candidate value <span class="math inline">\(\tilde{c}^{&lt;t&gt;}\)</span></li>
</ul>
<p><span class="math display">\[\begin{aligned}
    \text{Candidate Memory Cell:}\;\;\; &amp;   \tilde{c}^{&lt;t&gt;} &amp;&amp;= \tanh(\color{blue}{\Gamma_r} * W_c[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_c) \\
    \text{Gate:}\;\;\;                &amp; \Gamma_u &amp;&amp;= \sigma(W_u[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_u\\
    \color{blue}{\text{relevance:}}\;\;\;                &amp; \color{blue}{\Gamma_r} &amp;&amp;= \color{blue}{\sigma(W_r[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_r}\\
    \text{Memory Cell:}\;\;\; &amp; c^{&lt;t&gt;} &amp;&amp;= \Gamma_u * \tilde{c}^{&lt;t&gt;} +  (1-\Gamma_u) c^{&lt;t-1&gt;}\\
\end{aligned}\]</span></p>
<ul>
<li>Academic literature may use different notation, here used notation to be similar for GRU and LSTM
<ul>
<li>candidate memory cell <span class="math inline">\(\tilde{h}\)</span></li>
<li>Gate <span class="math inline">\(u\)</span></li>
<li>Relevance <span class="math inline">\(r\)</span></li>
<li>Memory cell <span class="math inline">\(h\)</span></li>
</ul></li>
</ul>
<p>LSTM</p>
<ul>
<li>another instantiations of the same idea</li>
</ul>
</div>
<div id="long-short-term-memory-lstm" class="section level2">
<h2>Long Short Term Memory (LSTM)</h2>
<ul>
<li>More powerful than GRU</li>
<li>Recall GRU equations</li>
</ul>
<p><span class="math display">\[\begin{aligned}
    \text{Candidate Memory Cell:}\;\;\; &amp;   \tilde{c}^{&lt;t&gt;} &amp;&amp;= \tanh(\Gamma_r * W_c[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_c) \\
    \text{Update Gate:}\;\;\;                &amp; \Gamma_u &amp;&amp;= \sigma(W_u[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_u\\
    \text{Relevance Gate:}\;\;\;                &amp; \Gamma_r &amp;&amp;= \sigma(W_r[c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_r\\
    \text{Memory Cell:}\;\;\; &amp; c^{&lt;t&gt;} &amp;&amp;= \Gamma_u * \tilde{c}^{&lt;t&gt;} +  (1-\Gamma_u) c^{&lt;t-1&gt;}\\
    \text{Activation Value:}\;\;\; &amp; a^{&lt;t&gt;} &amp;&amp;= c^{&lt;t&gt;}
\end{aligned}\]</span></p>
<p>LSTM</p>
<ul>
<li>As, unlike for GRU, <span class="math inline">\(a^{&lt;t&gt;} \neq c^{&lt;t&gt;}\)</span>, and so memory cell explicitly references the activation value, rather than the memory cell</li>
<li>Introduces a forget gate, which gives option of keeping old memory cell and adding to it.</li>
<li>Introduces output gate</li>
<li>So has three gates instead of two</li>
</ul>
<p><span class="math display">\[\begin{aligned}
    \text{Candidate Memory Cell:}\;\;\; &amp;   \tilde{c}^{&lt;t&gt;} &amp;&amp;= \tanh(W_c[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_c) \\
    \text{Update Gate:}\;\;\;                &amp; \Gamma_u &amp;&amp;= \sigma(W_u[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_u\\
    \text{Forget Gate:}\;\;\;                &amp; \Gamma_f &amp;&amp;= \sigma(W_f[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_f\\
    \text{Output Gate:}\;\;\;                &amp; \Gamma_o &amp;&amp;= \sigma(W_o[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_o\\
    \text{Memory Cell:}\;\;\; &amp; c^{&lt;t&gt;} &amp;&amp;= \Gamma_u * \tilde{c}^{&lt;t&gt;} +  \Gamma_f c^{&lt;t-1&gt;}\\
    \text{Activation Value:}\;\;\; &amp; a^{&lt;t&gt;} &amp;&amp;= \Gamma_o * \tanh c^{&lt;t&gt;}
\end{aligned}\]</span></p>
<div class="figure">
<img src="/img/recurrentNN/LSTM.png" alt="LSTM" />
<p class="caption">LSTM</p>
</div>
<ul>
<li><span class="math inline">\(a^{&lt;t-1&gt;}\)</span> and <span class="math inline">\(x^{&lt;t&gt;}\)</span> used to calculate all the gates (forget, update, output) and also go through the tanh to calculate <span class="math inline">\(\tilde{c}^{&lt;t&gt;}\)</span></li>
</ul>
<div class="figure">
<img src="/img/recurrentNN/LSTM_network.png" alt="LSTM Network" />
<p class="caption">LSTM Network</p>
</div>
<ul>
<li>So long as you set forget and update gates appropriately, it is relatively easy for LSTM to have some value <span class="math inline">\(c^{&lt;0&gt;}\)</span> and have it passed all the way to the right so that <span class="math inline">\(c^{&lt;3&gt;} = c^{&lt;0&gt;}\)</span></li>
</ul>
<p>Variations</p>
<ul>
<li>Have gate values also dependent on <span class="math inline">\(c^{&lt;t-1&gt;}\)</span>, known as a <em>peephole connection</em>, e.g., for output gate: <span class="math display">\[\Gamma_o = \sigma(W_o[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}, c^{&lt;t-1&gt;}] + b_o\]</span></li>
</ul>
<p>When to use GRU vs LSTM?</p>
<ul>
<li>LSTM became earlier than GRU (simplified version)</li>
<li>Different problems, different algorithm will win</li>
<li>Advantage of GRU is simpler and so easier to build bigger network (computationally faster and scales to building bigger models)</li>
<li>LSTM more flexible/powerful (historically more proven choice, although GRU usage is gaining momentum)</li>
</ul>
</div>
<div id="bidirectional-rnn-brnn" class="section level2">
<h2>Bidirectional RNN (BRNN)</h2>
<ul>
<li>To take information earlier and later in sequence</li>
<li>Inputs <span class="math inline">\(x^{&lt;1&gt;}, \ldots, x^{&lt;4&gt;}\)</span></li>
<li>Forward recurrent components <span class="math inline">\(\overrightarrow{a}^{&lt;1&gt;}, \overrightarrow{a}^{&lt;2&gt;}, \overrightarrow{a}^{&lt;3&gt;}, \overrightarrow{a}^{&lt;4&gt;}\)</span></li>
<li>Backward recurrent components <span class="math inline">\(\overleftarrow{a}^{&lt;4&gt;}, \overleftarrow{a}^{&lt;3&gt;}, \overleftarrow{a}^{&lt;2&gt;}, \overleftarrow{a}^{&lt;1&gt;}\)</span>. This is still forward prop.</li>
<li>Predictions <span class="math inline">\(\hat{y}^{&lt;1&gt;}, \ldots, \hat{y}^{&lt;4&gt;}\)</span> where <span class="math display">\[\hat{y}^{&lt;t&gt;} = g(W_y[\overrightarrow{a}^{&lt;t&gt;}, \overleftarrow{a}^{&lt;t&gt;}] + b_y)\]</span></li>
<li>Acyclic graph</li>
<li>BRNN can be with LSTM blocks both forward and backwards. Similarly with GRU.</li>
<li>Disadvantage: Need entire sequence of data before can make a prediction. Need to wait for person to stop talking, i.e. entire utterance. Where can get entire sentence at a time this standard BRNN is suitable.</li>
</ul>
</div>
<div id="deep-rnns" class="section level2">
<h2>Deep RNNs</h2>
<div class="figure">
<img src="/img/recurrentNN/deep_rnn.png" alt="Deep RNN" />
<p class="caption">Deep RNN</p>
</div>
<ul>
<li>May need to stack multiple layers of RNNs to build deeper RNNs</li>
<li>Use <span class="math inline">\(a^{[\text{layer}]&lt;\text{time}&gt;}\)</span></li>
<li>$a^{[2]&lt;3&gt;} = g(W_a^{[2]}[a^{[2]&lt;2&gt;}, a^{[1]&lt;3&gt;}] + b_a^{2}) $</li>
<li>First layer has parameters <span class="math inline">\(W_a^{[1]}, b_a^{1}\)</span>; second layer has parameters <span class="math inline">\(W_a^{[2]}, b_a^{2}\)</span>, and so on</li>
<li>Generally only have 3 recurrent layers (that is already pretty deep)</li>
<li>The recurrent blocks amy be RNN, GRU, LSTM</li>
<li>Sometimes have 3 recurrent layers, with additional non-connected layers on top</li>
</ul>
<div class="figure">
<img src="/img/recurrentNN/deep_RNN_On_Steroids.png" alt="Deep RNN" />
<p class="caption">Deep RNN</p>
</div>
</div>
</div>
<div id="week2" class="section level1">
<h1>Introduction to Word Embeddings</h1>
<div id="word-representation" class="section level2">
<h2>Word Representation</h2>
<div id="hot-representation" class="section level3">
<h3>1-Hot Representation</h3>
<p>1-hot representation: all zeros, with 1 for word. Use <span class="math inline">\(O_{i}\)</span> to represent 1-hot vector of zeros with a 1 at position <span class="math inline">\(i\)</span></p>
<p>Weakness:</p>
<ul>
<li>Each word treated as a thing (and doesn’t allow algorithm to generalise across words)</li>
<li>E.g. algorithm could learn that I want a glass of orange <strong>juice</strong> as a likely sentence, but it won’t be able to generalise that apple juice is a popular phrase</li>
<li>Inner product of King-Queen is zero and of Apple-Orange is also zero. I.e. doesn’t know that orange-apple are more similar than orange-king</li>
</ul>
</div>
<div id="word-embedding" class="section level3">
<h3>Word embedding</h3>
<p>Featurised representation; learn set of features for each word</p>
<table>
<thead>
<tr class="header">
<th>Feature</th>
<th>Man</th>
<th>Woman</th>
<th>King</th>
<th>Queen</th>
<th>Apple</th>
<th>Orange</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1-Hot Position</td>
<td>(5391)</td>
<td>(9853)</td>
<td>(4914)</td>
<td>(7157)</td>
<td>(456)</td>
<td>(657)</td>
</tr>
<tr class="even">
<td>Gender</td>
<td>-1</td>
<td>1</td>
<td>-0.95</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>Royal</td>
<td>0.01</td>
<td>0.02</td>
<td>0.93</td>
<td>0.95</td>
<td>-0.01</td>
<td>0.01</td>
</tr>
<tr class="even">
<td>Age</td>
<td>0.03</td>
<td>0.02</td>
<td>0.7</td>
<td>0.69</td>
<td>0.03</td>
<td>-0.02</td>
</tr>
<tr class="odd">
<td>Food</td>
<td>0.09</td>
<td>0.01</td>
<td>0.02</td>
<td>0.01</td>
<td>9.95</td>
<td>0.97</td>
</tr>
</tbody>
</table>
<p>Other features may include size, cost, colour…</p>
<p>Consider that there are 300 features, then use <span class="math inline">\(e_{5391}\)</span> to represent the 300-dimensional embedding vector for man.</p>
<p>You will notice that the embedding vectors for apple and orange to be similar, this will allow a algorithm that has learnt that orange juice to then know that apple juice is also a thing.</p>
<p>The components within the embedding vectors will not be easy to determine that they correspond to for example, gender, royal, etc. but the embedding vectors do help the algorithm.</p>
<div id="visualising-word-embeddings" class="section level4">
<h4>Visualising word embeddings</h4>
<p>t-SNE algorithm can be used to convert the 300 dimensional embedding for each word to a 2D space so that can visualise and show that word embedding algorithms can learn similar features for related concepts/words. The 300dimensional vectors are known as embeddings.</p>
<p>E.g. Word orange gets embedded into a point in the 300D space and apple gets embedded into another point in the 300-d space.</p>
</div>
</div>
</div>
<div id="using-word-embeddings" class="section level2">
<h2>Using word embeddings</h2>
<p>How to plug embeddings into NLP applications. Example: Named entity recognition example to determine which words are person’s names</p>
<p>Given the following sentences:</p>
<ul>
<li>Sally Johnson is an orange farmer</li>
<li>Robert Lin is an apple farmer</li>
<li>Harry James is a durian cultivator</li>
</ul>
<p>With word embeddings knowing that orange, apple and durian are fruit, and that farmer is like cultivator, if the NLP algorithm (named entity recognition) has learned that Sally Lin is a person, it can then deduce that Robert Lin and Harry James are also people.</p>
<p>Algorithms to learn word embeddings can examine very large text corpus’s (e.g. up to 100 billion words off Internet) so can know that durian is a fruit, cultivate is like a farmer (even though durian and cultivator might not be in NLP training set). I.e. named entity recognition task might have a much smaller training set. This is an example of transfer learning where you take information learned from large unlabelled data sets and transfer it to a named entity recognition task.</p>
<div id="transfer-learning-and-word-embeddings" class="section level3">
<h3>Transfer learning and word embeddings</h3>
<ol style="list-style-type: decimal">
<li>Learn word embeddings from large text corpus (1-100B words), or download pre-trained embedding online</li>
<li>Transfer embedding to a new task with smaller training set (say 100K). Rather than using a 10,000 dimensional 1-hot vector can now use a lower-dimension, e.g. 300-dimensional dense vector</li>
<li>Optional: Continue to fine-tune the word embeddings with new data (in practice would only do with a large data set for the NLP problem, i.e. task 3)</li>
</ol>
<p>Similar to transfer learning, most useful when have a tonne of data for task 1 and less for task 2.</p>
<p>Embedding has been used for the following NLP tasks:</p>
<ul>
<li>Named entity recognition</li>
<li>Text summarisation</li>
<li>Co-reference resolution</li>
<li>Parsing</li>
</ul>
<p>Embedding less useful for:</p>
<ul>
<li>Language modelling machine translation</li>
</ul>
</div>
<div id="relation-to-face-encoding" class="section level3">
<h3>Relation to face encoding</h3>
<p>For face-recognition; the term encoding is similar to embedding (word).</p>
<p>Difference:</p>
<ul>
<li>For face recognition, input = any face picture (unlimited sea of pictures)</li>
<li>Learning word embedding, input= fixed vocabulary (limited sea of words) and learn <span class="math inline">\(e_1, \ldots, e_{10,000}\)</span> learn embedding.</li>
</ul>
</div>
</div>
<div id="properties-of-word-embeddings" class="section level2">
<h2>Properties of word embeddings</h2>
<p>Can help with analogy reasoning, Man <span class="math inline">\(\rightarrow\)</span> Woman as King <span class="math inline">\(\rightarrow\)</span> Queen.</p>
<p>In above example, <span class="math inline">\(e_\textrm{man} - e_\text{woman} \approx [-2 0 0 0]^T\)</span>. Similarly, <span class="math inline">\(e_\textrm{king} - e_\text{queen} \approx [-2 0 0 0]^T\)</span>, i.e. main difference is gender.</p>
<div id="analogies-using-word-vectors" class="section level3">
<h3>Analogies using word vectors</h3>
<p>Man is to woman as king is to what: <span class="math inline">\(e_\textrm{man} - e_\text{woman} \approx e_\textrm{king} - ?\)</span></p>
<ul>
<li>Find word <span class="math inline">\(w\)</span> such that <span class="math inline">\(\arg \max\limits_{w} \text{sim}(e_w, e_\text{king} - e_\textrm{man} - e_\text{woman})\)</span></li>
</ul>
<p>Currently, get 30-75% accuracy on analogy where an analogy attempt is correct only if it guesses the exact right word.</p>
<p>Clarification of t-SNE.</p>
<ul>
<li>Mapping is very complex and non-linear</li>
<li>Cannot count on visualisation to show the parallels between words (this exists in the larger, e.g. 300-D, dimensional space)</li>
</ul>
<p>Similarity function:</p>
<ul>
<li>Most commonly used is cosine similarity <span class="math display">\[\text{sim}(u,v) = \frac{u^Tv}{||u||_2 ||v||_2}\]</span></li>
</ul>
<p>Ignoring the denominator, this is basically the inner product between <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>. if very similar, the inner product will be large.</p>
<p>This is the cosine of the angle between the two vectors.</p>
<p>For angle <span class="math inline">\(\phi\)</span>, then formula is cosine of angle.</p>
<ul>
<li>If <span class="math inline">\(\phi\)</span> = 0, then <span class="math inline">\(\cos \phi\)</span> = 1</li>
<li>If <span class="math inline">\(\phi\)</span> = 90, then <span class="math inline">\(\cos \phi\)</span> = 0</li>
<li><p>if <span class="math inline">\(\phi\)</span> = 180, then <span class="math inline">\(\cos \phi\)</span> = -1</p></li>
<li><p>Can also use Euclidean distance which is a technically a measure of dissimilarity, so would need to take negative <span class="math display">\[||u - v||^2\]</span></p></li>
</ul>
<p>Word embeddings can learn:</p>
<ul>
<li>Man:Woman as Boy:Girl</li>
<li>Ottawa:Canada as Nairobi:Kenya</li>
<li>Big:Bigger as Tall:Taller</li>
<li>Yen:Japan as Ruble:Russia</li>
</ul>
</div>
</div>
<div id="embedding-matrix" class="section level2">
<h2>Embedding matrix</h2>
<p>Algorithm to learn embedding results in embedding matrix.</p>
<p>e.g. with 10,000 words and unknown token and 300 features, get 300 <span class="math inline">\(\times\)</span> 10,001 embedding matrix.</p>
<p>Notation:</p>
<ul>
<li><span class="math inline">\(O_j\)</span> to denote the 1-hot vector with 0s with a 1 in position <span class="math inline">\(j\)</span>. This will be a 10,001 vector.<br />
</li>
<li><span class="math inline">\(E\)</span> for embedding matrix, e.g. dimension 300, 10001</li>
<li><span class="math inline">\(e_j\)</span> = <span class="math inline">\(E \centerdot O_j\)</span> will be of dimension (300,10001)(100001, 1) = (300,1) and so is grabbing the embedding vector for the word in position <span class="math inline">\(j\)</span>. This is the embedding vector for word <span class="math inline">\(j\)</span>.</li>
<li>In practice, do not implement as matrix multiplication (inefficient), but rather use a specialised function to look up an embedding.<br />
</li>
<li>In Keras, use Embedding layer</li>
</ul>
</div>
<div id="learning-word-embeddings" class="section level2">
<h2>Learning word embeddings</h2>
<p>Example: Building language model using a neural network, e.g. to predict the word in sentence <em>I want a glass of orange ….</em></p>
<table>
<thead>
<tr class="header">
<th>Word:</th>
<th>I</th>
<th>want</th>
<th>a</th>
<th>glass</th>
<th>of</th>
<th>orange</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Dictionary Position:</td>
<td>4343</td>
<td>9665</td>
<td>1</td>
<td>163</td>
<td>6257</td>
</tr>
<tr class="even">
<td>One hot vector:</td>
<td><span class="math inline">\(o_{4343}\)</span></td>
<td><span class="math inline">\(o_{9665}\)</span></td>
<td><span class="math inline">\(o_1\)</span></td>
<td><span class="math inline">\(o_{163}\)</span></td>
<td><span class="math inline">\(o_{6257}\)</span></td>
</tr>
<tr class="odd">
<td>Embedding vector:</td>
<td><span class="math inline">\(e_{4343}\)</span></td>
<td><span class="math inline">\(e_{9665}\)</span></td>
<td><span class="math inline">\(e_1\)</span></td>
<td><span class="math inline">\(e_{163}\)</span></td>
<td><span class="math inline">\(e_{6257}\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>Feed embedding vector into NN which will feed to a softmax (with its own parameters) of 10,000.</li>
<li>May only wish to use the last 4 words, which would mean would input a 1200 dimensional layer (as opposed to 1800).</li>
<li>Use backprop to perform gradient descent to maximize the likelihood of your training set to repeatedly predict the next word given four words in a sequence</li>
</ul>
<p>{EmbeddingMatrix.png}</p>
<p>Note: Same matrix <span class="math inline">\(E\)</span> used regardless of position</p>
<p>Contexts:</p>
<ul>
<li>Last 4 words (to predict next word)</li>
<li>4 words on left &amp; right (to predict word in middle)</li>
<li>Last 1 word, predict next word (would feed in embedding of previous word)</li>
<li>Nearby 1 word; what is a word that will be near glass (skip gram model)</li>
</ul>
<p>Posing these contexts allow you to learn good word embeddings</p>
</div>
<div id="word2vec" class="section level2">
<h2>Word2Vec</h2>
<p>Simple method to learn word embeddings</p>
<div id="skip-grams-model" class="section level3">
<h3>Skip-grams model</h3>
<ul>
<li>Randomly pick a word to be the context word</li>
<li>Randomly pick a word within some window of the context word which will be the target word, e.g.</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Context</th>
<th>Target</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>orange</td>
<td>juice</td>
</tr>
<tr class="even">
<td>orange</td>
<td>glass</td>
</tr>
<tr class="odd">
<td>orange</td>
<td>my</td>
</tr>
</tbody>
</table>
<p>Goal of setting up this supervised learning problem is not to predict words within an <span class="math inline">\(x\)</span>-word window but to learn the embeddings. (so even if do not so well with the supervised learning problem, the output will be useful as an embedding!)</p>
<p>Model:</p>
<ul>
<li>Vocab size = 10,000</li>
<li>Goal: Learn mapping from context word <span class="math inline">\(c\)</span> to some target <span class="math inline">\(t\)</span>, <span class="math inline">\(o_c \rightarrow E \rightarrow e_c \rightarrow \text{Softmax} \rightarrow \hat{y}\)</span></li>
<li>Context</li>
<li>Softmax: <span class="math display">\[p(t|c) = \frac{e^{\theta_t^Te_c}}{\sum^{10,000}_{j=1}e^{\theta_j^T e_c} }\]</span></li>
</ul>
<p>where <span class="math inline">\(\theta_t\)</span> = parameter associated with output <span class="math inline">\(t\)</span>.</p>
<p>Noting that loss function is <span class="math display">\[\mathcal{L}(\hat{y}, y) = -\sum^{10,000}_{i=1} y_i\log\hat{y}_i\]</span></p>
<p>Problem = Computational speed: everytime you want to evaluate the probability, need to sum over all 10,000 words in vocabulary.</p>
<p>Solutions to speed up softmax classification:</p>
<ul>
<li>Hierarchical softmax classifier: 1 classifier which says whether word is within 5,000 words, and then split, until you get to where it is. scales to log vocab size, rather than linear. Doesn’t generally use equal words in each branch , typically most common words are up the top so that need only a few traversals for those. Many algorithms exist to construct the tree.</li>
</ul>
<p>How to sample the context <span class="math inline">\(c\)</span>?</p>
<ul>
<li>Sample randomly from training corpus, but there are some words such as the, of, a, and, to, <span class="math inline">\(\ldots\)</span> that occur frequently. Then embedding will be updated many for those.</li>
<li>Heuristics exist to balance out sampling from common and less common words</li>
</ul>
<p>Two versions of Word2Vec algorithm within original paper: Skip-Gram and CBow</p>
</div>
</div>
<div id="negative-sampling" class="section level2">
<h2>Negative Sampling</h2>
<ul>
<li>Modified learning problem that allows you to do something similar to skip-gram with much more efficient learning algorithm (cf Softmax.</li>
<li>Example sentence: <em>I want a glass of orange juice to go along with my cereal</em></li>
<li>Sample context word, look around a window of $$10 words</li>
<li>Choose random words from dictionary to get negative targets to get, e.g..</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Source</th>
<th>Context (c)</th>
<th>Word (t)</th>
<th>Target (y)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\pm 10\)</span> word</td>
<td>orange</td>
<td>juice</td>
<td>1</td>
</tr>
<tr class="even">
<td>dictionary</td>
<td>orange</td>
<td>king</td>
<td>0</td>
</tr>
<tr class="odd">
<td>dictionary</td>
<td>orange</td>
<td>book</td>
<td>0</td>
</tr>
<tr class="even">
<td>dictionary</td>
<td>orange</td>
<td>the</td>
<td>0</td>
</tr>
<tr class="odd">
<td>dictionary</td>
<td>orange</td>
<td>of</td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(k\)</span></td>
<td><span class="math inline">\(k\)</span></td>
<td><span class="math inline">\(k\)</span></td>
<td><span class="math inline">\(k\)</span></td>
</tr>
</tbody>
</table>
<p>Choose <span class="math inline">\(k\)</span> = 2-5 for larger data set and <span class="math inline">\(k\)</span> = 5-20 for smaller datasets. In above example <span class="math inline">\(k\)</span> = 4 (i.e. number of words taken from dictionary)</p>
<p>Recall, softmax: <span class="math display">\[p(t|c) = \frac{e^{\theta_t^Te_c}}{\sum^{10,000}_{j=1}e^{\theta_j^T e_c} }\]</span></p>
<p>Model as logistic model: <span class="math display">\[P(y = 1|c,t) = \sigma (\theta_t^T, e_c)\]</span></p>
<p>Training data:</p>
<ul>
<li><span class="math inline">\(k\)</span>:1 negative:positive examples</li>
<li>If input word is orange (position 6257 in vocab), input one-hot vector pass through E and do multiplication to get embedding vector, i.e. <span class="math display">\[o_{6257} \rightarrow E \rightarrow e_{6257}\]</span> So have 10,000 possible logistic regression classification regression classifiers but only train <span class="math inline">\(k + 1\)</span> So instead of having one giant 10,000 way Softmax which is very expensive to compute, have turned it into 10,000 binary classification problems. On every iteration we’re only going to train <span class="math inline">\(k + 1\)</span> of them.<br />
</li>
<li>Called negative sampling, because you have a positive example and generate negative examples for which to train <span class="math inline">\(k\)</span> additional binary classifiers.</li>
</ul>
<p>Sampling the negative words:</p>
<ul>
<li>Sample according to the empirical frequency of words in your corpus, <span class="math inline">\(p(w_i)\)</span> but again get high representation of words like the, of, and, etc.</li>
<li>Other extreme is to use <span class="math inline">\(p(w_i) = 1/vocab size\)</span>, sample the negative examples uniformly at random</li>
<li>Middle ground: Take heuristic value of sampling between empirical and uniform, <span class="math inline">\(p(w_i) = \frac{f(w_i)^{3/4}}{\sum^{10000}_{j=1}f(w_j)^{3/4}}\)</span>. Not theoretically justified but seems to work well</li>
</ul>
<p>There are pre-trained word vectors and open source code.</p>
</div>
<div id="glove-word-vectors" class="section level2">
<h2>GloVe word vectors</h2>
<ul>
<li>Previously sampled pairs of words that appeared in close proximity to each other within the corpus</li>
<li><span class="math inline">\(c,t\)</span>: context, target</li>
<li><span class="math inline">\(x_{ij}\)</span>: number of times <span class="math inline">\(i\)</span> (the target, <span class="math inline">\(t\)</span>) appears in context of <span class="math inline">\(j\)</span> (the context, <span class="math inline">\(c\)</span>), i.e. how often two words appear close to each other</li>
<li>If within <span class="math inline">\(\pm 10\)</span>, then <span class="math inline">\(X_{ij} = X_{ji}\)</span></li>
</ul>
<p>Model:</p>
<ul>
<li><p>minimise <span class="math display">\[\sum^{10,000}_{i=1}\sum^{10,000}_{j=1} f(X_{ij}) (\theta^T_i e_j + b_i + b_j^\prime -\log X_{ij})^2\]</span></p></li>
<li>Solve <span class="math inline">\(\theta\)</span> and <span class="math inline">\(e\)</span> using gradient descent</li>
<li>Don’t want to sum over terms where <span class="math inline">\(X_{ij} = 0\)</span>, so use weighting term <span class="math inline">\(f(X_{ij}) = 0\)</span> if <span class="math inline">\(x_ij = 0\)</span>, assume <span class="math inline">\(0 \log 0 = 0\)</span>. It is also used to weight less common and more frequent words appropriately.<br />
</li>
<li>Note that <span class="math inline">\(\theta_i\)</span> and <span class="math inline">\(e_j\)</span> are symmetric, so can</li>
<li>Can initialise <span class="math inline">\(\theta\)</span> and <span class="math inline">\(e\)</span></li>
<li><p>For given word <span class="math inline">\(w\)</span> let <span class="math inline">\(e_w^\text{final} = \frac{e_w + \theta_w}{2}\)</span></p></li>
</ul>
<div id="properties" class="section level3">
<h3>Properties</h3>
<ul>
<li>Started with featurization view of word embeddings, but cannot guarantee that individual components of embedding are interpretable</li>
<li>Note that <span class="math inline">\((\theta^T_i e_j\)</span> can be replaced by <span class="math inline">\((A \theta_i)^T(A^T e_j)\)</span> where A is invertible. i.e. first component might be combination of gender, royal, age, etc. due to this linear transformation</li>
</ul>
</div>
</div>
<div id="sentiment-classification" class="section level2">
<h2>Sentiment Classification</h2>
</div>
<div id="debiasing-word-embeddings" class="section level2">
<h2>Debiasing word embeddings</h2>
</div>
</div>
<div id="week3" class="section level1">
<h1>Sequence Models &amp; Attention Mechanism</h1>
<div id="basic-models" class="section level2">
<h2>Basic Models</h2>
</div>
<div id="picking-the-most-likely-sentence" class="section level2">
<h2>Picking the most likely sentence</h2>
</div>
<div id="beam-search" class="section level2">
<h2>Beam Search</h2>
</div>
<div id="refinements-to-beam-search" class="section level2">
<h2>Refinements to Beam Search</h2>
</div>
<div id="error-analysis-in-beam-search" class="section level2">
<h2>Error analysis in beam search</h2>
</div>
<div id="bleu-score-optional" class="section level2">
<h2>Bleu Score (optional)</h2>
</div>
<div id="attention-model-intuition" class="section level2">
<h2>Attention Model Intuition</h2>
</div>
<div id="attention-model" class="section level2">
<h2>Attention Model</h2>
</div>
<div id="speech-recognition" class="section level2">
<h2>Speech recognition</h2>
</div>
<div id="trigger-word-detection" class="section level2">
<h2>Trigger Word Detection</h2>
</div>
<div id="conclusion-and-thank-you" class="section level2">
<h2>Conclusion and thank you</h2>
</div>
</div>
