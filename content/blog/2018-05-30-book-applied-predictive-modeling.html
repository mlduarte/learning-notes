---
title: 'Book: Applied Predictive Modeling'
author: Marie
date: '2018-05-30'
slug: book-applied-predictive-modeling
draft: TRUE
categories:
  - Study-Notes
tags:
  - R
  - Study-Notes
  - Book
---



<div id="overview" class="section level1">
<h1>Overview</h1>
<p>This post includes my notes and reproduction of examples in the book <a href="http://appliedpredictivemodeling.com">Applied Predictive Modeling</a> (2013), by Max Kuhn and Kjell Johnson.</p>
</div>
<div id="notes" class="section level1">
<h1>Notes</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Common reasons for predictive model failure:</p>
<ul>
<li>Inadequate pre-processing of data</li>
<li>Inadequate model validation</li>
<li>Unjustified extrapolation</li>
<li>Overfitting</li>
<li>Insufficient number of models explorer</li>
</ul>
<p>When prediction accuracy is the primary goal, should not choose a second-rate model for interpretability. To improve accuracy, generally need a more complex model which is more difficult to interpret.</p>
<p>Foundation for effective predictive model:</p>
<ul>
<li>intuition and deep knowledge (to obtain relevant data, eliminate noise)</li>
<li>Relevant data</li>
<li>Versatile computation toolbox (pre-processing, modelling, visulations) The combined force of predictive modelling and intuition will be better than the parts.</li>
</ul>
</div>
<div id="predictive-modeling-process" class="section level2">
<h2>Predictive Modeling Process</h2>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li>Understand data and modelling objectives; critical for a reliable and trustworthy model for predicting new samples; necessary before moving to the next steps.</li>
<li>Preprocess and split the data</li>
<li><p>Build &amp; evaluate model</p>
<ul>
<li>Split dataset into training set and validation set. Using MPG for 2010-2011 model year cars, if goal is to predict MPG for a new car line, then model could be created using all 2010 model cars and tested on the new 2011 cars. This, in contrast, to taking a random sample fo the data for model building. Use training set to try a number of techniques; only use validation set for a few strong candidate modesl. Repeatedly using test set negates its utility as final arbitrator</li>
<li>Alternatively can use resampling (cross-validation) to evaludate model</li>
</ul></li>
<li><p>Select model</p></li>
</ol>
<p>Themes</p>
<ul>
<li>Data splitting. How the model will be applied (e.g. whether it will be extrapolated to a new population) should determine how the training and test sets are determined. The amount of data available will also influence data splitting decisions. With a small dataset (and therefore test), resampling advised.</li>
<li>Predictors: feature selection</li>
<li>Estimating performance: Statistics (e.g. RMSE) and visualisations. Both are important.</li>
<li>Evaluating several models: There is no single model that will always do better than another.</li>
<li>Model selection: Involves choosing between models and selecting tuning parametesr (within model)</li>
</ul>
</div>
</div>
