---
title: 'Book: Applied Predictive Modeling (Part I)'
author: "Marie"
date: '2018-05-30T13:39:46+02:00'
categories: Study-Notes
slug: book-applied-predictive-modeling
tags:
- R
- Study-Notes
- Book
banner: img/banners/kuhn.png
---



<p><a href="#preprocessing">Preprocessing</a> <a href="#tuning">Over-Fitting and Model Tuning</a></p>
<div id="overview" class="section level1">
<h1>Overview</h1>
<p>This post includes my notes and reproduction of examples of <strong>Part I: General Strategies</strong> of the book <a href="http://appliedpredictivemodeling.com">Applied Predictive Modeling</a> (2013), by Max Kuhn and Kjell Johnson</p>
</div>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Common reasons for predictive model failure:</p>
<ul>
<li>Inadequate pre-processing of data</li>
<li>Inadequate model validation</li>
<li>Unjustified extrapolation</li>
<li>Overfitting</li>
<li>Insufficient number of models explorer</li>
</ul>
<p>When prediction accuracy is the primary goal, should not choose a second-rate model for interpretability. To improve accuracy, generally need a more complex model which is more difficult to interpret.</p>
<p>Foundation for effective predictive model:</p>
<ul>
<li>Intuition and deep knowledge (to obtain relevant data, eliminate noise)</li>
<li>Relevant data</li>
<li>Versatile computation toolbox (pre-processing, modelling, visualizations) The combined force of predictive modelling and intuition will be better than the parts.</li>
</ul>
<div id="predictive-modeling-process" class="section level2">
<h2>Predictive Modeling Process</h2>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li>Understand data and modelling objectives; critical for a reliable and trustworthy model for predicting new samples; necessary before moving to the next steps.</li>
<li>Preprocess and split the data</li>
<li><p>Build &amp; evaluate model</p>
<ul>
<li>Split dataset into training set and validation set. Using MPG for 2010-2011 model year cars, if goal is to predict MPG for a new car line, then model could be created using all 2010 model cars and tested on the new 2011 cars. This, in contrast, to taking a random sample of the data for model building. Use training set to try a number of techniques; only use validation set for a few strong candidate models. Repeatedly using test set negates its utility as final arbitrator</li>
<li>Alternatively can use resampling (cross-validation) to evaluate model</li>
</ul></li>
<li><p>Select model</p></li>
</ol>
<p>Themes</p>
<ul>
<li>Data splitting. How the model will be applied (e.g. whether it will be extrapolated to a new population) should determine how the training and test sets are determined. The amount of data available will also influence data splitting decisions. With a small dataset (and therefore test), resampling advised.</li>
<li>Predictors: feature selection</li>
<li>Estimating performance: Statistics (e.g. RMSE) and visualisations. Both are important.</li>
<li>Evaluating several models: There is no single model that will always do better than another.</li>
<li>Model selection: Involves choosing between models and selecting tuning parameters (within model)</li>
</ul>
</div>
</div>
<div id="data-pre-processing" class="section level1">
<h1>Data Pre-Processing <a id = preprocessing ></a></h1>
<ul>
<li>Addition, deletion or transformation of <strong>training set</strong> data</li>
<li>Can make or break model’s predictive ability</li>
<li>Feature extraction/feature engineering: how predictors are encoded, e.g. combinations, ratios</li>
<li>Feature selection: Include predictors to maximise accuracy</li>
<li>Method depends on model being used and true relationship with outcome</li>
<li>Model may require:</li>
<li>Predictors have common scale (e.g PLS)</li>
<li>Removal of outliers</li>
</ul>
<div id="data-tranformations-individual-predictors" class="section level2">
<h2>Data Tranformations (Individual Predictors)</h2>
<ul>
<li>Pre-processing techniques include:</li>
<li>Centering: Subtract average <span class="math inline">\(\rightarrow \bar{x} = 1\)</span></li>
<li>Scaling: Divide by standard deviation <span class="math inline">\(\rightarrow s = 1\)</span></li>
<li>Skewness transformations.<br />
</li>
<li>Disadvantage of transformations is loss of interpretability</li>
</ul>
<p><strong>Centering and Scaling</strong></p>
<p><strong>Skewness</strong> Skewed if <span class="math inline">\(\frac{x_\max}{x_\min} &gt; 20\)</span>, or if |skewness statistic| &gt;&gt; 0 <span class="math display">\[
    \begin{align}
      \text{skewness} &amp; = \frac{\sum (x_i - \bar{x})^3}{(n-1)v^{2/3}} \\
      v &amp; = \frac{\sum(x_i - \bar{x})^2}{(n-1)}
   \end{align}\]</span></p>
<p>Skewness can be removed by replacing data with log, square root or inverse, or by using the Box and Cox family of transformations and determining the appropriate parameter, <span class="math inline">\(\lambda\)</span>,</p>
<span class="math display">\[\begin{align}
  x^\star =  \begin{cases}\frac{x^\lambda - 1 }{\lambda} &amp; \text{if } \lambda \neq 0 \\
                    \log(x) &amp; \text{if } \lambda = 0
              \end{cases}
        
\end{align}\]</span>
<p>Note</p>
<ul>
<li>Square, <span class="math inline">\(\lambda = 2\)</span></li>
<li>Square root, <span class="math inline">\(\lambda = 0.5\)</span></li>
<li>Inverse, <span class="math inline">\(\lambda = -1\)</span></li>
</ul>
<p><span class="math inline">\(\lambda\)</span> can be estimated using training data for each feature with skewness. Transformations should only be applied if <span class="math inline">\(\lambda\)</span> outside <span class="math inline">\(1 \pm 0.02\)</span>.</p>
<p>The <code>MASS::boxcox</code> function can estimate <span class="math inline">\(\lambda\)</span> but will not create the transformed variables. The <code>caret::BoxCoxTrans</code> function will find the appropriate transformation and apply them to the new data.</p>
<p><strong>Example</strong>: Segmentation data</p>
<pre class="r"><code># load case study training data
data(&quot;segmentationOriginal&quot;)
segTrain &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)

## Remove response variables (first three columns)
segTrainId &lt;- segTrain$Cell
segTrainClass &lt;- segTrain$Class
segTrainCase &lt;- segTrain$Case
segTrainX &lt;- dplyr::select(segTrain, -c(Cell, Class, Case)) 
#%&gt;% select(-contains(&quot;Status&quot;))

# https://topepo.github.io/caret/pre-processing.html#the-preprocess-function

# Summarise the predictors
segMetrics &lt;- segTrainX %&gt;%
  select_if(is.numeric) %&gt;%
  summarise_all(funs(skewness, n_distinct, max, min)) %&gt;%
  gather(key=&quot;key&quot;, value=&quot;value&quot;) %&gt;%
  extract(key, c(&quot;variable&quot;, &quot;metric&quot;), &quot;(.*)_(skewness|n_distinct|max|min)&quot;) %&gt;%
  spread(metric, value) %&gt;%
  mutate(max_min_ratio = ifelse(min==0, (max+1)/(min+1), max/min)) %&gt;%
  select(variable, n_distinct, skewness, max_min_ratio, min, max)</code></pre>
<pre><code>## Warning: funs() is soft deprecated as of dplyr 0.8.0
## Please use a list of either functions or lambdas: 
## 
##   # Simple named list: 
##   list(mean = mean, median = median)
## 
##   # Auto named with `tibble::lst()`: 
##   tibble::lst(mean, median)
## 
##   # Using lambdas
##   list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))
## This warning is displayed once per session.</code></pre>
<pre class="r"><code>## Use caret&#39;s preProcess function to transform for skewness
segPP &lt;- caret::preProcess(segTrainX, method = &quot;BoxCox&quot;)

## Apply the transformations
segTrainTrans &lt;- predict(segPP, segTrainX)

#Transformation used
# pp2df &lt;- function(object, ...) {
#   vars &lt;- unlist(object$method)
#   nums &lt;- vapply(object$method, length, c(num = 0))
#   meth &lt;- rep(names(nums), nums)
#   
#   data.frame(variable = unname(vars), method = meth)
# }

# Record results
res &lt;- data.frame(lambda = sapply(segPP$bc, `[[`, 1))
res &lt;- data.frame(variable = rownames(res), res, row.names = NULL, stringsAsFactors = FALSE)

segMetrics &lt;- segMetrics %&gt;% 
  left_join(res, by=&quot;variable&quot;) %&gt;% 
  arrange(lambda)

head(segMetrics)</code></pre>
<pre><code>##                 variable n_distinct skewness max_min_ratio        min
## 1 ConvexHullAreaRatioCh1       1000 2.476582      2.878292   1.007653
## 2          EqCircDiamCh1        399 1.955530      3.806062  13.862944
## 3               PerimCh1        657 2.589488      9.631097  47.737594
## 4            ShapeLWRCh1       1009 2.490995      7.737628   1.002813
## 5                AreaCh1        399 3.525107     14.573333 150.000000
## 6    DiffIntenDensityCh1       1009 2.760473     16.563239  26.732283
##           max lambda
## 1    2.900320   -2.0
## 2   52.763230   -1.7
## 3  459.765378   -1.1
## 4    7.759391   -1.0
## 5 2186.000000   -0.9
## 6  442.773196   -0.9</code></pre>
<pre class="r"><code>rm(res)</code></pre>
<p>In this data set there were a total of 116 features, for which:</p>
<ul>
<li><p>69 were not transformed because they had a minimum value of <span class="math inline">\(\leq 0\)</span></p></li>
<li>the remaining features had a <span class="math inline">\(\lambda\)</span> between -2 and 2</li>
<li><p>Features with a lambda between 0.98 and 1.02 would not be transformed</p></li>
</ul>
<p><strong>Question</strong>: Why not add an offset variable to ensure all variables are positive? Note, that instead of using the log transformation, or Box-Cox transformations when predictors have values of zero, can instead use the Yeo-Johnson family of transformations. This family is similar to the Box-Cos transformations but can handle zero or negative predictor values.</p>
<p>It should be noted that transformations to reduce skewness might not always be successful. Under such circumstances, one should use models that are not unduly affected by skewed distributions (e.g. tree-based methods)</p>
<p>As an example, the feature <em>VarIntenCh3</em>, which records the standard deviation of pix intensity in actin filaments, had the following properties:</p>
<ul>
<li>Metrics</li>
</ul>
<pre class="r"><code>filter(segMetrics, variable == &quot;VarIntenCh3&quot;)</code></pre>
<pre><code>##      variable n_distinct skewness max_min_ratio       min     max lambda
## 1 VarIntenCh3       1009 2.391624      870.8872 0.8692526 757.021    0.1</code></pre>
<ul>
<li>Strong right skewness</li>
<li>Original data distribution</li>
</ul>
<pre class="r"><code>histogram(~segTrainX$VarIntenCh3,
          xlab = &quot;Natural Units&quot;,
          type = &quot;count&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/original_segmentation_features-1.png" width="672" /> * Transformed data distribution</p>
<pre class="r"><code>histogram(~log(segTrainX$VarIntenCh3),
          xlab = &quot;Log Units&quot;,
          ylab = &quot; &quot;,
          type = &quot;count&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/trans_segmentation_features-1.png" width="672" /></p>
</div>
<div id="data-transformations-multiple-predictors" class="section level2">
<h2>Data Transformations (Multiple Predictors)</h2>
<p><strong>Outliers</strong></p>
<ul>
<li>Is value valid or has recording error occurred?</li>
<li>Take care to remove or change values, especially if sample size is small (as could be a result of a skewed distribution without enough data to see the skewness, or could be an indication of a special part of the population)</li>
<li>Decision trees and SVMs are insensitive to outliers</li>
<li>The <em>spatial sign</em> transformation can minimize the problem of a model’s sensitivity to outliers</li>
</ul>
<p>Spatial Sign Transformation</p>
<ul>
<li>Projects predictor values onto a multidimensional sphere</li>
<li>Makes all samples the same distance from the centre of the sphere</li>
<li>Each sample is divided by its squared norm: <span class="math display">\[ x_{ij}^* = \frac{x_{ij}}{\sum^P_{j=1} x^2_{ij}}\]</span></li>
<li>The denominator measures the squared distance to the centre of the predictors distribution</li>
<li>It is important to center and scale the predictor data prior to using this transformation</li>
<li>The predictors are transformed as a group, making removal of a predictor problematic</li>
</ul>
<p>Example:</p>
<ul>
<li>Investigation of ~ 8 outliers shows valid but poorly sampled population (e.g. highly profitable customers)</li>
<li>Spatial sign transformation applied; outliers reside in northwest section of distribution but contracted inwards</li>
<li>Mitigates effect on model training</li>
</ul>
<pre class="r"><code>trellis.par.set(caretTheme())
featurePlot(x=iris[,-5], y=iris[,5], &quot;pairs&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/iris_spatial_trans-1.png" width="672" /></p>
<pre class="r"><code>featurePlot(spatialSign(scale(iris[,-5])), iris[,5], &quot;pairs&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/iris_spatial_trans-2.png" width="672" /></p>
<pre class="r"><code>set.seed(1)
n &lt;- 10000
tmp &lt;- data.frame(x=c(rnorm(n, 0, 0.02), -1, 1, 0.5),
                  y=c(rnorm(n, 0, 0.2), -1, 1, -2))

plot(tmp, asp=1, col=c(rep(1,n), 2, 3, 4), pch=19)
grid()</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/rand_spatial_trans-1.png" width="672" /></p>
<pre class="r"><code>plot(spatialSign(tmp), asp=1, col=c(rep(1,n), 2, 3, 4), pch=19)
grid()</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/rand_spatial_trans-2.png" width="672" /></p>
<p><strong>Data Reduction and Feature Extraction</strong></p>
<p>Data reduction techniques</p>
<ul>
<li>Generate smaller set of predictors that capture most of the information within the original variables</li>
<li><em>Signal (Feature) Extraction</em> techniques: New predictors are functions of original variables (therefore all original variables required)</li>
<li>PCA
<ul>
<li>Finds linear combinations of predictors (principal components) to capture most possible variance</li>
<li>First PC captures more variability than any other linear combination</li>
<li>Subsequent PCs are uncorrelated with all previous PCs</li>
<li>Principal Component can be written as: <span class="math display">\[PC_j = (a_{j1} x \text{Predictor} 1) + (a_{j2} x \text{Predictor} 2) + \ldots + (a_{jP} x \text{Predictor} P)\]</span> where P = # predictors, coefficients = component weights (or loadings) that show which predictors are important for given PC.</li>
<li>Advantage: creates uncorrelated components, which is required for stability of some models</li>
<li><p>Disadvantages: Seeks predictor-set variation without regard to predictor measurement scales/distributions or response variable; without guidance can summarize data characteristics that are irrelevant to structure of data and modelling objective</p>
<ul>
<li>Seeks linear combinations that maximize variability, and therefore will first summarise predictors with more variation. If original predictors are on measurements scales then first few components will summarise higher magnitude predictors; consequently it will focus on identifying data structure based on measurement scales rather than based on important relationships within the data for the current problem.</li>
<li>Unsupervised technique; does not consider consider modelling objective / response variability.</li>
</ul></li>
</ul></li>
<li>Should first transform skewed predictors and then center and scale prior to performing PCA; prevent PCA being influenced by original measurement scales</li>
<li>Consider Partial Least Squares (PLS) to derive components with response variable in mind.</li>
<li>Use scree plot to decide number of components to keep; in automated model building process, optimal number can be determined by cross-validation</li>
<li>Should also visually examine the PCs; plot first few PCs against each other and color points by, e.g., class labels. If PCA has captured sufficient amount of information in data, may demonstrate clusters of samples/outliers requiring closer examination. Ensure to use the same scale as later components will often have smaller data ranges and plotting on separate scales may lead to potential to over-interpret patterns.</li>
</ul>
<p>Example: PCA applied to two features</p>
<ul>
<li>For channel 1, Intensity Entropy is highly correlated with Fiber Width (0.93)</li>
<li>Could use just one predictor, or could use PCA to instead use a linear combination of these two predictors</li>
</ul>
<pre class="r"><code>## R&#39;s prcomp is used to conduct PCA
pr &lt;- prcomp(~ AvgIntenCh1 + EntropyIntenCh1, 
             data = segTrainTrans, #already pre-preprocessed for skewness
             scale. = TRUE)


transparentTheme(pchSize = .7, trans = .3)

xyplot(AvgIntenCh1 ~ EntropyIntenCh1,
       data = segTrainTrans,
       groups = segTrain$Class,
       xlab = &quot;Channel 1 Fiber Width&quot;,
       ylab = &quot;Intensity Entropy Channel 1&quot;,
       auto.key = list(columns = 2),
       type = c(&quot;p&quot;, &quot;g&quot;),
       main = &quot;Original Data&quot;,
       aspect = 1)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_PCA-1.png" width="672" /></p>
<pre class="r"><code>xyplot(PC2 ~ PC1,
       data = as.data.frame(pr$x),
       groups = segTrain$Class,
       xlab = &quot;Principal Component #1&quot;,
       ylab = &quot;Principal Component #2&quot;,
       main = &quot;Transformed&quot;,
       xlim = extendrange(pr$x),
       ylim = extendrange(pr$x),
       type = c(&quot;p&quot;, &quot;g&quot;),
       aspect = 1)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_PCA-2.png" width="672" /></p>
<ul>
<li>Because first PC summarises 96.6% variation and the second 3.42%, in this case could just use the first PC.</li>
</ul>
<p>Example: PCA applied to all features</p>
<p>The scree plot shows that four PCs would be retained.</p>
<pre class="r"><code>## Apply PCA to the entire set of predictors.

## There are a few predictors with only a single value, so we remove these first
## (since PCA uses variances, which would be zero)
isZV &lt;- apply(segTrainX, 2, function(x) length(unique(x)) == 1)

segPP &lt;- preProcess(segTrainX[, !isZV], c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;))
segTrainTrans &lt;- predict(segPP, segTrainX[, !isZV])

segPCA &lt;- prcomp(segTrainTrans, center = TRUE, scale. = TRUE)

tab &lt;- summary(segPCA)$importance[2,]
tab &lt;- data.frame(PC = names(tab), Var = tab, row.names=NULL, stringsAsFactors = FALSE) %&gt;% mutate(PC=parse_number(PC))
ggplot(tab, aes(x = PC, y=Var))+ 
         geom_line() + 
  geom_point()</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_pca_scree-1.png" width="672" /></p>
<p>Scatterplot matrix of first 3 PCs (with points coloured by class):</p>
<ul>
<li>Appears to be some separation between classes when plotting first and second component (but remember that these two components only explain 26.6% of variance and so don’t over-interpret!). However distribution of well-segmented cells roughly contained within poorly identified cells; cell types don’t appear to be easily separated. Don’t despair!; it does not mean other models, e.g. that can accommodate non-linear relationships, will reach the same conclusions.</li>
</ul>
<pre class="r"><code>## Plot a scatterplot matrix of the first three components
transparentTheme(pchSize = .8, trans = .3)
panelRange &lt;- extendrange(segPCA$x[, 1:3])
splom(as.data.frame(segPCA$x[, 1:3]),
      groups = segTrainClass,
      type = c(&quot;p&quot;, &quot;g&quot;),
      as.table = TRUE,
      auto.key = list(columns = 2),
      prepanel.limits = function(x) panelRange)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_pca_scatter-1.png" width="672" /></p>
<ul>
<li>Can also visualise which predictor is associated with each principal component. A coefficient (loading) close to zero within the PC linear equation indicates that that predictors did not contribute much to that component. In the following figure, each point corresponds to a predictor variable and is coloured by the optical channel used in the experiment. For the first PC, channel 1 (cell body) loadings are greater and therefore have largest effect on PC. However, even though cell body measurements account for more variation in the data, this does not imply that these variables will be associated with predicting segmentation quality.</li>
</ul>
<pre class="r"><code>## Format the rotation values for plotting
segRot &lt;- as.data.frame(segPCA$rotation[, 1:3])

## Derive the channel variable
vars &lt;- rownames(segPCA$rotation)
channel &lt;- rep(NA, length(vars))
channel[grepl(&quot;Ch1$&quot;, vars)] &lt;- &quot;Channel 1&quot;
channel[grepl(&quot;Ch2$&quot;, vars)] &lt;- &quot;Channel 2&quot;
channel[grepl(&quot;Ch3$&quot;, vars)] &lt;- &quot;Channel 3&quot;
channel[grepl(&quot;Ch4$&quot;, vars)] &lt;- &quot;Channel 4&quot;

segRot$Channel &lt;- channel
segRot &lt;- segRot[complete.cases(segRot),]
segRot$Channel &lt;- factor(as.character(segRot$Channel))

## Plot a scatterplot matrix of the first three rotation variable
transparentTheme(pchSize = .8, trans = .7)
panelRange &lt;- extendrange(segRot[, 1:3])
upperp &lt;- function(...)
  {
    args &lt;- list(...)
    circ1 &lt;- ellipse(diag(rep(1, 2)), t = .1)
    panel.xyplot(circ1[,1], circ1[,2],
                 type = &quot;l&quot;,
                 lty = trellis.par.get(&quot;reference.line&quot;)$lty,
                 col = trellis.par.get(&quot;reference.line&quot;)$col,
                 lwd = trellis.par.get(&quot;reference.line&quot;)$lwd)
    circ2 &lt;- ellipse(diag(rep(1, 2)), t = .2)
    panel.xyplot(circ2[,1], circ2[,2],
                 type = &quot;l&quot;,
                 lty = trellis.par.get(&quot;reference.line&quot;)$lty,
                 col = trellis.par.get(&quot;reference.line&quot;)$col,
                 lwd = trellis.par.get(&quot;reference.line&quot;)$lwd)
    circ3 &lt;- ellipse(diag(rep(1, 2)), t = .3)
    panel.xyplot(circ3[,1], circ3[,2],
                 type = &quot;l&quot;,
                 lty = trellis.par.get(&quot;reference.line&quot;)$lty,
                 col = trellis.par.get(&quot;reference.line&quot;)$col,
                 lwd = trellis.par.get(&quot;reference.line&quot;)$lwd)
    panel.xyplot(args$x, args$y, groups = args$groups, subscripts = args$subscripts)
}
# note this requires ellipse
lattice::splom(~segRot[, 1:3],
      groups = segRot$Channel,
      lower.panel = function(...){}, upper.panel = upperp,
      prepanel.limits = function(x) panelRange,
      auto.key = list(columns = 2))</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_pca_splom-1.png" width="672" /></p>
</div>
<div id="missing-values" class="section level2">
<h2>Missing Values</h2>
<p>Types of missing data:</p>
<ul>
<li>Structurally missing, e.g. number of children man has given birth to</li>
<li>Informative missingness: if related to outcome, can induce bias, e.g. customer rating are using polarised</li>
<li>Censored (this is not missing data, as something is known about it), but for predictive models, it may be treated as missing, or the censored value may be used as the observed value. E.g., for laboratory test which cannot measure below a limit, may use a random value between 0 and the limit as the observed value.</li>
</ul>
<p>Options:</p>
<ul>
<li>Remove samples (if small subset of large data set)</li>
<li>Specifically account for the missingness (e.g. tree-based techniques)</li>
<li>Impute by using information in the training set of predictors (i.e. predictive model within a predictive model).
<ul>
<li>Note that imputation for statistical inference is not the same as inference for predictive models (references given for the latter).<br />
</li>
<li>Incorporate imputation in resampling if being used to select tuning parameter values.<br />
</li>
<li>If number of predictors affected by missing values is small, best to perform exploratory analysis of relationships between predictors, e.g. using PCA or visualisations. If a variable with missing values is highly correlated with another then can use a focused model</li>
<li>KNN popular to impute by finding samples in training set closest to it and averages nearby points to fill it. Advantage: Confined to training set. Disadvantage: KNN requires entire training set and number of neighbours and method of determining “closeness” are tuning parameters.</li>
</ul></li>
</ul>
<pre class="r"><code>library(AppliedPredictiveModeling)
data(segmentationOriginal)

## Retain the original training set
segTrain &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)

## Remove the first three columns (identifier columns)
segTrainX &lt;- segTrain[, -(1:3)]
segTrainX &lt;- segTrainX[, -nearZeroVar(segTrainX)]

# Randomly sample 50 to be missing
set.seed(15103930)
ind_miss &lt;- sample(1:nrow(segTrainX), 50, replace = FALSE)
obs &lt;- segTrainX[ind_miss, ]
segTrainX$PerimCh1[ind_miss] &lt;- NA
summary(segTrainX$PerimCh1)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   47.74   64.37   79.02   91.95  103.86  459.77      50</code></pre>
<pre class="r"><code># Predict the missing values
set.seed(100)
knnTrans &lt;- preProcess(segTrainX, method = c(&quot;center&quot;, &quot;scale&quot;, &quot;knnImpute&quot;))
knnPred &lt;- predict(knnTrans, segTrainX)
pred &lt;- knnPred$PerimCh1[ind_miss]
obsTrans &lt;- predict(knnTrans, obs)$PerimCh1

ggplot(data.frame(obs = obsTrans, pred), aes(x=obs, y=pred)) + 
  geom_point() + 
  geom_smooth(method=&#39;lm&#39;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_imputation_knn-1.png" width="672" /></p>
<pre class="r"><code>cor(obsTrans, pred)</code></pre>
<pre><code>## [1] 0.9097433</code></pre>
<pre class="r"><code>#if(&quot;PerimCh1&quot; %in% preProcValues$method$scale) pred &lt;- pred * preProcValues$std[&quot;PerimCh1&quot;]
#if(&quot;PerimCh1&quot; %in% preProcValues$method$center) pred &lt;- pred + preProcValues$mean[&quot;PerimCh1&quot;]</code></pre>
<pre class="r"><code># use cell fiber lengtha nd cell size. to predict.  
segCorr &lt;- cor(segTrainX, use = &quot;pairwise.complete.obs&quot;)[,&quot;PerimCh1&quot;]
sort(segCorr, decreasing = TRUE)[2:3]</code></pre>
<pre><code>## FiberLengthCh1      LengthCh1 
##      0.9859845      0.9218499</code></pre>
<pre class="r"><code>mod &lt;- lm(PerimCh1 ~ FiberLengthCh1 + LengthCh1, data = segTrainX[-ind_miss, ])
pred &lt;- predict(mod, segTrainX[ind_miss, ])
ggplot(data.frame(obs = obsTrans, pred), aes(x=obs, y=pred)) + 
  geom_point() + 
  geom_smooth(method=&#39;lm&#39;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_imputation_regression-1.png" width="672" /></p>
<pre class="r"><code>cor(pred, obsTrans)</code></pre>
<pre><code>## [1] 0.9664605</code></pre>
</div>
<div id="removing-predictors" class="section level2">
<h2>Removing Predictors</h2>
<p>Advantage of less predictors:</p>
<ul>
<li>Decreased computation time</li>
<li>Decreased complexity / More parsimonious / More interpretable</li>
<li>Highly correlated predictors are measuring the same thing and so have no extra information</li>
<li>Better model performance / stability without problematic and correlated variables</li>
</ul>
<p>Predictors to remove:</p>
<ul>
<li>Zero Variance and near-zero</li>
<li>Correlated</li>
</ul>
</div>
<div id="zero-variance-and-near-zero-variance-predictors" class="section level2">
<h2>Zero Variance and Near-Zero Variance Predictors</h2>
<ul>
<li><strong>Zero variance predictors</strong> : Those with a single unique value</li>
<li><strong>Near-zero variance predictors</strong> : Those with a single value for most samples</li>
</ul>
<p>To determine:</p>
<ol style="list-style-type: decimal">
<li>Calculate number of unique variables / number of samples</li>
<li>Calculate frequency of unique values (or ratio of most common frequent to second most frequent)</li>
</ol>
<p>If (1) fraction of unique values over sample size is low, e.g. <span class="math inline">\(\leq\)</span> 10% and (2) ratio of frequency of most prevalent value to the second most prevalent value is large, e.g. <span class="math inline">\(\geq\)</span> 20</p>
</div>
<div id="correlated-variables" class="section level2">
<h2>Correlated Variables</h2>
<ul>
<li><strong>Collinearity</strong>: Correlated pair of predictor variables</li>
<li><strong>Multicollinearity</strong>: Relationships between multiple predictors at once</li>
</ul>
<p>To detect: 1. Visually.</p>
<pre class="r"><code>segData &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)[, -(1:3)]
isZV &lt;- apply(segData, 2, function(x) length(unique(x)) == 1)
segData &lt;- segData[, !isZV]
correlations &lt;- cor(segData)
corrplot::corrplot(correlations, order = &quot;hclust&quot;, tl.cex = 0.2)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_correlation_matrix-1.png" width="672" /></p>
<ol start="2" style="list-style-type: decimal">
<li><p>PCA: If first PCA accounts for large percentage of variance, then there is at least one group of predictors that represent the same information. Use PCA loadings to understand which predictors are associated with each component.</p></li>
<li><p>Variance Inflation Factor (VIF) statistic available as part of classical regression analysis, however only useful for linear regression. Whilst it determine collinear predictors it does not determine which should be removed to resolve the problem.</p></li>
</ol>
<p>To remove correlated predictors:</p>
<ol style="list-style-type: decimal">
<li>Use a heuristic algorithm to remove the minimum number of predictors such that all pairwise correlations are below a certain threshold. This only identifies colinearities in two dimensions, but can improve model performance. Algorithm is as follows:</li>
<li>Calculate correlation matrix</li>
<li>Determine pair of predictors with largest absolute correlation</li>
<li>Determine average correlation between A and other variables. Repeat for B.</li>
<li>Remove the predictor (A or B) with the largest average correlation</li>
<li>Repeat steps 2-3 until no absolute correlations above the threshold</li>
</ol>
<p>This algorithm is implemented using <code>caret::findCorrelation</code></p>
<pre class="r"><code>segTrain &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)
## Remove the first three columns (identifier columns)
segTrainX &lt;- segTrain[, -(1:3)]
isZV &lt;- apply(segTrainX, 2, function(x) length(unique(x)) == 1)
segTrainX &lt;- segTrainX[, !isZV]

segTrainClass &lt;- segTrain$Class



segPP &lt;- preProcess(segTrainX, c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;))
segTrainTrans &lt;- predict(segPP, segTrainX)


## Use caret&#39;s preProcess function to transform for skewness
segPP &lt;- preProcess(segTrainX, method = &quot;BoxCox&quot;)
## Apply the transformations
segTrainTrans &lt;- predict(segPP, segTrainX)
segCorr &lt;- cor(segTrainTrans)
#corrplot(segCorr, order = &quot;hclust&quot;, tl.cex = .35)

## caret&#39;s findCorrelation function is used to identify columns to remove.
highCorr &lt;- findCorrelation(segCorr, .75)
length(highCorr)</code></pre>
<pre><code>## [1] 43</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Feature extraction algorithms (e.g. PCA) can also be used to mitigate effect of correlations; the disadvantage is that they make connection between predictors more complex and as unsupervised no guarantee that resulting components will have relationship with outcome.</li>
<li>Simulated Annealing (e.g., <code>subselect</code> package)</li>
<li>Genetic Algorithms</li>
</ol>
</div>
<div id="adding-predictors" class="section level2">
<h2>Adding Predictors</h2>
<ul>
<li><p>It is common to decompose categorical predictors using dummy variables (indicator with zero/one). With 5 categories, only 4 dummy variables needed. If 5 were included in regression then would have numerical issues (intercept replaces the other); for other models, including all 5 may aid interpretation</p></li>
<li>Non-linear transformations, e.g. <span class="math inline">\(B^2\)</span>, where <span class="math inline">\(B\)</span> is a predictor</li>
<li><p>Combinations of data (e.g. class centroids, center of predictor data to each class)</p></li>
</ul>
</div>
<div id="binning-predictors" class="section level2">
<h2>Binning Predictors</h2>
<p>Disadvantages of manual binning:</p>
<ul>
<li>Loss of performance in the model</li>
<li>Loss in prediction in the predictions</li>
<li>Can lead to high rate of false positives (noisy predictors determined to be informative)</li>
</ul>
<p>Advantage: More interpretable. However the perceived improvement in interpretability by manual binning is usually offset by significant loss in performance (the goal of this book is prediction not interpretation and so manual binning not recommended).</p>
<p>Automatic Binning * E.G Classification / Regression Trees * Multivariate adaptive regression splines * Evaluate many variables simultaneously, based on statistically sound methodologies</p>
</div>
<div id="computing" class="section level2">
<h2>Computing</h2>
<div id="transformations" class="section level3">
<h3>Transformations</h3>
<ul>
<li><code>e1071::sknewness</code>: calculates sample skewness statistics for each predictor. Those that are highly skewed can be prioritised for distribution visualisations using <code>hist</code>, <code>lattice:histogram</code></li>
<li><code>MASS::boxcox</code>: to determine type of transformation to use; estimates <span class="math inline">\(\lambda\)</span> but will not create the transformed variable(s)</li>
<li><code>caret::BoxCoxTrans</code>: Find most appropriate transformation and apply to new data</li>
</ul>
<p>Example:</p>
<pre class="r"><code>segData &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)[, -(1:3)]
Ch1AreaTrans &lt;- BoxCoxTrans(segData$AreaCh1)
Ch1AreaTrans</code></pre>
<pre><code>## Box-Cox Transformation
## 
## 1009 data points used to estimate Lambda
## 
## Input data summary:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   150.0   194.0   256.0   325.1   376.0  2186.0 
## 
## Largest/Smallest: 14.6 
## Sample Skewness: 3.53 
## 
## Estimated Lambda: -0.9</code></pre>
<pre class="r"><code>data.frame(orig = head(segData$AreaCh1), auto.trans = predict(Ch1AreaTrans, head(segData$AreaCh1)), man.trans = ( head(segData$AreaCh1) ^ Ch1AreaTrans$lambda - 1 )/Ch1AreaTrans$lambda   )</code></pre>
<pre><code>##   orig auto.trans man.trans
## 1  819   1.108458  1.108458
## 2  431   1.106383  1.106383
## 3  298   1.104520  1.104520
## 4  256   1.103554  1.103554
## 5  258   1.103607  1.103607
## 6  358   1.105523  1.105523</code></pre>
<ul>
<li><code>prcomp</code>: For PCA</li>
</ul>
<p>Example:</p>
<pre class="r"><code>segData &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)[, -(1:3)]
isZV &lt;- apply(segData, 2, function(x) length(unique(x)) == 1)
segData &lt;- segData[, !isZV]

pcaObject &lt;- prcomp(segData, center = TRUE, scale. = TRUE)
# Calculate the cumulative percentage of variance which each component accounts for.
percentVariance &lt;- pcaObject$sd^2/sum(pcaObject$sd^2)*100
percentVariance[1:3]</code></pre>
<pre><code>## [1] 14.677086 11.734794  8.813733</code></pre>
<pre class="r"><code># Transformed variables stored in a `pcaObject` as a sub-object called x
head(pcaObject$x[ , 1:5])</code></pre>
<pre><code>##            PC1        PC2         PC3       PC4          PC5
## 2  -10.5813682  4.7022663  0.05567686 -1.792190  2.547714227
## 3    0.4154024  0.6974872 -1.38763482 -3.265656  1.672695719
## 4   -0.9779724 -1.8203879 -2.21605066  0.397754 -0.008029369
## 12   1.9842183 -0.3322714 -0.05032939 -2.517085  1.033381188
## 15   1.1217009 -0.1825613 -1.99234368 -3.021610  0.478149323
## 16   1.0824587  0.2026488 -1.16865264 -3.380599  1.200990054</code></pre>
<pre class="r"><code># Loadings
head(pcaObject$rotation[ , 1:3])</code></pre>
<pre><code>##                         PC1         PC2           PC3
## AngleCh1        0.001058888 -0.01186622  0.0008836706
## AngleStatusCh1  0.011793294  0.02083728 -0.0035682644
## AreaCh1        -0.211068512  0.09577310  0.0672000815
## AreaStatusCh1  -0.182337308  0.07292658  0.0546691134
## AvgIntenCh1     0.061268408  0.17388938  0.0413066884
## AvgIntenCh2     0.101313516  0.16201999  0.0349230837</code></pre>
<ul>
<li><code>caret::spatialSign</code>: Spatial sign transformation</li>
<li><code>impute::impute.knn</code> to estimate missing data; also possible using <code>caret::PreProcess</code> which applies imputation methods based on KNN or bagged trees</li>
<li><p><code>caret::PreProcess</code>: Has the ability to</p></li>
<li>Transform</li>
<li>Center</li>
<li>Scale</li>
<li>Impute values</li>
<li>Feature extraction</li>
<li><p>Apply spatial sign transformation</p></li>
</ul>
<p>(in this order) After calling the <code>preProcess</code> function, the <code>predict</code> method applies the transformation results to a set of data.</p>
</div>
<div id="filtering" class="section level3">
<h3>Filtering</h3>
<ul>
<li><code>caret::nearZeroVar</code>: To determine predictors with unique variables</li>
<li><code>cor</code>: To determine correlations</li>
<li><code>corrplot:corrplot</code>: Includes option to reorder variables in a way that reveals clusters of highly correlated predictors</li>
<li><code>caret::findCorrelation</code>: Recommends columns for deletion based on a given threshold of pairwise correlations</li>
<li><code>subselect</code> package also has methods for selecting predictors</li>
</ul>
</div>
<div id="creating-dummy-variables" class="section level3">
<h3>Creating Dummy Variables</h3>
<ul>
<li><code>caret::dummyVars</code>: To determine encoding for categorical predictors</li>
<li>All dummy variables recommended when using tree-based model</li>
</ul>
<pre class="r"><code>data(cars)
carSubset &lt;- select(cars, Price, Mileage)
type &lt;- c(&quot;convertible&quot;, &quot;coupe&quot;, &quot;hatchback&quot;, &quot;sedan&quot;, &quot;wagon&quot;)
carSubset$Type &lt;- factor(apply(cars[, 14:18], 1, function(x) type[which(x == 1)]))

simpleMod &lt;- dummyVars(~Mileage + Type,
                       data = carSubset,
                       ## Remove the variable name from the
                       ## column name
                       levelsOnly = TRUE)
simpleMod</code></pre>
<pre><code>## Dummy Variable Object
## 
## Formula: ~Mileage + Type
## 2 variables, 1 factors
## Factor variable names will be removed
## A less than full rank encoding is used</code></pre>
<pre class="r"><code># To generate the dummy variables fo rhte training set or any new samples, use the predict method with teh dummyVars objects
predict(simpleMod, head(carSubset))</code></pre>
<pre><code>##   Mileage convertible coupe hatchback sedan wagon
## 1   20105           0     0         0     1     0
## 2   13457           0     1         0     0     0
## 3   31655           1     0         0     0     0
## 4   22479           1     0         0     0     0
## 5   17590           1     0         0     0     0
## 6   23635           1     0         0     0     0</code></pre>
<pre class="r"><code>withInteraction &lt;- dummyVars(~Mileage + Type + Mileage:Type,
                             data = carSubset,
                             levelsOnly = TRUE)
withInteraction</code></pre>
<pre><code>## Dummy Variable Object
## 
## Formula: ~Mileage + Type + Mileage:Type
## 2 variables, 1 factors
## Factor variable names will be removed
## A less than full rank encoding is used</code></pre>
<pre class="r"><code>predict(withInteraction, head(carSubset))</code></pre>
<pre><code>##   Mileage convertible coupe hatchback sedan wagon Mileage:Typeconvertible
## 1   20105           0     0         0     1     0                       0
## 2   13457           0     1         0     0     0                       0
## 3   31655           1     0         0     0     0                   31655
## 4   22479           1     0         0     0     0                   22479
## 5   17590           1     0         0     0     0                   17590
## 6   23635           1     0         0     0     0                   23635
##   Mileage:Typecoupe Mileage:Typehatchback Mileage:Typesedan
## 1                 0                     0             20105
## 2             13457                     0                 0
## 3                 0                     0                 0
## 4                 0                     0                 0
## 5                 0                     0                 0
## 6                 0                     0                 0
##   Mileage:Typewagon
## 1                 0
## 2                 0
## 3                 0
## 4                 0
## 5                 0
## 6                 0</code></pre>
</div>
</div>
<div id="chapter-3-exercises" class="section level2">
<h2>Chapter 3 Exercises</h2>
<div id="exercise-3.1" class="section level3">
<h3>Exercise 3.1</h3>
<p>The UC Irvine Machine Learning Repository contains a data set related to glass identification. The data consist of 214 glass samples labeled as one of seven class categories. There are nine predictors, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.</p>
<pre class="r"><code>library(mlbench)
data(Glass)
str(Glass)</code></pre>
<pre><code>## &#39;data.frame&#39;:    214 obs. of  10 variables:
##  $ RI  : num  1.52 1.52 1.52 1.52 1.52 ...
##  $ Na  : num  13.6 13.9 13.5 13.2 13.3 ...
##  $ Mg  : num  4.49 3.6 3.55 3.69 3.62 3.61 3.6 3.61 3.58 3.6 ...
##  $ Al  : num  1.1 1.36 1.54 1.29 1.24 1.62 1.14 1.05 1.37 1.36 ...
##  $ Si  : num  71.8 72.7 73 72.6 73.1 ...
##  $ K   : num  0.06 0.48 0.39 0.57 0.55 0.64 0.58 0.57 0.56 0.57 ...
##  $ Ca  : num  8.75 7.83 7.78 8.22 8.07 8.07 8.17 8.24 8.3 8.4 ...
##  $ Ba  : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ Fe  : num  0 0 0 0 0 0.26 0 0 0 0.11 ...
##  $ Type: Factor w/ 6 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;5&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<pre class="r"><code>GlassPredictors &lt;- select(Glass, -Type) %&gt;%
  gather(Predictor, Value)</code></pre>
<ol style="list-style-type: lower-alpha">
<li>Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors.</li>
</ol>
<pre class="r"><code>ggplot(GlassPredictors, aes(x = Value)) +  
  geom_histogram() + 
  facet_wrap(~Predictor, scales = &quot;free&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/glass_hist-1.png" width="672" /></p>
<pre class="r"><code>ggplot(GlassPredictors, aes(x = Value)) +  
  geom_density() + 
  facet_wrap(~Predictor, scales = &quot;free&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/glass_dens-1.png" width="672" /></p>
<pre class="r"><code>library(GGally)
ggpairs(select(Glass, -Type))</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/glass_predictor_scatter-1.png" width="672" /></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Does there appear to be any outliers in the data? Are predictors skewed?</li>
</ol>
<ul>
<li>Several variables show signs of skewness (BA, Ca, FE, RI)</li>
<li>Variable K could be skewed or have outliers</li>
<li>Variables K and MG have possible second modes around zero, whereas Fe and Ba also have a high distribution of values around zero</li>
<li>Variables Ca, NA, RI and SI have concentrations of samples in the middle of the scale and a small number of data points at the edges</li>
<li>Variables Ca and Ri are positively correlated, and Ca and NA</li>
<li>Visually, Ca and Na appear to be negatively correlated, however the correlation score is -0.275</li>
</ul>
<ol start="3" style="list-style-type: lower-alpha">
<li>Are there any relevant transformations of one or more predictors that might improve the classification model?</li>
</ol>
<p>Due to variables containing zero, use the Yeo-Johnson family of transformations</p>
<pre class="r"><code>glassTrans &lt;- preProcess(select(Glass, -Type), method = &quot;YeoJohnson&quot;)
glassTransData &lt;- predict(glassTrans, select(Glass, -Type))
dat &lt;- glassTransData %&gt;%
  gather(Predictor, Value) %&gt;%
  mutate(Transformation = &quot;Yeo-Johnson&quot;) %&gt;%
  bind_rows(data.frame(GlassPredictors, Transformation = &quot;NA&quot;))


# Final result
res &lt;- list()
# How many points we want 
nPoints &lt;- 1e3

# Using simple loop to scale and calculate density
combinations &lt;- expand.grid(unique(dat$Predictor), unique(dat$Transformation))
for(i in 1:nrow(combinations)) {
    # Subset data
    foo &lt;- subset(dat, Predictor == combinations$Var1[i] &amp; Transformation == combinations$Var2[i])
    # Perform density on scaled signal
    densRes &lt;- density(x = scale(foo$Value), n = nPoints)
    # Position signal from 1 to wanted number of points
    res[[i]] &lt;- data.frame(x = 1:nPoints, y = densRes$y, 
                           pred = combinations$Var1[i], trans = combinations$Var2[i])
}
res &lt;- do.call(rbind, res)
ggplot(res, aes(x / nPoints, y, color = trans, linetype = trans)) +
    geom_line(alpha = 0.5, size = 1) +
    facet_wrap(~ pred, scales = &quot;free&quot;)  +
  labs(x = &quot;Value / Points&quot;, y = &quot;Scaled Density&quot;, colour = &quot;Transformation&quot;, linetype = &quot;Transformation&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/glass_yeo_trans-1.png" width="672" /></p>
<p>However, they don’t really help in terms of skewness …</p>
<p>To mitigate outliers, use the spatial sign transformation</p>
<pre class="r"><code>glassCSSSTrans &lt;- preProcess(select(Glass, -Type), method = c(&quot;center&quot;, &quot;scale&quot;, &quot;spatialSign&quot;))
glassCSSSData &lt;- predict(glassCSSSTrans, select(Glass, -Type))
ggpairs(glassCSSSData)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/glass_spatial_trans-1.png" width="672" /></p>
<p>Spatial sign transformation was effective in removing outliers.</p>
</div>
<div id="exercise-2" class="section level3">
<h3>Exercise 2</h3>
<p>The soybean data can also be found at the UC Irvine Machine Learning Repository. Data were collected to predict disease in 683 soybeans. The 35 predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes.</p>
<ol style="list-style-type: lower-alpha">
<li>Investigate the frequency distributions for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this chapter?</li>
</ol>
<p>Note: By degenerate, I believe it is meant whether the following exist:</p>
<ul>
<li>Skewness</li>
<li>Outliers</li>
<li>Missing values</li>
<li>Zero- and near-zero- variance</li>
</ul>
<pre class="r"><code>select(Soybean, -Class) %&gt;%
  select_if(negate(is.numeric)) %&gt;%
  gather(Variable, Value) %&gt;%
  ggplot(aes(x = Value)) + 
  geom_bar() + 
  facet_wrap(~ Variable, scales = &quot;free&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/soybean_hist-1.png" width="672" /></p>
<p>From these plots we see:</p>
<ul>
<li>Few observations in April (0)</li>
<li>Precipitation usually greater than normal (2)</li>
<li>Temperature usually normal</li>
<li>Temperature, precipitation and hail are missing values.</li>
</ul>
<p>A mosaic plot, fluctuation diagram, or faceted bar chart may be used to display two categorical variable, as detailed in <a href="https://vita.had.co.nz/papers/gpp.pdf">The Generalized Pairs Plot</a></p>
<pre class="r"><code>ggpairs(select(Soybean, date, precip, temp), diag = &#39;blankDiag&#39;, upper = list(discrete = &quot;ratio&quot;))</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/soybean_pairs-1.png" width="672" /></p>
<pre class="r"><code>library(vcd)
#library(ggmosaic)
#detach(&quot;package:kernlab&quot;, unload=TRUE)
Soybean2 &lt;- Soybean %&gt;%
  mutate(date = fct_recode(date, Apr = &quot;0&quot;, May = &quot;1&quot;, Jun = &quot;2&quot;, Jul = &quot;3&quot;, Aug = &quot;4&quot;, Sep = &quot;5&quot;, Oct = &quot;6&quot;), 
         temp = fct_recode(temp, &quot;lt-norm&quot; = &quot;0&quot;, &quot;norm&quot; = &quot;1&quot;, &quot;gt-norm&quot; = &quot;2&quot;))
vcd::mosaic(~date + temp, data = Soybean2)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/soybean_mosaic-1.png" width="672" /></p>
<pre class="r"><code>ggplot(Soybean2, aes(x = date, fill = temp)) + geom_bar()</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/soybean_mosaic-2.png" width="672" /></p>
<pre class="r"><code>Soybean2 %&gt;%
  group_by(date, temp) %&gt;%
  summarise(n = n()) %&gt;%
  group_by(date) %&gt;%
  mutate(pct = n/sum(n)) %&gt;%
  ggplot(aes(x = date, fill = temp, y = pct)) + geom_bar(stat = &quot;identity&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/soybean_mosaic-3.png" width="672" /></p>
<pre class="r"><code>Soybean2 %&gt;%
  group_by(date, temp) %&gt;%
  summarise(n = n()) %&gt;%
  group_by(temp) %&gt;%
  mutate(pct = n/sum(n)) %&gt;%
  ggplot(aes(x = temp, fill = date, y = pct)) + geom_bar(stat = &quot;identity&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/soybean_mosaic-4.png" width="672" /></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Roughly 18% of the data are missing. Are there particular predictors that are more likely to be missing? Is the pattern of missing data related to the classes?</li>
</ol>
<ul>
<li>Higher proportion of missing temperatures in April, although most missing temperatures are in July.</li>
<li>The pattern of missing data by class is shown below</li>
</ul>
<pre class="r"><code>n_nulls &lt;- function(x) sum(is.na(x))
select(Soybean, -Class) %&gt;%
    select_if(negate(is.numeric)) %&gt;%
    summarise_all(funs(n_distinct, n_nulls))  %&gt;%
    gather(key=&quot;key&quot;, value=&quot;value&quot;) %&gt;%
    extract(key, c(&quot;variable&quot;, &quot;metric&quot;), &quot;(.*)_(n_distinct$|n_nulls$)&quot;) %&gt;%
    tidyr::spread(metric, value) %&gt;%
    #filter(variable %in% c(&quot;date&quot;, &quot;precip&quot;, &quot;temp&quot;, &quot;hail&quot;)) %&gt;%
  mutate(n = nrow(Soybean), 
         prop_nulls = n_nulls/n) %&gt;%
  arrange(desc(prop_nulls))</code></pre>
<pre><code>##           variable n_distinct n_nulls   n  prop_nulls
## 1             hail          3     121 683 0.177159590
## 2          lodging          3     121 683 0.177159590
## 3         seed.tmt          4     121 683 0.177159590
## 4            sever          4     121 683 0.177159590
## 5             germ          4     112 683 0.163982430
## 6        leaf.mild          4     108 683 0.158125915
## 7      fruit.spots          5     106 683 0.155197657
## 8  fruiting.bodies          3     106 683 0.155197657
## 9    seed.discolor          3     106 683 0.155197657
## 10      shriveling          3     106 683 0.155197657
## 11     leaf.shread          3     100 683 0.146412884
## 12     mold.growth          3      92 683 0.134699854
## 13            seed          3      92 683 0.134699854
## 14       seed.size          3      92 683 0.134699854
## 15      fruit.pods          5      84 683 0.122986823
## 16       leaf.halo          4      84 683 0.122986823
## 17       leaf.malf          3      84 683 0.122986823
## 18       leaf.marg          4      84 683 0.122986823
## 19       leaf.size          4      84 683 0.122986823
## 20   canker.lesion          5      38 683 0.055636896
## 21       ext.decay          4      38 683 0.055636896
## 22    int.discolor          4      38 683 0.055636896
## 23        mycelium          3      38 683 0.055636896
## 24          precip          4      38 683 0.055636896
## 25       sclerotia          3      38 683 0.055636896
## 26    stem.cankers          5      38 683 0.055636896
## 27     plant.stand          3      36 683 0.052708638
## 28           roots          4      31 683 0.045387994
## 29            temp          4      30 683 0.043923865
## 30       crop.hist          5      16 683 0.023426061
## 31    plant.growth          3      16 683 0.023426061
## 32            stem          3      16 683 0.023426061
## 33        area.dam          5       1 683 0.001464129
## 34            date          8       1 683 0.001464129
## 35          leaves          2       0 683 0.000000000</code></pre>
<pre class="r"><code># Can also use YaleToolkit::whatis function
#YaleToolkit::whatis(select(Soybean, -Class))</code></pre>
<pre class="r"><code>table(Soybean$Class, complete.cases(Soybean))</code></pre>
<pre><code>##                              
##                               FALSE TRUE
##   2-4-d-injury                   16    0
##   alternarialeaf-spot             0   91
##   anthracnose                     0   44
##   bacterial-blight                0   20
##   bacterial-pustule               0   20
##   brown-spot                      0   92
##   brown-stem-rot                  0   44
##   charcoal-rot                    0   20
##   cyst-nematode                  14    0
##   diaporthe-pod-&amp;-stem-blight    15    0
##   diaporthe-stem-canker           0   20
##   downy-mildew                    0   20
##   frog-eye-leaf-spot              0   91
##   herbicide-injury                8    0
##   phyllosticta-leaf-spot          0   20
##   phytophthora-rot               68   20
##   powdery-mildew                  0   20
##   purple-seed-stain               0   20
##   rhizoctonia-root-rot            0   20</code></pre>
<p>Some classes have no complete cases, in particular: 2-4-d-injury, cyst-namatode, diaporthe-pod-&amp;-stem-blight, herbicide-injury and phytophthora-rot. Can check if there are just a few predictors causing this issue.</p>
<pre class="r"><code>n_nulls &lt;- function(x) sum(is.na(x))
Soybean %&gt;%
  filter(Class %in% c(&#39;2-4-d-injury&#39;, &#39;cyst-namatode&#39;,  &#39;diaporthe-pod-&amp;-stem-blight&#39;, &#39;herbicide-injury&#39;, &#39;phytophthora-rot&#39;)) %&gt;%
  group_by(Class) %&gt;%
    select_if(negate(is.numeric)) %&gt;%
    summarise_all(funs(n_nulls, n=n()))  %&gt;%
    gather(key=&quot;key&quot;, value=&quot;value&quot;, -Class) %&gt;%
    extract(key, c(&quot;variable&quot;, &quot;metric&quot;), &quot;(.*)_(n$|n_nulls$)&quot;) %&gt;%
    spread(metric, value) %&gt;%
  mutate(prop_nulls = n_nulls/n) %&gt;%
  select(Class, variable, prop_nulls) %&gt;%
  spread(Class, prop_nulls)</code></pre>
<pre><code>## # A tibble: 35 x 5
##    variable `2-4-d-injury` `diaporthe-pod-… `herbicide-inju…
##    &lt;chr&gt;             &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;
##  1 area.dam         0.0625              0                  0
##  2 canker.…         1                   0                  1
##  3 crop.hi…         1                   0                  0
##  4 date             0.0625              0                  0
##  5 ext.dec…         1                   0                  1
##  6 fruit.p…         1                   0                  0
##  7 fruit.s…         1                   0                  1
##  8 fruitin…         1                   0                  1
##  9 germ             1                   0.4                1
## 10 hail             1                   1                  1
## # … with 25 more rows, and 1 more variable: `phytophthora-rot` &lt;dbl&gt;</code></pre>
<p>This shows that many predictors are completely missing for some cases, e.g. the class 2-4-d-injury has no observations of the predictor canker.lesion, crop.hist, ext.decay, fruit.pots, etc.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Develop a strategy for handling missing data, either by eliminating predictors or imputation.</li>
</ol>
<p>In some cases, 100% of predictor values are missing and so imputation is unlikely to help. Options:</p>
<ul>
<li>Remove cases with high proportion of missing values</li>
<li>Encode missing as another level</li>
</ul>
<pre class="r"><code>dat &lt;- Soybean %&gt;%
  # consider only those that are complete cases
  filter_all(all_vars(!is.na(.))) %&gt;%
  # convert ordered variable to factor  
  mutate_if(is.ordered, function(x) factor(as.character(x)))

# generate binary predictors
dummyInfo &lt;- dummyVars(Class ~ ., data=dat)
dummies &lt;- predict(dummyInfo, dat)</code></pre>
<pre><code>## Warning in model.frame.default(Terms, newdata, na.action = na.action, xlev
## = object$lvls): variable &#39;Class&#39; is not a factor</code></pre>
<pre class="r"><code># Determine which variables are sparse
sparsity &lt;- nearZeroVar(dummies, saveMetrics = TRUE)
head(sparsity)</code></pre>
<pre><code>##        freqRatio percentUnique zeroVar   nzv
## date.0 28.578947     0.3558719   FALSE  TRUE
## date.1 10.019608     0.3558719   FALSE FALSE
## date.2  7.515152     0.3558719   FALSE FALSE
## date.3  5.534884     0.3558719   FALSE FALSE
## date.4  3.762712     0.3558719   FALSE FALSE
## date.5  3.014286     0.3558719   FALSE FALSE</code></pre>
<pre class="r"><code># Number and percentage of predictors to remove
sparsity %&gt;%
  summarise(n = n(), n_nzv = sum(nzv), pct_nzv = mean(nzv))</code></pre>
<pre><code>##    n n_nzv   pct_nzv
## 1 99    19 0.1919192</code></pre>
<p>To bypass the need to remove 19% of predictors, could use models insensitive to sparsity, such as tree- or rule- based models, or naive Bayes.</p>
</div>
<div id="exercise-3" class="section level3">
<h3>Exercise 3</h3>
<p>Chapter 5 introduces Quantitative Structure-Activity Relationship (QSAR) modeling where the characteristics of a chemical compound are used to predict other chemical properties. The caret package contains a QSAR data set from Mente and Lombardo (2005). Here, the ability of a chemical to permeate the blood-brain barrier was experimentally determined for 208 compounds. 134 descriptors were measured for each compound.</p>
<ol style="list-style-type: lower-alpha">
<li>Start R and use these commands to load the data:</li>
</ol>
<pre class="r"><code>#library(caret); library(tidyverse); library(e1071)
data(BloodBrain)
# use ?BloodBrain to see more details</code></pre>
<p>The numeric outcome is contained in the vector logBBB while the predictors are in the data frame bbbDescr.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Do any of the individual predictors have degenerate distributions? i.e., Need to look for: Skewness, outliers, missing values and zero- and near-zero- variance</li>
</ol>
<p>There are <code>ncol(bbbDescr)</code> descriptors and so will not plot all.</p>
<ul>
<li>Skewness is present if |skewness| is large or if the ratio of the maximum to minimum value is greater than 20.<br />
</li>
<li>(Near-)Zero Variance is present if percentage of unique values is <span class="math inline">\(\leq\)</span> 10 and frequency ratio (or most common over second-most common value) is <span class="math inline">\(\geq\)</span> 20.</li>
</ul>
<pre class="r"><code>zeroVar &lt;- function(x) nearZeroVar(x, saveMetrics = TRUE)$zeroVar
nzv &lt;- function(x) nearZeroVar(x, saveMetrics = TRUE)$nzv
freqRatio &lt;- function(x) nearZeroVar(x, saveMetrics = TRUE)$freqRatio
percentUnique &lt;- function(x) nearZeroVar(x, saveMetrics = TRUE)$percentUnique

  
bbbSumm &lt;- bbbDescr %&gt;% 
  summarise_all(funs(skewness, n_distinct, max, min, zeroVar, nzv, freqRatio, percentUnique)) %&gt;%
  gather(key=&quot;key&quot;, value=&quot;value&quot;) %&gt;%
  extract(key, c(&quot;variable&quot;, &quot;metric&quot;), &quot;(.*)_(skewness|n_distinct|max|min|zeroVar|nzv|freqRatio|percentUnique)&quot;) %&gt;%
  spread(metric, value) %&gt;%
  mutate(max_min_ratio = ifelse(min==0, (max+1)/(min+1), max/min)) %&gt;%
  select(variable, n_distinct, skewness, max_min_ratio, zeroVar, nzv, freqRatio, percentUnique, min, max) %&gt;%
  arrange(desc(abs(skewness)))

head(bbbSumm, n = 15)</code></pre>
<pre><code>##        variable n_distinct  skewness max_min_ratio zeroVar nzv  freqRatio
## 1      negative          2 14.214859    2.00000000       0   1 207.000000
## 2         alert          2  9.977967    2.00000000       0   1 103.000000
## 3        a_acid          3  5.400675    4.00000000       0   1  33.500000
## 4      vsa_acid          3  5.400675   41.70076000       0   1  33.500000
## 5          tcpa        189  5.331288   65.76978417       0   0   1.500000
## 6  frac.anion7.         12  4.293466    1.99900000       0   1  47.750000
## 7         wpsa2        206  4.042276   81.88304879       0   0   1.000000
## 8         inthb          2  3.966834    2.00000000       0   0  17.909091
## 9         wnsa2        206 -3.120316    0.01242556       0   0   1.000000
## 10 peoe_vsa.2.1         12  2.950651   45.60254000       0   1  25.571429
## 11     vsa_base          7  2.865661   59.21528000       0   0   4.032258
## 12        ppsa2        206  2.818707   25.94027225       0   0   1.000000
## 13        dpsa2        206  2.705874   20.91858303       0   0   1.000000
## 14 peoe_vsa.3.1         15  2.662580   36.86280000       0   1  21.000000
## 15      vsa_don         11  2.442425   59.91005000       0   0   1.431373
##    percentUnique        min         max
## 1      0.9615385     0.0000     1.00000
## 2      0.9615385     0.0000     1.00000
## 3      1.4423077     0.0000     3.00000
## 4      1.4423077     0.0000    40.70076
## 5     90.8653846     0.0278     1.82840
## 6      5.7692308     0.0000     0.99900
## 7     99.0384615    95.3229  7805.32959
## 8      0.9615385     0.0000     1.00000
## 9     99.0384615 -3199.7271   -39.75840
## 10     5.7692308     0.0000    44.60254
## 11     3.3653846     0.0000    58.21528
## 12    99.0384615   288.9730  7496.03809
## 13    99.0384615   505.2433 10568.97363
## 14     7.2115385     0.0000    35.86280
## 15     5.2884615     0.0000    58.91005</code></pre>
<p>There are a number of variables with near-zero variance:</p>
<p>These variables would be removed for models that are impacted detrimentally by near-zero variance predictors.</p>
<pre class="r"><code>select(bbbDescr, filter(bbbSumm, nzv == 1)$variable) %&gt;%
  gather(Measure, Value) %&gt;%
  
  ggplot() + 
  geom_histogram(aes(x=Value)) +
  facet_wrap(~ Measure)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/bloodBrain_nsv_hist-1.png" width="672" /></p>
<p>There are also a number of skewed variables. Data pre- and post-transformation (Yeo-Johnson) are shown below. May be best to determine manually which to transform.</p>
<pre class="r"><code>bbb_top_vars &lt;- filter(bbbSumm, ! variable %in% filter(bbbSumm, nzv == 1)$variable) %&gt;% 
  top_n(n = 16, wt = abs(skewness)) %&gt;%
  select(variable) %&gt;%
  pull()


bbbTrans &lt;- preProcess(select(bbbDescr, -c(filter(bbbSumm, nzv == 1)$variable)), method = &quot;YeoJohnson&quot;)
bbbTransData &lt;- predict(bbbTrans, select(bbbDescr, -c(filter(bbbSumm, nzv == 1)$variable))) 
dat &lt;- bbbTransData %&gt;%
  select(bbb_top_vars) %&gt;%
  gather(Predictor, Value) %&gt;%
  mutate(Transformation = &quot;Yeo-Johnson&quot;) %&gt;%
  bind_rows(data.frame(gather(select(bbbDescr, bbb_top_vars), Predictor, Value), Transformation = &quot;NA&quot;, stringsAsFactors = FALSE)) 
  
# Final result
res &lt;- list()
# How many points we want 
nPoints &lt;- 1e3

# Using simple loop to scale and calculate density
combinations &lt;- expand.grid(unique(dat$Predictor), unique(dat$Transformation))
for(i in 1:nrow(combinations)) {
    # Subset data
    foo &lt;- subset(dat, Predictor == combinations$Var1[i] &amp; Transformation == combinations$Var2[i])
    # Perform density on scaled signal
    densRes &lt;- density(x = scale(foo$Value), n = nPoints)
    # Position signal from 1 to wanted number of points
    res[[i]] &lt;- data.frame(x = 1:nPoints, y = densRes$y, 
                           pred = combinations$Var1[i], trans = combinations$Var2[i])
}
res &lt;- do.call(rbind, res)
ggplot(res, aes(x / nPoints, y, color = trans, linetype = trans)) +
    geom_line(alpha = 0.5, size = 1) +
    facet_wrap(~ pred, scales = &quot;free&quot;)  +
  labs(x = &quot;Value / Points&quot;, y = &quot;Scaled Density&quot;, colour = &quot;Transformation&quot;, linetype = &quot;Transformation&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/bloodBrain_dens_glimpse-1.png" width="672" /></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Generally speaking, are there strong relationships between the predictor data? If so, how could correlations in the predictor set be reduced? Does this have a dramatic effect on the number of predictors available for modeling?</li>
</ol>
<p>Samples in tails of skewed predictors may have significant effect on correlation structure and so best to review after transformation.</p>
<pre class="r"><code># Correlation plot with no transformation
bbb_cor_raw &lt;- cor(select(bbbDescr, -c(filter(bbbSumm, nzv == 1)$variable)))
corrplot(bbb_cor_raw, order = &quot;hclust&quot;, addgrid.col = NA, tl.pos = &quot;n&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/bloodBrain_correlation_plots-1.png" width="672" /></p>
<pre class="r"><code># Correlation plot of Yeo-Johnson transformed data
bbb_cor_yeo &lt;- cor(bbbTransData)
corrplot(bbb_cor_yeo, order = &quot;hclust&quot;, addgrid.col = NA, tl.pos = &quot;n&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/bloodBrain_correlation_plots-2.png" width="672" /></p>
<pre class="r"><code># Correlation plot of spatial sign transformed data
bbb_cor_sps &lt;- cor(spatialSign(scale(select(bbbDescr, -c(filter(bbbSumm, nzv == 1)$variable)))))
corrplot(bbb_cor_sps, order = &quot;hclust&quot;, addgrid.col = NA, tl.pos = &quot;n&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/bloodBrain_correlation_plots-3.png" width="672" /></p>
<p>This shows that correlations lessen with increasing levels of transformations</p>
<pre class="r"><code>corrInfo &lt;- function(x) summary(x[upper.tri(x)])
corrInfo(bbb_cor_raw)</code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -1.00000 -0.16173  0.06434  0.07068  0.28643  1.00000</code></pre>
<pre class="r"><code>corrInfo(bbb_cor_yeo)</code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.98924 -0.16609  0.05929  0.06798  0.28774  1.00000</code></pre>
<pre class="r"><code>corrInfo(bbb_cor_sps)</code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -1.00000 -0.16062  0.03862  0.05561  0.25545  1.00000</code></pre>
<p>However, rather than transform data (e.g., using PCA), it may be better to remove correlated predictors, using <code>findCorrelation</code> function.</p>
<p>The following chart shows the number of variables that would be removed based on varying correlation thresholds</p>
<pre class="r"><code>thresholds &lt;- seq(.25, .95, by = 0.05)
size &lt;- meanCorr &lt;- rep(NA, length(thresholds))
removals &lt;- vector(mode = &quot;list&quot;, length = length(thresholds))

for(i in seq_along(thresholds)){
  removals[[i]] &lt;- findCorrelation(bbb_cor_raw, thresholds[i]) # removed predictors
  subMat &lt;- bbb_cor_raw[-removals[[i]], -removals[[i]]]       # remaining predictors
  size[i] &lt;- ncol(bbb_cor_raw) -length(removals[[i]])         # number of remaining predictors
  meanCorr[i] &lt;- mean(abs(subMat[upper.tri(subMat)]))         # mean correlation of remaining predictors
}

corrData &lt;- data.frame(value = c(size, meanCorr),
                       threshold = c(thresholds, thresholds),
                       what = rep(c(&quot;Predictors&quot;, 
                                    &quot;Average Absolute Correlation&quot;),
                                  each = length(thresholds))) 

# xyplot(value~ threshold|what, data = corrData, 
#        scales = list(y = list(relation = &quot;free&quot;)),
#        type = c(&quot;p&quot;, &quot;g&quot;, &quot;smooth&quot;),
#        degree = 2,
#        ylab = &quot;&quot;)

ggplot(corrData, aes(x = threshold, y = value)) +
  geom_point() + 
  geom_smooth(method = &quot;loess&quot;, se = FALSE) + 
  facet_wrap(~ what, scales = &quot;free&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/bloodBrain_corr_heuristic-1.png" width="672" /></p>
<p>Other methods to remove correlated variables and search for quality subsets include simulated annealing and genetic algorithms, e.g. via the <code>subselect</code> package. These require that the correlation matrix is well-conditioned, which is possible using the <code>subselect::trim.matrix</code> function. This function is a less greedy method to remove perfect pair-wise correlations and relationships between three or more predictors.</p>
<pre class="r"><code>library(subselect)
ncol(bbb_cor_raw)
## [1] 127
trimmed &lt;- trim.matrix(bbb_cor_raw, tolval=1000*.Machine$double.eps)$trimmedmat
ncol(trimmed)
## [1] 119</code></pre>
<p>The Simulated Annealing algorithm seeks a k-variable subset and requires that the following variables are specified:</p>
<ul>
<li>kmin: Smallest subset that is wanted. Here kmin is set to the number of parameters (18) determined using the <code>caret::findCorrelation</code> solution at a threshold of 40%.</li>
<li>kmax: Largest subset that is wanted (=kmin by default).</li>
</ul>
<pre class="r"><code>set.seed(702)
sa &lt;- anneal(trimmed, kmin = 18, niter = 1000)
saMat &lt;- bbb_cor_raw[sa$bestsets[1,], sa$bestsets[1,]]</code></pre>
<p>The required parameters for the Genetic Algorithm algorithm are the same as for Simulated Annealing.</p>
<pre class="r"><code>set.seed(702)
ga &lt;- genetic(trimmed, kmin = 18, nger = 1000)
gaMat &lt;- bbb_cor_raw[ga$bestsets[1,], ga$bestsets[1,]]</code></pre>
<pre class="r"><code># caret::findCorrelation
fcMat &lt;- bbb_cor_raw[-removals[size == 18][[1]], 
                 -removals[size == 18][[1]]]
corrInfo(fcMat) </code></pre>
<pre><code>##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
## -0.254356 -0.097568  0.005174  0.012444  0.088337  0.380312</code></pre>
<pre class="r"><code># Simulated Annealing
corrInfo(saMat) </code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.95412 -0.11963  0.03580  0.05833  0.22176  0.95622</code></pre>
<pre class="r"><code># Genetic Algorithm
corrInfo(gaMat) </code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.88931 -0.16774  0.03500  0.03328  0.24175  0.92184</code></pre>
<p>The main difference between these results is that the greedy approach of <code>findCorrelation</code> is much more conservative than the techniques found in the <code>subselect</code> package.</p>
</div>
</div>
</div>
<div id="chapter-4-over-fitting-and-model-tuning" class="section level1">
<h1>Chapter 4: Over-Fitting and Model Tuning <a id="tuning"></a></h1>
<p>Aim: Strategies to avoid overfitting so that model can predict new samples with similar degree of accuracy as for training data. Will use model tuning (of model parameters) to maximise model performance. Traditionally, training set to build and tune model and test set to estimate performance. Modern approach is to split data into multiple training and test data sets.</p>
<div id="the-problem-of-over-fitting" class="section level2">
<h2>The Problem of Over-Fitting</h2>
<p>Over-fit model:</p>
<ul>
<li>Has learnt noise of data</li>
<li>Poor accuracy when applied to new data set</li>
</ul>
<p><strong>Apparent Performance</strong>: Error rate of model when applied to the training set</p>
</div>
<div id="model-tuning" class="section level2">
<h2>Model Tuning</h2>
<p><strong>Tuning parameter</strong>: Has no analytical formula to calculate an appropriate value, e.g. k in knn. Often control complexity and so poor choices can lead to over-fitting.</p>
<p>Methods for determining tuning parameters:</p>
<ul>
<li>Parameter Tuning Process:</li>
</ul>
<ol style="list-style-type: decimal">
<li>Define set of candidate values for tuning parameter(s)</li>
<li>For each candidate set:
<ol style="list-style-type: lower-alpha">
<li>Resample data</li>
<li>Fit model</li>
<li>Predictor hold-outs</li>
</ol></li>
<li>Aggregate resampling into performance profile</li>
<li>Determine final tuning parameters</li>
<li>Refit model with entire training set and final tuning parameters</li>
</ol>
<ul>
<li>Genetic Algorithms</li>
<li>Simplex search methods</li>
</ul>
</div>
<div id="data-splitting-single-test-set" class="section level2">
<h2>Data Splitting (Single Test Set)</h2>
<p>Not generally recommended.</p>
<p>Methods:</p>
<ul>
<li>Nonrandom; appropriate for example, if need to test on a different sample population, point in time or new phenomenon</li>
<li>Simple random sample</li>
<li>Stratified random sampling; apply to disproportionate classes to obtain similar distribution between training and test sets</li>
<li>Maximum dissimilarity sampling; distance between predictor values for two samples</li>
<li>Initialise with single sample</li>
<li>The second sample is chosen as the most dissimilar to the first</li>
<li>The third and subsequent samples are most dissimilar to the <strong>group</strong> of prior samples (e.g. average or minimum of the dissimilarity)</li>
</ul>
<p>Variants:</p>
<ul>
<li>Use stratified random sampling to select <span class="math inline">\(k\)</span> partitions so that the folds are balanced with respect to the outcome.</li>
<li>Leave-one-out cross-validation (LOOCV), <span class="math inline">\(k\)</span> = # samples</li>
<li>Repeated <span class="math inline">\(k\)</span>-fold cross-validation; can increase precision while maintaining small bias</li>
</ul>
<p>Choice of <span class="math inline">\(k\)</span> has a bias-variance trade-off;</p>
<p><strong>Bias</strong>: difference between estimated and true values of performance</p>
<p><strong>variance</strong>: Repeating the resampling procedure produces a very different value</p>
<p>Small <span class="math inline">\(k\)</span>, e.g. 2-3: Decreased variance, increased bias. (Bias is similar to bootstrap but with larger variance) <br> Large <span class="math inline">\(k\)</span>: Increased variance, decreased bias. More computationally taxing.</p>
<p>Typically use <span class="math inline">\(k\)</span> = 5 or 10.</p>
</div>
<div id="resampling-techniques" class="section level2">
<h2>Resampling Techniques</h2>
<div id="k-fold-cross-validation" class="section level3">
<h3>k-Fold Cross-Validation</h3>
<p>Method:</p>
<ul>
<li>Partition sample into <span class="math inline">\(k\)</span> sets (folds) of roughly equal size.</li>
<li>Repeat for each fold:</li>
<li>Fit model to all samples except the fold <span class="math inline">\(i\)</span></li>
<li>Predict held-out samples to estimate performance measures</li>
<li>Summarise <span class="math inline">\(k\)</span> resampled estimates of performance, e.g. mean, std. error to understand relationship between tuning parameters and model utility</li>
</ul>
</div>
<div id="generalised-cross-validation" class="section level3">
<h3>Generalised Cross-Validation</h3>
<p>For linear regression, can use generalised cross-validation (GCV) statistic to approximate the leave-one-out error rate, <span class="math display">\[\text{GCV} = \frac{1}{n}\sum^n_{i=1}\left(\frac{y_i - \hat{y}_i}{1-\text{df}/n}\right)^2\]</span> Models with similar precision but higher complexity will have a larger GCV value.</p>
</div>
<div id="repeated-trainingtest-splits-a.k.a-leave-group-out-cv-or-monte-carlo-cv" class="section level3">
<h3>Repeated Training/Test Splits (a.k.a Leave-Group-Out CV or Monte Carlo CV)</h3>
<p>Practitioner determines percent split between training (75-80%) and test, in addition to the number of repetitions (50-200). Can increase proportion of data in training set with increased repetitions in order to decrease bias. Increasing repetitions will decrease uncertainty of performance estimates.<br />
Unlike in <span class="math inline">\(k\)</span>-fold cross validation, a sample can be in multiple hold-out sets.</p>
</div>
<div id="the-bootstrap" class="section level3">
<h3>The Bootstrap</h3>
<p><strong>Bootstrap Sample</strong>: Random sample of data taken with replacement. Bootstrap sample is same size as original data set. Prediction is made on the <strong>out-of-bag samples</strong>, i.e. those samples not selected.</p>
<p>In comparison to cross-validation, bootstrap technique has:</p>
<ul>
<li>Less uncertainty than <span class="math inline">\(k\)</span>-fold cv</li>
<li>More bias (similar to 2-fold CV); bias will decrease as training set sample size becomes large (on average 63.2% of data points represented at least once)</li>
</ul>
<p>Modifications:</p>
<ul>
<li>632 method: combined bootstrap estimate and estimate from re-predicting the training set (the apparent error rate), error rate = .632 x simple bootstrap estimate + 0.368 x apparent error rate</li>
</ul>
</div>
</div>
<div id="case-study-credit-scoring" class="section level2">
<h2>Case Study: Credit Scoring</h2>
<p>Aim: predict probability that applicants have good credit <br> Dataset: German credit data set <br> Samples: 1000 <br> Class distribution: Good(70%), Bad(30%) <br> Baseline accuracy: Predict all to be good, such that accuracy is 70% <br> Predictors: Credit history, employment, account status, loan amount. Majority categorical and so converted to dummy variables. After conversion, there were 41 predictors.</p>
</div>
<div id="choosing-final-tuning-parameters" class="section level2">
<h2>Choosing Final Tuning Parameters</h2>
<p>Approach to choose final settings:</p>
<ul>
<li>Numerically optimal: Pick those with numerically best performance estimates. Disadvantage: May lead to choice of an overly complicated model.</li>
<li>One-standard error method: Find the numerically optical model then the simplest model whose performance is within a single standard error.</li>
<li>Tolerance method: Choose simpler model that has performance (<span class="math inline">\(X\)</span>) within tolerance of best (<span class="math inline">\(O\)</span>), i.e. such that <span class="math display">\[(X-O)/O &gt; \text{Tolerance}\]</span> e.g. if best accuracy is 75% then with 4% loss in accuracy acceptable, then can choose accuracy, <span class="math inline">\(X\)</span>, at <span class="math inline">\(O(1 + \text{Tolerance}) = .75*(1 - .04) = 0.72\)</span>.</li>
</ul>
</div>
<div id="data-splitting-recommendations" class="section level2">
<h2>Data Splitting Recommendations</h2>
<p>Strong case to use resampling</p>
<ul>
<li>Single test set has limited ability to characterise uncertainty</li>
<li>Proportionally large test sets increase bias</li>
<li>With small sample sizes, test set uncertainty can be considerably large</li>
</ul>
<p>No resampling method is uniformly better.</p>
<ul>
<li>Small sample size; recommend 10-fold cross-validation</li>
<li>Large sample size; difference becomes less pronounced and computational efficiency increases. Again 10-fold cross-validation recommended</li>
<li>To choose between models as opposed to getting best indicator of performance, consider bootstrap procedures as have low variance</li>
</ul>
<p>There may be some (often negligible) bias for final model chosen based on tuning parameter with smallest error rate.</p>
</div>
<div id="choosing-between-models" class="section level2">
<h2>Choosing Between Models</h2>
<p>Once tuning parameters chosen for each model, how do we determine between multiple models?</p>
<p>Approaches:</p>
<ul>
<li>Determine performance ceiling, then simplify</li>
<li>Start with least interpretable and most flexible model, e.g. boosted trees or support vector machines, to determine performance ceiling</li>
<li>Investigate simpler models that are less opaque, e.g. MARS, partial least squares, gams or naive Bayes</li>
<li>Choose the simplest model that reasonable approximates the performance ceiling</li>
<li>Use resampling results to compare models</li>
<li>Use identically resampled data sets</li>
<li>Use statistical methods for pair comparisons, e.g. use paired t-test to evaluate that models have equivalent accuracy or determine mean difference in accuracy and associated confidence interval</li>
</ul>
</div>
<div id="computing-1" class="section level2">
<h2>Computing</h2>
<div id="data-splitting" class="section level3">
<h3>Data Splitting</h3>
<p>The following functions can be used to partition the data:</p>
<ul>
<li><code>caret::sample</code>: simple random split</li>
<li><code>caret::createDataPartition</code>: stratified random split (based on the classes)</li>
<li><code>caret::maxdissim</code>: maximum dissimilarity sampling</li>
</ul>
<p>The following code performs stratified random splitting:</p>
<pre class="r"><code>set.seed(1)
trainingRows &lt;- createDataPartition(classes, p = 0.80, list = FALSE)
# training data
trainPredictors &lt;- predictors[trainingRows, ]
trainClasses &lt;- classes[trainingRows]
# test data
testPredictors &lt;- predictors[-trainingRows, ]
testClasses &lt;- classes[-trainingRows]
str(trainPredictors); str(testPredictors)</code></pre>
<pre><code>## &#39;data.frame&#39;:    167 obs. of  2 variables:
##  $ PredictorA: num  0.1582 0.6552 0.706 0.0658 0.3086 ...
##  $ PredictorB: num  0.161 0.492 0.633 0.179 0.28 ...</code></pre>
<pre><code>## &#39;data.frame&#39;:    41 obs. of  2 variables:
##  $ PredictorA: num  0.1992 0.3952 0.425 0.0847 0.2909 ...
##  $ PredictorB: num  0.0881 0.4152 0.2988 0.0548 0.3021 ...</code></pre>
</div>
<div id="resampling" class="section level3">
<h3>Resampling</h3>
<p>The following functions can be used to resample the data:</p>
<ul>
<li><code>caret::createDataPartition</code>: Repeated training/test splits, utilises the <code>times</code> argument</li>
<li><code>caret::createResamples</code>: Bootstrap samples</li>
<li><code>caret::createFolds</code>: k-Fold cross-validation</li>
<li><code>caret::createMultiFolds</code>: Repeated cross-validation</li>
</ul>
<p>The following code generates three resampled versions of the training set</p>
<pre class="r"><code>set.seed(1)
repeatedSplits &lt;- createDataPartition(trainClasses, p = 0.8, times = 3)
str(repeatedSplits)</code></pre>
<pre><code>## List of 3
##  $ Resample1: int [1:135] 1 2 3 4 6 7 9 10 11 12 ...
##  $ Resample2: int [1:135] 1 2 3 4 5 6 7 9 10 11 ...
##  $ Resample3: int [1:135] 1 2 3 4 5 7 8 9 11 12 ...</code></pre>
<p>The following code creates indicators for 10-fold cross-validation, where each fold is <span class="math inline">\((k-1)/k%\)</span> of the data.</p>
<pre class="r"><code>set.seed(1)
cvSplits &lt;- createFolds(trainClasses, k = 10, returnTrain = TRUE)
str(cvSplits)</code></pre>
<pre><code>## List of 10
##  $ Fold01: int [1:150] 1 2 4 5 6 7 8 10 11 13 ...
##  $ Fold02: int [1:150] 1 2 3 4 6 7 8 9 10 11 ...
##  $ Fold03: int [1:150] 1 3 4 5 6 7 8 9 10 11 ...
##  $ Fold04: int [1:150] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold05: int [1:150] 2 3 4 5 6 7 8 9 10 11 ...
##  $ Fold06: int [1:150] 1 2 3 4 5 6 7 8 9 11 ...
##  $ Fold07: int [1:150] 1 2 3 4 5 6 7 9 10 12 ...
##  $ Fold08: int [1:151] 1 2 3 4 5 6 8 9 10 11 ...
##  $ Fold09: int [1:151] 1 2 3 5 6 7 8 9 10 11 ...
##  $ Fold10: int [1:151] 1 2 3 4 5 7 8 9 10 11 ...</code></pre>
<pre class="r"><code>fold1 &lt;- cvSplits[[1]]
cvPredictors1 &lt;- trainPredictors[fold1, ]
cvClasses1 &lt;- trainClasses[fold1]
scales::percent(nrow(cvPredictors1)/nrow(trainPredictors))</code></pre>
<pre><code>## [1] &quot;89.8%&quot;</code></pre>
</div>
<div id="basic-model-building-in-r" class="section level3">
<h3>Basic Model Building in R</h3>
<p>To fit a 5-nearest neighbour classification model to the training data can use:</p>
<ul>
<li><code>MASS:knn</code></li>
<li><code>ipred:ipredknn</code></li>
<li><code>caret::knn3</code></li>
</ul>
<p>Conventions for specifying models:</p>
<ol style="list-style-type: decimal">
<li>Formula interface. e.g. <code>modelFunction(response_var ~ predictors, data = data)</code> <br>Advantages: Convenient; can specify transformations choose subset(s) of predictors in-line <br>Disadvantage: Formula information not efficiently stored and can slow down computations with a large number of predictors</li>
<li>Matrix (non-formula) interface, e.g.<code>modelFunction(x = predictors_matrix, y = response_var_vector)</code>. The predictors are typically a matrix or a dataframe.</li>
</ol>
<p>Not all R functions have both interfaces.</p>
<p>The following code estimated 5-nn using <code>caret:knn3</code>.</p>
<pre class="r"><code># Fit model
trainPredictros &lt;- as.matrix(trainPredictors)
knnFit &lt;- knn3(x = trainPredictors, y = as.factor(trainClasses), k = 5)
knnFit</code></pre>
<pre><code>## 5-nearest neighbor model
## Training set outcome distribution:
## 
## Class1 Class2 
##     89     78</code></pre>
<pre class="r"><code>testPredictions &lt;- predict(knnFit, newdata = testPredictors, type  = &quot;class&quot;)
head(testPredictions)</code></pre>
<pre><code>## [1] Class2 Class1 Class1 Class2 Class1 Class2
## Levels: Class1 Class2</code></pre>
<div id="determination-of-tuning-parameters" class="section level4">
<h4>Determination of Tuning Parameters</h4>
<p>R Packages / Functions:</p>
<ul>
<li>e1071::tune - can evaluate 4 types of models across a range of parameters</li>
<li>ipred::errorest can resample single models</li>
<li>caret::train - built in modules for 144 models with capability for different resampling methods, performance measures, and algorithms for choosing best model profile.</li>
</ul>
<p>SVM tuning parameters:</p>
<ul>
<li>Cost parameter</li>
<li>Kernel parameter <span class="math inline">\(\sigma\)</span> which impacts smoothness of the decision boundary of the kernel (which may be linear, radial basis, …)</li>
</ul>
<p>Options:</p>
<ol style="list-style-type: decimal">
<li>Try several combinations of both tuning parameters</li>
<li>There is an analytical formula for a reasonable estimate of <span class="math inline">\(\sigma\)</span>, use this and only tune the cost parameter.</li>
</ol>
<pre class="r"><code>data(GermanCredit)

GermanCredit &lt;- GermanCredit[, -nearZeroVar(GermanCredit)]
GermanCredit$CheckingAccountStatus.lt.0 &lt;- NULL
GermanCredit$SavingsAccountBonds.lt.100 &lt;- NULL
GermanCredit$EmploymentDuration.lt.1 &lt;- NULL
GermanCredit$EmploymentDuration.Unemployed &lt;- NULL
GermanCredit$Personal.Male.Married.Widowed &lt;- NULL
GermanCredit$Property.Unknown &lt;- NULL
GermanCredit$Housing.ForFree &lt;- NULL


## Split the data into training (80%) and test sets (20%)
set.seed(100)
inTrain &lt;- createDataPartition(GermanCredit$Class, p = .8)[[1]]
GermanCreditTrain &lt;- GermanCredit[ inTrain, ]
GermanCreditTest  &lt;- GermanCredit[-inTrain, ]

# Basic call with 
set.seed(1056)
svmFit &lt;- train(Class ~ .,
    data = GermanCreditTrain,
    method = &quot;svmRadial&quot;,
    preProc = c(&quot;center&quot;, &quot;scale&quot;),
    tuneLength = 10,  
    trControl = trainControl(method = &quot;repeatedcv&quot;,   # default is bootstrap
                             repeats = 5,
                             classProbs = TRUE))
svmFit

rm(inTrain)





svmFit &lt;- train(Class ~ .,
    data = GermanCreditTrain,
    method = &quot;svmRadial&quot;,
    preProc = c(&quot;center&quot;, &quot;scale&quot;),
    tuneLength = 10,  
    trControl = trainControl(method = &quot;cv&quot;,   # default is bootstrap
                             number = 2,
                             classProbs = TRUE))</code></pre>
<pre><code>## Support Vector Machines with Radial Basis Function Kernel 
## 
## 800 samples
##  41 predictor
##   2 classes: &#39;Bad&#39;, &#39;Good&#39; 
## 
## Pre-processing: centered (41), scaled (41) 
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 720, 720, 720, 720, 720, 720, ... 
## Resampling results across tuning parameters:
## 
##   C       Accuracy  Kappa    
##     0.25  0.74950   0.3639204
##     0.50  0.75075   0.3707124
##     1.00  0.75900   0.3723114
##     2.00  0.76375   0.3744096
##     4.00  0.75875   0.3470181
##     8.00  0.75425   0.3381316
##    16.00  0.74900   0.3217375
##    32.00  0.74400   0.3002030
##    64.00  0.74200   0.2972872
##   128.00  0.73575   0.2794031
## 
## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.01478465
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were sigma = 0.01478465 and C = 2.</code></pre>
<p>Line plot of the average performance profile of the SVM classification model:</p>
<pre class="r"><code>plot(svmFit, scales = list(x = list(log = 2)))</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/plotGermanCreditSVMTuneFit-1.png" width="672" /></p>
<p>To predict new samples:</p>
<pre class="r"><code>predictedClasses &lt;- predict(svmFit, GermanCreditTest)
str(predictedClasses)
rm(predictedClasses)</code></pre>
<pre><code>##  Factor w/ 2 levels &quot;Bad&quot;,&quot;Good&quot;: 2 2 2 2 2 2 2 2 1 2 ...</code></pre>
<p>Class probabilities</p>
<pre class="r"><code>predictedProbs &lt;- predict(svmFit, newdata = GermanCreditTest, type = &quot;prob&quot;)
head(predictedProbs)
rm(predictedProbs)</code></pre>
<pre><code>##          Bad      Good
## 1 0.40170150 0.5982985
## 2 0.09469587 0.9053041
## 3 0.12603008 0.8739699
## 4 0.29888025 0.7011197
## 5 0.35828615 0.6417139
## 6 0.08472826 0.9152717</code></pre>
<p>Other R packages/functions to estimate performance via resampling:</p>
<ul>
<li>Design::validate</li>
<li>ipred::errorest</li>
<li>e1071::tune</li>
</ul>
<div id="between-model-comparisons" class="section level5">
<h5>Between-Model Comparisons</h5>
<p>Logistic regression has no tuning parameters, but resampling can still be used to characterise the performance of the model.</p>
<pre class="r"><code>set.seed(1056)
lrFit &lt;- train(Class ~ .,
    data = GermanCreditTrain,
    method = &quot;glm&quot;,
    trControl = trainControl(method = &quot;cv&quot;,  
                             number = 2))



lrFit</code></pre>
<pre><code>## Generalized Linear Model 
## 
## 800 samples
##  41 predictor
##   2 classes: &#39;Bad&#39;, &#39;Good&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (2 fold) 
## Summary of sample sizes: 400, 400 
## Resampling results:
## 
##   Accuracy  Kappa    
##   0.735     0.3243985</code></pre>
<ul>
<li>Use <code>resamples</code> function to compare models based on cross-validation statistics</li>
<li>Use same seed so get same resamples as per SVM and therefore paired accuracy measurements exist.</li>
<li>First, create <code>resamples</code> object from the models</li>
</ul>
<pre class="r"><code>resamp &lt;- resamples(list(SVM = svmFit, Logistic = lrFit))
summary(resamp)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = resamp)
## 
## Models: SVM, Logistic 
## Number of resamples: 2 
## 
## Accuracy 
##           Min.  1st Qu.  Median    Mean  3rd Qu.   Max. NA&#39;s
## SVM      0.740 0.745625 0.75125 0.75125 0.756875 0.7625    0
## Logistic 0.735 0.735000 0.73500 0.73500 0.735000 0.7350    0
## 
## Kappa 
##               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## SVM      0.2896175 0.2999596 0.3103017 0.3103017 0.3206438 0.3309859    0
## Logistic 0.2679558 0.2961771 0.3243985 0.3243985 0.3526198 0.3808411    0</code></pre>
<p>Visualisation of paired values:</p>
<pre class="r"><code>p1 &lt;- xyplot(resamp)
p2 &lt;- parallelplot(resamp)
p3 &lt;- splom(resamp)
p4 &lt;- densityplot(resamp)
p5 &lt;- bwplot(resamp)
p6 &lt;- dotplot(resamp)
p7 &lt;- ggplot(resamp)
gridExtra::grid.arrange(p1, p3, p2, p4, p5, p6, p7, ncol = 2)</code></pre>
<pre><code>## Warning: Removed 1 rows containing missing values (geom_errorbar).</code></pre>
<pre class="r"><code>rm(list = ls(pattern = &quot;^p\\d$&quot;)) </code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/modelComparisonResamplePlot-1.png" width="768" /></p>
<p>To assess possible differences:</p>
<pre class="r"><code>modelDifferences &lt;- diff(resamp)
summary(modelDifferences)</code></pre>
<pre><code>## 
## Call:
## summary.diff.resamples(object = modelDifferences)
## 
## p-value adjustment: bonferroni 
## Upper diagonal: estimates of the difference
## Lower diagonal: p-value for H0: difference = 0
## 
## Accuracy 
##          SVM    Logistic
## SVM             0.01625 
## Logistic 0.3855         
## 
## Kappa 
##          SVM    Logistic
## SVM             -0.0141 
## Logistic 0.8849</code></pre>
<p>The p-values for the model comparisons are 0.386 for Accuracy and 0.885 for Kappa. Note that the Kappa statistic (chapter 11:) was originally designed to assess the agreement between 2 raters <span class="math display">\[\text{Kappa} = \frac{O-E}{1-E}\]</span> where <span class="math inline">\(O\)</span> = observed accuracy and <span class="math inline">\(E\)</span> is expected accuracy based on marginal totals of the confusion matrix. Statistic takes values between -1 and 1;</p>
<ul>
<li>0 :no agreement between observed and predicted</li>
<li>1: perfect concordance</li>
<li>-1: prediction is perfect opposite direction of truth</li>
</ul>
<p>When class distributions equivalent, overall accuracy and Kappa are proportional. Kappa values within 0.30 to 0.5 indicate reasonable agreement.</p>
<p>With high accuracy (90%) and high expected accuracy (85%), Kappa will show moderate agreement between observed and predicted classes <span class="math display">\[\text{Kappa} = \frac{90-85}{1-85} = 5/15 = 30%\]</span></p>
<p>Kappa statistic can be extended to evaluate concordance in problems with &gt; 2 classes. With ordinal classes, can use weighted Kappa to enact more substantial penalties to errors further away from true result.</p>
</div>
</div>
</div>
</div>
<div id="chapter-4-exercises" class="section level2">
<h2>Chapter 4 Exercises</h2>
<div id="exercise-4.1" class="section level3">
<h3>Exercise 4.1</h3>
<p>Consider the music genre data set described in Sect. 1.4. The objective for these data is to use the predictors to classify music samples into the appropriate music genre. (a) What data splitting method(s) would you use for these data? Explain. (b) Using tools described in this chapter, provide code for implementing your approach(es).</p>
</div>
<div id="exercise-4.2" class="section level3">
<h3>Exercise 4.2</h3>
<p>Consider the permeability data set described in Sect. 1.4. The objective for these data is to use the predictors to model compounds’ permeability. (a) What data splitting method(s) would you use for these data? Explain. (b) Using tools described in this chapter, provide code for implementing your approach(es).</p>
</div>
<div id="exercise-4.3" class="section level3">
<h3>Exercise 4.3</h3>
<p>Partial least squares (Sect. 6.3) was used to model the yield of a chemical manufacturing process (Sect. 1.4).</p>
<p>The objective of this analysis is to find the number of PLS components that yields the optimal R2 value (Sect. 5.1). PLS models with 1 through 10 components were each evaluated using five repeats of 10-fold cross-validation and the results are presented in the following table: (a) Using the “one-standard error” method, what number of PLS components provides the most parsimonious model? (b) Compute the tolerance values for this example. If a 10% loss in R2 is acceptable, then what is the optimal number of PLS components? (c) Several other models (discussed in Part II) with varying degrees of complexity were trained and tuned and the results are presented in Fig. 4.13. If the goal is to select the model that optimizes R2, then which model(s) would you choose, and why? (d) Prediction time, as well as model complexity (Sect. 4.8) are other factors to consider when selecting the optimal model(s). Given each model’s prediction time, model complexity, and R2 estimates, which model(s) would you choose, and why?</p>
<pre class="r"><code>library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)</code></pre>
</div>
<div id="exercise-4.4" class="section level3">
<h3>Exercise 4.4</h3>
<p>Brodnjak-Vonina et al. (2005) develop a methodology for food laboratories to determine the type of oil from a sample. In their procedure, they used a gas chromatograph (an instrument that separate chemicals in a sample) to measure seven different fatty acids in an oil. These measurements would then be used to predict the type of oil in a food samples. To create their model, they used 96 samples2 of seven types of oils. These data can be found in the caret package using data(oil). The oil types are contained in a factor variable called oilType. The types are pumpkin (coded as A), sunflower (B), peanut (C), olive (D), soybean (E), rapeseed (F) and corn (G).</p>
<ol style="list-style-type: lower-alpha">
<li>Use the sample function in base R to create a completely random sample of 60 oils. How closely do the frequencies of the random sample match the original samples? Repeat this procedure several times of understand the variation in the sampling process.</li>
<li><p>Use the caret package function createDataPartition to create a stratified random sample. How does this compare to the completely random samples?</p></li>
<li>With such a small samples size, what are the options for determining performance of the model? Should a test set be used?</li>
<li><p>One method for understanding the uncertainty of a test set is to use a confidence interval. To obtain a confidence interval for the overall accuracy, the based R function binom.test can be used. It requires the user to input the number of samples and the number correctly classified to calculate the interval. For example, suppose a test set sample of 20 oil samples was set aside and 76 were used for model training. The width of the 95% confidence interval is 37.9%. Try different samples sizes and accuracy rates to understand the trade-off between the uncertainty in the results, the model performance, and the test set size.</p></li>
</ol>
</div>
</div>
</div>
<div id="measuring-performance-in-regression-models" class="section level1">
<h1>Measuring Performance in Regression Models</h1>
<ul>
<li>Relying on single performance measure problematic</li>
<li>Use of visualisations critical</li>
</ul>
<div id="quantitative-measures-of-performance" class="section level2">
<h2>Quantitative Measures of Performance</h2>
<ul>
<li><strong>RMSE</strong>: How far (on average) residuals are from zero <span class="math inline">\(\equiv\)</span> average distance between observed values and model predictions</li>
<li><strong>Coefficient of Determination (<span class="math inline">\(R^2\)</span>)</strong>: Proportion of variation in data explained by model; usually equal to squared value of correlation between observed and predicted values.
<ul>
<li>Measure of correlation not accuracy. Looking at plot of observed vs predicted values may show that is under- or over- predicts for subsets of the data</li>
<li>Dependent on sample variance (and therefore unit of measurement) of response variable , e.g. with RMSE = 1 and <span class="math inline">\(s^2 = 4.2\)</span> gives <span class="math inline">\(R^2 = 1 - RMSE/s^2\)</span> = 76.2%, but with <span class="math inline">\(s^2 = 3\)</span>, the RSE is 50.0%</li>
</ul></li>
<li><strong>Spearman’s Rank Correlation</strong>: Correlation coefficient between ranks of observed and predicted outcomes, when focus is on ranking ability of model rather than predictive accuracy.</li>
</ul>
</div>
<div id="variance-bias-trade-off" class="section level2">
<h2>Variance-Bias Trade-Off</h2>
<p><span class="math display">\[\begin{aligned}
\text{MSE} &amp;= \frac{1}{n}\sum_{i = 1}^n (y_i - \hat{y}_i)^2 \\
  E(\text{MSE}) &amp;= \sigma^2 (\text{Model Bias})^2 + \text{Model Variance}
\end{aligned}\]</span></p>
<ul>
<li>Irreducible noise (<span class="math inline">\(\sigma^2\)</span>); cannot be eliminated by modelling. Based on assumption that <span class="math inline">\(e_i \sim ?(0, \sigma^2)\)</span></li>
<li>Bias(-squared): Closeness of model to true relationship between predictors and response</li>
<li>Model Variance: Changes to model fit with small perturbations in data</li>
</ul>
<p><strong>Overfitting</strong>: Complex models usually have low bias and high variance</p>
<p><strong>Underfitting</strong>: Simple models generally have high bias and low variance</p>
<p>Note: Highly correlated predictors can lead to collinearity and therefore high model variance. To mitigate, may be ideal to increase bias.</p>
<div id="computing-2" class="section level3">
<h3>Computing</h3>
<pre class="r"><code>observed &lt;- c(0.22, 0.83, -0.12, 0.89, -0.23, -1.30, -0.15, -1.4,0.62, 0.99, -0.18, 0.32, 0.34, -0.30, 0.04, -0.87,
0.55, -1.30, -1.15, 0.20)
predicted &lt;- c(0.24, 0.78, -0.66, 0.53, 0.70, -0.75, -0.41, -0.43,0.49, 0.79, -1.19, 0.06, 0.75, -0.07, 0.43, -0.42, -0.25, -0.64, -1.26, -0.07)

residualValues &lt;- observed - predicted
summary(residualValues)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -0.9700 -0.4200  0.0800 -0.0310  0.2625  1.0100</code></pre>
<p>Plot of observed vs predicted values: to understand how model fits</p>
<pre class="r"><code>ggplot(data.frame(observed, predicted), aes(observed, predicted)) + 
  geom_point() + 
  expand_limits(
    x =  c(min(observed, predicted), max(observed, predicted)),
    y = c(min(observed, predicted), max(observed, predicted))) + 
  geom_abline(intercept = 0, slope = 1)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/perf_plot_obs_pred-1.png" width="672" /></p>
<p>Plot of residuals vs predicted values: to check for any systematic patterns in predictions</p>
<pre class="r"><code>ggplot(data.frame(predicted, residualValues), aes(predicted, residualValues)) + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 0)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/perf_plot_pred_resid-1.png" width="672" /></p>
<p>Coefficient of Determination:</p>
<pre class="r"><code>caret::R2(predicted, observed)
cor(predicted, observed)^2</code></pre>
<pre><code>## [1] 0.5170123
## [1] 0.5170123</code></pre>
<p>Root Mean Square Error:</p>
<pre class="r"><code>caret::RMSE(predicted, observed)
sqrt(mean(residualValues^2))</code></pre>
<pre><code>## [1] 0.5234883
## [1] 0.5234883</code></pre>
<p>Rank Correlation:</p>
<pre class="r"><code>cor(predicted, observed, method = &quot;spearman&quot;)</code></pre>
<pre><code>## [1] 0.7554552</code></pre>
</div>
</div>
</div>
<div id="linear-regression-and-its-cousins" class="section level1">
<h1>Linear Regression and Its Cousins</h1>
<p>Form: <span class="math display">\[y_i = b_0 + b_1x_{i1} + b_2x_{i2} + \ldots + b_P x_{iP} + e_i\]</span></p>
<p>Models that are linear in the parameters:</p>
<ul>
<li>Linear regression</li>
<li>Partial least squares (PLS)</li>
<li>Penalised models:
<ul>
<li>Ridge regression</li>
<li>The lasso</li>
<li>Elastic net</li>
</ul></li>
</ul>
<p>Modelling objective: Estimate parameters such that SSE minimised.</p>
<ul>
<li>Linear regression - minimises bias, higher model variance</li>
<li>Penalised model - lower model variance</li>
</ul>
<p>Advantage of linear models:</p>
<ul>
<li>Interpretable</li>
<li>Relationships between predictors can be interpreted through coefficients ???</li>
<li>Can compute standard errors of coefficients (if willing to make assumptions regarding the distribution of model residuals) and therefore assess statistical significance of the predictors. However inference is not goal of this book, but rather model prediction</li>
</ul>
<p>Disadvantage:</p>
<ul>
<li>Only appropriate when relationship between predictors and response falls along a hyperplane; although can augment with higher order terms to capture curvature but may not be appropriate (in which case, nonlinear regression models and regression trees should be investigated)</li>
</ul>
<div id="linear-regression" class="section level2">
<h2>Linear Regression</h2>
<p>Aim: Find plane that minimizes SSE between observed and predicted response: <span class="math display">\[\text{SSE} = \sum^n_{i=1}(y_i-\hat{y}_i)^2\]</span></p>
<p>Ordinary Least Squares: Optimal plane that minimises the bias component of the bias-variance trade off, given by: <span class="math display">\[(X^T X)^{-1} X^T y\]</span></p>
<p>For proof and details around following theorem, see <a href="https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf">OLS in Matrix Form, by Prof. Michael J Rosenfeld (Stanford University)</a></p>
<p>Gauss-Markov Theorem: Given the following assumptions, there will be no other linear and unbiased estimated of the coefficients that has a smaller sampling variance. I.e., the OLS estimates or the Best Linear, Unbiased and Efficient estimated (BLUE). Assumptions:</p>
<ul>
<li>There is a linear relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(X\)</span></li>
<li>Identification condition: There is no perfect multicollinearities, i.e. the columns of <span class="math inline">\(X\)</span> are linearly independent</li>
<li>Zero conditional mean assumption: Disturbances average out to 0 for any value of <span class="math inline">\(X\)</span>.<br />
<span class="math display">\[E(\epsilon|X) = 0\]</span> Implies that <span class="math inline">\(E(y) = X\beta\)</span> and therefore that get the mean function right</li>
<li>Assumption of homoskedasticity and no autocorrelation, <span class="math inline">\(E(\epsilon \epsilon^\prime|X) = \sigma^2I\)</span>. Homoskedasticity states that the variance of <span class="math inline">\(\epsilon_i\)</span> is the same (<span class="math inline">\(\sigma^2\)</span>) for all <span class="math inline">\(i\)</span>, i.e., <span class="math inline">\(\text{var}[\epsilon_i|X] = \sigma^2 \forall i\)</span>. The assumption of no autocorrelation (uncorrelated errors) means that <span class="math inline">\(\text{cov}[\epsilon_i, \epsilon_j|X] = 0 \forall i,j\)</span>, i.e. knowing something about the residual term for one observation tells nothing about the residual term for any other observation.</li>
<li><span class="math inline">\(X\)</span> must be generated by a mechanism that is unrelated to <span class="math inline">\(\epsilon\)</span></li>
<li>Not required but typically assumed to make hypothesis testing easier: <span class="math display">\[\epsilon|X \sim N(0, \sigma^2I)\]</span></li>
</ul>
<p>Training and validation techniques required to understand predictive ability of data, even though there are no tuning parameters.</p>
<p>Disadvantages of linear regression:</p>
<div id="requires-the-following-conditions-for-unique-set-of-regression-coefficient" class="section level5">
<h5>1. Requires the following conditions for unique set of regression coefficient,</h5>
<p>noting that optimal plane includes the term <span class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span>)</p>
<ol style="list-style-type: lower-alpha">
<li>No perfect multicollinearities (no predictor can be determined from a combination of <span class="math inline">\(\geq\)</span> ` other predictors). If not, can:</li>
</ol>
<ul>
<li>Replace <span class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span> with conditional inverse (???)</li>
<li>Remove predictors that are collinear. By default, <span class="math inline">\(R\)</span> fits the largest identifiable model by removing variables the reverse order of appearance in the model formula.</li>
</ul>
<p>Impact: Regression coefficients to determine predictions are not unique and therefore lose ability to meaningfully interpret coefficients.</p>
<p>Solutions:</p>
<ul>
<li>With small number of predictors, can manually remove offending predictors. But with many predictors or those with complex relationship, may be better to choose a model that can tolerate collinearity. Additionally, linear combinations of predictors may still be correlated with other predictors</li>
<li>Use PCA, however new predictors will be linear combinations of original predictors which can impede practical understanding. Note that this is known as principal component regression (PCR); see below..</li>
</ul>
<p>b.# samples &gt; # predictors. If not,</p>
<ul>
<li>Use pre-processing techniques to remove pairwise correlated predictors and therefore reduce number of overall predictors</li>
<li><p>If problem remains, consider PCA, simultaneous dimension reduction, regression via PLS or methods that shrink parameter estimates (ridge regression, the lasso, or the elastic net).</p></li>
<li><p>Beware with Bootstrapping/cross-validation: Resampling may lead to less observations in training data than number of predictors.</p></li>
</ul>
<p>Note: After removing pairwise correlated predictors, regression may still be impacted by multicollinearity (which is when one ore more of the predictors are functions of <span class="math inline">\(\geq\)</span> 2 other predictors).</p>
<p>The article <a href="http://blog.minitab.com/blog/adventures-in-statistics-2/what-are-the-effects-of-multicollinearity-and-when-can-i-ignore-them">What Are the Effects of Multicollinearity and When Can I Ignore Them?</a> states the following:</p>
<ul>
<li>Moderate multicollinearity may not be problematic</li>
<li>Severe multicollinearity is because it can increase the variance of the coefficient estimates and make the estimates very sensitive to minor changes in the model. This makes the coefficient estimates unstable and difficult to interpret. It can cause the coefficients to switch signs and makes it more difficult to specify the correct model.</li>
</ul>
<p>In short the article states that, multicollinearity</p>
<ul>
<li>Can make choosing the correct predictors more difficult</li>
<li>Interferes in determining precise effect of each predictors</li>
<li>Doesn’t affect overall fit of the model or produce bad predictions. (so depending on goals, multicollinearity may not be a problem)</li>
</ul>
<p>Multicollinearity can be diagnosed by investigating the variance inflation factors. High VIFs (&gt;5) suggest that coefficients are poorly estimated and that one should be wary of their p-values.</p>
<p>Note:Higher-order terms (squared and cubed predictors) are correlated with main effect terms because they include the main effects term. To reduce high VIFs produced by interaction and higher-order terms, you can standardise the continuous predictor variables. Centering the variables will remove the multicollinearity produced by interaction and higher-order terms and has the added benefit of not changing the interpretation of coefficients; the coefficients continue to estimate the change in the mean response per unit increase in <span class="math inline">\(X\)</span> when all other predictors are held constant.</p>
</div>
<div id="solution-is-linear-in-parameters-and-therefore-does-not-account-for-curvature-or-nonlinear-structures." class="section level5">
<h5>2. Solution is linear in parameters and therefore does not account for curvature or nonlinear structures.</h5>
<p>To identify, examine diagnostic plots, looking for curvature in predicted-versus-residual plot. Can then include quadratic , cubic and interactions, but may make data matrix have more predictors than observations and matrix won’t be invertible.</p>
</div>
<div id="impacted-by-outliers-influencers." class="section level5">
<h5>3. Impacted by “outliers” (<em>influencers</em>).</h5>
<p>OLS aims to minimise SSE; outliers will have exponentially large residuals and so linear regression will adjust parameter estimates to better accommodate the outliers.</p>
<p>Robust regression has been developed to address these kind of issues:</p>
<ul>
<li>Use alternative metric to SSE that is less sensitive to large outliers, e.g. find parameter estimates that minimise the sum of absolute errors, or use Huber function that uses squared residuals when they are small and the residuals themselves when above a threshold.</li>
</ul>
</div>
</div>
<div id="principal-component-regression" class="section level2">
<h2>Principal Component Regression</h2>
<p>Two step regression:</p>
<ol style="list-style-type: decimal">
<li>Dimension reduction via PCA</li>
<li>Regression on PCA components</li>
</ol>
<p>Advantages:</p>
<ul>
<li>Components uncorrelated</li>
<li>If variability in predictor space related to response variability then PCR has good change of identifying predictive relationship</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>Not recommended; use instead PLS when there are correlated predictors and a linear regression-type solution desired</li>
<li>New predictors will be linear combinations of original predictors which can impede practical understanding</li>
<li>Components may not explain response variable; i.e. components are not selected with response in mind.</li>
</ul>
</div>
<div id="partial-least-squares" class="section level2">
<h2>Partial Least Squares</h2>
<p>Based on nonlinear iterative partial least squares (NIPALS) algorithm:</p>
<p>Iteratively seeks to find underlying, or latent, relationships among predictors which are highly correlated with response. For univariate response, each iteration:</p>
<ul>
<li>Assesses relationship between predictors <span class="math inline">\(\mathbf{X}\)</span> and response <span class="math inline">\(\mathbf{y}\)</span> and summarises with a vector of weights <span class="math inline">\(\mathbf{w}\)</span> (directions)</li>
<li>Orthogonally projects predictor data onto the direction to generate scores <span class="math inline">\(\mathbf{t}\)</span></li>
<li>Scores used to generate loadings <span class="math inline">\(\mathbf{p}\)</span>, which measure the correlation of the score vector to the original predictors</li>
<li>Deflates predictors and the response by subtracting current estimate of predictor and response, respectively. These are then used to generate next set of weights, scores and loadings.</li>
</ul>
<table>
<colgroup>
<col width="44%" />
<col width="55%" />
</colgroup>
<thead>
<tr class="header">
<th>PLSR</th>
<th>PCR</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simultaneously summarises variation of predictors while being optimally correlated with the outcome</td>
<td>Finds linear combinations of predictors to maximally summarise predictor space variability</td>
</tr>
<tr class="even">
<td>Supervised dimension reduction</td>
<td>Unsupervised procedure</td>
</tr>
</tbody>
</table>
<p>Preprocessing required for PLS: Centre and scale; otherwise will be draw towards predictors with large variation</p>
<p>Tuning parameters: Number of components to retain; can be determined using resampling techniques.</p>
<p>Variable importance in the projection for <span class="math inline">\(j\)</span>th predictor: <span class="math display">\[\text{VIP} = \sqrt{\frac{\sum^h_{k=1}R^2(y,t_k)(w_{kj}/||w_k||)^2}{(1/p)\sum^h_{k=1}R^2(y,t_k)}},\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(p\)</span> = # predictor variables</li>
<li><span class="math inline">\(h\)</span> = # Components</li>
<li><span class="math inline">\(w_{kj}\)</span> = Weight of <span class="math inline">\(j\)</span>th predictor variable in component <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(R^2(y,t_k)\)</span> = response variation explained by component <span class="math inline">\(k\)</span></li>
<li>Numerator: Weighted sum of normalised weights corresponding to the <span class="math inline">\(j\)</span>th predictor, where the <span class="math inline">\(j\)</span>th normalised weight is scaled by the amount of variation in the response explained by the <span class="math inline">\(k\)</span>th component</li>
<li>Denominator: Total amount of response variation explained by all <span class="math inline">\(k\)</span> components</li>
<li>The larger the normalised weight and amount of response variation explained by the component, the more important the predictor</li>
</ul>
<p>Rule of thumb: Variables with VIP &gt; 1 contain predictive information. Variables with small PLS regression coefficients and small VIP values not likely important and should be considered as candidates for removal from the model</p>
<div id="algorithmic-variations-of-pls" class="section level3">
<h3>Algorithmic Variations of PLS</h3>
<div id="alternatives-to-address-computational-inefficiencies" class="section level4">
<h4>Alternatives to address computational inefficiencies:</h4>
<p>With <span class="math inline">\(\geq\)</span> 2500 samples or <span class="math inline">\(\geq\)</span> 20 predictors NIPALS becomes inefficient. As both predictor matrix and response deflated, an <span class="math inline">\(n \times P\)</span> matrix and an <span class="math inline">\(n \times 1\)</span> vector must be recomputed, operated on, and stored in each iteration</p>
<p>Alternative approaches:</p>
<ul>
<li>kernel matrix approach</li>
<li>SIMPLS</li>
<li>etc</li>
</ul>
</div>
<div id="alternatives-to-consider-nonlinearity" class="section level4">
<h4>Alternatives to consider nonlinearity</h4>
<p>Recall: PLS components summarise data through linear hyperplanes of original predictor space. Can allow for non-linearity by:</p>
<ul>
<li>Augmenting predictors with squared / cubic predictors. Note: no need to add cross product terms</li>
<li>Using GIFI approach which bins predictors thought to have a nonlinear approach (based on prior knowledge or characteristics of data.</li>
</ul>
<p>However: Other predictive modeling technique can more naturally identify nonlinear structures (and are recommended) without having to modify predictor space.</p>
</div>
</div>
</div>
<div id="penalised-a.ka.a-shrinkage-models" class="section level2">
<h2>Penalised (a.ka.a Shrinkage) Models</h2>
<p>OLS coefficients unbiased and of all unbiased linear techniques will have lowest variance. However may be possible to produce model with smaller SME by sacrificing some bias for a much more substantial drop in variance. Predictors with large correlations -&gt; large variance, can combat collinearity using biased models.</p>
<div id="ridge-regression" class="section level3">
<h3>Ridge Regression</h3>
<p>To create bias, add penalty to SSE if estimates become large.</p>
<p><span class="math display">\[\text{SSE}_{L_2} = \sum^n_{i=1}(y_i-\hat{y}_i)^2 + \lambda \sum^P_{j=1}\beta_j^2\]</span></p>
<p><span class="math inline">\(L_2\)</span>: Square (second-order penalty) being used; parameter estimates can only get large if proportional reduction in SSE. Method shrinks estimates towards 0 as <span class="math inline">\(\lambda\)</span> penalty becomes large. Cross validation is used to optimise the penalty value.</p>
<p>Model does not perform feature selection; i.e. no parameter estimate will be set to 0</p>
<p>Plots:</p>
<ol style="list-style-type: decimal">
<li><p>Penalty vs standardized coefficients; as the penalty increases the standardized coefficients tend towards zero</p></li>
<li><p>Fraction of full solution vs standardized coefficients.</p></li>
</ol>
<p>Fraction of full solution: Amount that the ridge regression coefficient estimates have been shrunken towards zero; small value indicates they have been shrunken very close to zero. <span class="math display">\[||\hat{\beta}_\lambda^R||/||\hat{\beta}||_2\]</span> where:</p>
<ul>
<li><span class="math inline">\(\hat{\beta}_\lambda^R\)</span> = ridge regression coefficients</li>
<li><span class="math inline">\(\hat{\beta}\)</span> =least squares coefficient estimates</li>
<li><span class="math inline">\(||\beta||_2 = \sqrt{\sum^P_{j=1}\beta_j^2}\)</span> is the <span class="math inline">\(\ell_2\)</span> norm, measures the distance of <span class="math inline">\(\beta\)</span> from zero.</li>
</ul>
<p>As <span class="math inline">\(\lambda\)</span> increases, the <span class="math inline">\(\ell_2\)</span> norm of <span class="math inline">\(\hat{\beta}_\lambda^R\)</span> will decrease and so will the fraction of the full solution.</p>
<p>The fraction of full solution ranges form 1 (when <span class="math inline">\(\lambda = 0\)</span> when ridge regression coefficient = OLS) to 0 (when <span class="math inline">\(\lambda = \infty\)</span>)</p>
</div>
<div id="lasso-least-absolute-shrinkage-and-selection-operator-model" class="section level3">
<h3>Lasso (Least Absolute Shrinkage and Selection Operator Model)</h3>
<p><span class="math display">\[\text{SSE}_{L_1} = \sum^n_{i=1}(y_i-\hat{y}_i)^2 + \lambda \sum^P_{j=1}|\beta_j|\]</span></p>
<p>Lasso Coefficient Path: Trace plot showing the relationship between the penalty and the standardised coefficients; as the penalty</p>
<p>The lasso has been extended to:</p>
<ul>
<li>Linear discriminant analysis</li>
<li>PLS</li>
<li>PCA</li>
</ul>
</div>
<div id="lasso-vs-ridge" class="section level3">
<h3>Lasso vs Ridge</h3>
<table>
<colgroup>
<col width="46%" />
<col width="53%" />
</colgroup>
<thead>
<tr class="header">
<th>Ridge</th>
<th>Lasso</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>L2 Norm</td>
<td>L1 Norm</td>
</tr>
<tr class="even">
<td>Coefficients are not set to zero</td>
<td>Performs features selection</td>
</tr>
<tr class="odd">
<td>Shrinks coefficients of correlated predicted towards each other</td>
<td>Tends to pick one of the correlated predictors and ignore the rest</td>
</tr>
</tbody>
</table>
<p>Why Lasso Results in Feature Selection, but Not Ridge:</p>
<p>Optimisation formulations:</p>
<p>Ridge: <span class="math display">\[\matrix{\text{minimize}\\\beta} \left\{ \sum^n_{i=1} \left(y_i - \beta_0 - \sum^p_{j=1}\beta_j x_{ij}\right)^2 \right\} \;\;\;\text{subject to }\sum^p_{j=1}\beta_j^2 \leq s\]</span></p>
<p>Lasso: <span class="math display">\[\matrix{\text{minimize}\\\beta} \left\{ \sum^n_{i=1} \left(y_i - \beta_0 - \sum^p_{j=1}\beta_j x_{ij}\right)^2 \right\} \;\;\;\text{subject to }\sum^p_{j=1}|\beta_j| \leq s\]</span></p>
<p>i.e., <span class="math inline">\(\forall \lambda\)</span> there is a value of <span class="math inline">\(s\)</span> that will give the same ridge/lasso coefficient estimates.</p>
<p>When <span class="math inline">\(p = 2\)</span>,</p>
<ul>
<li>Ridge: smallest RSS out of all points lies within diamond defined by <span class="math inline">\(|\beta_1| + |\beta_2| \leq s\)</span>.</li>
<li>Ridge: smallest RSS out of all points lies within circle defined by <span class="math inline">\(\beta^2_1 + \beta^2_2 \leq s\)</span>.</li>
</ul>
<p>When <span class="math inline">\(s\)</span> is large then constraint region is also large and so coefficient estimates can be large.</p>
<p>In the following chart:</p>
<ul>
<li><span class="math inline">\(\hat{\beta}\)</span> is the least squared solution</li>
<li>The blue circle is the ridge regression constraint</li>
<li>The black diamond is the lasso regression constraint</li>
<li>Each quadrant relates to a different regression problem</li>
<li>The ellipses in each quadrant are the contours of the residual sum of squares (RSS); all points on a given ellipse share a common value of the RSS. As the ellipses expand away from <span class="math inline">\(\hat{\beta}\)</span>, the RSS increases.</li>
<li>The lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region.</li>
</ul>
<p>Ridge regression constraint is circular and no sharp points and so intersection will not generally occur on an axis.<br />
The lasso constraint has corners and so will often intersect at the corner (and therefore one of the coefficients will equal zero).</p>
<div class="figure">
<img src="LassoVsRidge.JPG" alt="Lasso/ Ridge Optimisation" />
<p class="caption">Lasso/ Ridge Optimisation</p>
</div>
<p>Further reasoning as to why Lasso leads to feature selection, as per <a href="https://stats.stackexchange.com/questions/74542/why-does-the-lasso-provide-variable-selection/74569">Cross Validated, Why does the Lasso provide Variable Selection?</a>:</p>
<p>Consider the model <span class="math inline">\(y = \beta x + e\)</span>, with an L1 penalty on <span class="math inline">\(\hat{\beta}\)</span> and a least-squares loss function on <span class="math inline">\(\hat{e}\)</span>. We can expand the expression to be minimized as:</p>
<p><span class="math display">\[\min y^Ty -2 y^Tx\hat{\beta} + \hat{\beta} x^Tx\hat{\beta} + 2\lambda|\hat{\beta}|\]</span></p>
<p>Assume the least-squares solution is some <span class="math inline">\(\hat{\beta} &gt; 0\)</span>, which is equivalent to assuming that <span class="math inline">\(y^Tx &gt; 0\)</span>, and see what happens when we add the L1 penalty. With <span class="math inline">\(\hat{\beta}&gt;0\)</span>, <span class="math inline">\(|\hat{\beta}| = \hat{\beta}\)</span>, so the penalty term is equal to <span class="math inline">\(2\lambda\beta\)</span>. The derivative of the objective function w.r.t. <span class="math inline">\(\hat{\beta}\)</span> is:</p>
<p><span class="math display">\[-2y^Tx +2x^Tx\hat{\beta} + 2\lambda\]</span></p>
<p>which evidently has solution <span class="math inline">\(\hat{\beta} = (y^Tx - \lambda)/(x^Tx)\)</span>.</p>
<p>Obviously by increasing <span class="math inline">\(\lambda\)</span> we can drive <span class="math inline">\(\hat{\beta}\)</span> to zero (at <span class="math inline">\(\lambda = y^Tx\)</span>). However, once <span class="math inline">\(\hat{\beta} = 0\)</span>, increasing <span class="math inline">\(\lambda\)</span> won’t drive it negative, because, writing loosely, the instant <span class="math inline">\(\hat{\beta}\)</span> becomes negative, the derivative of the objective function changes to:</p>
<p><span class="math display">\[-2y^Tx +2x^Tx\hat{\beta} - 2\lambda\]</span></p>
<p>where the flip in the sign of <span class="math inline">\(\lambda\)</span> is due to the absolute value nature of the penalty term; with <span class="math inline">\(\hat{\beta}&lt; 0\)</span>, <span class="math inline">\(|\hat{\beta}|\neq \hat{\beta}\)</span>. As the derivative is w.r.t. to <span class="math inline">\(\hat{\beta}\)</span> the sign flips on that term; not on the <span class="math inline">\(-2y^Tx\hat{\beta}\)</span> term because that term includes <span class="math inline">\(\hat{\beta}\)</span> not <span class="math inline">\(|\hat{\beta}|\)</span>.</p>
<p>So when <span class="math inline">\(\beta\)</span> becomes negative, the penalty term becomes equal to <span class="math inline">\(-2\lambda\beta\)</span>, and taking the derivative w.r.t. <span class="math inline">\(\beta\)</span> results in <span class="math inline">\(-2\lambda\)</span>. This leads to the solution <span class="math inline">\(\hat{\beta} = (y^Tx + \lambda)/(x^Tx)\)</span>, which is obviously inconsistent with <span class="math inline">\(\hat{\beta} &lt; 0\)</span> (given that the least squares solution <span class="math inline">\(&gt; 0\)</span>, which implies <span class="math inline">\(y^Tx &gt; 0\)</span>, and <span class="math inline">\(\lambda &gt; 0\)</span>). There is an increase in the L1 penalty AND an increase in the squared error term (as we are moving farther from the least squares solution) when moving <span class="math inline">\(\hat{\beta}\)</span> from <span class="math inline">\(0\)</span> to $ &lt; 0$, so we don’t, we just stick at <span class="math inline">\(\hat{\beta}=0\)</span>.</p>
<p>It should be intuitively clear the same logic applies, with appropriate sign changes, for a least squares solution with <span class="math inline">\(\hat{\beta} &lt; 0\)</span>.</p>
<p>With the least squares penalty <span class="math inline">\(\lambda\hat{\beta}^2\)</span>, however, the derivative becomes:</p>
<p><span class="math display">\[-2y^Tx +2x^Tx\hat{\beta} + 2\lambda\hat{\beta}\]</span></p>
<p>which evidently has solution <span class="math inline">\(\hat{\beta} = y^Tx/(x^Tx + \lambda)\)</span>. Obviously no increase in <span class="math inline">\(\lambda\)</span> will drive this all the way to zero. So the L2 penalty can’t act as a variable selection tool without some mild ad-hockery such as “set the parameter estimate equal to zero if it is less than <span class="math inline">\(\epsilon\)</span>”.</p>
<p>Obviously things can change when you move to multivariate models, for example, moving one parameter estimate around might force another one to change sign, but the general principle is the same: the L2 penalty function can’t get you all the way to zero, because, writing very heuristically, it in effect adds to the “denominator” of the expression for <span class="math inline">\(\hat{\beta}\)</span>, but the L1 penalty function can, because it in effect adds to the “numerator”.</p>
<p>Source:</p>
<ul>
<li>An Introduction to Statistical Learning with Applications in R, Casella, Fienberg and Olkin</li>
<li><a href="https://towardsdatascience.com/regularization-in-machine-learning-connecting-the-dots-c6e030bfaddd">Regularization in Machine Learning: Connect the dots</a></li>
<li><a href="https://stats.stackexchange.com/questions/74542/why-does-the-lasso-provide-variable-selection">Cross Validated, Why does the Lasso provide Variable Selection?</a></li>
</ul>
</div>
<div id="least-angle-regression-lars" class="section level3">
<h3>Least Angle Regression (LARS)</h3>
<p>Broad framework (that envelopes lasso) that can be used to fit lasso models more efficiently</p>
</div>
<div id="elastic-net" class="section level3">
<h3>Elastic Net</h3>
<p>Combines ridge and lasso: <span class="math display">\[\text{SSE}_{\text{E}_\text{net}} = \sum^n_{i=1}(y_i-\hat{y}_i)^2 + \lambda_1 \sum^P_{j=1}\beta^2_j + \lambda_2 \sum^P_{j=1}|\beta_j|\]</span></p>
<p>Advantage:</p>
<ul>
<li>Enables regularisation via ridge-type penalty</li>
<li>Feature selection via lasso penalty</li>
</ul>
<p>Both penalties require tuning.</p>
</div>
</div>
<div id="computing---case-study-quantitative-structure-activity-relationship-modeling" class="section level2">
<h2>Computing - Case Study: Quantitative Structure-Activity Relationship Modeling</h2>
<p>Solubility Data:</p>
<ul>
<li>Predictors for test and training test sets in data frames called <code>solTrainX</code> and <code>solTestX</code></li>
<li>Alternative versions that have been Box-Cox transformed are in the data frames <code>solTrainXTrans</code> and <code>solTestXTrans</code></li>
<li><p>Solubility (response value) for each compound contained in <code>solTrainY</code> and <code>solTestY</code></p></li>
<li>Each column of data corresponds to a predictor and the rows correspond to compounds. There are 228 columns in the data. Random sample of column names below:
<ul>
<li>FP085</li>
<li>FP207</li>
<li>FP198</li>
<li>FP006</li>
<li>FP160</li>
<li>FP136</li>
<li>FP017</li>
<li>FP093</li>
</ul></li>
</ul>
<p>Predictors:</p>
<ul>
<li>208 binary descriptors</li>
<li>16 count descriptors</li>
<li>4 continuous descriptors</li>
</ul>
<p>Sample size: 1,267</p>
<ul>
<li><p>Data has been split into training and test sets:</p>
<ul>
<li>Training Set (n = 951
<ul>
<li>To tune and estimate models</li>
<li>To determine initial estimates of performance using repeated 10-fold cross-validation</li>
</ul></li>
<li>Test set n = 316): For final characterisation of models of interest</li>
</ul></li>
</ul>
<p><strong>Data Exploration</strong></p>
<pre class="r"><code>res &lt;- cor(rbind(solTrainX, solTestX))
res[lower.tri(res, diag = TRUE)] &lt;- NA
res &lt;- as.data.frame(res) %&gt;%
    rownames_to_column(var = &quot;VariableA&quot;) %&gt;%
    gather(key = &quot;VariableB&quot;, value = &quot;Correlation&quot;, -VariableA) %&gt;%
    filter(!is.na(Correlation)) %&gt;%
    mutate(AbsCorrelation = abs(Correlation)) %&gt;%
    arrange(desc(AbsCorrelation))

# sum(res$Correlation &gt; 0.9)</code></pre>
<p>On average, descriptors are uncorrelated (average absolute correlation = 0.14), however there are 47 pairs of descriptors with a correlation greater than 0.9. One pair of descriptors (SurfaceArea1 and SurfaceArea2) are identical for 87.0% of compounds, but small differences can contain certain information for prediction.</p>
<p>The relationship between Solubility and</p>
<ol style="list-style-type: lower-alpha">
<li>Molecular Weight; as molecular weight increases, solubility generally decreases</li>
<li>FP100</li>
</ol>
<pre class="r"><code>library(lattice)
library(corrplot)

### Some initial plots of the data

p1 &lt;- xyplot(solTrainY ~ solTrainX$MolWeight, type = c(&quot;p&quot;, &quot;g&quot;),
       ylab = &quot;Solubility (log)&quot;,
       main = &quot;(a)&quot;,
       xlab = &quot;Molecular Weight&quot;)
# xyplot(solTrainY ~ solTrainX$NumRotBonds, type = c(&quot;p&quot;, &quot;g&quot;),
#        ylab = &quot;Solubility (log)&quot;,
#        xlab = &quot;Number of Rotatable Bonds&quot;)
p2 &lt;- bwplot(solTrainY ~ ifelse(solTrainX[,100] == 1, 
                          &quot;structure present&quot;, 
                          &quot;structure absent&quot;),
       ylab = &quot;Solubility (log)&quot;,
       main = &quot;(b)&quot;,
       horizontal = FALSE)
gridExtra::grid.arrange(p1, p2, nrow = 1)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/solubilityRelationshipEgs-1.png" width="672" /></p>
<pre class="r"><code>rm(p1, p2)</code></pre>
<p>Skewness; recall that variables are considered skewed if max/min &gt; 20 or if |skewness statistic| much greater than zero.</p>
<pre class="r"><code>res &lt;- solTrainX %&gt;%
  select(-starts_with(&quot;FP&quot;)) %&gt;%
  summarise_all(funs(e1071::skewness, n_distinct, max, min)) %&gt;%
  gather(key=&quot;key&quot;, value=&quot;value&quot;) %&gt;%
  extract(key, c(&quot;variable&quot;, &quot;metric&quot;), &quot;(.*)_(e1071::skewness|n_distinct|max|min)&quot;) %&gt;%
    mutate(metric = ifelse(metric == &quot;e1071::skewness&quot;, &quot;skewness&quot;, metric)) %&gt;%
  spread(metric, value) %&gt;%
  mutate(max_min_ratio = ifelse(min==0, (max+1)/(min+1), max/min)) %&gt;%
  select(variable, n_distinct, skewness, max_min_ratio, min, max) %&gt;%
    mutate(AbsSkewness = abs(skewness)) </code></pre>
<p>The mean skewness of continuous variables in training set was 1.6 (minimum 0.7), maximum 3.8), indicating variables have tendency to be right-skewed.</p>
<p>Box-Cox transformation were therefore applied to all predictors.</p>
<p>To check for linear relationships between predictors and solubility, scatter plots with regression line used which suggests both linear, e.g. MolWeight, and non-linear, e.g., NumChlorine, relationships exist (may want to include some quadratic terms)</p>
<pre class="r"><code>featurePlot(solTrainXtrans[, -Fingerprints],
            solTrainY,
            between = list(x = 1, y = 1),
            type = c(&quot;g&quot;, &quot;p&quot;, &quot;smooth&quot;),
            labels = rep(&quot;&quot;, 2))</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/solubilityScatters-1.png" width="672" /></p>
<p>To check for significant between-predictor correlations, PCA used on full set of transformed predictors.</p>
<!--Errata: Figure 6.4, not cumulative-->
<pre class="r"><code>solPCA &lt;- prcomp(solTrainXtrans, center = TRUE, scale = TRUE)
#pctVar &lt;-  solPCA$sdev^2/sum(solPCA$sdev^2)*100
#head(pctVar)

tab &lt;- summary(solPCA)$importance[2,]
tab &lt;- data.frame(PC = names(tab), Var = tab, row.names=NULL, stringsAsFactors = FALSE) %&gt;% mutate(PC=parse_number(PC))


ggplot(tab, aes(x = PC, y=Var))+ 
         geom_line() + 
    scale_y_continuous(name = &quot;% Variance&quot;, labels = scales::percent) +
    xlab(&quot;Components&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/solubilityPCA-1.png" width="672" /></p>
<pre class="r"><code>rm(tab, solPCA)</code></pre>
<p>Scree plot shows that amount of variability drops sharply, with no one component accounting for more than 13% of variance. This indicates that the structure of the data is contained in a much smaller number of dimensions than the number of dimensions in the original space; often due to large number of collinearities in the predictors.<br />
Collinearity can create problems in developing some models (e.g. linear regression) and will require appropriate pre-processing steps.</p>
<p>The correlation structure is shown for the transformed continuous predictor and shows many strong positive correlations.</p>
<pre class="r"><code>### Full namespace used to call this function because the pls
### package (also used in this chapter) has a function with the same
### name.
corrplot::corrplot(cor(solTrainXtrans[, -Fingerprints]), 
                   order = &quot;hclust&quot;, 
                   tl.cex = .8)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/solubilityCorrplot-1.png" width="672" /></p>
<div id="ordinary-linear-regression" class="section level3">
<h3>Ordinary Linear Regression</h3>
<ul>
<li>Standard function <code>lm</code> requires predictors and response variable in same dataframe</li>
</ul>
<pre class="r"><code># all predictors
trainingData &lt;- solTrainXtrans
trainingData$Solubility &lt;- solTrainY
lmFitAllPredictors &lt;- lm(Solubility ~ ., data = trainingData)
#summary(lmFitAllPredictors)
# RMSE
#sqrt(mean(lmFitAllPredictors$residuals^2))

# R^2
#summary(lmFitAllPredictors)$r.squared</code></pre>
<p>Training set performance:</p>
<ul>
<li>RMSE = 0.481</li>
<li><span class="math inline">\(R^2\)</span> = 0.945</li>
</ul>
<p>Can predict new samples using <code>predict</code> function:</p>
<pre class="r"><code>lmPred1 &lt;- predict(lmFitAllPredictors, solTestXtrans)
head(lmPred1)</code></pre>
<pre><code>##          20          21          23          25          28          31 
##  0.99370933  0.06834627 -0.69877632  0.84796356 -0.16578324  1.40815083</code></pre>
<p>Test set performance estimated using <code>caret::defaultSummary</code>:</p>
<pre class="r"><code>lmValues1 &lt;- data.frame(obs = solTestY, pred = lmPred1)
defaultSummary(lmValues1)</code></pre>
<pre><code>##      RMSE  Rsquared       MAE 
## 0.7455802 0.8722236 0.5497605</code></pre>
<p><strong>Resampling estimate of performance</strong></p>
<ul>
<li>With ~1000 training samples, should be sufficient to use 10-fold cross-validation as opposed to repeated cross-validation in order to obtain reasonable estimates of model performance. Use the function <code>trainControl</code> to specify type of resampling:</li>
</ul>
<pre class="r"><code>### Create a control function that will be used across models. We
### create the fold assignments explicitly instead of relying on the random number seed being set to identical values.
set.seed(100)
indx &lt;- createFolds(solTrainY, returnTrain = TRUE)
ctrl &lt;- trainControl(method = &quot;cv&quot;, index = indx)
rm(indx)

#ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 10)

set.seed(100)
lmFit1 &lt;- train(x = solTrainXtrans, y = solTrainY, method = &quot;lm&quot;, trControl = ctrl)</code></pre>
<pre><code>## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient
## fit may be misleading

## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient
## fit may be misleading</code></pre>
<pre class="r"><code>lmFit1</code></pre>
<pre><code>## Linear Regression 
## 
## 951 samples
## 228 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 856, 855, 857, 856, 856, 855, ... 
## Resampling results:
## 
##   RMSE       Rsquared   MAE      
##   0.7170016  0.8792751  0.5298775
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<ul>
<li>For models built to explain, it is important to check model assumptions, such as the residual distribution.</li>
<li>For predictive models, some of the same diagnostic techniques can highlight where model is not predicting well.</li>
</ul>
<p>Plot of residuals vs predicted values:</p>
<pre class="r"><code>## plot the points (type = &#39;p&#39;) and a background grid (&#39;g&#39;)
xyplot(resid(lmFit1) ~ predict(lmFit1), type = c(&quot;p&quot;, &quot;g&quot;), xlab = &quot;Predicted&quot;, ylab = &quot;Residuals&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/plotlmSolTrainResidPred-1.png" width="672" /></p>
<ul>
<li>A random cloud of points should provide comfort that no major terms (e.g.. quadratics) missing from model</li>
<li><code>resid</code> function generates model residuals for training set</li>
<li><code>predict</code> function without additional dataset generates predicted values for training set</li>
</ul>
<p>Predicted vs observed values to assess how close predictions are to the actual values:</p>
<pre class="r"><code>xyplot(solTrainY ~ predict(lmFit1), type = c(&quot;p&quot;, &quot;g&quot;), xlab = &quot;Predicted&quot;, ylab = &quot;Observed&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/plotlmSolTrainObsPred-1.png" width="672" /></p>
<p>Basic diagnostic tests do not suggest any bias in prediction; distribution between predictive values and residuals appear to be random about zero.</p>
<p>Can remove highly correlated predictors and build smaller model:</p>
<p>Identify predictors that have high pairwise correlation and remove predictors so that no absolute pairwise correlation is greater than some pre-specified level (e.g. 0.9). Note: In practice the threshold would need to be be smaller , however in this instance it would also remove important variables. The section on Feature Selection will further discuss this data and how to investigate how terms fit into the model, interactions and nonlinear transformations.</p>
<!-- # Errata here: SolTrainY in book not solTestY -->
<pre class="r"><code>### Linear regression model with all of the predictors. This will produce some warnings that a &#39;rank-deficient fit may be  misleading&#39;. This is related to the predictors being so highly correlated that some of the math has broken down.
# set.seed(100)
# lmTune0 &lt;- train(x = solTrainXtrans, y = solTrainY,
#                 method = &quot;lm&quot;,
#                 trControl = ctrl)
# lmTune0 

### Now use a set of predictors reduced by unsupervised filtering; apply a filter to reduce extreme between-predictor correlations and note the lack of warnings.

corThresh &lt;- .9
tooHigh &lt;- findCorrelation(cor(solTrainXtrans), corThresh)
corrPred &lt;- names(solTrainXtrans)[tooHigh]
trainXfiltered &lt;- solTrainXtrans[, -tooHigh]
testXfiltered &lt;- solTestXtrans[, -tooHigh]

set.seed(100)
lmFiltered &lt;- train(trainXfiltered, solTrainY, method = &quot;lm&quot;, trControl = ctrl)
lmFiltered

# To obtain training R2
lmValuesF &lt;- data.frame(obs = solTrainY, pred = lmFiltered$finalModel$fitted.values)

rm(tooHigh, trainXfiltered, testXfiltered, corrPred, corThresh)</code></pre>
<pre><code>## Linear Regression 
## 
## 951 samples
## 190 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 856, 855, 857, 856, 856, 855, ... 
## Resampling results:
## 
##   RMSE       Rsquared   MAE    
##   0.7112743  0.8805826  0.53382
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<table>
<thead>
<tr class="header">
<th>Data</th>
<th>RMSE</th>
<th><span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Training (All predictors)</td>
<td>0.481</td>
<td>0.945</td>
</tr>
<tr class="even">
<td>Test (All predictors)</td>
<td>0.746</td>
<td>0.872</td>
</tr>
<tr class="odd">
<td>Train (Filtered)</td>
<td>0.525</td>
<td>0.934</td>
</tr>
<tr class="even">
<td>Test (Filtered)</td>
<td>0.711</td>
<td>0.881</td>
</tr>
</tbody>
</table>
<p><strong>Robust Linear Regression Model</strong>, <code>MASS::rlm</code> uses Huber function by default.</p>
<pre class="r"><code># converged in 18 iterations
rlmFitAllPredictors &lt;- MASS::rlm(Solubility ~ ., data = trainingData)
rm(rlmFitAllPredictors)</code></pre>
<p>However, it is recommended to first ensure that the covariance matrix is not singular, as <code>rlm</code> does not allow singularity.</p>
<p>Note that MASS::rlm solution converges without pre-processing of data, but caret::train with method = rlm does not converge. <strong>Question</strong>: Why not? Perhaps because performing 10-fold CV?</p>
<pre class="r"><code>set.seed(100)
rlmPCA &lt;- train(solTrainXtrans, solTrainY, method = &quot;rlm&quot;, preProcess = &quot;pca&quot;, trControl = ctrl, maxit = 30)

rlmPCA</code></pre>
<pre><code>## Robust Linear Model 
## 
## 951 samples
## 228 predictors
## 
## Pre-processing: principal component signal extraction (228),
##  centered (228), scaled (228) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 856, 855, 857, 856, 856, 855, ... 
## Resampling results across tuning parameters:
## 
##   intercept  psi           RMSE       Rsquared   MAE      
##   FALSE      psi.huber     2.8259554  0.8512363  2.7142644
##   FALSE      psi.hampel    2.8258408  0.8512527  2.7141467
##   FALSE      psi.bisquare  2.8261587  0.8511998  2.7144914
##    TRUE      psi.huber     0.7943194  0.8499849  0.6054924
##    TRUE      psi.hampel    0.7929511  0.8505782  0.6062351
##    TRUE      psi.bisquare  0.7993123  0.8480131  0.6076413
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were intercept = TRUE and psi
##  = psi.hampel.</code></pre>
<pre class="r"><code>rm(rlmPCA)</code></pre>
<div id="partial-least-squares-1" class="section level4">
<h4>Partial Least Squares</h4>
<p><code>pls::plsr</code> function</p>
<ul>
<li>Requires model formula</li>
<li>The first Dayal and MacGregor kernel algorithm used; can specify others using the <code>method</code> argument, e.g. “oscorespls”, “simpls”, or “widekernelpls”.</li>
<li>Use <code>ncomp</code> argument to set number of components, otherwise maximum number of components will be calculated.</li>
<li>Use <code>validation</code> argument for K-fold or leave-one-out CV</li>
</ul>
<p>Helper functions to extract PLS components:</p>
<ul>
<li><code>loadings</code></li>
<li><code>scores</code></li>
<li><code>plot</code></li>
</ul>
<p>Given that many predictors highly correlated and overall information within predictors space contained in smaller number of dimensions, predictor conditions are favourable for applying PLS.</p>
<p>Use cross validation to determine optimal number of PLS components to retain in order to minimise RMSE.</p>
<p>In the following code, PCR is performed using the same cross-validation sets to compare its performance to PLS.</p>
<p>A comparison of PLS to PCR is below; PLS found a minimum RMSE (0.698) with 20 components, whereas PCR found a minimum RMSE (0.734) with 35 components.</p>
<pre class="r"><code>plsResamples &lt;- plsTune$results
plsResamples$Model &lt;- &quot;PLS&quot;

pcrResamples &lt;- pcrTune$results
pcrResamples$Model &lt;- &quot;PCR&quot;

plsPlotData &lt;- rbind(plsResamples, pcrResamples)
xyplot(RMSE ~ ncomp,
       data = plsPlotData,
       #aspect = 1,
       xlab = &quot;# Components&quot;,
       ylab = &quot;RMSE (Cross-Validation)&quot;,
       auto.key = list(columns = 2),
       groups = Model,
       type = c(&quot;o&quot;, &quot;g&quot;))</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/solubilityPlsVsPCR-1.png" width="672" /></p>
<p>Although predictive ability is similar, PLS finds a simpler model that uses far fewer components than PCR. Using the one-standard error method (simpler solution within 1 standard error of numerically optimal value) reduces this to 9 components for PLS and 33 for PCR</p>
<p>The following figure shows the relationship between the first two components of both PLS and PCR; the correlation is greater for the PLS components and illustrates why PLS is more quickly steered towards underlying relationship with the response.</p>
<pre class="r"><code>as.data.frame(unclass(plsTune$finalModel$scores)) %&gt;%
    mutate(Model = &quot;PLS&quot;, Solubility = solTrainY) %&gt;%
    gather(Component, Score, -Solubility, -Model) %&gt;%
    rbind(
        as.data.frame(unclass(pcrTune$finalModel$scores)) %&gt;%
            mutate(Model = &quot;PCR&quot;, Solubility = solTrainY) %&gt;%
            gather(Component, Score, -Solubility, -Model)) %&gt;%
    filter(Component %in% c(&quot;Comp 1&quot;, &quot;Comp 2&quot;)) %&gt;%
    ggplot(aes(Score, Solubility)) + 
    geom_point() + 
    facet_grid(Model ~ Component)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/solubilityScoresVsREsponse-1.png" width="672" /></p>
<p>Variable importance for the PLS model shown below:</p>
<pre class="r"><code>plsImp &lt;- varImp(plsTune, scale = FALSE)</code></pre>
<pre><code>## 
## Attaching package: &#39;pls&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:corrplot&#39;:
## 
##     corrplot</code></pre>
<pre><code>## The following object is masked from &#39;package:caret&#39;:
## 
##     R2</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     loadings</code></pre>
<pre class="r"><code>plot(plsImp, top = 25, scales = list(y = list(cex = .95)))</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/solubilityPlsVarIMp-1.png" width="672" /></p>
<pre class="r"><code>rm(plsImp, plsTune, plsPlotData, plsResamples, pcrTune, pcrResamples)</code></pre>
</div>
<div id="penalized-regression-models" class="section level4">
<h4>Penalized Regression Models</h4>
<p>Ridge Regression</p>
<ul>
<li>MASS::lm.ridge</li>
<li>elasticnet::enet;
<ul>
<li>lambda specifies the ridge-regression penalty</li>
<li>lasso penalty can be computed efficiently for many values of the penalty, predict.enet generates predictions for one or more values of the lasso penalty simultaneously using the <code>s</code> and <code>mode</code> argument. For ridge regression, desire lasso penalty of 0 and so specify the full solution using <code>s=1</code> and <code>mode = &quot;fraction&quot;</code></li>
</ul></li>
</ul>
<p><strong>Notes on using <code>predict.enet</code></strong>: The meaning of <code>s</code> depends on the value of <code>model</code>. For example, if <code>mode</code> = “norm” and <code>s</code> = 2000, then predict.enet will extract the coefficient vector with L1 norm = 2000. If <code>mode</code> = “fraction” and <code>s</code>=0.45, then predict.enet will extract the coefficient vector with L1 norm fraction = 0.45.</p>
<pre class="r"><code>ridgeModel &lt;- elasticnet::enet(x = as.matrix(solTrainXtrans), y = solTrainY, lambda = 0.001)

ridgePred &lt;- predict(ridgeModel, newx = as.matrix(solTestXtrans), s = 1, mode = &quot;fraction&quot;, type = &quot;fit&quot;)

rm(ridgeModel)
head(ridgePred$fit)</code></pre>
<pre><code>##          20          21          23          25          28          31 
##  0.96795590  0.06918538 -0.54365077  0.96072014 -0.03594693  1.59284535</code></pre>
<pre class="r"><code>rm(ridgePred)</code></pre>
<p>The following figure shows how the RMSE changes with <span class="math inline">\(\lambda\)</span>;</p>
<pre class="r"><code># Define the candidate set of values
ridgeGrid &lt;- data.frame(lambda = seq(0, .1, length = 15))
# ridgeGrid &lt;- expand.grid(lambda = seq(0, .1, length = 15))

set.seed(100)
ridgeRegFit &lt;- train(x = solTrainXtrans, 
                     y = solTrainY, 
                     method = &quot;ridge&quot;, 
                     tuneGrid = ridgeGrid, 
                     trControl = ctrl, 
                     preProc = c(&quot;center&quot;, &quot;scale&quot;))

ridgeRegFit
print(update(plot(ridgeRegFit ), xlab = &quot;Penalty&quot;))</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/ridgeTune-1.png" width="672" /></p>
<pre class="r"><code>rm(ridgeGrid)</code></pre>
<pre><code>## Ridge Regression 
## 
## 951 samples
## 228 predictors
## 
## Pre-processing: centered (228), scaled (228) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 856, 855, 857, 856, 856, 855, ... 
## Resampling results across tuning parameters:
## 
##   lambda       RMSE       Rsquared   MAE      
##   0.000000000  0.7169416  0.8793300  0.5298092
##   0.007142857  0.6966645  0.8852028  0.5254374
##   0.014285714  0.6888520  0.8877355  0.5221313
##   0.021428571  0.6861436  0.8886873  0.5216569
##   0.028571429  0.6856894  0.8889716  0.5225741
##   0.035714286  0.6864634  0.8889140  0.5242680
##   0.042857143  0.6879978  0.8886625  0.5264494
##   0.050000000  0.6900472  0.8882942  0.5287584
##   0.057142857  0.6924712  0.8878533  0.5310894
##   0.064285714  0.6951834  0.8873666  0.5335339
##   0.071428571  0.6981285  0.8868514  0.5361135
##   0.078571429  0.7012695  0.8863189  0.5388772
##   0.085714286  0.7045812  0.8857770  0.5416857
##   0.092857143  0.7080454  0.8852307  0.5447911
##   0.100000000  0.7116491  0.8846838  0.5480085
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was lambda = 0.02857143.</code></pre>
<p>with no <span class="math inline">\(\lambda\)</span> the error is inflated, when increased, the error drops from 0.7169416 to 0.7116491</p>
<p>Lasso model</p>
<ul>
<li>lars::lars</li>
<li>elastnet::enet; predictor data must be a matrix object. Use normalize argument to center and scale prior to modelling. lambda controls ridge regression penalty, setting to 0 fits the lass model. Lasso penalty does not need to be specified until the time of prediction.</li>
<li>glmnet::enet</li>
<li>biglars (large data sets)</li>
<li>FLLat (fused lasso)</li>
<li>grplasso (group lasso)</li>
<li>penalized</li>
<li>relazo (relazes lasso)</li>
</ul>
<p>Predictors should be centered and scaled prior to modelling.</p>
<pre class="r"><code>enetModel &lt;- elasticnet::enet(x = as.matrix(solTrainXtrans), y = solTrainY, lambda = 0, normalize = TRUE)

enetPred &lt;- predict(enetModel, newx = as.matrix(solTestXtrans), s = .1, mode = &quot;fraction&quot;, type = &quot;fit&quot;)

head(enetPred$fit)
rm(enetPred)</code></pre>
<pre><code>##         20         21         23         25         28         31 
##  0.5156807  0.2891629 -0.5002878  0.6396277 -0.1264201  1.3809965</code></pre>
<pre class="r"><code>enetCoef&lt;- predict(enetModel, newx = as.matrix(solTestXtrans), s = .1, mode = &quot;fraction&quot;, type = &quot;coefficients&quot;)
tail(enetCoef$coefficients)

rm(enetModel, enetCoef)</code></pre>
<pre><code>##       NumChlorine        NumHalogen          NumRings HydrophilicFactor 
##        -0.1073073         0.0000000         0.0000000         0.0000000 
##      SurfaceArea1      SurfaceArea2 
##         0.2575574         0.0000000</code></pre>
<p><strong>Question</strong>: How to recreate coefficient path …</p>
<p>The following plot shows the performance profiles across three values of the ridge penalty and 20 values of the lasso penalty.</p>
<p>The pure lasso model (with <span class="math inline">\(\lambda_1\)</span> = 0) has an initial drop in the error and then an increase when the fraction of the full solution (amount that the ridge regression coefficient estimates have been shrunken towards zero; small value indicates they have been shrunken very close to zero) is greater than 0.2. The two models with nonzero values of the ridge penalty have minimum errors with a larger model.</p>
<pre class="r"><code>enetGrid &lt;- expand.grid(.lambda = c(0, 0.01, .1), .fraction = seq(.05, 1, length = 20))

set.seed(100)
enetTune &lt;- train(solTrainXtrans, solTrainY,
                  method = &quot;enet&quot;,
                  tuneGrid = enetGrid,
                  trControl = ctrl,
                  preProc = c(&quot;center&quot;, &quot;scale&quot;))

plot(enetTune)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/lassoTune-1.png" width="672" /></p>
<pre class="r"><code>coeffs &lt;- predict.enet(enetTune$finalModel, type = &quot;coefficients&quot;, mode = &quot;fraction&quot;)

as.data.frame(unclass(coeffs$coefficients)) %&gt;%
 mutate(Fraction = coeffs$fraction) %&gt;%
 gather(Variable, Coefficient, -Fraction) %&gt;%
 mutate(Col = ifelse(Variable %in% c(&quot;NumNonHAtoms&quot;, &quot;NumNonHBonds&quot;, &quot;NumMultBonds&quot;), Variable, &quot;Other&quot;),
        Col = factor(Col, levels = c(&quot;NumNonHAtoms&quot;, &quot;NumNonHBonds&quot;, &quot;NumMultBonds&quot;, &quot;Other&quot;))) %&gt;%
 ggplot(aes(x = Fraction, y = Coefficient, group=Variable, colour = Col)) +
 geom_line() +
     scale_color_manual(name = &quot;&quot;, values = c(&quot;NumNonHAtoms&quot; = &quot;green&quot;, &quot;NumNonHBonds&quot; = &quot;purple&quot;, &quot;NumMultBonds&quot; = &quot;orange&quot;, &quot;Other&quot; = &quot;gray&quot;), breaks = c(&quot;NumNonHAtoms&quot;, &quot;NumNonHBonds&quot;, &quot;NumMultBonds&quot;)) + 
    theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/lassoTune-2.png" width="672" /></p>
<pre class="r"><code>rm(coeffs, enetGrid)</code></pre>
<p>In the end, the optimal performance was associated with the lasso model (<span class="math inline">\(\lambda = 0\)</span>) with a fraction of 0.6, corresponding to 18 predictors out of a possible 228.</p>
<p>Note: To extract the coefficients used in the final model, use the following code, either specify the optimal fraction (s = 0.6) or enter programmatically as follows:</p>
<pre class="r"><code>sum(predict.enet(enetTune$finalModel, s=enetTune$bestTune[1, &quot;fraction&quot;], type=&quot;coef&quot;, mode=&quot;fraction&quot;)$coefficients != 0)

rm(enetTune)</code></pre>
<pre class="r"><code>rm(ctrl, solTestX, solTestXtrans, solTrainX, solTrainXtrans, trainingData, solTestY, solTrainY, Fingerprints, SolubilityCounts)</code></pre>
</div>
</div>
</div>
</div>
<div id="nonlinear-regression-models" class="section level1">
<h1>Nonlinear Regression Models</h1>
<p>Linear regression models can be adapted to nonlinear trends in data by manually adding model terms but requires knowledge of specific nature of nonlinearity in data.</p>
<p>Advantage of nonlinear models (NN, MARS, SVN, KNN, tree models) is that don’t need to know, or specify, form of nonlinearity prior to model training.</p>
<div id="neural-networks" class="section level2">
<h2>Neural Networks</h2>
<ul>
<li>Hidden units:
<ul>
<li>Intermediary set of unobserved variables used in modelling (similar to PLS)</li>
<li>Linear combination of original predictors, but typically transformed by a nonlinear function <span class="math inline">\(g(\centerdot)\)</span>, e.g. the logistic/sigmoidal function <span class="math display">\[
\begin{aligned}
h_k(\mathbf{x}) = g\left(\beta_{0k} + \sum^P_{i=1}x_j\beta_{jk}\right), \;\; \text{where}
\\
g(u) = \frac{1}{1 + e^{-u}}\end{aligned}
\]</span></li>
</ul>
<p>where <span class="math inline">\(\beta_{jk}\)</span> is the effect of the <span class="math inline">\(j\)</span>th predictor on the <span class="math inline">\(k\)</span>th hidden unit</p>
<ul>
<li>Not estimated in hierarchical fashion (as per PLS)</li>
</ul></li>
<li><p>Another linear combination connects the hidden units to the outcome: <span class="math display">\[f(\mathbf{x}) = \gamma_o + \sum^{H}_{k = 1} \gamma_k h_k\]</span></p></li>
</ul>
<p>For <span class="math inline">\(H\)</span> hidden units and <span class="math inline">\(P\)</span> predictors, there are <span class="math inline">\(H(P+1) + (H + 1)\)</span> parameters being estimated.</p>
<p>Solution:</p>
<ul>
<li>Parameters usually optimised to minimize the sum of squared residuals</li>
<li>Parameters usually initialised to random variables</li>
<li>Back-propagation algorithm efficient, but solution is not a global solution, meaning no guarantee that resulting set of parameters are uniformly better than any other set</li>
</ul>
<p>Limitations</p>
<ul>
<li>No global optimum</li>
<li>Tendency to overfit; solutions:
<ul>
<li>Early stopping: Prematurely halt iterative algorithm for solving the regression equations, e.g. stop when error rate starts to increase (instead of using a numerical tolerance to indicate parameter estimates or error rates are stable)… but how to estimate the error rate? Training error rate often optimistic and further splitting training set can be problematic</li>
<li>Use weight decay, penalisation method to regularize the model (similar to ridge regression); add penalty (<span class="math inline">\(\lambda\)</span>) so that any large value must have significant effect on model errors to be tolerated; aim would therefore be to minimize <span class="math display">\[\sum^n_{i=1}(y_i - f_i(x))^2 + \lambda\sum^H_{k=1}\sum^P_{j=0}\beta_{jk}^2 + \lambda\sum^H_{k=0}\gamma_k^2\]</span></li>
</ul></li>
</ul>
<p>Tuning parameters for weight decay NN include:</p>
<ul>
<li>Regularization value, usually between 0 and 0.1</li>
<li>Number of hidden units.</li>
</ul>
<p>Since regression coefficients are being summed, should be centred and scaled prior to modelling (such that on same scale)</p>
<p>Neural network models may differ to the single-layer feed forward NN represented here; e.g. can have:</p>
<ul>
<li>More than one layer of hidden units</li>
<li>Loops in both directions</li>
<li>Bayesian approaches</li>
</ul>
<p>Limitations</p>
<ul>
<li>Parameter estimates are locally optimal (unlikely to be a global optimum)
<ul>
<li>To overcome, use <strong>model averaging</strong>, i.e., create several models with different starting values and average the results to produce a more stable prediction</li>
<li>Adversely affected by high correlation among predictor variables (since use gradients to optimise the model parameters). To overcome use the following techniques which have the added advantage of fewer model terms thus improving computational time:
<ul>
<li>Pre-filter predictors to remove predictors that are associated with high correlations.</li>
<li>Use a feature extraction technique, i.e., principal component analysis prior to modelling to eliminate correlations.</li>
</ul></li>
</ul></li>
</ul>
<p><strong>Self-organisation maps</strong></p>
<ul>
<li>Similar model to NN</li>
<li>Can be used as unsupervised, exploratory technique or in supervised fashion for prediction</li>
</ul>
</div>
<div id="multivariate-adaptive-regression-splines-mars" class="section level2">
<h2>Multivariate Adaptive Regression Splines (MARS)</h2>
<ul>
<li>Uses surrogate (instead of original) features</li>
<li>Surrogate features:
<ul>
<li>Contrasted version of a predictor</li>
<li>Usually a function of one or two predictors</li>
</ul></li>
<li><p>For each original predictor, have left-hand and right-hand surrogate features which are essentially piecewise linear and operate on isolated portion of original data. The cutpoint / spline is determined by considering each data point as a candidate; that which achieves the minimum error is used. New features described by a hinge function: <span class="math display">\[h(x) = \begin{cases} x \; x &gt; 0 \\
                    0 \; x \leq 0
                    \end{cases}\]</span> A <strong>pair</strong> of hinge functions can be written as:</p></li>
<li><span class="math inline">\(h(x-a)\)</span> which is non-zero when <span class="math inline">\(x&gt;a\)</span>, and</li>
<li><p><span class="math inline">\(h(a - x)\)</span> which is non-zero when <span class="math inline">\(x &lt; a\)</span></p></li>
</ul>
<p>Model Building Strategy:</p>
<ul>
<li>Similar to forward stepwise linear regression, but instead of using original inputs, can use hinge functions, so model has form: <span class="math display">\[f(X) = \beta_0 + \sum^M_{i=1}\beta_m h_m(x)\]</span></li>
<li>Given a choice for <span class="math inline">\(h_m\)</span>, coefficients <span class="math inline">\(\beta_m\)</span> estimating by minimising RSS (i.e. by standard linear regression), but how to choose <span class="math inline">\(h_m\)</span>?</li>
</ul>
<p>The collection of basis functions is <span class="math display">\[C = {(X_j-t)+, (t - X_j)+}, t \in {x_{ij},x_{2j}, \ldots x_{Nj}}, j = 1, 2, \ldots, p. \]</span> If all input values distinct then there are <span class="math inline">\(2Np\)</span> basis functions. The basis function from the collection that produces the largest decrease in training error is selected. The process is continued until model set contains some preset maximum number of terms.<br />
* Typically a backward deletion procedure is applied. The term whose removal causes the smallest increase in residual squared error is deleted to produce an estimated best model for each number of terms <span class="math inline">\(\lambda\)</span>.</p>
<ul>
<li>Iteratively look for next set of features that yield best model fit</li>
<li>Consider each data point for predictor as candidate cut point by creating linear regression model with candidate features and calculate corresponding model error.</li>
</ul>
<p>Note that MARS can select a predictor more than once during the iterations.<br />
see <a href="http://uc-r.github.io/mars" class="uri">http://uc-r.github.io/mars</a></p>
<p>MARS can also build models where features involve multiple predictors at once.</p>
<ul>
<li>Second-degree MARS model: Conduct same search of single term and after creating initial pair of features, instigate another search to create new cuts to couple with each of the original features, e.g. to include terms <span class="math inline">\(A\)</span>, <span class="math inline">\(A \times B\)</span>, <span class="math inline">\(A \times C\)</span></li>
</ul>
<p>Tuning parameters:</p>
<ul>
<li>Degree of features</li>
<li>Number of retained terms
<ul>
<li>Can be automatically determined using default pruning procedure (GCV). Evaluates individual model. Since it does not reflect uncertainty from feature selection, suffers from selection bias.<br />
</li>
<li>Set by user</li>
<li>Determined using external resampling technique (e.g. cross-validation). Exposed to variation in entire model building process, including feature selection.</li>
</ul></li>
</ul>
<p>Advantages:</p>
<ul>
<li>Model automatically conducts feature selection therefore potentially things predictor set using same algorithm used to build model. Feature selection routine has a direct connection to functional performance.</li>
<li>Interpretability. Each hinge function responsible for specific region.
<ul>
<li>When model is additive (one-degree), contribution of each predictor can be isolated without need to consider others.<br />
</li>
<li>When model is non-additive, (e.g. 2-degree and 2 predictors, 3 out 4 regions will be zero) can isolate interaction.</li>
</ul></li>
<li>Requires little pre-processing. Data transformations and filtering of predictors not required; * a zero variance predictor will never be chosen for a split since it offers no possible predictive information.
<ul>
<li>Correlated predictors do not drastically impact model performance but can complicate model interpretation.</li>
</ul></li>
</ul>
<p>Variable Model Importance: To understand how predictors affect model</p>
<ul>
<li>Track reduction in RMSE (GCV stat) when adding feature to model (and attribute to original feature)</li>
</ul>
<p>Interpreting additive model: Can original predictor and predicted response value when all others are held constant at their mean level. The additive nature of the model allows each predictor to be viewed in isolation. Changing the values of the other predictor variables will not alter the shape of the profile, only the location on the <span class="math inline">\(y\)</span>-axis where the profile starts.</p>
</div>
<div id="support-vector-machines" class="section level2">
<h2>Support Vector Machines</h2>
<ul>
<li>Originally developed for classification models</li>
<li>Used for robust regression; to minimize effect of outliers</li>
<li>Many types; here <span class="math inline">\(\epsilon\)</span>-insensitive regression</li>
<li>Recall: regression aim to minimize SSE, but can be influenced by just one observation falling far from data trend.</li>
<li>With influential observations, consider alternative minimisation metrics:
<ul>
<li>Huber function; squared residuals when small and absolute residuals when large</li>
</ul></li>
<li>SVM for regression function <span class="math inline">\(\sim\)</span> Huber function; for given threshold (<span class="math inline">\(\epsilon\)</span>)
<ul>
<li>Data points with residuals within threshold do not contribute to regression fit <span class="math inline">\(\rightarrow\)</span> Samples that the model fits well have <em>no</em> effect on regression equation. With large <span class="math inline">\(\epsilon\)</span> only outliers define regression line</li>
<li>Data points with absolute difference &gt; <span class="math inline">\(\epsilon\)</span> contribute linear-scale amount; i.e. squared residuals not used.</li>
<li>Aim: to choose regression coefficients that minimise: <span class="math display">\[\text{Cost}\sum^n_{i=1}L_e(y_i - \hat{y}_i) + \sigma^P_{j=1}\beta_j^2,\]</span> where <span class="math inline">\(L_e(\centerdot)\)</span> is the <span class="math inline">\(\epsilon\)</span>-insensitive function and the <em>cost</em> parameter is used to penalise large residuals (unlike weight decay in neural networks it is attached to residuals and not parameters)</li>
</ul></li>
</ul>
<p>Prediction with simple linear regression for new sample <span class="math inline">\(u\)</span>: <span class="math display">\[\hat{y} = \beta_0 + \sum^P_{j=1}\beta_ju_j\]</span></p>
<p>Prediction with <span class="math inline">\(\epsilon\)</span>-insensitive regression: Replace <span class="math inline">\(\beta_j\)</span> with <span class="math inline">\(\sum^n_{i=1}\alpha_ix_{ij}\)</span>, such that prediction is <span class="math display">\[\begin{aligned}
\hat{y} &amp;= \beta_0 + \sum^P_{j=1}u_j\left(\sum^n_{i=1}\alpha_ix_{ij}\right)\\
        &amp;= \beta_0 + \sum^n_{i=1}\alpha_i \left(\sum^P_{j=1}x_{ij}u_j\right)\\
        &amp;= \beta_0 + \sum^n_{i=1}\alpha_i K(\mathbf{x_i}, \mathbf{u})
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(K(\mathbf{x_i}, \mathbf{u})\)</span> is referred to as the <em>kernel function</em></p>
<p>Noting that:</p>
<ul>
<li>Number <span class="math inline">\(\alpha\)</span> parameters = number data points. Cost parameter regularizes model to prevent over-parameterisation</li>
<li>Require <em>subset of</em> training data points (the outliers for which <span class="math inline">\(\alpha \neq 0\)</span>) to make new predictions. Since the regression line is determined using these samples, they are called the <strong>support vectors</strong>.</li>
<li>Kernel function above was linear; can replace with dot product <span class="math inline">\(\mathbf{x}^\prime \mathbf{u}\)</span>. There are other types of kernel functions <span class="math display">\[\begin{aligned}
\text{linear} &amp;= \mathbf{x}^\prime \mathbf{u}\\
\text{polynomial} &amp;= (\phi(\mathbf{x}^\prime \mathbf{u}) + 1) ^\text{degree}\\
\text{radial basis function} &amp;= \exp(-\sigma||\mathbf{x}- \mathbf{u}||^2\\
\text{hyperbolic tangent} &amp;= \tanh(\phi({x}^\prime \mathbf{u}) + 1)\\
\end{aligned}\]</span> where <span class="math inline">\(\phi\)</span> and <span class="math inline">\(sigma\)</span> are scaling parameters. Radial basis function shown to be effective but when regression line is truly linear, linear kernel function will be the better choice.</li>
</ul>
<p>Tuning parameters:</p>
<ul>
<li><strong>Cost</strong> value; main tool to adjust complexity of model. Large cost <span class="math inline">\(\rightarrow\)</span> flexible model as effect of errors amplified. Small cost <span class="math inline">\(\rightarrow\)</span> model will stiffen and become less likely to over-fit (but more likely to underfit) as contribution of squared parameters (<span class="math inline">\(\sum^P_{j=1}\beta_j^2\)</span>) proportionally large in modified error function</li>
<li>Size funnel <span class="math inline">\(\epsilon\)</span>, but suggestion is to keep if fixed and tune over other parameters.<br />
</li>
<li>Kernel specific tuning parameters
<ul>
<li>Polynomial kernel function: </li>
<li>Radial basis function: scale parameter <span class="math inline">\(\sigma\)</span>; but can calculate computationally using 10th and 90th percentile of <span class="math inline">\(||x - x^\prime||^2\)</span> as a range for <span class="math inline">\(\sigma\)</span> and use the midpoint of the percentiles..</li>
</ul></li>
</ul>
<p>Predictors enter model as sum of cross products and so differences in predictor scales can affect model. Recommendation: centre and scale predictors prior to building SVM model.</p>
</div>
<div id="k-nearest-neighbours" class="section level2">
<h2><span class="math inline">\(K\)</span>-Nearest Neighbours</h2>
<ul>
<li>Predicted response = summary statistic(response of the <span class="math inline">\(K\)</span> nearest neighbours in the training data set).</li>
<li>For summary statistics can use mean or median</li>
<li>Nearest neighbours = f(distance definition):
<ul>
<li>Euclidean distance = straight line distance <span class="math display">\[\left(\sum^P_{j = 1}(x_{aj}-x_{bj})^2\right)^P\]</span> where <span class="math inline">\(\mathbf{x_a}\)</span> and <span class="math inline">\(\mathbf{x_b}\)</span> are two individual samples</li>
<li>Minkowski distance = generalisation of Euclidean distance <span class="math display">\[\left(\sum^P_{j = 1}|x_{aj}-x_{bj}|^2\right)^P\]</span>
<ul>
<li><span class="math inline">\(q = 1\)</span>: Manhattan = city block, common for samples with binary predictors</li>
<li><span class="math inline">\(q = 2\)</span> Euclidean</li>
</ul></li>
<li>Tanimoto - computational chemistry problems when molecules described using binary fingerprints</li>
<li>Hamming</li>
<li>Cosine</li>
</ul></li>
</ul>
<p>As <span class="math inline">\(K\)</span>NN depends on distance, predictor scale has big impact on samples. Data with predictors on different scales will generate distances weighted towards predictors with the largest scales. Recommendation: Centre and scale prior to performing <span class="math inline">\(K\)</span>NN</p>
<p><strong>Missing values</strong>: Makes it impossible to compute distance between samples. Options:</p>
<ul>
<li>Exclude samples or predictors with missing values</li>
<li>Impute missing data using naive estimators such as mean of the predictor, or nearest neighbour using only predictors with complete information</li>
</ul>
<p>Steps:</p>
<ul>
<li>Pre-process data</li>
<li>Select distance metric</li>
<li>Find optimal number of neighbours = tuning parameter
<ul>
<li>Resampling</li>
</ul></li>
</ul>
<p>Limitations:</p>
<ul>
<li>Computational time; to predict sample distances between sample and all other samples needs to be computed. Computation time increases with <span class="math inline">\(n\)</span> because training data must be loaded into memory and new sample and all training samples must be computed. Replace original data with less memory-intensive representation that describes locations, e.g. <span class="math inline">\(k-d\)</span> tree (<span class="math inline">\(k\)</span>-dimensional tree), which orthogonally partitions predictor space using tree approach. Once tree grown, distances for new sample only computed for training observations in tree close to sample. Provides significant computational improvement when <span class="math inline">\(n\)</span> &gt;&gt; <span class="math inline">\(p\)</span>.</li>
<li>Disconnect between local structure and predictive ability (i.e. when irrelevant or noisy predictors are in the neighbourhood of the new sample). To overcome:
<ul>
<li>Remove irrelevant, noise-laden predictors</li>
<li>Weight neighbour’s contribution based on distance to new sample</li>
</ul></li>
</ul>
</div>
</div>
<div id="chapter-8-regression-trees-and-rule-based-models" class="section level1">
<h1>Chapter 8: Regression Trees and Rule-Based Models</h1>
<ul>
<li>Tree-based model = <span class="math inline">\(\geq\)</span> 1 nested <em>if-then</em> statements</li>
<li>Rule-based model = simplified tree-based model in which samples may be covered by multiple rules?</li>
</ul>
<p>Strengths:</p>
<ul>
<li>Highly interpretable</li>
<li>Easy to implement</li>
<li>Can handle many types of predictors (sparse, skewed, continuous, categorical,etc) without need to pre-process</li>
<li>No requirement to specify form of predictor’s relationship to response (such as in linear regression)</li>
<li>Implicitly handles missing data</li>
<li>Implicitly conducts feature selection</li>
</ul>
<p>Weaknesses:</p>
<ul>
<li>Model instability; slight changes in data can drastically change tree/rule structure and therefore interpretation</li>
<li>Less-than-optimal predictive performance as models define rectangular regions that may not adequately define the relationship between predictors and the response</li>
</ul>
<p>To overcome weaknesses, ensembles used which combine many trees (or rule-based) models into one.</p>
<div id="basic-regression-trees" class="section level2">
<h2>Basic Regression Trees</h2>
<p>partition data into smaller groups that are more homogeneous with respect to response, by determining:</p>
<ul>
<li>Predictor to split on and value of split</li>
<li>Depth /complexity of tree</li>
<li>Prediction equation in terminal nodes</li>
</ul>
<p>Techniques for constructing regression trees:</p>
<ul>
<li>CART (classification and regression tree)</li>
</ul>
<p><strong>CART method</strong>:</p>
<ul>
<li>Start with entire data set <span class="math inline">\(S\)</span></li>
<li>Tree growing step: Search each distinct value of very predictor to find predictor and split value that partitions the data into two groups <span class="math inline">\(S_1\)</span> and $S_2# such that overall sums of squares are minimised: <span class="math display">\[\text{SSE} =  \sum_{i \in S_1}(y_i - \bar{y}_1)^2 + \sum_{i \in S_2}(y_i - \bar{y}_2)^2\]</span> where <span class="math inline">\(\bar{y}_i\)</span> is average of training set outcome within group <span class="math inline">\(S_i\)</span>. Recursively partition the <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> based on this formula, until number of samples in splits falls below some threshold (e.g. 20 samples)</li>
<li>Pruning step: Fully grown tree may be large and likely over-fit training set. Pruning methods:
<ul>
<li>Cost-complexity tuning: Penalise error rate using size of tree using a <strong>complexity parameter</strong> <span class="math inline">\(c_p\)</span>. For given value of <span class="math inline">\(c_p\)</span> find lowest penalised error rate <span class="math display">\[\text{SSE}_C_p = \text{SSE} + c_p \times (\text{# Terminal Nodes})\]</span></li>
</ul></li>
</ul>
<p>Splitting continuous predictors: Order data Splitting binary predictors: Only one possible split point Splitting categorical predictors: Options: * Decompose categorical predictors into set of binary dummy variables (one vs all split) * Allow the model to determine how to split the values</p>
</div>
<div id="regression-tree-models" class="section level2">
<h2>Regression Tree Models</h2>
</div>
<div id="rule-based-models" class="section level2">
<h2>Rule-Based Models</h2>
</div>
<div id="bagged-trees" class="section level2">
<h2>Bagged Trees</h2>
</div>
<div id="random-forests" class="section level2">
<h2>Random Forests</h2>
</div>
<div id="boosting" class="section level2">
<h2>Boosting</h2>
</div>
<div id="cubist" class="section level2">
<h2>Cubist</h2>
</div>
</div>
<div id="chapter-11-measuring-performance-in-classification-models" class="section level1">
<h1>Chapter 11: Measuring Performance in Classification Models</h1>
<pre class="r"><code>library(tidyverse)
library(datasets)
library(caret)
# load the iris dataset
data(iris)
split_index &lt;- createDataPartition(iris$Species, p=0.80, list=FALSE)
# use  80% of data to train model
iris_tr &lt;- iris[split_index,]

# use 20% of the data for validation
iris_te &lt;- iris[-split_index,]

# define training control
train_control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3)
# train the model
model &lt;- train(Species~., data=select(iris_tr, Sepal.Length, Sepal.Width, Species), trControl=train_control, method=&quot;rf&quot;)</code></pre>
<pre><code>## note: only 1 unique complexity parameters in default grid. Truncating the grid to 1 .</code></pre>
<pre class="r"><code># summarize results
print(model)</code></pre>
<pre><code>## Random Forest 
## 
## 120 samples
##   2 predictor
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... 
## Resampling results:
## 
##   Accuracy   Kappa
##   0.7166667  0.575
## 
## Tuning parameter &#39;mtry&#39; was held constant at a value of 2</code></pre>
<pre class="r"><code>predictions &lt;- predict(model, iris_te, type = &quot;prob&quot;)
predictions.class &lt;- predictions %&gt;%  mutate(&#39;class&#39;=names(.)[apply(., 1, which.max)]) %&gt;% dplyr::select(class) %&gt;%
    mutate(class = as.factor(class)) %&gt;% pull()

confusionMatrix(predictions.class, iris_te$Species)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         10          0         0
##   versicolor      0          3         2
##   virginica       0          7         8
## 
## Overall Statistics
##                                          
##                Accuracy : 0.7            
##                  95% CI : (0.506, 0.8527)
##     No Information Rate : 0.3333         
##     P-Value [Acc &gt; NIR] : 4.433e-05      
##                                          
##                   Kappa : 0.55           
##                                          
##  Mcnemar&#39;s Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            0.3000           0.8000
## Specificity                 1.0000            0.9000           0.6500
## Pos Pred Value              1.0000            0.6000           0.5333
## Neg Pred Value              1.0000            0.7200           0.8667
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.1000           0.2667
## Detection Prevalence        0.3333            0.1667           0.5000
## Balanced Accuracy           1.0000            0.6000           0.7250</code></pre>
<pre class="r"><code>getMultinomialCM &lt;- function(prediction, target){
    data &lt;- cbind(prediction, target)
    
    cm &lt;- with(dat, table(target, prediction))
    # recall
    recall &lt;- diag(cm / apply(cm, 1, sum))
    # precision
    precision &lt;- diag(cm/apply(cm, 2, sum))
    # accuracy
    accuracy &lt;- sum(diag(cm))/sum(cm)
    
    cm &lt;- cm %&gt;% cbind(recall) %&gt;%
        rbind(precision = c(precision, accuracy))
    cm
}
getMultinomialCM(predictions.class, iris_te$Species)</code></pre>
<pre><code>##            setosa versicolor virginica recall
## setosa         10        0.0 0.0000000    1.0
## versicolor      0        3.0 7.0000000    0.3
## virginica       0        2.0 8.0000000    0.8
## precision       1        0.6 0.5333333    0.7</code></pre>
<p>Calibration Plots</p>
<pre class="r"><code>obs &lt;- model.matrix( ~Species-1, data = iris_te %&gt;% dplyr::select(Species))

i &lt;- 1
cal &lt;- NULL
for (i in 1:ncol(predictions)){
    dat &lt;- data.frame(obs = obs[,i], pred = predictions[,i]) 
    dat$pred_bin &lt;- cut(dat$pred, breaks = c(-0.001, seq(0.1, 1, 0.1)))
    dat$midpoint &lt;- seq(.1/2, 1-.1/2, by = 0.1)[as.numeric(dat$pred_bin)]
    
    dat &lt;- dat %&gt;% group_by(midpoint) %&gt;%
        summarise(n = n(), 
                  events = sum(obs)) %&gt;%
        mutate(event_rate = events / n)
    
    dat$class &lt;- names(predictions)[i]
    cal &lt;- bind_rows(cal, dat)
}

ggplot(cal, aes(midpoint, event_rate, colour = class)) + 
    geom_line() + 
    geom_point()</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/unnamed-chunk-3-1.png" width="672" /> Heatmap</p>
<pre class="r"><code>cbind(predictions, Target = iris_te$Species) %&gt;%
    gather(Pred_Species, Pred_Probability, -Target) %&gt;%
    ggplot(aes(Pred_Species, Target, fill = Pred_Probability)) + 
           geom_tile(colour = &quot;white&quot;) + 
    scale_fill_gradient(low = &quot;white&quot;, high = &quot;steelblue&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="chapter-12-discriminant-analysis-and-other-linear-classification-models" class="section level1">
<h1>Chapter 12: Discriminant Analysis and Other Linear Classification Models</h1>
</div>
<div id="chapter-13-nonlinear-classification-models" class="section level1">
<h1>Chapter 13: Nonlinear Classification Models</h1>
</div>
<div id="chapter-14-classification-trees-and-rule-based-models" class="section level1">
<h1>Chapter 14: Classification Trees and Rule-Based Models</h1>
<hr />
</div>
<div id="chapter-16-remedies-for-severe-class-imbalance" class="section level1">
<h1>Chapter 16: Remedies for Severe Class Imbalance</h1>
<p>Overview:</p>
<ul>
<li>Impact of class imbalance on performance measures</li>
<li>Methodologies for post-processing model predictions</li>
<li>Predictive models that can mitigate issue during training</li>
</ul>
<div id="case-study-predicting-caravan-policy-ownership" class="section level2">
<h2>Case Study: Predicting Caravan Policy Ownership</h2>
<p>TODO: Read PDF.</p>
<ul>
<li>Imbalanced; only 6% purchased policies</li>
<li>85 Predictors; see <a href="https://rdrr.io/rforge/DWD/man/ticdata.html">here</a> for more details
<ul>
<li>Customer subtype: 39 unique values; though many comprise &lt; 5% of customers</li>
<li>Demographic factors; religion, education level, social class, income + 38 others</li>
<li>Product ownership information; number (or contribution) to policies</li>
</ul></li>
<li>Many categorical predictors had <span class="math inline">\(\geq\)</span> 10 levels</li>
<li>Count-based predicted fairly sparse (few non-zero values)</li>
<li>Stratified random sampling (strata <span class="math inline">\(\equiv\)</span> response variable), with rate of customers with caravan policies ~6% in each.
<ul>
<li>Training set (<span class="math inline">\(n = 6877\)</span>) to estimate model parameters and tune model</li>
<li>Evaluation set (<span class="math inline">\(n = 983\)</span>) to develop post-processing techniques, e.g. probability cut-offs</li>
<li>Customer set (<span class="math inline">\(n = 1962\)</span>) for final model evaluation</li>
</ul></li>
</ul>
</div>
<div id="effect-of-class-imbalance" class="section level2">
<h2>Effect of Class Imbalance</h2>
<p>Models:</p>
<ul>
<li>Random forest. 1000 trees in forest tuned over 5 values of <span class="math inline">\(m_\text{try}\)</span> parameter with optimal value of <span class="math inline">\(m_\text{try} = 126\)</span></li>
<li>Flexible discriminant analysis (MARS hinge functions) (FDA): Used first-degree features; tuned over 25 values for the number of retained terms. Resampling process determined that 13 model terms appropriate.</li>
<li>Logistic regression. Simple additive model (no interactions or non-linear terms) with reduced predictor set (many near-zero variance predictors removed so that model resulted in a stable solution)</li>
</ul>
<p>Model tuning:</p>
<ul>
<li>10-fold cross-validation, each holdout sample with ~ 687 customers to provide reasonable estimates of uncertainty</li>
</ul>
<p>Performance metrics:</p>
<ul>
<li>Overall accuracy</li>
<li>Kappa</li>
<li>AUROC (used to choose optimal model)</li>
<li>Sensitivity</li>
<li>Specificity</li>
</ul>
<p>Findings:</p>
<ul>
<li>Models only predicted 13/59 customer would purchase. Had good specificity (since almost every customer predicted no insurance), but poor sensitivity</li>
<li>82% of customers had predicted probability of <span class="math inline">\(\leq\)</span> 10% of having insurance</li>
<li>Lift plots: How many individuals would need to be contacted in order to capture a specific percent of those who might purchase the policy, i.e. to find 60% of those with policies, around 30% of the population would need to be sampled</li>
<li>ROC curves did not differentiate the models</li>
<li>when classes are balanced, lift and ROC curves are not so similar</li>
</ul>
<p>Strategies for overcoming class imbalances:</p>
<ul>
<li><strong>Model Tuning</strong>: Tune model to maximise accuracy of minority class(es)</li>
<li><strong>Aternative Cutoffs</strong>: Examine ROC to select cut-offs to:
<ul>
<li>Achieve targets for sensitivity/specificity (if given)</li>
<li>Minimise distance between ROC curve and perfect model (100% Sensitivity and Specificity, i.e. top LH corner)</li>
<li>Maximise Youden’s <span class="math inline">\(J\)</span> index model, which measures the proportion of correctly predicted samples for both event and non-event groups</li>
</ul></li>
</ul>
<p>Note that it is important, especially for small sample sizes, to use an independent data set to derive the cut-off. If use:</p>
<ul>
<li>Training set <span class="math inline">\(\rightarrow\)</span> large optimistic bias in the class probabilities <span class="math inline">\(\rightarrow\)</span> inaccurate assessments of the sensitivity and specificity.</li>
<li>Test set <span class="math inline">\(\rightarrow\)</span> lose unbiased source to judge model performance.</li>
</ul>
<p>Alternative cut-offs does not impact overall predictive effectiveness of the model, but rather impacts the trade-ff between the two types of errors.</p>
<ul>
<li><p><strong>Adjusting Prior Probabilities</strong>: Priors that reflect natural class imbalance will bias predictions to majority class; better to use balanced priors, or balanced training set (e.g. in naive Bayes and discriminant analysis classifiers). However, the new class probabilities are unlikely to change the rankings or AUROC; gain this is just impacting the the trade-off between sensitivity and specificity</p></li>
<li><p><strong>Unequal Case Weights</strong>: Can increase weights, e.g., in logistic regression, boosting and cart tree models, for samples in the minority classes; effectively upsampling (or duplicating data points) within the minority class(es) (therefore related to sampling methods)</p></li>
<li><strong>Sampling Methods</strong>: Select training set sample to have roughly equal event rates during initial data collection; so that model doesn’t have to deal with imbalance. Will however require that test set sampled to be more consistent with “real life”, to obtain true estimates of future performance. Post-hoc sampling approaches:
<ul>
<li>Down-sampling: Reduce number of samples to improve class balance. Methods:
<ul>
<li>Randomly sample majority classes</li>
<li>Bootstrap sample so that classes are balanced in the bootstrap sample. Advantage is that can be ran many times to obtain an estimate of variation about the down-sampling.</li>
</ul></li>
<li>Up-sampling: simulates or imputes additional data points. Majority class only appears once; minority class are sampled with replacement.</li>
<li>Synthetic minority over-sampling technique (SMOTE): Uses up- and down-sampling.</li>
<li>Uses three operational parameters; (1) amount of up=sampling, (2) amount of down-champing, and (3) number of neighbours used to impute new cases.<br />
</li>
<li>To up-sample the minority class, SMOTE synthesises new data points based on its KNNs.<br />
</li>
<li>Down-sampling based on random sampling</li>
</ul></li>
</ul>
<p><strong>Warning re: Sampling Methods:</strong> When using modified versions of the training set, resampled estimates of model performance can become biased, i.e., if data up-sampled, resampling procedures likely to have same sample in the cases that are used to build the model as well as the holdout set, leading to optimistic results. Despite this, resampling methods can be effective at tuning the models.</p>
<p><strong>Findings re: Sampling Methods</strong>: Sampling approaches have benefit of enabling better trade-offs between sensitivity and specificity.</p>
<ul>
<li><p><strong>Cost-Sensitivty Training</strong>: instead of optimising accuracy/impurity, can instead optimise cost or loss function that deferentially weights specific types of errors. By assigning higher cost to misrepresented true cases (false negatives) may bias model to under-represented class. This method has the potential to make true improvements to classifier, rather than simply altering the cut-off to make trade-offs between sensitivity and specificity.</p></li>
<li>With SVM, can specify costs for specific classes. In current implementation will not be able to perform ROC curve, instead use Kappa, sensitivity or specificity. TODO check: apply cost to class not specific type of error.</li>
<li><p>Many classification trees (CART and C5.0) can also incorporate differential costs.</p></li>
</ul>
<p>TODO: READ CH. 14<br />
<span class="math inline">\(\text{Pr(misclassification)} = Pr[2|1]\pi_1 + Pr[1|2]\pi_2\)</span> where * <span class="math inline">\(pi_i\)</span> = prior probability of sample being in class <span class="math inline">\(i\)</span> * $Pr[j|i] = probability of mistakenly predicting a class <span class="math inline">\(i\)</span> as class <span class="math inline">\(j\)</span>.</p>
<p>Use <span class="math inline">\(p_i\)</span> as estimate of <span class="math inline">\(Pr[j|i]\)</span></p>
<p>For given sample, classify into class 1 if  &gt; </p>
</div>
</div>
<div id="new-r-commands" class="section level1">
<h1>New R commands</h1>
<p><code>apropos</code>: search R packages for a given term in currently loaded packages</p>
<p><code>RSiteSearch</code>: find function in any package</p>
</div>
