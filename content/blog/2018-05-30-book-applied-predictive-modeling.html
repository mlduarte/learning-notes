---
title: 'Book: Applied Predictive Modeling (Part I)'
author: "Marie"
date: '2018-05-30'
categories:
  - Study-Notes
slug: book-applied-predictive-modeling
tags:
- R
- Study-Notes
- Book
banner: img/banners/kuhn.png
---



<div id="overview" class="section level1">
<h1>Overview</h1>
<p>This post includes my notes and reproduction of examples of <strong>Part I: General Strategies</strong> of the book <a href="http://appliedpredictivemodeling.com">Applied Predictive Modeling</a> (2013), by Max Kuhn and Kjell Johnson</p>
</div>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Common reasons for predictive model failure:</p>
<ul>
<li>Inadequate pre-processing of data</li>
<li>Inadequate model validation</li>
<li>Unjustified extrapolation</li>
<li>Overfitting</li>
<li>Insufficient number of models explorer</li>
</ul>
<p>When prediction accuracy is the primary goal, should not choose a second-rate model for interpretability. To improve accuracy, generally need a more complex model which is more difficult to interpret.</p>
<p>Foundation for effective predictive model:</p>
<ul>
<li>Intuition and deep knowledge (to obtain relevant data, eliminate noise)</li>
<li>Relevant data</li>
<li>Versatile computation toolbox (pre-processing, modelling, visualizations) The combined force of predictive modelling and intuition will be better than the parts.</li>
</ul>
<div id="predictive-modeling-process" class="section level2">
<h2>Predictive Modeling Process</h2>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li>Understand data and modelling objectives; critical for a reliable and trustworthy model for predicting new samples; necessary before moving to the next steps.</li>
<li>Preprocess and split the data</li>
<li><p>Build &amp; evaluate model</p>
<ul>
<li>Split dataset into training set and validation set. Using MPG for 2010-2011 model year cars, if goal is to predict MPG for a new car line, then model could be created using all 2010 model cars and tested on the new 2011 cars. This, in contrast, to taking a random sample of the data for model building. Use training set to try a number of techniques; only use validation set for a few strong candidate models. Repeatedly using test set negates its utility as final arbitrator</li>
<li>Alternatively can use resampling (cross-validation) to evaluate model</li>
</ul></li>
<li><p>Select model</p></li>
</ol>
<p>Themes</p>
<ul>
<li>Data splitting. How the model will be applied (e.g. whether it will be extrapolated to a new population) should determine how the training and test sets are determined. The amount of data available will also influence data splitting decisions. With a small dataset (and therefore test), resampling advised.</li>
<li>Predictors: feature selection</li>
<li>Estimating performance: Statistics (e.g. RMSE) and visualisations. Both are important.</li>
<li>Evaluating several models: There is no single model that will always do better than another.</li>
<li>Model selection: Involves choosing between models and selecting tuning parameters (within model)</li>
</ul>
</div>
</div>
<div id="data-pre-processing" class="section level1">
<h1>Data Pre-Processing</h1>
<ul>
<li>Addition, deletion or transformation of <strong>training set</strong> data</li>
<li>Can make or break model’s predictive ability</li>
<li>Feature extraction/feature engineering: how predictors are encoded, e.g. combinations, ratios</li>
<li>Feature selection: Include predictors to maximise accuracy</li>
<li>Method depends on model being used and true relationship with outcome</li>
<li>Model may require:</li>
<li>Predictors have common scale (e.g PLS)</li>
<li>Removal of outliers</li>
</ul>
<div id="data-tranformations-individual-predictors" class="section level2">
<h2>Data Tranformations (Individual Predictors)</h2>
<ul>
<li>Pre-processing techniques include:</li>
<li>Centering: Subtract average <span class="math inline">\(\rightarrow \bar{x} = 1\)</span></li>
<li>Scaling: Divide by standard deviation <span class="math inline">\(\rightarrow s = 1\)</span></li>
<li>Skewness transformations.<br />
</li>
<li>Disadvantage of transformations is loss of interpretability</li>
</ul>
<p><strong>Centering and Scaling</strong></p>
<p><strong>Skewness</strong> Skewed if <span class="math inline">\(\frac{x_\max}{x_\min} &gt; 20\)</span>, or if |skewness statistic| &gt;&gt; 0 <span class="math display">\[
    \begin{align}
      \text{skewness} &amp; = \frac{\sum (x_i - \bar{x})^3}{(n-1)v^{2/3}} \\
      v &amp; = \frac{\sum(x_i - \bar{x})^2}{(n-1)}
   \end{align}\]</span></p>
<p>Skewness can be removed by replacing data with log, square root or inverse, or by using the Box and Cox family of transformations and determining the appropriate parameter, <span class="math inline">\(\lambda\)</span>,</p>
<span class="math display">\[\begin{align}
  x^\star =  \begin{cases}\frac{x^\lambda - 1 }{\lambda} &amp; \text{if } \lambda \neq 0 \\
                    \log(x) &amp; \text{if } \lambda = 0
              \end{cases}
        
\end{align}\]</span>
<p>Note</p>
<ul>
<li>Square, <span class="math inline">\(\lambda = 2\)</span></li>
<li>Square root, <span class="math inline">\(\lambda = 0.5\)</span></li>
<li>Inverse, <span class="math inline">\(\lambda = -1\)</span></li>
</ul>
<p><span class="math inline">\(\lambda\)</span> can be estimated using training data for each feature with skewness. Transformations should only be applied if <span class="math inline">\(\lambda\)</span> outside <span class="math inline">\(1 \pm 0.02\)</span>.</p>
<p>The <code>MASS::boxcox</code> function can estimate <span class="math inline">\(\lambda\)</span> but will not create the transformed variables. The <code>caret::BoxCoxTrans</code> function will find the appropriate transformation and apply them to the new data.</p>
<p><strong>Example</strong>: Segmentation data</p>
<pre class="r"><code># load case study training data
data(&quot;segmentationOriginal&quot;)
segTrain &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)

## Remove response variables (first three columns)
segTrainId &lt;- segTrain$Cell
segTrainClass &lt;- segTrain$Class
segTrainCase &lt;- segTrain$Case
segTrainX &lt;- dplyr::select(segTrain, -c(Cell, Class, Case)) 
#%&gt;% select(-contains(&quot;Status&quot;))

# https://topepo.github.io/caret/pre-processing.html#the-preprocess-function

# Summarise the predictors
segMetrics &lt;- segTrainX %&gt;%
  select_if(is.numeric) %&gt;%
  summarise_all(funs(skewness, n_distinct, max, min)) %&gt;%
  gather(key=&quot;key&quot;, value=&quot;value&quot;) %&gt;%
  extract(key, c(&quot;variable&quot;, &quot;metric&quot;), &quot;(.*)_(skewness|n_distinct|max|min)&quot;) %&gt;%
  spread(metric, value) %&gt;%
  mutate(max_min_ratio = ifelse(min==0, (max+1)/(min+1), max/min)) %&gt;%
  select(variable, n_distinct, skewness, max_min_ratio, min, max)

## Use caret&#39;s preProcess function to transform for skewness
segPP &lt;- caret::preProcess(segTrainX, method = &quot;BoxCox&quot;)

## Apply the transformations
segTrainTrans &lt;- predict(segPP, segTrainX)

#Transformation used
# pp2df &lt;- function(object, ...) {
#   vars &lt;- unlist(object$method)
#   nums &lt;- vapply(object$method, length, c(num = 0))
#   meth &lt;- rep(names(nums), nums)
#   
#   data.frame(variable = unname(vars), method = meth)
# }

# Record results
res &lt;- data.frame(lambda = sapply(segPP$bc, `[[`, 1))
res &lt;- data.frame(variable = rownames(res), res, row.names = NULL, stringsAsFactors = FALSE)

segMetrics &lt;- segMetrics %&gt;% 
  left_join(res, by=&quot;variable&quot;) %&gt;% 
  arrange(lambda)

head(segMetrics)</code></pre>
<pre><code>##                 variable n_distinct skewness max_min_ratio        min
## 1 ConvexHullAreaRatioCh1       1000 2.476582      2.878292   1.007653
## 2          EqCircDiamCh1        399 1.955530      3.806062  13.862944
## 3               PerimCh1        657 2.589488      9.631097  47.737594
## 4            ShapeLWRCh1       1009 2.490995      7.737628   1.002813
## 5                AreaCh1        399 3.525107     14.573333 150.000000
## 6    DiffIntenDensityCh1       1009 2.760473     16.563239  26.732283
##           max lambda
## 1    2.900320   -2.0
## 2   52.763230   -1.7
## 3  459.765378   -1.1
## 4    7.759391   -1.0
## 5 2186.000000   -0.9
## 6  442.773196   -0.9</code></pre>
<pre class="r"><code>rm(res)</code></pre>
<p>In this data set there were a total of 116 features, for which:</p>
<ul>
<li><p>69 were not transformed because they had a minimum value of <span class="math inline">\(\leq 0\)</span></p></li>
<li>the remaining features had a <span class="math inline">\(\lambda\)</span> between -2 and 2</li>
<li><p>Features with a lambda between 0.98 and 1.02 would not be transformed</p></li>
</ul>
<p><strong>Question</strong>: Why not add an offset variable to ensure all variables are positive? Note, that instead of using the log transformation, or Box-Cox transformations when predictors have values of zero, can instead use the Yeo-Johnson family of transformations. This family is similar to the Box-Cos transformations but can handle zero or negative predictor values.</p>
<p>It should be noted that transformations to reduce skewness might not always be successful. Under such circumstances, one should use models that are not unduly affected by skewed distributions (e.g. tree-based methods)</p>
<p>As an example, the feature <em>VarIntenCh3</em>, which records the standard deviation of pix intensity in actin filaments, had the following properties:</p>
<ul>
<li>Metrics</li>
</ul>
<pre class="r"><code>filter(segMetrics, variable == &quot;VarIntenCh3&quot;)</code></pre>
<pre><code>##      variable n_distinct skewness max_min_ratio       min     max lambda
## 1 VarIntenCh3       1009 2.391624      870.8872 0.8692526 757.021    0.1</code></pre>
<ul>
<li>Strong right skewness</li>
<li>Original data distribution</li>
</ul>
<pre class="r"><code>histogram(~segTrainX$VarIntenCh3,
          xlab = &quot;Natural Units&quot;,
          type = &quot;count&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/original_segmentation_features-1.png" width="672" /> * Transformed data distribution</p>
<pre class="r"><code>histogram(~log(segTrainX$VarIntenCh3),
          xlab = &quot;Log Units&quot;,
          ylab = &quot; &quot;,
          type = &quot;count&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/trans_segmentation_features-1.png" width="672" /></p>
</div>
<div id="data-transformations-multiple-predictors" class="section level2">
<h2>Data Transformations (Multiple Predictors)</h2>
<p><strong>Outliers</strong></p>
<ul>
<li>Is value valid or has recording error occurred?</li>
<li>Take care to remove or change values, especially if sample size is small (as could be a result of a skewed distribution without enough data to see the skewness, or could be an indication of a special part of the population)</li>
<li>Decision trees and SVMs are insensitive to outliers</li>
<li>The <em>spatial sign</em> transformation can minimize the problem of a model’s sensitivity to outliers</li>
</ul>
<p>Spatial Sign Transformation</p>
<ul>
<li>Projects predictor values onto a multidimensional sphere</li>
<li>Makes all samples the same distance from the centre of the sphere</li>
<li>Each sample is divided by its squared norm: <span class="math display">\[ x_{ij}^* = \frac{x_{ij}}{\sum^P_{j=1} x^2_{ij}}\]</span></li>
<li>The denominator measures the squared distance to the centre of the predictors distribution</li>
<li>It is important to center and scale the predictor data prior to using this transformation</li>
<li>The predictors are transformed as a group, making removal of a predictor problematic</li>
</ul>
<p>Example:</p>
<ul>
<li>Investigation of ~ 8 outliers shows valid but poorly sampled population (e.g. highly profitable customers)</li>
<li>Spatial sign transformation applied; outliers reside in northwest section of distribution but contracted inwards</li>
<li>Mitigates effect on model training</li>
</ul>
<pre class="r"><code>trellis.par.set(caretTheme())
featurePlot(x=iris[,-5], y=iris[,5], &quot;pairs&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/iris_spatial_trans-1.png" width="672" /></p>
<pre class="r"><code>featurePlot(spatialSign(scale(iris[,-5])), iris[,5], &quot;pairs&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/iris_spatial_trans-2.png" width="672" /></p>
<pre class="r"><code>set.seed(1)
n &lt;- 10000
tmp &lt;- data.frame(x=c(rnorm(n, 0, 0.02), -1, 1, 0.5),
                  y=c(rnorm(n, 0, 0.2), -1, 1, -2))

plot(tmp, asp=1, col=c(rep(1,n), 2, 3, 4), pch=19)
grid()</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/rand_spatial_trans-1.png" width="672" /></p>
<pre class="r"><code>plot(spatialSign(tmp), asp=1, col=c(rep(1,n), 2, 3, 4), pch=19)
grid()</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/rand_spatial_trans-2.png" width="672" /></p>
<p><strong>Data Reduction and Feature Extraction</strong></p>
<p>Data reduction techniques</p>
<ul>
<li>Generate smaller set of predictors that capture most of the information within the original variables</li>
<li><em>Signal (Feature) Extraction</em> techniques: New predictors are functions of original variables (therefore all original variables required)</li>
<li>PCA
<ul>
<li>Finds linear combinations of predictors (principal components) to capture most possible variance</li>
<li>First PC captures more variability than any other linear combination</li>
<li>Subsequent PCs are uncorrelated with all previous PCs</li>
<li>Principal Component can be written as: <span class="math display">\[PC_j = (a_{j1} x \text{Predictor} 1) + (a_{j2} x \text{Predictor} 2) + \ldots + (a_{jP} x \text{Predictor} P)\]</span> where P = # predictors, coefficients = component weights (or loadings) that show which predictors are important for given PC.</li>
<li>Advantage: creates uncorrelated components, which is required for stability of some models</li>
<li><p>Disadvantages: Seeks predictor-set variation without regard to predictor measurement scales/distributions or response variable; without guidance can summarize data characteristics that are irrelevant to structure of data and modelling objective</p>
<ul>
<li>Seeks linear combinations that maximize variability, and therefore will first summarise predictors with more variation. If original predictors are on measurements scales then first few components will summarise higher magnitude predictors; consequently it will focus on identifying data structure based on measurement scales rather than based on important relationships within the data for the current problem.</li>
<li>Unsupervised technique; does not consider consider modelling objective / response variability.</li>
</ul></li>
</ul></li>
<li>Should first transform skewed predictors and then center and scale prior to performing PCA; prevent PCA being influenced by original measurement scales</li>
<li>Consider Partial Least Squares (PLS) to derive components with response variable in mind.</li>
<li>Use scree plot to decide number of components to keep; in automated model building process, optimal number can be determined by cross-validation</li>
<li>Should also visually examine the PCs; plot first few PCs against each other and color points by, e.g., class labels. If PCA has captured sufficient amount of information in data, may demonstrate clusters of samples/outliers requiring closer examination. Ensure to use the same scale as later components will often have smaller data ranges and plotting on separate scales may lead to potential to over-interpret patterns.</li>
</ul>
<p>Example: PCA applied to two features</p>
<ul>
<li>For channel 1, Intensity Entropy is highly correlated with Fiber Width (0.93)</li>
<li>Could use just one predictor, or could use PCA to instead use a linear combination of these two predictors</li>
</ul>
<pre class="r"><code>## R&#39;s prcomp is used to conduct PCA
pr &lt;- prcomp(~ AvgIntenCh1 + EntropyIntenCh1, 
             data = segTrainTrans, #already pre-preprocessed for skewness
             scale. = TRUE)


transparentTheme(pchSize = .7, trans = .3)

xyplot(AvgIntenCh1 ~ EntropyIntenCh1,
       data = segTrainTrans,
       groups = segTrain$Class,
       xlab = &quot;Channel 1 Fiber Width&quot;,
       ylab = &quot;Intensity Entropy Channel 1&quot;,
       auto.key = list(columns = 2),
       type = c(&quot;p&quot;, &quot;g&quot;),
       main = &quot;Original Data&quot;,
       aspect = 1)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_PCA-1.png" width="672" /></p>
<pre class="r"><code>xyplot(PC2 ~ PC1,
       data = as.data.frame(pr$x),
       groups = segTrain$Class,
       xlab = &quot;Principal Component #1&quot;,
       ylab = &quot;Principal Component #2&quot;,
       main = &quot;Transformed&quot;,
       xlim = extendrange(pr$x),
       ylim = extendrange(pr$x),
       type = c(&quot;p&quot;, &quot;g&quot;),
       aspect = 1)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_PCA-2.png" width="672" /></p>
<ul>
<li>Because first PC summarises 96.6% variation and the second 3.42%, in this case could just use the first PC.</li>
</ul>
<p>Example: PCA applied to all features</p>
<p>The scree plot shows that four PCs would be retained.</p>
<pre class="r"><code>## Apply PCA to the entire set of predictors.

## There are a few predictors with only a single value, so we remove these first
## (since PCA uses variances, which would be zero)
isZV &lt;- apply(segTrainX, 2, function(x) length(unique(x)) == 1)

segPP &lt;- preProcess(segTrainX[, !isZV], c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;))
segTrainTrans &lt;- predict(segPP, segTrainX[, !isZV])

segPCA &lt;- prcomp(segTrainTrans, center = TRUE, scale. = TRUE)

tab &lt;- summary(segPCA)$importance[2,]
tab &lt;- data.frame(PC = names(tab), Var = tab, row.names=NULL, stringsAsFactors = FALSE) %&gt;% mutate(PC=parse_number(PC))
ggplot(tab, aes(x = PC, y=Var))+ 
         geom_line() + 
  geom_point()</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_pca_scree-1.png" width="672" /></p>
<p>Scatterplot matrix of first 3 PCs (with points coloured by class):</p>
<ul>
<li>Appears to be some separation between classes when plotting first and second component (but remember that these two components only explain 26.6% of variance and so don’t over-interpret!). However distribution of well-segmented cells roughly contained within poorly identified cells; cell types don’t appear to be easily separated. Don’t despair!; it does not mean other models, e.g. that can accommodate non-linear relationships, will reach the same conclusions.</li>
</ul>
<pre class="r"><code>## Plot a scatterplot matrix of the first three components
transparentTheme(pchSize = .8, trans = .3)
panelRange &lt;- extendrange(segPCA$x[, 1:3])
splom(as.data.frame(segPCA$x[, 1:3]),
      groups = segTrainClass,
      type = c(&quot;p&quot;, &quot;g&quot;),
      as.table = TRUE,
      auto.key = list(columns = 2),
      prepanel.limits = function(x) panelRange)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_pca_scatter-1.png" width="672" /></p>
<ul>
<li>Can also visualise which predictor is associated with each principal component. A coefficient (loading) close to zero within the PC linear equation indicates that that predictors did not contribute much to that component. In the following figure, each point corresponds to a predictor variable and is coloured by the optical channel used in the experiment. For the first PC, channel 1 (cell body) loadings are greater and therefore have largest effect on PC. However, even though cell body measurements account for more variation in the data, this does not imply that these variables will be associated with predicting segmentation quality.</li>
</ul>
<pre class="r"><code>## Format the rotation values for plotting
segRot &lt;- as.data.frame(segPCA$rotation[, 1:3])

## Derive the channel variable
vars &lt;- rownames(segPCA$rotation)
channel &lt;- rep(NA, length(vars))
channel[grepl(&quot;Ch1$&quot;, vars)] &lt;- &quot;Channel 1&quot;
channel[grepl(&quot;Ch2$&quot;, vars)] &lt;- &quot;Channel 2&quot;
channel[grepl(&quot;Ch3$&quot;, vars)] &lt;- &quot;Channel 3&quot;
channel[grepl(&quot;Ch4$&quot;, vars)] &lt;- &quot;Channel 4&quot;

segRot$Channel &lt;- channel
segRot &lt;- segRot[complete.cases(segRot),]
segRot$Channel &lt;- factor(as.character(segRot$Channel))

## Plot a scatterplot matrix of the first three rotation variable
transparentTheme(pchSize = .8, trans = .7)
panelRange &lt;- extendrange(segRot[, 1:3])
upperp &lt;- function(...)
  {
    args &lt;- list(...)
    circ1 &lt;- ellipse(diag(rep(1, 2)), t = .1)
    panel.xyplot(circ1[,1], circ1[,2],
                 type = &quot;l&quot;,
                 lty = trellis.par.get(&quot;reference.line&quot;)$lty,
                 col = trellis.par.get(&quot;reference.line&quot;)$col,
                 lwd = trellis.par.get(&quot;reference.line&quot;)$lwd)
    circ2 &lt;- ellipse(diag(rep(1, 2)), t = .2)
    panel.xyplot(circ2[,1], circ2[,2],
                 type = &quot;l&quot;,
                 lty = trellis.par.get(&quot;reference.line&quot;)$lty,
                 col = trellis.par.get(&quot;reference.line&quot;)$col,
                 lwd = trellis.par.get(&quot;reference.line&quot;)$lwd)
    circ3 &lt;- ellipse(diag(rep(1, 2)), t = .3)
    panel.xyplot(circ3[,1], circ3[,2],
                 type = &quot;l&quot;,
                 lty = trellis.par.get(&quot;reference.line&quot;)$lty,
                 col = trellis.par.get(&quot;reference.line&quot;)$col,
                 lwd = trellis.par.get(&quot;reference.line&quot;)$lwd)
    panel.xyplot(args$x, args$y, groups = args$groups, subscripts = args$subscripts)
}
# note this requires ellipse
lattice::splom(~segRot[, 1:3],
      groups = segRot$Channel,
      lower.panel = function(...){}, upper.panel = upperp,
      prepanel.limits = function(x) panelRange,
      auto.key = list(columns = 2))</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_pca_splom-1.png" width="672" /></p>
</div>
<div id="missing-values" class="section level2">
<h2>Missing Values</h2>
<p>Types of missing data:</p>
<ul>
<li>Structurally missing, e.g. number of children man has given birth to</li>
<li>Informative missingness: if related to outcome, can induce bias, e.g. customer rating are using polarised</li>
<li>Censored (this is not missing data, as something is known about it), but for predictive models, it may be treated as missing, or the censored value may be used as the observed value. E.g., for laboratory test which cannot measure below a limit, may use a random value between 0 and the limit as the observed value.</li>
</ul>
<p>Options:</p>
<ul>
<li>Remove samples (if small subset of large data set)</li>
<li>Specifically account for the missingness (e.g. tree-based techniques)</li>
<li>Impute by using information in the training set of predictors (i.e. predictive model within a predictive model).
<ul>
<li>Note that imputation for statistical inference is not the same as inference for predictive models (references given for the latter).<br />
</li>
<li>Incorporate imputation in resampling if being used to select tuning parameter values.<br />
</li>
<li>If number of predictors affected by missing values is small, best to perform exploratory analysis of relationships between predictors, e.g. using PCA or visualisations. If a variable with missing values is highly correlated with another then can use a focused model</li>
<li>KNN popular to impute by finding samples in training set closest to it and averages nearby points to fill it. Advantage: Confined to training set. Disadvantage: KNN requires entire training set and number of neighbours and method of determining “closeness” are tuning parameters.</li>
</ul></li>
</ul>
<pre class="r"><code>require(AppliedPredictiveModeling)
data(segmentationOriginal)

## Retain the original training set
segTrain &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)

## Remove the first three columns (identifier columns)
segTrainX &lt;- segTrain[, -(1:3)]
segTrainX &lt;- segTrainX[, -nearZeroVar(segTrainX)]

# Randomly sample 50 to be missing
set.seed(15103930)
ind_miss &lt;- sample(1:nrow(segTrainX), 50, replace = FALSE)
obs &lt;- segTrainX[ind_miss, ]
segTrainX$PerimCh1[ind_miss] &lt;- NA
summary(segTrainX$PerimCh1)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   47.74   64.92   79.09   92.20  103.67  459.77      50</code></pre>
<pre class="r"><code># Predict the missing values
set.seed(100)
knnTrans &lt;- preProcess(segTrainX, method = c(&quot;center&quot;, &quot;scale&quot;, &quot;knnImpute&quot;))
knnPred &lt;- predict(knnTrans, segTrainX)
pred &lt;- knnPred$PerimCh1[ind_miss]
obsTrans &lt;- predict(knnTrans, obs)$PerimCh1

ggplot(data.frame(obs = obsTrans, pred), aes(x=obs, y=pred)) + 
  geom_point() + 
  geom_smooth(method=&#39;lm&#39;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_imputation_knn-1.png" width="672" /></p>
<pre class="r"><code>cor(obsTrans, pred)</code></pre>
<pre><code>## [1] 0.8817321</code></pre>
<pre class="r"><code>#if(&quot;PerimCh1&quot; %in% preProcValues$method$scale) pred &lt;- pred * preProcValues$std[&quot;PerimCh1&quot;]
#if(&quot;PerimCh1&quot; %in% preProcValues$method$center) pred &lt;- pred + preProcValues$mean[&quot;PerimCh1&quot;]</code></pre>
<pre class="r"><code># use cell fiber lengtha nd cell size. to predict.  
segCorr &lt;- cor(segTrainX, use = &quot;pairwise.complete.obs&quot;)[,&quot;PerimCh1&quot;]
sort(segCorr, decreasing = TRUE)[2:3]</code></pre>
<pre><code>## FiberLengthCh1      LengthCh1 
##      0.9855599      0.9223834</code></pre>
<pre class="r"><code>mod &lt;- lm(PerimCh1 ~ FiberLengthCh1 + LengthCh1, data = segTrainX[-ind_miss, ])
pred &lt;- predict(mod, segTrainX[ind_miss, ])
ggplot(data.frame(obs = obsTrans, pred), aes(x=obs, y=pred)) + 
  geom_point() + 
  geom_smooth(method=&#39;lm&#39;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_imputation_regression-1.png" width="672" /></p>
<pre class="r"><code>cor(pred, obsTrans)</code></pre>
<pre><code>## [1] 0.9752604</code></pre>
</div>
<div id="removing-predictors" class="section level2">
<h2>Removing Predictors</h2>
<p>Advantage of less predictors:</p>
<ul>
<li>Decreased computation time</li>
<li>Decreased complexity / More parsimonious / More interpretable</li>
<li>Highly correlated predictors are measuring the same thing and so have no extra information</li>
<li>Better model performance / stability without problematic and correlated variables</li>
</ul>
<p>Predictors to remove: * Zero Variance and near-zero * Correlated</p>
</div>
<div id="zero-variance-and-near-zero-variance-predictors" class="section level2">
<h2>Zero Variance and Near-Zero Variance Predictors</h2>
<ul>
<li><strong>Zero variance predictors</strong> : Those with a single unique value</li>
<li><strong>Near-zero variance predictors</strong> : Those with a single value for most samples</li>
</ul>
<p>To determine:</p>
<p>1.Calculate number of unique variables / number of samples 2. Calculate frequency of unique values (or ratio of most common frequent to second most frequent)</p>
<p>If (1) fraction of unique values over sample size is low, e.g. <span class="math inline">\(\leq\)</span> 10% and (2) ratio of frequency of most prevalent value to the second most prevalent value is large, e.g. <span class="math inline">\(\geq\)</span> 20</p>
</div>
<div id="correlated-variables" class="section level2">
<h2>Correlated Variables</h2>
<ul>
<li><strong>Collinearity</strong>: Correlated pair of predictor variables</li>
<li><strong>Multicollinearity</strong>: Relationships between multiple predictors at once</li>
</ul>
<p>To detect: 1. Visually.</p>
<pre class="r"><code>segData &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)[, -(1:3)]
isZV &lt;- apply(segData, 2, function(x) length(unique(x)) == 1)
segData &lt;- segData[, !isZV]
correlations &lt;- cor(segData)
corrplot::corrplot(correlations, order = &quot;hclust&quot;, tl.cex = 0.2)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_correlation_matrix-1.png" width="672" /></p>
<ol start="2" style="list-style-type: decimal">
<li><p>PCA: If first PCA accounts for large percentage of variance, then there is at least one group of predictors that represent the same information. Use PCA loadings to understand which predictors are associated with each component.</p></li>
<li><p>Variance Inflation Factor (VIF) statistic available as part of classical regression analysis, however only useful for linear regression. Whilst it determine collinear predictors it does not determine which should be removed to resolve the problem.</p></li>
</ol>
<p>To remove correlated predictors:</p>
<ol style="list-style-type: decimal">
<li>Use a heuristic algorithm to remove the minimum number of predictors such that all pairwise correlations are below a certain threshold. This only identifies colinearities in two dimensions, but can improve model performance. Algorithm is as follows:</li>
<li>Calculate correlation matrix</li>
<li>Determine pair of predictors with largest absolute correlation</li>
<li>Determine average correlation between A and other variables. Repeat for B.</li>
<li>Remove the predictor (A or B) with the largest average correlation</li>
<li>Repeat steps 2-3 until no absolute correlations above the threshold</li>
</ol>
<p>This algorithm is implemented using <code>caret::findCorrelation</code></p>
<pre class="r"><code>segTrain &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)
## Remove the first three columns (identifier columns)
segTrainX &lt;- segTrain[, -(1:3)]
isZV &lt;- apply(segTrainX, 2, function(x) length(unique(x)) == 1)
segTrainX &lt;- segTrainX[, !isZV]

segTrainClass &lt;- segTrain$Class



segPP &lt;- preProcess(segTrainX, c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;))
segTrainTrans &lt;- predict(segPP, segTrainX)


## Use caret&#39;s preProcess function to transform for skewness
segPP &lt;- preProcess(segTrainX, method = &quot;BoxCox&quot;)
## Apply the transformations
segTrainTrans &lt;- predict(segPP, segTrainX)
segCorr &lt;- cor(segTrainTrans)
#corrplot(segCorr, order = &quot;hclust&quot;, tl.cex = .35)

## caret&#39;s findCorrelation function is used to identify columns to remove.
highCorr &lt;- findCorrelation(segCorr, .75)
length(highCorr)</code></pre>
<pre><code>## [1] 43</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Feature exraction algorithms (e.g. PCA) can also be used to mitigate effect of correlations; the disadvantage is that they make connection between predictors more complex and as unsupervised no gurantee that resulting components will have relationship with outcome.</li>
<li>Simulated Annealing (e.g., <code>subselect</code> package)</li>
<li>Genetic Algorithms</li>
</ol>
</div>
<div id="adding-predictors" class="section level2">
<h2>Adding Predictors</h2>
<ul>
<li><p>It is common to decompose categorical predictors using dummy variables (indicator with zero/one). With 5 categories, only 4 dummy variables needed. If 5 were included in regression then would have numerical issues (intercept replaces the other); for other models, including all 5 may aid interpretation</p></li>
<li>Non-linear transformations, e.g. <span class="math inline">\(B^2\)</span>, where <span class="math inline">\(B\)</span> is a predictor</li>
<li><p>Combinations of data (e.g. class centroids, center of predictor data to each class)</p></li>
</ul>
</div>
<div id="binning-predictors" class="section level2">
<h2>Binning Predictors</h2>
<p>Disadvantages of manual binning:</p>
<ul>
<li>Loss of performance in the model</li>
<li>Loss in prediction in the predictions</li>
<li>Can lead to high rate of false positives (noisy predictors determined to be informative)</li>
</ul>
<p>Advantage: More interpretable. However the perceived improvement in interpretability by manual binning is usually offset by significant loss in performance (the goal of this book is prediction not interpretation and so manual binning not recommended).</p>
<p>Automatic Binning * E.G Classification / Regression Trees * Multivariate adaptive regression splines * Evaluate many variables simultaneously, based on statistically sound methodologies</p>
</div>
<div id="computing" class="section level2">
<h2>Computing</h2>
<div id="transformations" class="section level3">
<h3>Transformations</h3>
<ul>
<li><code>e1071::sknewness</code>: calculates sample skewness statistics for each predictor. Those that are highly skewed can be prioritised for distribution visualisations using <code>hist</code>, <code>lattice:histogram</code></li>
<li><code>MASS::boxcox</code>: to determine type of transformation to use; estimates <span class="math inline">\(\lambda\)</span> but will not create the transformed variable(s)</li>
<li><code>caret::BoxCoxTrans</code>: Find most appropriate transformation and apply to new data</li>
</ul>
<p>Example:</p>
<pre class="r"><code>segData &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)[, -(1:3)]
Ch1AreaTrans &lt;- BoxCoxTrans(segData$AreaCh1)
Ch1AreaTrans</code></pre>
<pre><code>## Box-Cox Transformation
## 
## 1009 data points used to estimate Lambda
## 
## Input data summary:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   150.0   194.0   256.0   325.1   376.0  2186.0 
## 
## Largest/Smallest: 14.6 
## Sample Skewness: 3.53 
## 
## Estimated Lambda: -0.9</code></pre>
<pre class="r"><code>data.frame(orig = head(segData$AreaCh1), auto.trans = predict(Ch1AreaTrans, head(segData$AreaCh1)), man.trans = ( head(segData$AreaCh1) ^ Ch1AreaTrans$lambda - 1 )/Ch1AreaTrans$lambda   )</code></pre>
<pre><code>##   orig auto.trans man.trans
## 1  819   1.108458  1.108458
## 2  431   1.106383  1.106383
## 3  298   1.104520  1.104520
## 4  256   1.103554  1.103554
## 5  258   1.103607  1.103607
## 6  358   1.105523  1.105523</code></pre>
<ul>
<li><code>prcomp</code>: For PCA</li>
</ul>
<p>Example:</p>
<pre class="r"><code>segData &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)[, -(1:3)]
isZV &lt;- apply(segData, 2, function(x) length(unique(x)) == 1)
segData &lt;- segData[, !isZV]

pcaObject &lt;- prcomp(segData, center = TRUE, scale. = TRUE)
# Calculate the cumulative percentage of variance which each component accounts for.
percentVariance &lt;- pcaObject$sd^2/sum(pcaObject$sd^2)*100
percentVariance[1:3]</code></pre>
<pre><code>## [1] 14.677086 11.734794  8.813733</code></pre>
<pre class="r"><code># Transformed variables stored in a `pcaObject` as a sub-object called x
head(pcaObject$x[ , 1:5])</code></pre>
<pre><code>##            PC1        PC2         PC3       PC4          PC5
## 2  -10.5813682  4.7022663  0.05567686 -1.792190  2.547714227
## 3    0.4154024  0.6974872 -1.38763482 -3.265656  1.672695719
## 4   -0.9779724 -1.8203879 -2.21605066  0.397754 -0.008029369
## 12   1.9842183 -0.3322714 -0.05032939 -2.517085  1.033381188
## 15   1.1217009 -0.1825613 -1.99234368 -3.021610  0.478149323
## 16   1.0824587  0.2026488 -1.16865264 -3.380599  1.200990054</code></pre>
<pre class="r"><code># Loadings
head(pcaObject$rotation[ , 1:3])</code></pre>
<pre><code>##                         PC1         PC2           PC3
## AngleCh1        0.001058888 -0.01186622  0.0008836706
## AngleStatusCh1  0.011793294  0.02083728 -0.0035682644
## AreaCh1        -0.211068512  0.09577310  0.0672000815
## AreaStatusCh1  -0.182337308  0.07292658  0.0546691134
## AvgIntenCh1     0.061268408  0.17388938  0.0413066884
## AvgIntenCh2     0.101313516  0.16201999  0.0349230837</code></pre>
<ul>
<li><code>caret::spatialSign</code>: Spatial sign transformation</li>
<li><code>impute::impute.knn</code> to estimate missing data; also possible using <code>caret::PreProcess</code> which applies imputation methods based on KNN or bagged trees</li>
<li><p><code>caret::PreProcess</code>: Has the ability to</p></li>
<li>Transform</li>
<li>Center</li>
<li>Scale</li>
<li>Impute values</li>
<li>Feature extraction</li>
<li><p>Apply spatial sign transformation</p></li>
</ul>
<p>(in this order) After calling the <code>preProcess</code> function, the <code>predict</code> method applies the transformation results to a set of data.</p>
</div>
<div id="filtering" class="section level3">
<h3>Filtering</h3>
<ul>
<li><code>caret::nearZeroVar</code>: To determine predictors with unique variables</li>
<li><code>cor</code>: To determine correlations</li>
<li><code>corrplot:corrplot</code>: Includes option to reorder variables in a way that reveals clusters of highly correlated predictors</li>
<li><code>caret::findCorrelation</code>: Recommends columns for deletion based on a given threshold of pairwise correlations</li>
<li><code>subselect</code> package also has methods for selecting predictors</li>
</ul>
</div>
<div id="creating-dummy-variables" class="section level3">
<h3>Creating Dummy Variables</h3>
<ul>
<li><code>caret::dummyVars</code>: To determine encoding for categorical predictors</li>
<li>All dummy variables recommended when using tree-based model</li>
</ul>
<pre class="r"><code>data(cars)
carSubset &lt;- select(cars, Price, Mileage)
type &lt;- c(&quot;convertible&quot;, &quot;coupe&quot;, &quot;hatchback&quot;, &quot;sedan&quot;, &quot;wagon&quot;)
carSubset$Type &lt;- factor(apply(cars[, 14:18], 1, function(x) type[which(x == 1)]))

simpleMod &lt;- dummyVars(~Mileage + Type,
                       data = carSubset,
                       ## Remove the variable name from the
                       ## column name
                       levelsOnly = TRUE)
simpleMod</code></pre>
<pre><code>## Dummy Variable Object
## 
## Formula: ~Mileage + Type
## 2 variables, 1 factors
## Factor variable names will be removed
## A less than full rank encoding is used</code></pre>
<pre class="r"><code># To generate the dummy variables fo rhte training set or any new samples, use the predict method with teh dummyVars objects
predict(simpleMod, head(carSubset))</code></pre>
<pre><code>##   Mileage convertible coupe hatchback sedan wagon
## 1   20105           0     0         0     1     0
## 2   13457           0     1         0     0     0
## 3   31655           1     0         0     0     0
## 4   22479           1     0         0     0     0
## 5   17590           1     0         0     0     0
## 6   23635           1     0         0     0     0</code></pre>
<pre class="r"><code>withInteraction &lt;- dummyVars(~Mileage + Type + Mileage:Type,
                             data = carSubset,
                             levelsOnly = TRUE)
withInteraction</code></pre>
<pre><code>## Dummy Variable Object
## 
## Formula: ~Mileage + Type + Mileage:Type
## 2 variables, 1 factors
## Factor variable names will be removed
## A less than full rank encoding is used</code></pre>
<pre class="r"><code>predict(withInteraction, head(carSubset))</code></pre>
<pre><code>##   Mileage convertible coupe hatchback sedan wagon Mileage:convertible
## 1   20105           0     0         0     1     0                   0
## 2   13457           0     1         0     0     0                   0
## 3   31655           1     0         0     0     0               31655
## 4   22479           1     0         0     0     0               22479
## 5   17590           1     0         0     0     0               17590
## 6   23635           1     0         0     0     0               23635
##   Mileage:coupe Mileage:hatchback Mileage:sedan Mileage:wagon
## 1             0                 0         20105             0
## 2         13457                 0             0             0
## 3             0                 0             0             0
## 4             0                 0             0             0
## 5             0                 0             0             0
## 6             0                 0             0             0</code></pre>
</div>
</div>
<div id="chapter-3-exercises" class="section level2">
<h2>Chapter 3 Exercises</h2>
<div id="exercise-3.1" class="section level3">
<h3>Exercise 3.1</h3>
<p>The UC Irvine Machine Learning Repository contains a data set related to glass identification. The data consist of 214 glass samples labeled as one of seven class categories. There are nine predictors, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.</p>
<pre class="r"><code>require(mlbench)
data(Glass)
str(Glass)</code></pre>
<pre><code>## &#39;data.frame&#39;:    214 obs. of  10 variables:
##  $ RI  : num  1.52 1.52 1.52 1.52 1.52 ...
##  $ Na  : num  13.6 13.9 13.5 13.2 13.3 ...
##  $ Mg  : num  4.49 3.6 3.55 3.69 3.62 3.61 3.6 3.61 3.58 3.6 ...
##  $ Al  : num  1.1 1.36 1.54 1.29 1.24 1.62 1.14 1.05 1.37 1.36 ...
##  $ Si  : num  71.8 72.7 73 72.6 73.1 ...
##  $ K   : num  0.06 0.48 0.39 0.57 0.55 0.64 0.58 0.57 0.56 0.57 ...
##  $ Ca  : num  8.75 7.83 7.78 8.22 8.07 8.07 8.17 8.24 8.3 8.4 ...
##  $ Ba  : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ Fe  : num  0 0 0 0 0 0.26 0 0 0 0.11 ...
##  $ Type: Factor w/ 6 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;5&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<pre class="r"><code>GlassPredictors &lt;- select(Glass, -Type) %&gt;%
  gather(Predictor, Value)</code></pre>
<ol style="list-style-type: lower-alpha">
<li>Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors.</li>
</ol>
<pre class="r"><code>ggplot(GlassPredictors, aes(x = Value)) +  
  geom_histogram() + 
  facet_wrap(~Predictor, scales = &quot;free&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/glass_hist-1.png" width="672" /></p>
<pre class="r"><code>ggplot(GlassPredictors, aes(x = Value)) +  
  geom_density() + 
  facet_wrap(~Predictor, scales = &quot;free&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/glass_dens-1.png" width="672" /></p>
<pre class="r"><code>require(GGally)
ggpairs(select(Glass, -Type))</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/glass_predictor_scatter-1.png" width="672" /></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Does there appear to be any outliers in the data? Are predictors skewed?</li>
</ol>
<ul>
<li>Several variables show signs of skewness (BA, Ca, FE, RI)</li>
<li>Variable K could be skewed or have outliers</li>
<li>Variables K and MG have possible second modes around zero, whereas Fe and Ba also have a high distribution of values around zero</li>
<li>Variables Ca, NA, RI and SI have concentrations of samples in the middle of the scale and a small number of data points at the edges</li>
<li>Variables Ca and Ri are positively correlated, and Ca and NA</li>
<li>Visually, Ca and Na appear to be negatively correlated, however the correlation score is -0.275</li>
</ul>
<ol start="3" style="list-style-type: lower-alpha">
<li>Are there any relevant transformations of one or more predictors that might improve the classification model?</li>
</ol>
<p>Due to variables containing zero, use the Yeo-Johnson family of transformations</p>
<pre class="r"><code>glassTrans &lt;- preProcess(select(Glass, -Type), method = &quot;YeoJohnson&quot;)
glassTransData &lt;- predict(glassTrans, select(Glass, -Type))
dat &lt;- glassTransData %&gt;%
  gather(Predictor, Value) %&gt;%
  mutate(Transformation = &quot;Yeo-Johnson&quot;) %&gt;%
  bind_rows(data.frame(GlassPredictors, Transformation = &quot;NA&quot;))


# Final result
res &lt;- list()
# How many points we want 
nPoints &lt;- 1e3

# Using simple loop to scale and calculate density
combinations &lt;- expand.grid(unique(dat$Predictor), unique(dat$Transformation))
for(i in 1:nrow(combinations)) {
    # Subset data
    foo &lt;- subset(dat, Predictor == combinations$Var1[i] &amp; Transformation == combinations$Var2[i])
    # Perform density on scaled signal
    densRes &lt;- density(x = scale(foo$Value), n = nPoints)
    # Position signal from 1 to wanted number of points
    res[[i]] &lt;- data.frame(x = 1:nPoints, y = densRes$y, 
                           pred = combinations$Var1[i], trans = combinations$Var2[i])
}
res &lt;- do.call(rbind, res)
ggplot(res, aes(x / nPoints, y, color = trans, linetype = trans)) +
    geom_line(alpha = 0.5, size = 1) +
    facet_wrap(~ pred, scales = &quot;free&quot;)  +
  labs(x = &quot;Value / Points&quot;, y = &quot;Scaled Density&quot;, colour = &quot;Transformation&quot;, linetype = &quot;Transformation&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/glass_yeo_trans-1.png" width="672" /></p>
<p>However, they don’t really help in terms of skewness …</p>
<p>To mitigate outliers, use the spatial sign transformation</p>
<pre class="r"><code>glassCSSSTrans &lt;- preProcess(select(Glass, -Type), method = c(&quot;center&quot;, &quot;scale&quot;, &quot;spatialSign&quot;))
glassCSSSData &lt;- predict(glassCSSSTrans, select(Glass, -Type))
ggpairs(glassCSSSData)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/glass_spatial_trans-1.png" width="672" /></p>
<p>Spatial sign transformation was effective in removing outliers.</p>
</div>
<div id="exercise-2" class="section level3">
<h3>Exercise 2</h3>
<p>The soybean data can also be found at the UC Irvine Machine Learning Repository. Data were collected to predict disease in 683 soybeans. The 35 predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes.</p>
<ol style="list-style-type: lower-alpha">
<li>Investigate the frequency distributions for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this chapter?</li>
</ol>
<p>Note: By degenerate, I believe it is meant whether the following exist:</p>
<ul>
<li>Skewness</li>
<li>Outliers</li>
<li>Missing values</li>
<li>Zero- and near-zero- variance</li>
</ul>
<pre class="r"><code>select(Soybean, -Class) %&gt;%
  select_if(negate(is.numeric)) %&gt;%
  gather(Variable, Value) %&gt;%
  ggplot(aes(x = Value)) + 
  geom_bar() + 
  facet_wrap(~ Variable, scales = &quot;free&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/soybean_hist-1.png" width="672" /></p>
<p>From these plots we see:</p>
<ul>
<li>Few observations in April (0)</li>
<li>Precipitation usually greater than normal (2)</li>
<li>Temperature usually normal</li>
<li>Temperature, precipitation and hail are missing values.</li>
</ul>
<p>A mosaic plot, fluctuation diagram, or faceted bar chart may be used to display two categorical variable, as detailed in <a href="https://vita.had.co.nz/papers/gpp.pdf">The Generalized Pairs Plot</a></p>
<pre class="r"><code>ggpairs(select(Soybean, date, precip, temp), diag = &#39;blankDiag&#39;, upper = list(discrete = &quot;ratio&quot;))</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/soybean_pairs-1.png" width="672" /></p>
<pre class="r"><code>require(vcd)
#require(ggmosaic)
#detach(&quot;package:kernlab&quot;, unload=TRUE)
Soybean2 &lt;- Soybean %&gt;%
  mutate(date = fct_recode(date, Apr = &quot;0&quot;, May = &quot;1&quot;, Jun = &quot;2&quot;, Jul = &quot;3&quot;, Aug = &quot;4&quot;, Sep = &quot;5&quot;, Oct = &quot;6&quot;), 
         temp = fct_recode(temp, &quot;lt-norm&quot; = &quot;0&quot;, &quot;norm&quot; = &quot;1&quot;, &quot;gt-norm&quot; = &quot;2&quot;))
vcd::mosaic(~date + temp, data = Soybean2)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/soybean_mosaic-1.png" width="672" /></p>
<pre class="r"><code>ggplot(Soybean2, aes(x = date, fill = temp)) + geom_bar()</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/soybean_mosaic-2.png" width="672" /></p>
<pre class="r"><code>Soybean2 %&gt;%
  group_by(date, temp) %&gt;%
  summarise(n = n()) %&gt;%
  group_by(date) %&gt;%
  mutate(pct = n/sum(n)) %&gt;%
  ggplot(aes(x = date, fill = temp, y = pct)) + geom_bar(stat = &quot;identity&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/soybean_mosaic-3.png" width="672" /></p>
<pre class="r"><code>Soybean2 %&gt;%
  group_by(date, temp) %&gt;%
  summarise(n = n()) %&gt;%
  group_by(temp) %&gt;%
  mutate(pct = n/sum(n)) %&gt;%
  ggplot(aes(x = temp, fill = date, y = pct)) + geom_bar(stat = &quot;identity&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/soybean_mosaic-4.png" width="672" /></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Roughly 18% of the data are missing. Are there particular predictors that are more likely to be missing? Is the pattern of missing data related to the classes?</li>
</ol>
<ul>
<li>Higher proportion of missing temperatures in April, although most missing temperatures are in July.</li>
<li>The pattern of missing data by class is shown below</li>
</ul>
<pre class="r"><code>n_nulls &lt;- function(x) sum(is.na(x))
select(Soybean, -Class) %&gt;%
    select_if(negate(is.numeric)) %&gt;%
    summarise_all(funs(n_distinct, n_nulls))  %&gt;%
    gather(key=&quot;key&quot;, value=&quot;value&quot;) %&gt;%
    extract(key, c(&quot;variable&quot;, &quot;metric&quot;), &quot;(.*)_(n_distinct$|n_nulls$)&quot;) %&gt;%
    tidyr::spread(metric, value) %&gt;%
    #filter(variable %in% c(&quot;date&quot;, &quot;precip&quot;, &quot;temp&quot;, &quot;hail&quot;)) %&gt;%
  mutate(n = nrow(Soybean), 
         prop_nulls = n_nulls/n) %&gt;%
  arrange(desc(prop_nulls))</code></pre>
<pre><code>##           variable n_distinct n_nulls   n  prop_nulls
## 1             hail          3     121 683 0.177159590
## 2          lodging          3     121 683 0.177159590
## 3         seed.tmt          4     121 683 0.177159590
## 4            sever          4     121 683 0.177159590
## 5             germ          4     112 683 0.163982430
## 6        leaf.mild          4     108 683 0.158125915
## 7      fruit.spots          5     106 683 0.155197657
## 8  fruiting.bodies          3     106 683 0.155197657
## 9    seed.discolor          3     106 683 0.155197657
## 10      shriveling          3     106 683 0.155197657
## 11     leaf.shread          3     100 683 0.146412884
## 12     mold.growth          3      92 683 0.134699854
## 13            seed          3      92 683 0.134699854
## 14       seed.size          3      92 683 0.134699854
## 15      fruit.pods          5      84 683 0.122986823
## 16       leaf.halo          4      84 683 0.122986823
## 17       leaf.malf          3      84 683 0.122986823
## 18       leaf.marg          4      84 683 0.122986823
## 19       leaf.size          4      84 683 0.122986823
## 20   canker.lesion          5      38 683 0.055636896
## 21       ext.decay          4      38 683 0.055636896
## 22    int.discolor          4      38 683 0.055636896
## 23        mycelium          3      38 683 0.055636896
## 24          precip          4      38 683 0.055636896
## 25       sclerotia          3      38 683 0.055636896
## 26    stem.cankers          5      38 683 0.055636896
## 27     plant.stand          3      36 683 0.052708638
## 28           roots          4      31 683 0.045387994
## 29            temp          4      30 683 0.043923865
## 30       crop.hist          5      16 683 0.023426061
## 31    plant.growth          3      16 683 0.023426061
## 32            stem          3      16 683 0.023426061
## 33        area.dam          5       1 683 0.001464129
## 34            date          8       1 683 0.001464129
## 35          leaves          2       0 683 0.000000000</code></pre>
<pre class="r"><code># Can also use YaleToolkit::whatis function
#YaleToolkit::whatis(select(Soybean, -Class))</code></pre>
<pre class="r"><code>table(Soybean$Class, complete.cases(Soybean))</code></pre>
<pre><code>##                              
##                               FALSE TRUE
##   2-4-d-injury                   16    0
##   alternarialeaf-spot             0   91
##   anthracnose                     0   44
##   bacterial-blight                0   20
##   bacterial-pustule               0   20
##   brown-spot                      0   92
##   brown-stem-rot                  0   44
##   charcoal-rot                    0   20
##   cyst-nematode                  14    0
##   diaporthe-pod-&amp;-stem-blight    15    0
##   diaporthe-stem-canker           0   20
##   downy-mildew                    0   20
##   frog-eye-leaf-spot              0   91
##   herbicide-injury                8    0
##   phyllosticta-leaf-spot          0   20
##   phytophthora-rot               68   20
##   powdery-mildew                  0   20
##   purple-seed-stain               0   20
##   rhizoctonia-root-rot            0   20</code></pre>
<p>Some classes have no complete cases, in particular: 2-4-d-injury, cyst-namatode, diaporthe-pod-&amp;-stem-blight, herbicide-injury and phytophthora-rot. Can check if there are just a few predictors causing this issue.</p>
<pre class="r"><code>n_nulls &lt;- function(x) sum(is.na(x))
Soybean %&gt;%
  filter(Class %in% c(&#39;2-4-d-injury&#39;, &#39;cyst-namatode&#39;,  &#39;diaporthe-pod-&amp;-stem-blight&#39;, &#39;herbicide-injury&#39;, &#39;phytophthora-rot&#39;)) %&gt;%
  group_by(Class) %&gt;%
    select_if(negate(is.numeric)) %&gt;%
    summarise_all(funs(n_nulls, n=n()))  %&gt;%
    gather(key=&quot;key&quot;, value=&quot;value&quot;, -Class) %&gt;%
    extract(key, c(&quot;variable&quot;, &quot;metric&quot;), &quot;(.*)_(n$|n_nulls$)&quot;) %&gt;%
    spread(metric, value) %&gt;%
  mutate(prop_nulls = n_nulls/n) %&gt;%
  select(Class, variable, prop_nulls) %&gt;%
  spread(Class, prop_nulls)</code></pre>
<pre><code>## # A tibble: 35 x 5
##    variable      `2-4-d-injury` `diaporthe-pod-&amp;-stem-b… `herbicide-injur…
##    &lt;chr&gt;                  &lt;dbl&gt;                    &lt;dbl&gt;             &lt;dbl&gt;
##  1 area.dam              0.0625                      0                   0
##  2 canker.lesion         1                           0                   1
##  3 crop.hist             1                           0                   0
##  4 date                  0.0625                      0                   0
##  5 ext.decay             1                           0                   1
##  6 fruit.pods            1                           0                   0
##  7 fruit.spots           1                           0                   1
##  8 fruiting.bod…         1                           0                   1
##  9 germ                  1                           0.4                 1
## 10 hail                  1                           1                   1
## # ... with 25 more rows, and 1 more variable: `phytophthora-rot` &lt;dbl&gt;</code></pre>
<p>This shows that many predictors are completely missing for some cases, e.g. the class 2-4-d-injury has no observations of the predictor canker.lesion, crop.hist, ext.decay, fruit.pots, etc.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Develop a strategy for handling missing data, either by eliminating predictors or imputation.</li>
</ol>
<p>In some cases, 100% of predictor values are missing and so imputation is unlikely to help. Options: * Remove cases with high proportion of missing values * Encode missing as another level</p>
<pre class="r"><code>dat &lt;- Soybean %&gt;%
  # consider only those that are complete cases
  filter_all(all_vars(!is.na(.))) %&gt;%
  # convert ordered variable to factor  
  mutate_if(is.ordered, function(x) factor(as.character(x)))

# generate binary predictors
dummyInfo &lt;- dummyVars(Class ~ ., data=dat)
dummies &lt;- predict(dummyInfo, dat)</code></pre>
<pre><code>## Warning in model.frame.default(Terms, newdata, na.action = na.action, xlev
## = object$lvls): variable &#39;Class&#39; is not a factor</code></pre>
<pre class="r"><code># Determine which variables are sparse
sparsity &lt;- nearZeroVar(dummies, saveMetrics = TRUE)
head(sparsity)</code></pre>
<pre><code>##        freqRatio percentUnique zeroVar   nzv
## date.0 28.578947     0.3558719   FALSE  TRUE
## date.1 10.019608     0.3558719   FALSE FALSE
## date.2  7.515152     0.3558719   FALSE FALSE
## date.3  5.534884     0.3558719   FALSE FALSE
## date.4  3.762712     0.3558719   FALSE FALSE
## date.5  3.014286     0.3558719   FALSE FALSE</code></pre>
<pre class="r"><code># Number and percentage of predictors to remove
sparsity %&gt;%
  summarise(n = n(), n_nzv = sum(nzv), pct_nzv = mean(nzv))</code></pre>
<pre><code>##    n n_nzv   pct_nzv
## 1 99    19 0.1919192</code></pre>
<p>To bypass the need to remove 19% of predictors, could use models insensitive to sparsity, such as tree- or rule- based models, or naive Bayes.</p>
</div>
<div id="exercise-3" class="section level3">
<h3>Exercise 3</h3>
<p>Chapter 5 introduces Quantitative Structure-Activity Relationship (QSAR) modeling where the characteristics of a chemical compound are used to predict other chemical properties. The caret package contains a QSAR data set from Mente and Lombardo (2005). Here, the ability of a chemical to permeate the blood-brain barrier was experimentally determined for 208 compounds. 134 descriptors were measured for each compound.</p>
<ol style="list-style-type: lower-alpha">
<li>Start R and use these commands to load the data:</li>
</ol>
<pre class="r"><code>#require(caret); require(tidyverse); require(e1071)
data(BloodBrain)
# use ?BloodBrain to see more details</code></pre>
<p>The numeric outcome is contained in the vector logBBB while the predictors are in the data frame bbbDescr.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Do any of the individual predictors have degenerate distributions? i.e., Need to look for: Skewness, outliers, missing values and zero- and near-zero- variance</li>
</ol>
<p>There are <code>ncol(bbbDescr)</code> descriptors and so will not plot all.</p>
<ul>
<li>Skewness is present if |skewness| is large or if the ratio of the maximum to minimum value is greater than 20.<br />
</li>
<li>(Near-)Zero Variance is present if percentage of unique values is <span class="math inline">\(\leq\)</span> 10 and frequency ratio (or most common over second-most common value) is <span class="math inline">\(\geq\)</span> 20.</li>
</ul>
<pre class="r"><code>zeroVar &lt;- function(x) nearZeroVar(x, saveMetrics = TRUE)$zeroVar
nzv &lt;- function(x) nearZeroVar(x, saveMetrics = TRUE)$nzv
freqRatio &lt;- function(x) nearZeroVar(x, saveMetrics = TRUE)$freqRatio
percentUnique &lt;- function(x) nearZeroVar(x, saveMetrics = TRUE)$percentUnique

  
bbbSumm &lt;- bbbDescr %&gt;% 
  summarise_all(funs(skewness, n_distinct, max, min, zeroVar, nzv, freqRatio, percentUnique)) %&gt;%
  gather(key=&quot;key&quot;, value=&quot;value&quot;) %&gt;%
  extract(key, c(&quot;variable&quot;, &quot;metric&quot;), &quot;(.*)_(skewness|n_distinct|max|min|zeroVar|nzv|freqRatio|percentUnique)&quot;) %&gt;%
  spread(metric, value) %&gt;%
  mutate(max_min_ratio = ifelse(min==0, (max+1)/(min+1), max/min)) %&gt;%
  select(variable, n_distinct, skewness, max_min_ratio, zeroVar, nzv, freqRatio, percentUnique, min, max) %&gt;%
  arrange(desc(abs(skewness)))

head(bbbSumm, n = 15)</code></pre>
<pre><code>##        variable n_distinct  skewness max_min_ratio zeroVar nzv  freqRatio
## 1      negative          2 14.214859    2.00000000       0   1 207.000000
## 2         alert          2  9.977967    2.00000000       0   1 103.000000
## 3        a_acid          3  5.400675    4.00000000       0   1  33.500000
## 4      vsa_acid          3  5.400675   41.70076000       0   1  33.500000
## 5          tcpa        189  5.331288   65.76978417       0   0   1.500000
## 6  frac.anion7.         12  4.293466    1.99900000       0   1  47.750000
## 7         wpsa2        206  4.042276   81.88304879       0   0   1.000000
## 8         inthb          2  3.966834    2.00000000       0   0  17.909091
## 9         wnsa2        206 -3.120316    0.01242556       0   0   1.000000
## 10 peoe_vsa.2.1         12  2.950651   45.60254000       0   1  25.571429
## 11     vsa_base          7  2.865661   59.21528000       0   0   4.032258
## 12        ppsa2        206  2.818707   25.94027225       0   0   1.000000
## 13        dpsa2        206  2.705874   20.91858303       0   0   1.000000
## 14 peoe_vsa.3.1         15  2.662580   36.86280000       0   1  21.000000
## 15      vsa_don         11  2.442425   59.91005000       0   0   1.431373
##    percentUnique        min         max
## 1      0.9615385     0.0000     1.00000
## 2      0.9615385     0.0000     1.00000
## 3      1.4423077     0.0000     3.00000
## 4      1.4423077     0.0000    40.70076
## 5     90.8653846     0.0278     1.82840
## 6      5.7692308     0.0000     0.99900
## 7     99.0384615    95.3229  7805.32959
## 8      0.9615385     0.0000     1.00000
## 9     99.0384615 -3199.7271   -39.75840
## 10     5.7692308     0.0000    44.60254
## 11     3.3653846     0.0000    58.21528
## 12    99.0384615   288.9730  7496.03809
## 13    99.0384615   505.2433 10568.97363
## 14     7.2115385     0.0000    35.86280
## 15     5.2884615     0.0000    58.91005</code></pre>
<p>There are a number of variables with near-zero variance:</p>
<p>These variables would be removed for models that are impacted detrimentally by near-zero variance predictors.</p>
<pre class="r"><code>select(bbbDescr, filter(bbbSumm, nzv == 1)$variable) %&gt;%
  gather(Measure, Value) %&gt;%
  
  ggplot() + 
  geom_histogram(aes(x=Value)) +
  facet_wrap(~ Measure)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/bloodBrain_nsv_hist-1.png" width="672" /></p>
<p>There are also a number of skewed variables. Data pre- and post-transformation (Yeo-Johnson) are shown below. May be best to determine manually which to transform.</p>
<pre class="r"><code>bbb_top_vars &lt;- filter(bbbSumm, ! variable %in% filter(bbbSumm, nzv == 1)$variable) %&gt;% 
  top_n(n = 16, wt = abs(skewness)) %&gt;%
  select(variable) %&gt;%
  pull()


bbbTrans &lt;- preProcess(select(bbbDescr, -c(filter(bbbSumm, nzv == 1)$variable)), method = &quot;YeoJohnson&quot;)
bbbTransData &lt;- predict(bbbTrans, select(bbbDescr, -c(filter(bbbSumm, nzv == 1)$variable))) 
dat &lt;- bbbTransData %&gt;%
  select(bbb_top_vars) %&gt;%
  gather(Predictor, Value) %&gt;%
  mutate(Transformation = &quot;Yeo-Johnson&quot;) %&gt;%
  bind_rows(data.frame(gather(select(bbbDescr, bbb_top_vars), Predictor, Value), Transformation = &quot;NA&quot;, stringsAsFactors = FALSE)) 
  
# Final result
res &lt;- list()
# How many points we want 
nPoints &lt;- 1e3

# Using simple loop to scale and calculate density
combinations &lt;- expand.grid(unique(dat$Predictor), unique(dat$Transformation))
for(i in 1:nrow(combinations)) {
    # Subset data
    foo &lt;- subset(dat, Predictor == combinations$Var1[i] &amp; Transformation == combinations$Var2[i])
    # Perform density on scaled signal
    densRes &lt;- density(x = scale(foo$Value), n = nPoints)
    # Position signal from 1 to wanted number of points
    res[[i]] &lt;- data.frame(x = 1:nPoints, y = densRes$y, 
                           pred = combinations$Var1[i], trans = combinations$Var2[i])
}
res &lt;- do.call(rbind, res)
ggplot(res, aes(x / nPoints, y, color = trans, linetype = trans)) +
    geom_line(alpha = 0.5, size = 1) +
    facet_wrap(~ pred, scales = &quot;free&quot;)  +
  labs(x = &quot;Value / Points&quot;, y = &quot;Scaled Density&quot;, colour = &quot;Transformation&quot;, linetype = &quot;Transformation&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/bloodBrain_dens_glimpse-1.png" width="672" /></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Generally speaking, are there strong relationships between the predictor data? If so, how could correlations in the predictor set be reduced? Does this have a dramatic effect on the number of predictors available for modeling?</li>
</ol>
<p>Samples in tails of skewed predictors may have significant effect on correlation structure and so best to review after transformation.</p>
<pre class="r"><code># Correlation plot with no transformation
bbb_cor_raw &lt;- cor(select(bbbDescr, -c(filter(bbbSumm, nzv == 1)$variable)))
corrplot(bbb_cor_raw, order = &quot;hclust&quot;, addgrid.col = NA, tl.pos = &quot;n&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/bloodBrain_correlation_plots-1.png" width="672" /></p>
<pre class="r"><code># Correlation plot of Yeo-Johnson transformed data
bbb_cor_yeo &lt;- cor(bbbTransData)
corrplot(bbb_cor_yeo, order = &quot;hclust&quot;, addgrid.col = NA, tl.pos = &quot;n&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/bloodBrain_correlation_plots-2.png" width="672" /></p>
<pre class="r"><code># Correlation plot of spatial sign transformed data
bbb_cor_sps &lt;- cor(spatialSign(scale(select(bbbDescr, -c(filter(bbbSumm, nzv == 1)$variable)))))
corrplot(bbb_cor_sps, order = &quot;hclust&quot;, addgrid.col = NA, tl.pos = &quot;n&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/bloodBrain_correlation_plots-3.png" width="672" /></p>
<p>This shows that correlations lessen with increasing levels of transformations</p>
<pre class="r"><code>corrInfo &lt;- function(x) summary(x[upper.tri(x)])
corrInfo(bbb_cor_raw)</code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -1.00000 -0.16173  0.06434  0.07068  0.28643  1.00000</code></pre>
<pre class="r"><code>corrInfo(bbb_cor_yeo)</code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.98924 -0.16609  0.05929  0.06798  0.28774  1.00000</code></pre>
<pre class="r"><code>corrInfo(bbb_cor_sps)</code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -1.00000 -0.16062  0.03862  0.05561  0.25545  1.00000</code></pre>
<p>However, rather than transform data (e.g., using PCA), it may be better to remove correlated predictors, using <code>findCorrelation</code> function.</p>
<p>The following chart shows the number of variables that would be removed based on varying correlation thresholds</p>
<pre class="r"><code>thresholds &lt;- seq(.25, .95, by = 0.05)
size &lt;- meanCorr &lt;- rep(NA, length(thresholds))
removals &lt;- vector(mode = &quot;list&quot;, length = length(thresholds))

for(i in seq_along(thresholds)){
  removals[[i]] &lt;- findCorrelation(bbb_cor_raw, thresholds[i]) # removed predictors
  subMat &lt;- bbb_cor_raw[-removals[[i]], -removals[[i]]]       # remaining predictors
  size[i] &lt;- ncol(bbb_cor_raw) -length(removals[[i]])         # number of remaining predictors
  meanCorr[i] &lt;- mean(abs(subMat[upper.tri(subMat)]))         # mean correlation of remaining predictors
}

corrData &lt;- data.frame(value = c(size, meanCorr),
                       threshold = c(thresholds, thresholds),
                       what = rep(c(&quot;Predictors&quot;, 
                                    &quot;Average Absolute Correlation&quot;),
                                  each = length(thresholds))) 

# xyplot(value~ threshold|what, data = corrData, 
#        scales = list(y = list(relation = &quot;free&quot;)),
#        type = c(&quot;p&quot;, &quot;g&quot;, &quot;smooth&quot;),
#        degree = 2,
#        ylab = &quot;&quot;)

ggplot(corrData, aes(x = threshold, y = value)) +
  geom_point() + 
  geom_smooth(method = &quot;loess&quot;, se = FALSE) + 
  facet_wrap(~ what, scales = &quot;free&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/bloodBrain_corr_heuristic-1.png" width="672" /></p>
<p>Other methods to remove correlated variables and search for quality subsets include simulated annealing and genetic algorithms, e.g. via the <code>subselect</code> package. These require that the correlation matrix is well-conditioned, which is possible using the <code>subselect::trim.matrix</code> function. This function is a less greedy method to remove perfect pair-wise correlations and relationships between three or more predictors.</p>
<pre class="r"><code>require(subselect)</code></pre>
<pre><code>## Loading required package: subselect</code></pre>
<pre class="r"><code>ncol(bbb_cor_raw)</code></pre>
<pre><code>## [1] 127</code></pre>
<pre class="r"><code>trimmed &lt;- trim.matrix(bbb_cor_raw, tolval=1000*.Machine$double.eps)$trimmedmat
ncol(trimmed)</code></pre>
<pre><code>## [1] 119</code></pre>
<p>The Simulated Annealing algorithm seeks a k-variable subset and requires that the following variables are specified:</p>
<ul>
<li>kmin: Smallest subset that is wanted. Here kmin is set to the number of parameters (18) determined using the <code>caret::findCorrelation</code> solution at a threshold of 40%.</li>
<li>kmax: Largest subset that is wanted (=kmin by default).</li>
</ul>
<pre class="r"><code>set.seed(702)
sa &lt;- anneal(trimmed, kmin = 18, niter = 1000)
saMat &lt;- bbb_cor_raw[sa$bestsets[1,], sa$bestsets[1,]]</code></pre>
<p>The required parameters for the Genetic Algorithm algorithm are the same as for Simulated Annealing.</p>
<pre class="r"><code>set.seed(702)
ga &lt;- genetic(trimmed, kmin = 18, nger = 1000)
gaMat &lt;- bbb_cor_raw[ga$bestsets[1,], ga$bestsets[1,]]</code></pre>
<pre class="r"><code># caret::findCorrelation
fcMat &lt;- bbb_cor_raw[-removals[size == 18][[1]], 
                 -removals[size == 18][[1]]]
corrInfo(fcMat) </code></pre>
<pre><code>##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
## -0.254356 -0.097568  0.005174  0.012444  0.088337  0.380312</code></pre>
<pre class="r"><code># Simulated Annealing
corrInfo(saMat) </code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.95412 -0.11963  0.03580  0.05833  0.22176  0.95622</code></pre>
<pre class="r"><code># Genetic Algorithm
corrInfo(gaMat) </code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.88931 -0.16774  0.03500  0.03328  0.24175  0.92184</code></pre>
<p>The main difference between these results is that the greedy approach of  is much more conservative than the techniques found in the  package.</p>
</div>
</div>
</div>
<div id="chapter-4-over-fiting-and-model-tuning" class="section level1">
<h1>Chapter 4: Over-Fiting and Model Tuning</h1>
<p>Aim: Strategies to avoid overfitting so that model can predict new samples with similar degree of accuracy as for training data. Will use model tuning (of model parameters) to maximise model performance. Traditionally, training set to build and tune model and test set to estimate performance. Modern approach is to split data into multiple training and test data sets.</p>
<div id="the-problem-of-over-fitting" class="section level2">
<h2>The Problem of Over-Fitting</h2>
<p>Over-fit model:</p>
<ul>
<li>Has learnt noise of data</li>
<li>Poor accuracy when applied to new data set</li>
</ul>
<p><strong>Apparent Performance</strong>: Error rate of model when applied to the training set</p>
</div>
<div id="model-tuning" class="section level2">
<h2>Model Tuning</h2>
<p><strong>Tuning parameter</strong>: Has no analytical formula to calculate an appropriate value, e.g. k in knn. Often control complexity and so poor choices can lead to over-fitting.</p>
<p>Methods for determining tuning parameters:</p>
<ul>
<li>Parameter Tuning Process:</li>
</ul>
<ol style="list-style-type: decimal">
<li>Define set of candidate values for tuning parameter(s)</li>
<li>For each candidate set:
<ol style="list-style-type: lower-alpha">
<li>Resample data</li>
<li>Fit model</li>
<li>Predictor hold-outs</li>
</ol></li>
<li>Aggregate resampling into performance profile</li>
<li>determine final tuning parameters</li>
<li>Refit model with entire training set and final tuning parameters</li>
</ol>
<ul>
<li>Genetic Algorithms</li>
<li>Simplex search methods</li>
</ul>
</div>
<div id="data-splitting-single-test-set" class="section level2">
<h2>Data Splitting (Single Test Set)</h2>
<p>Not generally recommended.</p>
<p>Methods:</p>
<ul>
<li>Nonrandom; appropriate for example, if need to test on a different sample population, point in time or new phenomenon</li>
<li>Simple random sample</li>
<li>Stratified random sampling; apply to disproportionate classes to obtain similar distribution between training and test sets</li>
<li>Maximum dissimilarity sampling; distance between predictor values for two samples</li>
<li>Initialise with single sample</li>
<li>The second sample is chosen as the most dissimilar to the first</li>
<li>The third and subsequent samples are most dissimilar to the <strong>group</strong> of prior samples (e.g. average or minimum of the dissimilarity)</li>
</ul>
<p>Variants:</p>
<ul>
<li>Use stratified random sampling to select <span class="math inline">\(k\)</span> partitions so that the folds are balanced with respect to the outcome.</li>
<li>Leave-one-out cross-validation (LOOCV), <span class="math inline">\(k\)</span> = # samples</li>
<li>Repeated <span class="math inline">\(k\)</span>-fold cross-validation; can increase precision while maintaining small bias</li>
</ul>
<p>Choice of <span class="math inline">\(k\)</span> has a bias-variance trade-off;</p>
<p><strong>bias</strong>: difference between estimated and true values of performance</p>
<p><strong>variance</strong>: Repeating the resampling procedure produces a very different value</p>
<p>Small <span class="math inline">\(k\)</span>, e.g. 2-3: Decreased variance, increased bias. (Bias is similar to bootstrap but with larger variance) </br> Large <span class="math inline">\(k\)</span>: Increased variance, decreased bias. More computationally taxing.</p>
<p>Typically use <span class="math inline">\(k\)</span> = 5 or 10.</p>
</div>
<div id="resampling-techniques" class="section level2">
<h2>Resampling Techniques</h2>
<div id="k-fold-cross-validation" class="section level3">
<h3>k-Fold Cross-Validation</h3>
<p>Method:</p>
<ul>
<li>Partition sample into <span class="math inline">\(k\)</span> sets (folds) of roughly equal size.</li>
<li>Repeat for each fold:</li>
<li>Fit model to all samples except the fold <span class="math inline">\(i\)</span></li>
<li>Predict held-out samples to estimate performance measures</li>
<li>Summarise <span class="math inline">\(k\)</span> resampled estimates of performance, e.g. mean, std. error to understand relationship between tuning parameters and model utility</li>
</ul>
</div>
<div id="generalised-cross-validation" class="section level3">
<h3>Generalised Cross-Validation</h3>
<p>For linear regression, can use generalised cross-validation (GCV) statistic to approximate the leave-one-out error rate, <span class="math display">\[\text{GCV} = \frac{1}{n}\sum^n_{i=1}\left(\frac{y_i - \hat{y}_i}{1-\text{df}/n}\right)^2\]</span> Models with similar precision but higher complexity will have a larger GCV value.</p>
</div>
<div id="repeated-trainingtest-splits-a.k.a-leave-group-out-cv-or-monte-carlo-cv" class="section level3">
<h3>Repeated Training/Test Splits (a.k.a Leave-Group-Out CV or Monte Carlo CV)</h3>
<p>Practitioner determine percent split between training (75-80%) and test, in addition to the number of repetitions (50-200). Can increase proportion of data in training set with increased repetitions in order to decrease bias.Increasing repetitions will decrease uncertainty of performance estimates.<br />
Unlike in k-fold cross validation, a sample can be in multiple hold-out sets.</p>
</div>
<div id="the-bootstrap" class="section level3">
<h3>The Bootstrap</h3>
<p><strong>Bootstrap Sample</strong>: Random sample of data taken with replacement. Bootstrap sample is same size as original data set. Prediction is made on the <strong>out-of-bag samples</strong>, i.e. those samples not selected.</p>
<p>In comparison to cross-validation, bootstrap technique has:</p>
<ul>
<li>less uncertainty than k-fold cv</li>
<li>more bias (similar to 2-fold CV); bias will decease as training set sample size becomes large (on average 63.2% of data points represented at least once)</li>
</ul>
<p>Modifications:</p>
<ul>
<li>632 method: combined bootstrap estimate and estimate from re-predicting the training set (the apparent error rate), error rate = .632 x simple bootstrap estimate + 0.368 x apparent error rate</li>
</ul>
</div>
</div>
<div id="case-study-credit-scoring" class="section level2">
<h2>Case Study: Credit Scoring</h2>
<p>Aim: predict probability that applicants have good credit Dataset: German credit dat set Samples: 1000 Class distribution: Good(70%), Bad(30%) Baseline accuracy: Predict all to be good, such that accuracy is 70% Predictors: credit history, employment, account status, loan amount. Majority categorical and so converted to dummy variables. After conversion, there were 41 predictors.</p>
</div>
<div id="choosing-final-tuning-parameters" class="section level2">
<h2>Choosing Final Tuning Parameters</h2>
<p>Approach to choose final settings:</p>
<ul>
<li>Numerically optimal: Pick those with numerically best performance estimates. Disadvantage: May lead to choice of an overly complicated model.</li>
<li>One-standard error method: Find the numerically optical model then the simplest model whose performance is within a single standard error.</li>
<li>Tolerance method: Choose simpler model that has performance (<span class="math inline">\(X\)</span>) within tolerance of best (<span class="math inline">\(O\)</span>), i.e. such that <span class="math display">\[(X-O)/O &gt; \text{Tolerance}\]</span> e.g. if best accuracy is 75% then with 4% loss in accuracy acceptable, then can choose accuracy at <span class="math inline">\(O(1 + \text{Tolerance} = .75*(1 - .04) = 0.72\)</span>.</li>
</ul>
</div>
<div id="data-splitting-recommendations" class="section level2">
<h2>Data Splitting Recommendations</h2>
<p>Strong case to use resampling</p>
<ul>
<li>Single test set has limited ability to characterise uncertainty</li>
<li>Proportionally large test sets increase bias</li>
<li>With small sample sizes, test set uncertainty can be considerably large</li>
</ul>
<p>No resampling method is uniformly better.</p>
<ul>
<li>Small sample size; recommend 10-fold cross-validation</li>
<li>Large sample size; difference becomes less pronounced and computational efficiency increases. Again 10-fold cross-validation recommended</li>
<li>To choose between models as opposed to getting best indicator of performance, consider bootstrap procedures as have low variance</li>
</ul>
<p>There may be some (often negligible) bias for final model chosen based on tuning parameter with smallest error rate.</p>
</div>
<div id="choosing-between-models" class="section level2">
<h2>Choosing Between Models</h2>
<p>Once tuning parameters chosen for each model, how do we determine between multiple models?</p>
<p>Approaches:</p>
<ul>
<li>Determine performance ceiling, then simplify</li>
<li>Start with least interpretable and most flexible model, e.g. boosted trees or support vector machines, to determine performance ceiling</li>
<li>Investigate simpler models that are less opaque, e.g. MARS, partial least squares, gams or naive Bayes</li>
<li>Choose the simplest model that reasonable approximates the performance ceiling</li>
<li>Use resampling results to compare models</li>
<li>Use identically resampled data sets</li>
<li>Use statistical methods for pair comparisons, e.g. use paired t-test to evaluate that models have equivalent accuracy or determine mean difference in accuracy and associated confidence interval</li>
</ul>
</div>
<div id="computing-1" class="section level2">
<h2>Computing</h2>
<div id="data-splitting" class="section level3">
<h3>Data Splitting</h3>
<p>The following functions can be used to partition the data:</p>
<ul>
<li><code>caret::sample</code>: simple random split</li>
<li><code>caret::createDataPartition</code>: stratified random split (based on the classes)</li>
<li><code>caret::maxdissim</code>: maximum dissimilarity sampling</li>
</ul>
<p>The following code performs stratified random splitting:</p>
<pre class="r"><code>set.seed(1)
trainingRows &lt;- createDataPartition(classes, p = 0.80, list = FALSE)
# training data
trainPredictors &lt;- predictors[trainingRows, ]
trainClasses &lt;- classes[trainingRows]
# test data
testPredictors &lt;- predictors[-trainingRows, ]
testClasses &lt;- classes[-trainingRows]
str(trainPredictors); str(testPredictors)</code></pre>
<pre><code>## &#39;data.frame&#39;:    167 obs. of  2 variables:
##  $ PredictorA: num  0.158 0.655 0.706 0.199 0.395 ...
##  $ PredictorB: num  0.1609 0.4918 0.6333 0.0881 0.4152 ...</code></pre>
<pre><code>## &#39;data.frame&#39;:    41 obs. of  2 variables:
##  $ PredictorA: num  0.0658 0.1056 0.2909 0.4129 0.0472 ...
##  $ PredictorB: num  0.1786 0.0801 0.3021 0.2869 0.0414 ...</code></pre>
</div>
<div id="resampling" class="section level3">
<h3>Resampling</h3>
<p>The following functions can be used to resample the data:</p>
<ul>
<li><code>caret::createDataPartition</code>: Repeated training/test splits, utilises the <code>times</code> argument</li>
<li><code>caret::createResamples</code>: Bootstrap samples</li>
<li><code>caret::createFolds</code>: k-Fold cross-validation</li>
<li><code>caret::createMultiFolds</code>: Repeated cross-validation</li>
</ul>
<p>The following code generates three resampled versions of the training set</p>
<pre class="r"><code>set.seed(1)
repeatedSplits &lt;- createDataPartition(trainClasses, p = 0.8, times = 3)
str(repeatedSplits)</code></pre>
<pre><code>## List of 3
##  $ Resample1: int [1:135] 1 2 4 5 6 8 9 10 11 12 ...
##  $ Resample2: int [1:135] 2 3 4 6 7 8 9 11 14 15 ...
##  $ Resample3: int [1:135] 4 5 6 7 8 9 11 13 14 15 ...</code></pre>
<p>The following code creates indicators for 10-fold cross-validation, where each fold is <span class="math inline">\((k-1)/k%\)</span> of the data.</p>
<pre class="r"><code>set.seed(1)
cvSplits &lt;- createFolds(trainClasses, k = 10, returnTrain = TRUE)
str(cvSplits)</code></pre>
<pre><code>## List of 10
##  $ Fold01: int [1:151] 2 3 4 5 6 7 11 12 13 14 ...
##  $ Fold02: int [1:150] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold03: int [1:150] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold04: int [1:151] 1 2 3 4 5 7 8 9 10 11 ...
##  $ Fold05: int [1:150] 1 2 3 5 6 7 8 9 10 11 ...
##  $ Fold06: int [1:150] 1 2 3 4 5 6 8 9 10 11 ...
##  $ Fold07: int [1:150] 1 3 4 5 6 7 8 9 10 11 ...
##  $ Fold08: int [1:151] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold09: int [1:150] 1 2 4 5 6 7 8 9 10 12 ...
##  $ Fold10: int [1:150] 1 2 3 4 6 7 8 9 10 11 ...</code></pre>
<pre class="r"><code>fold1 &lt;- cvSplits[[1]]
cvPredictors1 &lt;- trainPredictors[fold1, ]
cvClasses1 &lt;- trainClasses[fold1]
scales::percent(nrow(cvPredictors1)/nrow(trainPredictors))</code></pre>
<pre><code>## [1] &quot;90.4%&quot;</code></pre>
</div>
<div id="basic-model-building-in-r" class="section level3">
<h3>Basic Model Building in R</h3>
<p>To fit a 5-nearest neighbour classification model to the training data can use:</p>
<ul>
<li><code>MASS:knn</code></li>
<li><code>ipre:ipredknn</code></li>
<li><code>caret::knn3</code></li>
</ul>
<p>Conventions for specifying models:</p>
<ol style="list-style-type: decimal">
<li>Formula interface. e.g. <code>modelFunction(response_var ~ predictors, data = data)</code> &lt;&gt;Advantages: Convenient; can specify transformations choose subset(s) of predictors in-line &lt;&gt;Disadvantage: Formula information not efficiently stored and can slow down computations with a large number of predictors</li>
<li>Matrix (non-formula) interface, e.g.<code>modelFunction(x = predictors_matrix, y = response_var_vector)</code>. The predictors are typically a matrix or a dataframe.</li>
</ol>
<p>Not all R functions have both interfaces.</p>
<p>The following code estimated 5-nn using <code>caret:knn3</code>.</p>
<pre class="r"><code># Fit model
trainPredictros &lt;- as.matrix(trainPredictors)
knnFit &lt;- knn3(x = trainPredictors, y = as.factor(trainClasses), k = 5)
knnFit</code></pre>
<pre><code>## 5-nearest neighbor model
## Training set outcome distribution:
## 
## Class1 Class2 
##     89     78</code></pre>
<pre class="r"><code>testPredictions &lt;- predict(knnFit, newdata = testPredictors, type  = &quot;class&quot;)
head(testPredictions)</code></pre>
<pre><code>## [1] Class2 Class2 Class1 Class1 Class2 Class2
## Levels: Class1 Class2</code></pre>
</div>
<div id="determination-of-tuning-parameters" class="section level3">
<h3>Determination of Tuning Parameters</h3>
</div>
<div id="between-model-comparisons" class="section level3">
<h3>Between-Model Comparisons</h3>
</div>
</div>
<div id="chapter-4-exercises" class="section level2">
<h2>Chapter 4 Exercises</h2>
<div id="exercise-4.1" class="section level3">
<h3>Exercise 4.1</h3>
<p>Consider the music genre data set described in Sect. 1.4. The objective for these data is to use the predictors to classify music samples into the appropriate music genre. (a) What data splitting method(s) would you use for these data? Explain. (b) Using tools described in this chapter, provide code for implementing your approach(es).</p>
</div>
<div id="exercise-4.2" class="section level3">
<h3>Exercise 4.2</h3>
<p>Consider the permeability data set described in Sect. 1.4. The objective for these data is to use the predictors to model compounds’ permeability. (a) What data splitting method(s) would you use for these data? Explain. (b) Using tools described in this chapter, provide code for implementing your approach(es).</p>
</div>
<div id="exercise-4.3" class="section level3">
<h3>Exercise 4.3</h3>
<p>Partial least squares (Sect. 6.3) was used to model the yield of a chemical manufacturing process (Sect. 1.4).</p>
<p>The objective of this analysis is to find the number of PLS components that yields the optimal R2 value (Sect. 5.1). PLS models with 1 through 10 components were each evaluated using five repeats of 10-fold cross-validation and the results are presented in the following table: (a) Using the “one-standard error” method, what number of PLS components provides the most parsimonious model? (b) Compute the tolerance values for this example. If a 10% loss in R2 is acceptable, then what is the optimal number of PLS components? (c) Several other models (discussed in Part II) with varying degrees of complexity were trained and tuned and the results are presented in Fig. 4.13. If the goal is to select the model that optimizes R2, then which model(s) would you choose, and why? (d) Prediction time, as well as model complexity (Sect. 4.8) are other factors to consider when selecting the optimal model(s). Given each model’s prediction time, model complexity, and R2 estimates, which model(s) would you choose, and why?</p>
<pre class="r"><code>library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)</code></pre>
</div>
<div id="exercise-4.4" class="section level3">
<h3>Exercise 4.4</h3>
<p>Brodnjak-Vonina et al. (2005) develop a methodology for food laboratories to determine the type of oil from a sample. In their procedure, they used a gas chromatograph (an instrument that separate chemicals in a sample) to measure seven different fatty acids in an oil. These measurements would then be used to predict the type of oil in a food samples. To create their model, they used 96 samples2 of seven types of oils. These data can be found in the caret package using data(oil). The oil types are contained in a factor variable called oilType. The types are pumpkin (coded as A), sunflower (B), peanut (C), olive (D), soybean (E), rapeseed (F) and corn (G).</p>
<ol style="list-style-type: lower-alpha">
<li>Use the sample function in base R to create a completely random sample of 60 oils. How closely do the frequencies of the random sample match the original samples? Repeat this procedure several times of understand the variation in the sampling process.</li>
<li><p>Use the caret package function createDataPartition to create a stratified random sample. How does this compare to the completely random samples?</p></li>
<li>With such a small samples size, what are the options for determining performance of the model? Should a test set be used?</li>
<li><p>One method for understanding the uncertainty of a test set is to use a confidence interval. To obtain a confidence interval for the overall accuracy, the based R function binom.test can be used. It requires the user to input the number of samples and the number correctly classified to calculate the interval. For example, suppose a test set sample of 20 oil samples was set aside and 76 were used for model training. The width of the 95% confidence interval is 37.9%. Try different samples sizes and accuracy rates to understand the trade-off between the uncertainty in the results, the model performance, and the test set size.</p></li>
</ol>
</div>
</div>
</div>
<div id="new-r-commands" class="section level1">
<h1>New R commands</h1>
<p><code>apropos</code>: search R packages for a given term in currently loaded packages</p>
<p><code>RSiteSearch</code>: find function in any package</p>
</div>
