---
title: 'Book: Applied Predictive Modeling'
author: Marie
date: '2018-05-30'
slug: book-applied-predictive-modeling
banner: "img/banners/kuhn.png"
categories:
  - Study-Notes
tags:
  - R
  - Study-Notes
  - Book
---



<div id="overview" class="section level1">
<h1>Overview</h1>
<p>This post includes my notes and reproduction of examples in the book <a href="http://appliedpredictivemodeling.com">Applied Predictive Modeling</a> (2013), by Max Kuhn and Kjell Johnson.</p>
</div>
<div id="notes" class="section level1">
<h1>Notes</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Common reasons for predictive model failure:</p>
<ul>
<li>Inadequate pre-processing of data</li>
<li>Inadequate model validation</li>
<li>Unjustified extrapolation</li>
<li>Overfitting</li>
<li>Insufficient number of models explorer</li>
</ul>
<p>When prediction accuracy is the primary goal, should not choose a second-rate model for interpretability. To improve accuracy, generally need a more complex model which is more difficult to interpret.</p>
<p>Foundation for effective predictive model:</p>
<ul>
<li>Intuition and deep knowledge (to obtain relevant data, eliminate noise)</li>
<li>Relevant data</li>
<li>Versatile computation toolbox (pre-processing, modelling, visualizations) The combined force of predictive modelling and intuition will be better than the parts.</li>
</ul>
</div>
<div id="predictive-modeling-process" class="section level2">
<h2>Predictive Modeling Process</h2>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li>Understand data and modelling objectives; critical for a reliable and trustworthy model for predicting new samples; necessary before moving to the next steps.</li>
<li>Preprocess and split the data</li>
<li><p>Build &amp; evaluate model</p>
<ul>
<li>Split dataset into training set and validation set. Using MPG for 2010-2011 model year cars, if goal is to predict MPG for a new car line, then model could be created using all 2010 model cars and tested on the new 2011 cars. This, in contrast, to taking a random sample of the data for model building. Use training set to try a number of techniques; only use validation set for a few strong candidate models. Repeatedly using test set negates its utility as final arbitrator</li>
<li>Alternatively can use resampling (cross-validation) to evaluate model</li>
</ul></li>
<li><p>Select model</p></li>
</ol>
<p>Themes</p>
<ul>
<li>Data splitting. How the model will be applied (e.g. whether it will be extrapolated to a new population) should determine how the training and test sets are determined. The amount of data available will also influence data splitting decisions. With a small dataset (and therefore test), resampling advised.</li>
<li>Predictors: feature selection</li>
<li>Estimating performance: Statistics (e.g. RMSE) and visualisations. Both are important.</li>
<li>Evaluating several models: There is no single model that will always do better than another.</li>
<li>Model selection: Involves choosing between models and selecting tuning parameters (within model)</li>
</ul>
</div>
<div id="data-pre-processing" class="section level2">
<h2>Data Pre-Processing</h2>
<ul>
<li>Addition, deletion or transformation of <strong>training set</strong> data</li>
<li>Can make or break model’s predictive ability</li>
<li>Feature extraction/feature engineering: how predictors are encoded, e.g. combinations, ratios</li>
<li>Feature selection: Include predictors to maximise accuracy</li>
<li>Method depends on model being used and true relationship with outcome</li>
<li>Model may require:</li>
<li>Predictors have common scale (e.g PLS)</li>
<li>Removal of outliers</li>
</ul>
<div id="data-tranformations-individual-predictors" class="section level3">
<h3>Data Tranformations (Individual Predictors)</h3>
<ul>
<li>Pre-processing techniques include:</li>
<li>Centering: Subtract average <span class="math inline">\(\rightarrow \bar{x} = 1\)</span></li>
<li>Scaling: Divide by standard deviation <span class="math inline">\(\rightarrow s = 1\)</span></li>
<li>Skewness transformations.<br />
</li>
<li>Disadvantage of transformations is loss of interpretability</li>
</ul>
<p><strong>Centering and Scaling</strong></p>
<p><strong>Skewness</strong> Skewed if <span class="math inline">\(\frac{x_\max}{x_\min} &gt; 20\)</span>, or if |skewness statistic| &gt;&gt; 0 <span class="math display">\[
    \begin{align}
      \text{skewness} &amp; = \frac{\sum (x_i - \bar{x})^3}{(n-1)v^{2/3}} \\
      v &amp; = \frac{\sum(x_i - \bar{x})^2}{(n-1)}
   \end{align}\]</span></p>
<p>Skewness can be removed by replacing data with log, square root or inverse, or by using the Box and Cox family of transformations and determining the appropriate parameter, <span class="math inline">\(\lambda\)</span>,</p>
<span class="math display">\[\begin{align}
  x^\star =  \begin{cases}\frac{x^\lambda - 1 }{\lambda} &amp; \text{if } \lambda \neq 0 \\
                    \log(x) &amp; \text{if } \lambda = 0
              \end{cases}
        
\end{align}\]</span>
<p>Note</p>
<ul>
<li>Square, <span class="math inline">\(\lambda = 2\)</span></li>
<li>Square root, <span class="math inline">\(\lambda = 0.5\)</span></li>
<li>Inverse, <span class="math inline">\(\lambda = -1\)</span></li>
</ul>
<p><span class="math inline">\(\lambda\)</span> can be estimated using training data for each feature with skewness. Transformations should only be applied if <span class="math inline">\(\lambda\)</span> outside <span class="math inline">\(1 \pm 0.02\)</span>.</p>
<p>The <code>MASS::boxcox</code> function can estimate <span class="math inline">\(\lambda\)</span> but will not create the transformed variables. The <code>caret::BoxCoxTrans</code> function will find the appropriate transformation and apply them to the new data.</p>
<p><strong>Example</strong>: Segmentation data</p>
<pre class="r"><code># load case study training data
data(&quot;segmentationOriginal&quot;)
segTrain &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)

## Remove response variables (first three columns)
segTrainId &lt;- segTrain$Cell
segTrainClass &lt;- segTrain$Class
segTrainCase &lt;- segTrain$Case
segTrainX &lt;- select(segTrain, -c(Cell, Class, Case)) 
#%&gt;% select(-contains(&quot;Status&quot;))

# https://topepo.github.io/caret/pre-processing.html#the-preprocess-function

# Summarise the predictors
segMetrics &lt;- segTrainX %&gt;%
  select_if(is.numeric) %&gt;%
  summarise_all(funs(skewness, n_distinct, max, min)) %&gt;%
  gather(key=&quot;key&quot;, value=&quot;value&quot;) %&gt;%
  extract(key, c(&quot;variable&quot;, &quot;metric&quot;), &quot;(.*)_(skewness|n_distinct|max|min)&quot;) %&gt;%
  tidyr::spread(metric, value) %&gt;%
  mutate(max_min_ratio = ifelse(min==0, (max+1)/(min+1), max/min)) %&gt;%
  select(variable, n_distinct, skewness, max_min_ratio, min, max)

## Use caret&#39;s preProcess function to transform for skewness
segPP &lt;- caret::preProcess(segTrainX, method = &quot;BoxCox&quot;)

## Apply the transformations
segTrainTrans &lt;- predict(segPP, segTrainX)

#Transformation used
# pp2df &lt;- function(object, ...) {
#   vars &lt;- unlist(object$method)
#   nums &lt;- vapply(object$method, length, c(num = 0))
#   meth &lt;- rep(names(nums), nums)
#   
#   data.frame(variable = unname(vars), method = meth)
# }

# Record results
res &lt;- data.frame(lambda = sapply(segPP$bc, `[[`, 1))
res &lt;- data.frame(variable = rownames(res), res, row.names = NULL, stringsAsFactors = FALSE)

segMetrics &lt;- segMetrics %&gt;% 
  left_join(res, by=&quot;variable&quot;) %&gt;% 
  arrange(lambda)

head(segMetrics)</code></pre>
<pre><code>##                 variable n_distinct skewness max_min_ratio        min
## 1 ConvexHullAreaRatioCh1       1000 2.476582      2.878292   1.007653
## 2          EqCircDiamCh1        399 1.955530      3.806062  13.862944
## 3               PerimCh1        657 2.589488      9.631097  47.737594
## 4            ShapeLWRCh1       1009 2.490995      7.737628   1.002813
## 5                AreaCh1        399 3.525107     14.573333 150.000000
## 6    DiffIntenDensityCh1       1009 2.760473     16.563239  26.732283
##           max lambda
## 1    2.900320   -2.0
## 2   52.763230   -1.7
## 3  459.765378   -1.1
## 4    7.759391   -1.0
## 5 2186.000000   -0.9
## 6  442.773196   -0.9</code></pre>
<pre class="r"><code>rm(res)</code></pre>
<p>In this data set there were a total of 116 features, for which:</p>
<ul>
<li><p>69 were not transformed because they had a minimum value of <span class="math inline">\(\leq 0\)</span></p></li>
<li>the remaining features had a <span class="math inline">\(\lambda\)</span> between -2 and 2</li>
<li><p>Features with a lambda between 0.98 and 1.02 would not be transformed</p></li>
</ul>
<p><strong>Question</strong>: Why not add an offset variable to ensure all variables are positive? Note, that instead of using the log transformation, or Box-Cox transformations when predictors have values of zero, can instaed use the Yeo-Johnson family of transformations. This family is similar to the Box-Cos transformations but can handle zero or negative predictor values.</p>
<p>As an example, the feature <em>VarIntenCh3</em>, which records the standard deviation of pix intensity in actin filaments, had the following properties:</p>
<ul>
<li>Metrics</li>
</ul>
<pre class="r"><code>filter(segMetrics, variable == &quot;VarIntenCh3&quot;)</code></pre>
<pre><code>##      variable n_distinct skewness max_min_ratio       min     max lambda
## 1 VarIntenCh3       1009 2.391624      870.8872 0.8692526 757.021    0.1</code></pre>
<ul>
<li>Strong right skewness</li>
<li>Original data distribution</li>
</ul>
<pre class="r"><code>histogram(~segTrainX$VarIntenCh3,
          xlab = &quot;Natural Units&quot;,
          type = &quot;count&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_featureEG_orig-1.png" width="672" /> * Transformed data distribution</p>
<pre class="r"><code>histogram(~log(segTrainX$VarIntenCh3),
          xlab = &quot;Log Units&quot;,
          ylab = &quot; &quot;,
          type = &quot;count&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segmentation_featureEG_trans-1.png" width="672" /></p>
</div>
<div id="data-transformations-multiple-predictors" class="section level3">
<h3>Data Transformations (Multiple Predictors)</h3>
<p><strong>Outliers</strong></p>
<ul>
<li>Is value valid or has recording error occurred?</li>
<li>Take care to remove or change values, especially if sample size is small (as could be a result of a skewed distribution without enough data to see the skewness, or could be an indication of a special part of the population)</li>
<li>Decision trees and SVMs are insensitive to outliers</li>
<li>The <em>spatial sign</em> transformation can minimize the problem of a model’s sensitivity to outliers</li>
</ul>
<p>Spatial Sign Transformation</p>
<ul>
<li>Projects predictor values onto a multidimensional sphere</li>
<li>Makes all samples the same distance from the centre of the sphere</li>
<li>Each sample is divided by its squared norm: <span class="math display">\[ x_{ij}^* = \frac{x_{ij}}{\sum^P_{j=1} x^2_{ij}}\]</span></li>
<li>The denominator measures the squared distance to the centre of the predictors distribution</li>
<li>It is important to center and scale the predictor data prior to using this transformation</li>
<li>The predictors are transformed as a group, making removal of a predictor problematic</li>
</ul>
<p>Example:</p>
<ul>
<li>Investigation of ~ 8 outliers shows valid but poorly sampled population (e.g. highly profitable customers)</li>
<li>Spatial sign transformation applied; outliers reside in northwest section of distribution but contracted inwards</li>
<li>Mitigates effect on model training</li>
</ul>
<pre class="r"><code>trellis.par.set(caretTheme())
featurePlot(x=iris[,-5], y=iris[,5], &quot;pairs&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/spatialsign_egIris-1.png" width="672" /></p>
<pre class="r"><code>featurePlot(spatialSign(scale(iris[,-5])), iris[,5], &quot;pairs&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/spatialsign_egIris-2.png" width="672" /></p>
<pre class="r"><code>set.seed(1)
n &lt;- 10000
tmp &lt;- data.frame(x=c(rnorm(n, 0, 0.02), -1, 1, 0.5),
                  y=c(rnorm(n, 0, 0.2), -1, 1, -2))

plot(tmp, asp=1, col=c(rep(1,n), 2, 3, 4), pch=19)
grid()</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/spatialsign_eg_rand-1.png" width="672" /></p>
<pre class="r"><code>plot(spatialSign(tmp), asp=1, col=c(rep(1,n), 2, 3, 4), pch=19)
grid()</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/spatialsign_eg_rand-2.png" width="672" /></p>
<p><strong>Data Reduction and Feature Extraction</strong></p>
<p>Data reduction techniques</p>
<ul>
<li>Generate smaller set of predictors that capture most of the information within the original variables</li>
<li><em>Signal (Feature) Extraction</em> techniques: New predictors are functions of original variables (therefore all original variables required)</li>
<li>PCA
<ul>
<li>Finds linear combinations of predictors (principal components) to capture most possible variance</li>
<li>First PC captures more variability than any other linear combination</li>
<li>Subsequent PCs are uncorrelated with all previous PCs</li>
<li>Principal Component can be written as: <span class="math display">\[PC_j = (a_{j1} x \text{Predictor} 1) + (a_{j2} x \text{Predictor} 2) + \ldots + (a_{jP} x \text{Predictor} P)\]</span> where P = # predictors, coefficients = component weights (or loadings) that show which predictors are important for given PC.</li>
<li>Advantage: creates uncorrelated components, which is required for stability of some models</li>
<li><p>Disadvantages: Seeks predictor-set variation without regard to predictor measurement scales/distributions or response variable; without guidance can summarize data characteristics that are irrelevant to structure of data and modelling objective</p>
<ul>
<li>Seeks linear combinations that maximize variability, and therefore will first summarise predictors with more variation. If original predictors are on measurements scales then first few components will summarise higher magnitude predictors; consequently it will focus on identifying data structure based on measurement scales rather than based on important relationships within the data for the current problem.</li>
<li>Unsupervised technique; does not consider consider modelling objective / response variability.</li>
</ul></li>
</ul></li>
<li>Should first transform skewed predictors and then center and scale prior to performing PCA; prevent PCA being influenced by original measurement scales</li>
<li>Consider Partial Least Squares (PLS) to derive components with response variable in mind.</li>
<li>Use scree plot to decide number of components to keep; in automated model building process, optimal number can be determined by cross-validation</li>
<li>Should also visually examine the PCs; plot first few PCs against each other and color points by, e.g., class labels. If PCA has captured sufficient amount of information in data, may demonstrate clusters of samples/outliers requiring closer examination. Ensure to use the same scale as later components will often have smaller data ranges and plotting on separate scales may lead to potential to over-interpret patterns.</li>
</ul>
<p>Example: PCA applied to two features</p>
<ul>
<li>For channel 1, Intensity Entropy is highly correlated with Fiber Width (0.93)</li>
<li>Could use just one predictor, or could use PCA to instead use a linear combination of these two predictors</li>
</ul>
<pre class="r"><code>## R&#39;s prcomp is used to conduct PCA
pr &lt;- prcomp(~ AvgIntenCh1 + EntropyIntenCh1, 
             data = segTrainTrans, #already pre-preprocessed for skewness
             scale. = TRUE)


transparentTheme(pchSize = .7, trans = .3)

xyplot(AvgIntenCh1 ~ EntropyIntenCh1,
       data = segTrainTrans,
       groups = segTrain$Class,
       xlab = &quot;Channel 1 Fiber Width&quot;,
       ylab = &quot;Intensity Entropy Channel 1&quot;,
       auto.key = list(columns = 2),
       type = c(&quot;p&quot;, &quot;g&quot;),
       main = &quot;Original Data&quot;,
       aspect = 1)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segPCS_EGplot-1.png" width="672" /></p>
<pre class="r"><code>xyplot(PC2 ~ PC1,
       data = as.data.frame(pr$x),
       groups = segTrain$Class,
       xlab = &quot;Principal Component #1&quot;,
       ylab = &quot;Principal Component #2&quot;,
       main = &quot;Transformed&quot;,
       xlim = extendrange(pr$x),
       ylim = extendrange(pr$x),
       type = c(&quot;p&quot;, &quot;g&quot;),
       aspect = 1)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segPCS_EGplot-2.png" width="672" /></p>
<ul>
<li>Because first PC summarises 96.6% variation and the second 3.42%, in this case could just use the first PC.</li>
</ul>
<p>Example: PCA applied to all features</p>
<p>The scree plot shows that four PCs would be retained.</p>
<pre class="r"><code>## Apply PCA to the entire set of predictors.

## There are a few predictors with only a single value, so we remove these first
## (since PCA uses variances, which would be zero)
isZV &lt;- apply(segTrainX, 2, function(x) length(unique(x)) == 1)

segPP &lt;- preProcess(segTrainX[, !isZV], c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;))
segTrainTrans &lt;- predict(segPP, segTrainX[, !isZV])

segPCA &lt;- prcomp(segTrainTrans, center = TRUE, scale. = TRUE)

tab &lt;- summary(segPCA)$importance[2,]
tab &lt;- data.frame(PC = names(tab), Var = tab, row.names=NULL, stringsAsFactors = FALSE) %&gt;% mutate(PC=parse_number(PC))
ggplot(tab, aes(x = PC, y=Var))+ 
         geom_line() + 
  geom_point()</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segPCS_EGscree-1.png" width="672" /></p>
<p>Scatterplot matrix of first 3 PCs (with points coloured by class):</p>
<ul>
<li>Appears to be some separation between classes when plotting first and second component (but remember that these two components only explain 26.6% of variance and so don’t over-interpret!). However distribution of well-segmented cells roughly contained within poorly identified cells; cell types don’t appear to be easily separated. Don’t despair!; it does not mean other models, e.g. that can accommodate non-linear relationships, will reach the same conclusions.</li>
</ul>
<pre class="r"><code>## Plot a scatterplot matrix of the first three components
transparentTheme(pchSize = .8, trans = .3)
panelRange &lt;- extendrange(segPCA$x[, 1:3])
splom(as.data.frame(segPCA$x[, 1:3]),
      groups = segTrainClass,
      type = c(&quot;p&quot;, &quot;g&quot;),
      as.table = TRUE,
      auto.key = list(columns = 2),
      prepanel.limits = function(x) panelRange)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segPCS_EGscat-1.png" width="672" /></p>
<ul>
<li>Can also visualise which predictor is associated with each principal component. A coefficient (loading) close to zero within the PC linear equation indicates that that predictors did not contribute much to that component. In the following figure, each point corresponds to a predictor variable and is coloured by thte opitcal channel used in the exerpiment. For the first PC, channel 1 (cell body) loadings are greater and therefore have largest effect on PC. However, even though cell body measurements account fo rmore variation in the data, this does not imply that these variables will be associated with predicting segmentation quality.</li>
</ul>
<pre class="r"><code>## Format the rotation values for plotting
segRot &lt;- as.data.frame(segPCA$rotation[, 1:3])

## Derive the channel variable
vars &lt;- rownames(segPCA$rotation)
channel &lt;- rep(NA, length(vars))
channel[grepl(&quot;Ch1$&quot;, vars)] &lt;- &quot;Channel 1&quot;
channel[grepl(&quot;Ch2$&quot;, vars)] &lt;- &quot;Channel 2&quot;
channel[grepl(&quot;Ch3$&quot;, vars)] &lt;- &quot;Channel 3&quot;
channel[grepl(&quot;Ch4$&quot;, vars)] &lt;- &quot;Channel 4&quot;

segRot$Channel &lt;- channel
segRot &lt;- segRot[complete.cases(segRot),]
segRot$Channel &lt;- factor(as.character(segRot$Channel))

## Plot a scatterplot matrix of the first three rotation variable
transparentTheme(pchSize = .8, trans = .7)
panelRange &lt;- extendrange(segRot[, 1:3])
upperp &lt;- function(...)
  {
    args &lt;- list(...)
    circ1 &lt;- ellipse(diag(rep(1, 2)), t = .1)
    panel.xyplot(circ1[,1], circ1[,2],
                 type = &quot;l&quot;,
                 lty = trellis.par.get(&quot;reference.line&quot;)$lty,
                 col = trellis.par.get(&quot;reference.line&quot;)$col,
                 lwd = trellis.par.get(&quot;reference.line&quot;)$lwd)
    circ2 &lt;- ellipse(diag(rep(1, 2)), t = .2)
    panel.xyplot(circ2[,1], circ2[,2],
                 type = &quot;l&quot;,
                 lty = trellis.par.get(&quot;reference.line&quot;)$lty,
                 col = trellis.par.get(&quot;reference.line&quot;)$col,
                 lwd = trellis.par.get(&quot;reference.line&quot;)$lwd)
    circ3 &lt;- ellipse(diag(rep(1, 2)), t = .3)
    panel.xyplot(circ3[,1], circ3[,2],
                 type = &quot;l&quot;,
                 lty = trellis.par.get(&quot;reference.line&quot;)$lty,
                 col = trellis.par.get(&quot;reference.line&quot;)$col,
                 lwd = trellis.par.get(&quot;reference.line&quot;)$lwd)
    panel.xyplot(args$x, args$y, groups = args$groups, subscripts = args$subscripts)
  }
splom(~segRot[, 1:3],
      groups = segRot$Channel,
      lower.panel = function(...){}, upper.panel = upperp,
      prepanel.limits = function(x) panelRange,
      auto.key = list(columns = 2))</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/segPCS_EGrot-1.png" width="672" /></p>
</div>
<div id="missing-values" class="section level3">
<h3>Missing Values</h3>
<p>Types of missing data:</p>
<ul>
<li>Structurally missing, e.g. number of children man has given birth to</li>
<li>Informative missingness: if related to outcome, can induce bias, e.g. customer rating are using polarised</li>
<li>Censored (this is not missing data, as something isknown about it), but for predictive models, it may be treated as missing, or hte censored value may be used as the observed value. E.g., for labatory test which cannot measure below a limit, may use a random value between 0 and the limit as the observed value.</li>
</ul>
<p>Options:</p>
<ul>
<li>Remove samples (if small subset of large data set)</li>
<li>Specifically account for the missingness (e.g. tree-based techniques)</li>
<li>Impute by using information in the training set of predictors (i.e. predictive model within a predictive model).
<ul>
<li>Note that imputation for statistical inference is not the same as inference for predictive models (references given for the latter).<br />
</li>
<li>Incorporate imputation in resampling if being used to select tuning parmater values.<br />
</li>
<li>If number of predictors affected by missing values is small, best to perform exploratory analysis of relationships between predictors, e.g. using PCA or visualisations. If a variable with missing values is highly correlated with another thne can use a focussed model</li>
<li>KNN popular to impute by finding samples in training set closest to it and averages nearby points to fill it. Advantage: Confined to training set. Disadvantage: KNN requires entire training set and number of neighbours and method of determinig “closeness” are tuning parameters.</li>
</ul></li>
</ul>
<pre class="r"><code>require(AppliedPredictiveModeling)
data(segmentationOriginal)

## Retain the original training set
segTrain &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)

## Remove the first three columns (identifier columns)
segTrainX &lt;- segTrain[, -(1:3)]
segTrainX &lt;- segTrainX[, -nearZeroVar(segTrainX)]

# Randomly sample 50 to be missing
set.seed(15103930)
ind_miss &lt;- sample(1:nrow(segTrainX), 50, replace = FALSE)
obs &lt;- segTrainX[ind_miss, ]
segTrainX$PerimCh1[ind_miss] &lt;- NA
summary(segTrainX$PerimCh1)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   47.74   64.92   79.09   92.20  103.67  459.77      50</code></pre>
<pre class="r"><code># Predict the missing values
set.seed(100)
knnTrans &lt;- preProcess(segTrainX, method = c(&quot;center&quot;, &quot;scale&quot;, &quot;knnImpute&quot;))
knnPred &lt;- predict(knnTrans, segTrainX)
pred &lt;- knnPred$PerimCh1[ind_miss]
obsTrans &lt;- predict(knnTrans, obs)$PerimCh1

ggplot(data.frame(obs = obsTrans, pred), aes(x=obs, y=pred)) + 
  geom_point() + 
  geom_smooth(method=&#39;lm&#39;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/imputationViaKNN-1.png" width="672" /></p>
<pre class="r"><code>cor(obsTrans, pred)</code></pre>
<pre><code>## [1] 0.8817321</code></pre>
<pre class="r"><code>#if(&quot;PerimCh1&quot; %in% preProcValues$method$scale) pred &lt;- pred * preProcValues$std[&quot;PerimCh1&quot;]
#if(&quot;PerimCh1&quot; %in% preProcValues$method$center) pred &lt;- pred + preProcValues$mean[&quot;PerimCh1&quot;]</code></pre>
<p>TODO: Simple regression method ..need to first find correlated variables</p>
<pre class="r"><code># use cell fiber lengtha nd cell size. to predict.  
segCorr &lt;- cor(segTrainX, use = &quot;pairwise.complete.obs&quot;)[,&quot;PerimCh1&quot;]
sort(segCorr, decreasing = TRUE)[2:3]</code></pre>
<pre><code>## FiberLengthCh1      LengthCh1 
##      0.9855599      0.9223834</code></pre>
<pre class="r"><code>mod &lt;- lm(PerimCh1 ~ FiberLengthCh1 + LengthCh1, data = segTrainX[-ind_miss, ])
pred &lt;- predict(mod, segTrainX[ind_miss, ])
ggplot(data.frame(obs = obsTrans, pred), aes(x=obs, y=pred)) + 
  geom_point() + 
  geom_smooth(method=&#39;lm&#39;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/imputationViaRegression-1.png" width="672" /></p>
<pre class="r"><code>cor(pred, obsTrans)</code></pre>
<pre><code>## [1] 0.9752604</code></pre>
</div>
<div id="removing-predictors" class="section level3">
<h3>Removing Predictors</h3>
<p>Advantage of less predictors:</p>
<ul>
<li>Decreased computation time</li>
<li>Decreased complexity / More pasimonious / More interpretable</li>
<li>Highly correlated predictors are measring the same thing and so have no extra information</li>
<li>Better model performance / stability without problematic and correlated variabels</li>
</ul>
<p>Predictors to remove:</p>
</div>
<div id="zero-variance-and-near-zero-variance-predictors" class="section level3">
<h3>Zero Variance and Near-Zero Variance Predictors</h3>
<ul>
<li><strong>Zero variance predictors</strong> : Those with a single unique value</li>
<li><strong>Near-zero variance predictors</strong> : Thos with a single value for most samples</li>
</ul>
<p>To determine: 1. Calculate number of unique variables / number of samples 2. Calculate frequency of unique values (or ratio of most common frequent to second most frequent)</p>
<p>If (1) faction of unique values over sample size is loq, e.g. <span class="math inline">\(\leq\)</span> 10% and (2) ratio of frequency of most prevalent value to the second most prevalent value is large, e.g. <span class="math inline">\(\geq\)</span> 20</p>
</div>
<div id="correlated-variables" class="section level3">
<h3>Correlated Variables</h3>
<ul>
<li><strong>Collinearity</strong>: Correlated pair of predictor variables</li>
<li><strong>Multicollinearity</strong>: Relationships between multiple predictors at once</li>
</ul>
<p>To detect: 1. Visually. TODO: Review Everitt</p>
<pre class="r"><code>segData &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)[, -(1:3)]
isZV &lt;- apply(segData, 2, function(x) length(unique(x)) == 1)
segData &lt;- segData[, !isZV]
correlations &lt;- cor(segData)
corrplot::corrplot(correlations, order = &quot;hclust&quot;, tl.cex = 0.2)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/plotCorrelationMatrix-1.png" width="672" /></p>
<ol start="2" style="list-style-type: decimal">
<li>PCA: If first PCA accounts for large percentage of variance, then there is at least one group of predictors that represent the same information. Use PCA loadings to understand which predictos are associated with each component. Note though that as unsupervised, no guarnatee that resulting predictors will have relationship with the outcome.</li>
</ol>
<p>TODO: Need to understand PCA better</p>
<ol start="3" style="list-style-type: decimal">
<li>Variance Inflation Factor (VIF) statistic available as part of classical regression anallysis, however only useful for linear regression. Whilst it determine collinear predictors it does not determine which should be removed to resolve the problem</li>
<li>Remove the minimum number of predictors to determien that all pairwise correlations are below a certain threshold. This only identifies collineariites in two dimensions, but can improve model performance. Algorithm is as follows:</li>
<li>Calculate correlation matrix</li>
<li>Determine pair of predictors with largest absolute correlation</li>
<li>Determine average correlation between A and other variables. Repeat for B.</li>
<li>Remove the predictor (A or B) with the largest average corelation</li>
<li>Repeat steps 2-3 until no absolute correlations above the threshold</li>
</ol>
<p>This algorithm is implemented using <code>caret::findCorrelation</code></p>
<pre class="r"><code>segTrain &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)
## Remove the first three columns (identifier columns)
segTrainX &lt;- segTrain[, -(1:3)]
isZV &lt;- apply(segTrainX, 2, function(x) length(unique(x)) == 1)
segTrainX &lt;- segTrainX[, !isZV]

segTrainClass &lt;- segTrain$Class



segPP &lt;- preProcess(segTrainX, c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;))
segTrainTrans &lt;- predict(segPP, segTrainX)


## Use caret&#39;s preProcess function to transform for skewness
segPP &lt;- preProcess(segTrainX, method = &quot;BoxCox&quot;)
## Apply the transformations
segTrainTrans &lt;- predict(segPP, segTrainX)
segCorr &lt;- cor(segTrainTrans)
#corrplot(segCorr, order = &quot;hclust&quot;, tl.cex = .35)

## caret&#39;s findCorrelation function is used to identify columns to remove.
highCorr &lt;- findCorrelation(segCorr, .75)
length(highCorr)</code></pre>
<pre><code>## [1] 43</code></pre>
</div>
<div id="adding-predictors" class="section level3">
<h3>Adding Predictors</h3>
<ul>
<li><p>It is common to decompose categorical predictors usng dummy variables (indicator with zero/one). With 5 categories, only 4 dummy variables needed. If 5 were included in regression then would have numerical isseus (intercept replaces the other); for other models, including all 5 may aid interpretation</p></li>
<li>Non-linear transformations, e.g. <span class="math inline">\(B^2\)</span>, where <span class="math inline">\(B\)</span> is a predictor</li>
<li><p>Combinations of data (e.g. class centroids, center of predictor data to each class)</p></li>
</ul>
</div>
<div id="binning-predictors" class="section level3">
<h3>Binning Predictors</h3>
<p>Disadvantages of manual binning:</p>
<ul>
<li>Loss of performance in the model</li>
<li>Loss in predicion in the predictions</li>
<li>Can lead to high rate of false positives (noisy predictors determined to be informative)</li>
</ul>
<p>Advantage: More interpretable. However the perceived improvement in interpretability by manual binning is usually offest by siginficant loss in perofrmance (the goal of this book is prediction not interpretation and so manual binning not recommended).</p>
<p>Automatic Binning * E.G Classification / Regression Trees * Multivariate adaptive regression splies * Evaluate many variables simulatabiuosly, based on staticscally sound methodogloges</p>
</div>
<div id="computing" class="section level3">
<h3>Computing</h3>
</div>
<div id="transformations" class="section level3">
<h3>Transformations</h3>
<ul>
<li><code>e1071::sknewness</code>: calculates sample skewness statistics for each predictor. Those that are highly skewed can be priorritsed for distribution visualations using <code>hist</code>, <code>lattice:histogram</code></li>
<li><code>MASS::boxcox</code>: to determine type of tranformation to use; estimates <span class="math inline">\(\lambda\)</span> but will not create the transformed variable(s)</li>
<li><code>caret::BoxCoxTrans</code>: Find most appropriate transformation and apply to new data</li>
</ul>
<p>Example:</p>
<pre class="r"><code>segData &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)[, -(1:3)]
Ch1AreaTrans &lt;- BoxCoxTrans(segData$AreaCh1)
Ch1AreaTrans</code></pre>
<pre><code>## Box-Cox Transformation
## 
## 1009 data points used to estimate Lambda
## 
## Input data summary:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   150.0   194.0   256.0   325.1   376.0  2186.0 
## 
## Largest/Smallest: 14.6 
## Sample Skewness: 3.53 
## 
## Estimated Lambda: -0.9</code></pre>
<pre class="r"><code>data.frame(orig = head(segData$AreaCh1), auto.trans = predict(Ch1AreaTrans, head(segData$AreaCh1)), man.trans = ( head(segData$AreaCh1) ^ Ch1AreaTrans$lambda - 1 )/Ch1AreaTrans$lambda   )</code></pre>
<pre><code>##   orig auto.trans man.trans
## 1  819   1.108458  1.108458
## 2  431   1.106383  1.106383
## 3  298   1.104520  1.104520
## 4  256   1.103554  1.103554
## 5  258   1.103607  1.103607
## 6  358   1.105523  1.105523</code></pre>
<ul>
<li><code>prcomp</code>: For PCA</li>
</ul>
<p>Example:</p>
<pre class="r"><code>segData &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)[, -(1:3)]
isZV &lt;- apply(segData, 2, function(x) length(unique(x)) == 1)
segData &lt;- segData[, !isZV]

pcaObject &lt;- prcomp(segData, center = TRUE, scale. = TRUE)
# Calculate the cumulative percentage of variance which each component accounts for.
percentVariance &lt;- pcaObject$sd^2/sum(pcaObject$sd^2)*100
percentVariance[1:3]</code></pre>
<pre><code>## [1] 14.677086 11.734794  8.813733</code></pre>
<pre class="r"><code># Transformed variables stored in a `pcaObject` as a sub-object called x
head(pcaObject$x[ , 1:5])</code></pre>
<pre><code>##            PC1        PC2         PC3       PC4          PC5
## 2  -10.5813682  4.7022663  0.05567686 -1.792190  2.547714227
## 3    0.4154024  0.6974872 -1.38763482 -3.265656  1.672695719
## 4   -0.9779724 -1.8203879 -2.21605066  0.397754 -0.008029369
## 12   1.9842183 -0.3322714 -0.05032939 -2.517085  1.033381188
## 15   1.1217009 -0.1825613 -1.99234368 -3.021610  0.478149323
## 16   1.0824587  0.2026488 -1.16865264 -3.380599  1.200990054</code></pre>
<pre class="r"><code># Loadings
head(pcaObject$rotation[ , 1:3])</code></pre>
<pre><code>##                         PC1         PC2           PC3
## AngleCh1        0.001058888 -0.01186622  0.0008836706
## AngleStatusCh1  0.011793294  0.02083728 -0.0035682644
## AreaCh1        -0.211068512  0.09577310  0.0672000815
## AreaStatusCh1  -0.182337308  0.07292658  0.0546691134
## AvgIntenCh1     0.061268408  0.17388938  0.0413066884
## AvgIntenCh2     0.101313516  0.16201999  0.0349230837</code></pre>
<ul>
<li><code>caret::spatialSign</code>: Spatial sign transfomration</li>
<li><code>impute::impute.knn</code> to estimate missing data; also possible using <code>caret::PreProcess</code> which applies imputation methods based on KNN or bagged trees</li>
<li><code>caret::PreProcess</code>: Has the ability to</li>
<li>Transform</li>
<li>Center</li>
<li>Scale</li>
<li>Impute values</li>
<li>Feature extraction</li>
<li>Apply spatial sign transformation</li>
</ul>
<p>(in this order) After calling the <code>preProcess</code> function, the <code>predict</code> method applies the tranformationr esults to a set of data.</p>
</div>
<div id="filtering" class="section level3">
<h3>Filtering</h3>
<ul>
<li><code>caret::nearZeroVar</code>: To determine predictors with unique variables</li>
<li><code>cor</code>: To determine correlations</li>
<li><code>corrplot:corrplot</code>: Includes option to reorder variables in a way that reveals clusters of highly correlated predictors</li>
<li><code>caret::findCorrelation</code>: Recommends columns for deleteion based on a given threshold of pairwise correlations</li>
<li><code>subselect</code> package also has methods for selecting predictors</li>
</ul>
</div>
<div id="creating-dummy-variables" class="section level3">
<h3>Creating Dummy Variables</h3>
<ul>
<li><code>caret::dummyVars</code>: To determine encodings for clategorical predictors</li>
<li>All dummy variables recommended when using tree-based model</li>
</ul>
<pre class="r"><code>data(cars)
carSubset &lt;- select(cars, Price, Mileage)
type &lt;- c(&quot;convertible&quot;, &quot;coupe&quot;, &quot;hatchback&quot;, &quot;sedan&quot;, &quot;wagon&quot;)
carSubset$Type &lt;- factor(apply(cars[, 14:18], 1, function(x) type[which(x == 1)]))

simpleMod &lt;- dummyVars(~Mileage + Type,
                       data = carSubset,
                       ## Remove the variable name from the
                       ## column name
                       levelsOnly = TRUE)
simpleMod</code></pre>
<pre><code>## Dummy Variable Object
## 
## Formula: ~Mileage + Type
## 2 variables, 1 factors
## Factor variable names will be removed
## A less than full rank encoding is used</code></pre>
<pre class="r"><code># To generate the dummy variables fo rhte training set or any new samples, use the predict method with teh dummyVars objects
predict(simpleMod, head(carSubset))</code></pre>
<pre><code>##   Mileage convertible coupe hatchback sedan wagon
## 1   20105           0     0         0     1     0
## 2   13457           0     1         0     0     0
## 3   31655           1     0         0     0     0
## 4   22479           1     0         0     0     0
## 5   17590           1     0         0     0     0
## 6   23635           1     0         0     0     0</code></pre>
<pre class="r"><code>withInteraction &lt;- dummyVars(~Mileage + Type + Mileage:Type,
                             data = carSubset,
                             levelsOnly = TRUE)
withInteraction</code></pre>
<pre><code>## Dummy Variable Object
## 
## Formula: ~Mileage + Type + Mileage:Type
## 2 variables, 1 factors
## Factor variable names will be removed
## A less than full rank encoding is used</code></pre>
<pre class="r"><code>predict(withInteraction, head(carSubset))</code></pre>
<pre><code>##   Mileage convertible coupe hatchback sedan wagon Mileage:convertible
## 1   20105           0     0         0     1     0                   0
## 2   13457           0     1         0     0     0                   0
## 3   31655           1     0         0     0     0               31655
## 4   22479           1     0         0     0     0               22479
## 5   17590           1     0         0     0     0               17590
## 6   23635           1     0         0     0     0               23635
##   Mileage:coupe Mileage:hatchback Mileage:sedan Mileage:wagon
## 1             0                 0         20105             0
## 2         13457                 0             0             0
## 3             0                 0             0             0
## 4             0                 0             0             0
## 5             0                 0             0             0
## 6             0                 0             0             0</code></pre>
</div>
<div id="chapter-3-exercises" class="section level3">
<h3>Chapter 3 Exercises</h3>
<div id="exercise-3.1" class="section level4">
<h4>Exercise 3.1</h4>
<p>The UC Irvine Machine Learning Repository contains a data set related to glass identification. The data consist of 214 glass samples labeled as one of seven class categories. There are nine predictors, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.</p>
<pre class="r"><code>require(mlbench)</code></pre>
<pre><code>## Loading required package: mlbench</code></pre>
<pre class="r"><code>data(Glass)
str(Glass)</code></pre>
<pre><code>## &#39;data.frame&#39;:    214 obs. of  10 variables:
##  $ RI  : num  1.52 1.52 1.52 1.52 1.52 ...
##  $ Na  : num  13.6 13.9 13.5 13.2 13.3 ...
##  $ Mg  : num  4.49 3.6 3.55 3.69 3.62 3.61 3.6 3.61 3.58 3.6 ...
##  $ Al  : num  1.1 1.36 1.54 1.29 1.24 1.62 1.14 1.05 1.37 1.36 ...
##  $ Si  : num  71.8 72.7 73 72.6 73.1 ...
##  $ K   : num  0.06 0.48 0.39 0.57 0.55 0.64 0.58 0.57 0.56 0.57 ...
##  $ Ca  : num  8.75 7.83 7.78 8.22 8.07 8.07 8.17 8.24 8.3 8.4 ...
##  $ Ba  : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ Fe  : num  0 0 0 0 0 0.26 0 0 0 0.11 ...
##  $ Type: Factor w/ 6 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;5&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<pre class="r"><code>GlassPredictors &lt;- select(Glass, -Type) %&gt;%
  gather(Variable, Value)</code></pre>
<ol style="list-style-type: lower-alpha">
<li>Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors.</li>
</ol>
<pre class="r"><code>ggplot(GlassPredictors, aes(x = Value)) +  
  geom_histogram() + 
  facet_wrap(~Variable, scales = &quot;free&quot;)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/ex31_hist-1.png" width="672" /></p>
<pre class="r"><code>ggplot(GlassPredictors, aes(x = Value)) +  
  geom_density() + 
  facet_wrap(~Variable, scales = &quot;free&quot;)</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/ex31_dens-1.png" width="672" /></p>
<pre class="r"><code>require(GGally)
ggpairs(select(Glass, -Type))</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/ex31_scatter-1.png" width="672" /></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Does there appear to be any outliers in the data? Are predictors skewed?</li>
</ol>
<ul>
<li>Several variables show signs of skewness (BA, Ca, FE, RI)</li>
<li>Variable K could be skewed or have outliers</li>
<li>Variables K and MG have possible second modes around zero, whereas Fe and Ba also have a high distribution of values around zero</li>
<li>Variables Ca, NA, RI and SI have concentrations of samples in the middle of the scale and a small number of data points at the edges</li>
<li>Variables Ca and Ri are positively correlated, and Ca and NA</li>
<li>Visually, Ca and Na appear to be negatively correlated, however the correlation score is -0.275</li>
</ul>
<ol start="3" style="list-style-type: lower-alpha">
<li>Are there any relevant transformations of one or more predictors that might improve the classification model?</li>
</ol>
<p>Due to variables containing zero, use the Yeo-Johnson family of transformations</p>
<pre class="r"><code>glassTrans &lt;- preProcess(select(Glass, -Type), method = &quot;YeoJohnson&quot;)
glassTransData &lt;- predict(glassTrans, select(Glass, -Type))
glassTransData %&gt;%
  gather(Variable, Value) %&gt;%
  mutate(Transformation = &quot;Yeo-Johnson&quot;) %&gt;%
  bind_rows(data.frame(GlassPredictors, Transformation = &quot;NA&quot;)) %&gt;%
ggplot(aes(x = Value, color = Transformation)) +  
  geom_density() + 
  facet_wrap(~Variable, scales = &quot;free&quot;)</code></pre>
<pre><code>## Warning in bind_rows_(x, .id): binding character and factor vector,
## coercing into character vector</code></pre>
<p><img src="/blog/2018-05-30-book-applied-predictive-modeling_files/figure-html/ex31_yeo-1.png" width="672" /> However, they don’t really help in terms of skewness …</p>
<p>To mitigate outliers, use the spatial sign transformation</p>
</div>
</div>
</div>
</div>
<div id="new-r-commands" class="section level1">
<h1>New R commands</h1>
<p><code>apropos</code>: search R packages for a given term in currently loaded packages</p>
<p><code>RSiteSearch</code>: find function in any package</p>
</div>
