---
title: Sequence Models
author: ''
date: 2019-03-04T13:39:46+02:00
slug: sequence-models
categories: [Study-Notes]
tags: 
- Coursera
- Study-Notes
banner: img/banners/sequenceNN.png
---


# Recurrent Neural Networks {#week1}

## Why sequence models

* Speech recognition
    * Input: Audio clip (sequence)
    * Output: Sequence of words (sequence)
* Music generation
    * Input: e.g. Genre to generate
    * Output: Music (sequence)
* Sentiment classification
    * Input: Phrase (sequence)
    * Output: Stars
* DNA sequence analysis
    * Input: DNA sequence
    * Output: Which part of sequence corresponds to a protein
* Machine translation
    * Input: sentence, e.g. in french
    * Output: same sentence in different language
* Video activity recognition
    * Input: Sequence of video frames
    * Output: Activity
* Name entity recognition, which can be used to find people's names, companies names, times, locations, countries, currency names, etc in different types of text
    * Input: Sentence
    * Output: List of names in sentence
    
* Supervised learning examples
* Can have both X and Y as sequences (which may or may not have the same length), or either X or either Y as sequence only.

## Notation

Motivating example: Named entity recognition,

* $x$: Harry Potter and Hermoine Granger invented a new spell
* $y$: For each part of input word, is it part of a person's name?  (more sophisticated alternatives: Start/stop for names)

* $x^{<t>}$: index to position into input sequence, e.g. $x^{<2>}$ in above example, would be Potter as it is the second word in the sequence.
* $x^{(i)<t>}$: input sequence position $t$ of $i^{\text{th}}$ training example
* $y^{<t>}$: index to position into output sequence, e.g. $y^{<2>}$ in above example, would be 1 as Potter is part of a name
* $y^{(i)<t>}$: output sequence position $t$ of $i^{\text{th}}$ training example
* $T_x$: length of input sequence, e.g. 9 in above example
* $T_y$: length of output sequence, e.g. 9 in above example
* $T_x^{(i)}$: length of input sequence in $i^{\text{th}}$ training example
* $T_y^{(i)}$: length of output sequence in $i^{\text{th}}$ training example


How to represent individual words in a sentence?

* Start with a vocabulary / dictionary

$\begin{bmatrix} \text{a} \\ \vdots \\ \text{and} \\ \text{harry} \\ \vdots \\ \text{zulu} \end{bmatrix}$

* Number each word in vobulary sequentially, e.g. with 10,000 words in dictionary (quite small); 30-50K more common.  100K not uncommon.  Internet companies: 1Million

Word | Index
----------| -----
a   | 1
aaron | 2
$\vdots$ | $\dots$
and | 367
$\vdots$ | $\dots$
harry | 4,075
$\vdots$ | $\dots$
potter | 6,830
$\vdots$ | $\dots$
zulu | 10,000


Use one-hot representations:

* $x^{1}$ would be a 10,000 length vector of zeros, with exception of the value 1 at index 4075 (_Harry_)
* $x^{2}$ would be a 10,000 length vector of zeros, with exception of the value 1 at index 6830 (_Potter_)
* $x^{7}$ would be a 10,000 length vector of zeros, with exception of the value 1 at index 1 (_a_)

So each $x^{<t>}$ is a 10,000 (vocabulary size) one-hot vector (one-hot = only one is on, and zero everywhere else)
Use <UNK> if come across a word not in vocabulary

## Recurrent Neural Network Model

Why couldn't you use a standard neural network?

Problems:

* Inputs and and output can be different lengths in different examples.  (not a good idea to pad, zero pad inputs to consider inputs up to a maximum length)
* Doesn't share features learned across different positions of text, so that Harry learned as a name in position 1 would generalise to other parts
* As standard model, would need weights for 100,000 $\times$ maximum number of words 

Recurrent Neural Networks

* Assume read from left to right
* When reads second word, it takes information from what computed for first word, and so on
![Recurrent Neural Network](/img/recurrentNN/RNN.png)  
* Alternative representation, where square is one-step
![Recurrent Neural Network (De-Rowed)](/img/recurrentNN/RNN_derowed.png)  
* Parameters, $W_{ax}$, $W_{ay}$  and $W_{aa}$, for each time step are shared
![RNN Parameters](/img/recurrentNN/RNN_with_params.png)  
* Weakness: Only uses information that is earlier in the sequences, e.g. cannot determine "Teddy" is a name with only first word in the following sentences; need the later words: "Teddy Roosevelt was a great President" and "Teddy bears are on sale!".  Will look at bidrectional RNN (BRNN) later in course.

![Recurrent Neural Network](/img/recurrentNN/RNN.png)  
Forward propagation (unidirectional RNN)

* Start with initial activations, $a^{<0>}$ of zeros
* Activation at time step 1: generally use tanh (most common) or ReLU activation.  Other ways of preventing vanishing gradient problem
    \[a^{<1>} = g(W_{aa} a^{<0>} + W_{ax} x^{<1>} + b_a)\]
* Prediction at time set 1; 
    * binary classification: sigmoid activation function
    * k-way: softmax
\[\hat{y}^{<i>} =  g(W_{ya} a^{<1>} + b_y)\]
* More generally, at time $t$:
$$\begin{aligned}
    a^{<t>}         &= g(W_{aa} a^{<t-1>} + W_{ax} x^{<t>} + b_a) \\
    \hat{y}^{<t>}   &=  g(W_{ya} a^{<t>} + b_y)
\end{aligned}$$

Can simplify as:
$$\begin{aligned}
    a^{<t>}         &= g(W_{a} [a^{<t-1>}, x^{<t>}] + b_a) \\
    \hat{y}^{<t>}   &=  g(W_{ya} a^{<t>} + b_y)
\end{aligned}$$

where $$\begin{aligned}
    W_a &= [W_{aa} | W_{ax}] \\
    [a^{<t-1>}, x^{<t>}] &= \begin{bmatrix} a^{<t-1>} \\ x^{<t>} \end{bmatrix} 
\end{aligned}$$
 
Example: If $a$ 100-dimensional and $x$ 10,000-dimensional, then

* $W_{aa}$ would be 100 $\times$ 100
* $W_{ax}$ would be 100 $\times$ 10,000
* $W_a$ would be 100 $\times$ 10,100
* $a^{<t-1>}$ would be 100 $\times$ 1
* $x^{<t>}$ would be $10,000 $\times$ 1

Note: 

* $b_a$: bias
* $W_\text{xy}$ Multiply by some $y$-like quantity to compute some $x$-like quantity


## Backpropagation through time

* Programming framework would automatically take care of backpropagation
* Given inputs $x^{<1>}, x^{<2>}, \ldots, x^{<T_x>}$, initial activation $a_0$ and parameters $W_a$ and $b_a$, calculate activations $a^{<1>}, a^{<2>}, \ldots, a^{<T_x>}$
* Given parameters $W_y$, $b_y$ and activations, can calculate predictions $\hat{y}^{<1>}, \hat{y}^{<2>}, \ldots, \hat{y}^{<T_x>}$
* To perform back-propagation, require loss function, standard logistic regression loss function (aka cross-entropy loss)
    \[\mathcal{L}(\hat{y}^{<t>}, y^{<t>}) = -y^{<t>} \log \hat{y}^{<t>}  - (1 - y^{<t>}) \log (1 - \hat{y}^{<t>})\]
![Backpropagation Through Time](/img/recurrentNN/BackpropThruTime.png)    

## Different types of RNNs

Wider range of RNN architectures, e.g. when output $T_x \neq T_y$ input sequence length

* Many-to-many, $T_x,  T_y > 1$.  
    * $T_x = T_y$; as per motivating example above </br>
    ![Many to many, same](/img/recurrentNN/architecture_many_to_many_same.png)   
    * $T_x \neq T_y$, e.g machine translation, "como estas" $\rightarrow$ "how are you"</br>
    ![Many to many, same](/img/recurrentNN/architecture_many_to_many_diff.png)   
* Many-to-one, e.g. output 0 or 1 (or a rating from 1 to 5) to indicate sentiment from sentence</br>
![Many to one](/img/recurrentNN/architecture_many_to_one.png)   
* One-to-many, e.g. music generation where goal is to output set of notes from input such as genre</br>
![One to Many](/img/recurrentNN/architecture_one_to_many.png)   
* One-to-one (standard)</br>
![One to One](/img/recurrentNN/architecture_one_to_one.png)  
* attention-based (to discuss later)


## Language model and sequence generation



* A language model takes as input a sentence (a sequence, but use notation $y$ rather than $x$) and determines the probability
* To build a language model, 
    1. Obtain  training set: large corpus (body) of english text
    2. Require vocabulary (dictionary)
    3. Map each word to the dictionary tokens using one-hot vectors
        * Each word in sentence is represented as $y^{<1>}, y^{<2>}, \ldots$$
        * May use <EOS> token for end of sentence (period)
        * Use <UNK> for words not in vocabulary
    4. What is the probability of each word in the sentence
        * First word:   What is the probability of the first word being any word in the dictionary?  With 10,000-word, each output is a 10,000-way soft max output (incl. EOS and UNK).  As input, pass zero-vector
        * Second word: What is probability of second word being each word in dictionary, given first word in sentence
        * $\ldots$
        * Each step looks at preceding words, RNN learns to predict one word a time going from left to right
Note that 15 was considered a word
![Cats average 15 hours of sleep a day](/img/recurrentNN/example_model_cats_sleep.png)  
    
* Cost function: at time t with tru word $y^{<t>}$ and prediction $\hat{y}^{<t>}$ then loss function is
$$\begin{aligned}
    \mathcal{L}(\hat{y}^{<t>}, y^{<t>}) &= -\sum_i y^{<t>}_i \log \hat{y}^{<t>}_i \\
    \mathcal{L} &= \sum_t \mathcal{L}^{<t>}(\hat{y}^{<t>}, y^{<t>}) 
\end{aligned}$$

Note conditional probabilities used to determine the probability of a three-word sentence:
$P(y^{<1>}, y^{<2>}, y^{<3>}) = P(y^{<1>})P(y^{<2>}|y^{<1>})P(y^{<2>})P(y^{<1>},y^{<2>})$


## Sampling novel sequences

To get a sense of what is learned

* Sample first word; randomly sample soft max distribution use` np.random.choice`
* Take $x_2 = \hat{y}^{<1>}$ to predict $\hat{y}^{<2>}$
* Sample until EOS token (but if no EOS, choose sentence length beforehand)
* You may get an UNK token; can reject and resample until not UNK, or leave in output

Character-level vocabulary

* Have been using a word-level vocabulary; alternative is to  use a character-level vocabulary 
* Example vocabulary: [a, b,c, $\ldots$, z, space, ., ;, 0, $\ldots$ 9, A, Z]
* Can use corpus to determine complete vocabulary
* Advantage: Don't have to worry about unknown tokens
* Disadvantages: 
    * Much longer sequences, 10-20 words = 100+ characters.  
    * Harder to capture longer-range dependencies.  
    * More computationally expensive
* As computers are getting more efficient, people are starting to use character-level vocabularies, but practice not widely spread

Sampling novel sentences from a model trained using newspaper corpuses gives very different results to those trained using a shakespear corpus.


## Vanishing gradients with RNNs

* RNNs tend to run in to vanishing gradient problems
* Consider:
    * The __cat__, which already ate $\ldots$, __was__ full
    * The __cats__, which already ate $\ldots$, __were__ full
* Basic RNN not good at picking up very long-range dependency
* Recall concept of vanishing gradients with very deep neural networks (e.g. 100 layers); gradient at last layer will have hard time impacting gradient at earlier level.  Similar occurs with basic RNN.  
* Basic RNN has mainly local influences, i.e. preceding 1-3 words have much more influence than words preceding the current word by, e.g. 10 places
* Exploding gradients
    * Not as common as Vanishing
    * But when exists, can be catestrophic
    * If you see "NaN" in gradient vectors, can apply gradient clipping; i.e. set gradient = max(X, gradient)
 
As vanishing gradient harder to deal with, will be discussed in next few sections.

See [Improving Deep Learning Networks](/blog/2018/12/07/2018-12-07-improving-deep-learning-networks#vanishing)

## Gated Recurrent Unit (GRU)

* Better than basic RNN at capturing long range connections and helps with vanishing gradient problems
* Recall formula for activation at time $t$, \[a^{<t>}  = g(W_{aa} a^{<t-1>} + W_{ax} x^{<t>} + b_a)\]
    * activation from last time step
    * Input token
    * g could be tahn activation function
![Basic RNN activation function](/img/recurrentNN/basic_rnn_activation.png)  

* Motivating example
    * The __cat__, which already ate $\ldots$, __was__ full
    * The __cats__, which already ate $\ldots$, __were__ full
* Formulas:

$$\begin{aligned}
    \text{Candidate Memory Cell:} &   \tilde{c}^{<t>} &= \tanh(w_c[c^{<t-1>}, x^{<t>}] + b_c) \\
    \text{Gate:}                    & \Gamma_u &= \sigma(w_u[c^{<t-1>}, x^{<t>}] + b_u\\
    \text{Memory Cell:} & c^{<t>} &= \Gamma_u \tilde{c}^{<t>} +  (1-\Gamma_u) c^{<t-1>}\\
\end{aligned}$$

* Use $c$ for memory _c_ell
* The GRU will output an activation that is the same as the memory cell, but this notation is used as these will differ for other models, e.g., LSTM
\[c^{<t>} = a^{<t>}\]
* At each time step, consider overriding memory cell with the candidate memory cell based on the value of the gate
    * As the gate is a sigmoid function it will usually be practically zero or practically one
    * If the gate function is 0, the memory cell $c^{<t>}$ remains the same as at $t-1$, i.e.,  $c^{<t>} =  c^{<t-1>}$.  If the gate function is 1, the memory cell is updated to the candidate value
* For example, if the cell $c$ captures concept of plural/singular, the gate would have a value of 1 at the _cat_ token remain at zero until after a new noun captures after the word _was_.
* The dimensions of the memory cell $c^{<t>}$, candidate memory cell $\tilde{c}^{<t>}$ and gate $\Gamma_u$ are the same.
    * So if all are 100-dimensional vector; most are zeros or ones and tells which are the bits you want to update.  (in practice, may be in-between zero)
    * The asterix is element-wise in the memory cell formula
    * Different elements (bits) may be used for different concepts; i.e. one for singular/plural, another for the concept of food, etc.

![Simplified GRU](/img/recurrentNN/simplified_gru.png)  

Full GRU

* Uses $r$ to measure the relevance of the current memory cell $c^{<t-1>}$ when calculating the candidate value $\tilde{c}^{<t>}$

$$\begin{aligned}
    \text{Candidate Memory Cell:}\;\;\; &   \tilde{c}^{<t>} &&= \tanh(\color{blue}{\Gamma_r} * W_c[c^{<t-1>}, x^{<t>}] + b_c) \\
    \text{Gate:}\;\;\;                & \Gamma_u &&= \sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u\\
    \color{blue}{\text{relevance:}}\;\;\;                & \color{blue}{\Gamma_r} &&= \color{blue}{\sigma(W_r[c^{<t-1>}, x^{<t>}] + b_r}\\
    \text{Memory Cell:}\;\;\; & c^{<t>} &&= \Gamma_u * \tilde{c}^{<t>} +  (1-\Gamma_u) c^{<t-1>}\\
\end{aligned}$$

* Academic literature may use different notation, here used notation to be similar for GRU and LSTM
    * candidate memory cell $\tilde{h}$
    * Gate $u$
    * Relevance $r$
    * Memory cell $h$

LSTM 

* another instantiations of the same idea


## Long Short Term Memory (LSTM)

* More powerful than GRU
* Recall GRU equations

$$\begin{aligned}
    \text{Candidate Memory Cell:}\;\;\; &   \tilde{c}^{<t>} &&= \tanh(\Gamma_r * W_c[c^{<t-1>}, x^{<t>}] + b_c) \\
    \text{Update Gate:}\;\;\;                & \Gamma_u &&= \sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u\\
    \text{Relevance Gate:}\;\;\;                & \Gamma_r &&= \sigma(W_r[c^{<t-1>}, x^{<t>}] + b_r\\
    \text{Memory Cell:}\;\;\; & c^{<t>} &&= \Gamma_u * \tilde{c}^{<t>} +  (1-\Gamma_u) c^{<t-1>}\\
    \text{Activation Value:}\;\;\; & a^{<t>} &&= c^{<t>}
\end{aligned}$$


LSTM

* As, unlike for GRU, $a^{<t>} \neq c^{<t>}$, and so memory cell explicitly references the activation value, rather than the memory cell
* Introduces a forget gate, which gives option of keeping old memory cell and adding to it.
* Introduces output gate
* So has three gates instead of two

$$\begin{aligned}
    \text{Candidate Memory Cell:}\;\;\; &   \tilde{c}^{<t>} &&= \tanh(W_c[a^{<t-1>}, x^{<t>}] + b_c) \\
    \text{Update Gate:}\;\;\;                & \Gamma_u &&= \sigma(W_u[a^{<t-1>}, x^{<t>}] + b_u\\
    \text{Forget Gate:}\;\;\;                & \Gamma_f &&= \sigma(W_f[a^{<t-1>}, x^{<t>}] + b_f\\
    \text{Output Gate:}\;\;\;                & \Gamma_o &&= \sigma(W_o[a^{<t-1>}, x^{<t>}] + b_o\\
    \text{Memory Cell:}\;\;\; & c^{<t>} &&= \Gamma_u * \tilde{c}^{<t>} +  \Gamma_f c^{<t-1>}\\
    \text{Activation Value:}\;\;\; & a^{<t>} &&= \Gamma_o * \tanh c^{<t>}
\end{aligned}$$

![LSTM](/img/recurrentNN/LSTM.png) 

* $a^{<t-1>}$ and $x^{<t>}$ used to calculate all the gates (forget, update, output) and also go through the tanh to calculate $\tilde{c}^{<t>}$

![LSTM Network](/img/recurrentNN/LSTM_network.png) 

* So long as you set forget and update gates appropriately, it is relatively easy for LSTM to have some value $c^{<0>}$ and have it passed all the way to the right so that $c^{<3>} = c^{<0>}$

Variations

* Have gate values also dependent on $c^{<t-1>}$, known as a _peephole connection_, e.g., for output gate:
\[\Gamma_o = \sigma(W_o[a^{<t-1>}, x^{<t>}, c^{<t-1>}] + b_o\]


When to use GRU vs LSTM?

* LSTM became earlier than GRU (simplified version)
* Different problems, different algorithm will win
* Advantage of GRU is simpler and so easier to build bigger network (computationally faster and scales to building bigger models)
* LSTM more flexible/powerful (historically more proven choice, although GRU usage is gaining momentum)

## Bidirectional RNN (BRNN)

* To take information earlier and later in sequence
* Inputs $x^{<1>}, \ldots, x^{<4>}$
* Forward recurrent components $\overrightarrow{a}^{<1>}, \overrightarrow{a}^{<2>}, \overrightarrow{a}^{<3>}, \overrightarrow{a}^{<4>}$
* Backward recurrent components $\overleftarrow{a}^{<4>}, \overleftarrow{a}^{<3>}, \overleftarrow{a}^{<2>}, \overleftarrow{a}^{<1>}$.  This is still forward prop.
* Predictions $\hat{y}^{<1>}, \ldots, \hat{y}^{<4>}$ where \[\hat{y}^{<t>} = g(W_y[\overrightarrow{a}^{<t>}, \overleftarrow{a}^{<t>}] + b_y)\]
* Acyclic graph
* BRNN can be with LSTM blocks both forward and backwards.  Similarly with GRU.
* Disadvantage: Need entire sequence of data before can make a prediction.  Need to wait for person to stop talking, i.e. entire utterance.  Where can get entire sentence at a time this standard BRNN is suitable.


## Deep RNNs

![Deep RNN](/img/recurrentNN/deep_rnn.png) 

* May need to stack multiple layers of RNNs to build deeper RNNs
* Use $a^{[\text{layer}]<\text{time}>}$
* $a^{[2]<3>} = g(W_a^{[2]}[a^{[2]<2>}, a^{[1]<3>}] + b_a^{2}) $
* First layer has parameters $W_a^{[1]}, b_a^{1}$; second layer has parameters $W_a^{[2]}, b_a^{2}$, and so on
* Generally only have 3 recurrent layers (that is already pretty deep)
* The recurrent blocks amy be RNN, GRU, LSTM
* Sometimes have 3 recurrent layers, with additional non-connected layers on top

![Deep RNN](/img/recurrentNN/deep_RNN_On_Steroids.png) 

# Introduction to Word Embeddings {#week2}

## Word Representation
### 1-Hot Representation
1-hot representation: all zeros, with 1 for word.  Use $O_{i}$ to represent 1-hot vector of zeros with a 1 at position $i$

Weakness:

* Each word treated as a thing (and doesn't allow algorithm to generalise across words)
* E.g. algorithm could learn that I want a glass of orange __juice__ as a likely sentence, but it won't be able to generalise that apple juice is a popular phrase
* Inner product of King-Queen is zero and of Apple-Orange is also zero.  I.e. doesn't know that orange-apple are more similar than orange-king

### Word embedding
Featurised representation; learn set of features for each word

Feature | Man | Woman | King | Queen | Apple | Orange
--------| ----| ------| -----| ------| ------| ------
1-Hot Position | (5391) | (9853) | (4914) | (7157) | (456) | (657)
Gender  | -1 | 1 | -0.95 | 0 | 0
Royal | 0.01 | 0.02 | 0.93 | 0.95 | -0.01 | 0.01
Age | 0.03 | 0.02 | 0.7 | 0.69 | 0.03 | -0.02
Food | 0.09 | 0.01 | 0.02 | 0.01 | 9.95 | 0.97

Other features may include size, cost, colour...

Consider that there are 300 features, then use $e_{5391}$ to represent the 300-dimensional embedding vector for man.

You will notice that the embedding vectors for apple and orange to be similar, this will allow a algorithm that has learnt that orange juice to then know that apple juice is also a thing.

The components within the embedding vectors will not be easy to determine that they correspond to for example, gender, royal, etc. but the embedding vectors do help the algorithm.

#### Visualising word embeddings

t-SNE algorithm can be used to convert the 300 dimensional embedding for each word to a 2D space so that can visualise and show that word embedding algorithms can learn similar features for related concepts/words.  The 300dimensional vectors are known as embeddings.

E.g. Word orange gets embedded into a point in the 300D space and apple gets embedded into another point in the 300-d space.

## Using word embeddings


How to plug embeddings into NLP applications.
Example: Named entity recognition example to determine which words are person's names 

Given the following  sentences:

* Sally Johnson is an orange farmer
* Robert Lin is an apple farmer
* Harry James is a durian cultivator
 
With word embeddings knowing that orange, apple and durian are fruit, and that farmer is like cultivator, if the NLP algorithm (named entity recognition) has learned that Sally Lin is a person, it can then deduce that Robert Lin and Harry James are also people.

Algorithms to learn word embeddings can examine very large text corpus's (e.g. up to 100 billion words off Internet) so can know that durian is a fruit, cultivate is like a farmer (even though durian and cultivator might not be in NLP training set).  I.e. named entity recognition task might have a much smaller training set.  This is an example of transfer learning where you take information learned from large unlabelled data sets and transfer it to a named entity recognition task.


### Transfer learning and word embeddings

1. Learn word embeddings from large text corpus (1-100B words), or download pre-trained embedding online
2.  Transfer embedding to a new task with smaller training set (say 100K).  Rather than using a 10,000 dimensional 1-hot vector can now use a lower-dimension, e.g. 300-dimensional dense  vector
3.  Optional: Continue to fine-tune the word embeddings with new data (in practice would only do with a large data set for the NLP problem, i.e. task 3)

Similar to transfer learning, most useful when have a tonne of data for task 1 and less for task 2.


Embedding has been used for the following NLP tasks:

* Named entity recognition
* Text summarisation
* Co-reference resolution
* Parsing

Embedding less useful for:

* Language modelling machine translation

### Relation to face encoding

For face-recognition; the term encoding is similar to embedding (word).

Difference:

* For face recognition, input = any face picture (unlimited sea of pictures)
* Learning word embedding, input= fixed vocabulary (limited sea of words) and learn $e_1, \ldots, e_{10,000}$ learn embedding.

## Properties of word embeddings

Can help with analogy reasoning, Man $\rightarrow$ Woman as King $\rightarrow$ Queen.

In above example, $e_\textrm{man} - e_\text{woman} \approx [-2 0 0 0]^T$.  Similarly,  $e_\textrm{king} - e_\text{queen} \approx [-2 0 0 0]^T$, i.e. main difference is gender.

### Analogies using word vectors

Man is to woman as king is to what: 
$e_\textrm{man} - e_\text{woman} \approx e_\textrm{king} - ?$

* Find word $w$ such that $\arg \max\limits_{w} \text{sim}(e_w, e_\text{king} - e_\textrm{man} - e_\text{woman})$

Currently, get 30-75% accuracy on analogy where an analogy attempt is correct only if it guesses the exact right word.

Clarification of t-SNE.

* Mapping is very complex and non-linear
* Cannot count on visualisation to show the parallels between words (this exists in the larger, e.g. 300-D, dimensional space)

Similarity function:

* Most commonly used is cosine similarity
\[\text{sim}(u,v) = \frac{u^Tv}{||u||_2 ||v||_2}\]

Ignoring the denominator, this is basically the inner product between $u$ and $v$. if very similar, the inner product will be large.  

This is the cosine of the angle between the two vectors.  

For angle $\phi$, then formula is cosine of angle.

* If $\phi$ = 0, then $\cos \phi$ = 1
* If $\phi$ = 90, then $\cos \phi$ = 0
* if $\phi$ = 180, then  $\cos \phi$ = -1

* Can also use Euclidean distance which is a technically a measure of dissimilarity, so would need to take negative \[||u - v||^2\]

Word embeddings can learn:

* Man:Woman as Boy:Girl
* Ottawa:Canada as Nairobi:Kenya
* Big:Bigger as Tall:Taller
* Yen:Japan as Ruble:Russia

## Embedding matrix

Algorithm to learn embedding results in embedding matrix.

e.g. with 10,000 words and unknown token and 300 features, get 300 $\times$ 10,001 embedding matrix.

Notation:

* $O_j$ to denote the 1-hot vector with 0s with a 1 in position $j$.  This will be a 10,001 vector.  
* $E$ for embedding matrix, e.g. dimension 300, 10001
* $e_j$ = $E \centerdot O_j$ will be of dimension  (300,10001)(100001, 1) = (300,1) and so is grabbing the embedding vector for the word in position $j$.  This is the embedding vector for word $j$.
* In practice, do not implement as matrix multiplication (inefficient), but rather use a specialised function to look up an embedding.  
* In Keras, use Embedding layer

## Learning word embeddings

Example: Building language model using a neural network, e.g. to predict the word in sentence _I want a glass of orange ...._

Word: | I | want | a | glass | of | orange
------| ----| -----| -----| -----|-----| ----------
Dictionary Position: | 4343 | 9665 | 1 | 163 | 6257
One hot vector: | $o_{4343}$ | $o_{9665}$ |$o_1$ |$o_{163}$ | $o_{6257}$
Embedding vector: | $e_{4343}$ | $e_{9665}$ |$e_1$ |$e_{163}$ | $e_{6257}$


* Feed embedding vector into NN which will feed to a softmax (with its own parameters) of 10,000.
* May only wish to use the last 4 words, which would mean would input a 1200 dimensional layer (as opposed to 1800).
* Use backprop to perform gradient descent to maximize the likelihood of your training set to  repeatedly predict the next word given four words in a sequence

{EmbeddingMatrix.png}

Note: Same matrix $E$ used regardless of position

Contexts:

* Last 4 words (to predict next word)
* 4 words on left & right (to predict word in middle)
* Last 1 word, predict next word (would feed in embedding of previous word)
* Nearby 1 word; what is a word that will be near glass (skip gram model)

Posing these contexts allow you to learn good word embeddings

## Word2Vec
Simple method to learn word embeddings

### Skip-grams model

* Randomly pick a word to be the context word
* Randomly pick a word within some window of the context word which will be the target word, e.g. 

Context | Target
-------| ---------
orange | juice
orange | glass
orange | my

Goal of setting up this supervised learning problem is not to predict words within an $x$-word window but to learn the embeddings. (so even if do not so well with the supervised learning problem, the output will be useful as an embedding!)

Model:

* Vocab size = 10,000
* Goal: Learn mapping from context word $c$ to some target $t$, $o_c \rightarrow E \rightarrow e_c \rightarrow \text{Softmax} \rightarrow \hat{y}$
* Context
* Softmax: \[p(t|c) = \frac{e^{\theta_t^Te_c}}{\sum^{10,000}_{j=1}e^{\theta_j^T e_c} }\]

where $\theta_t$ = parameter associated with output $t$.

Noting that loss function is \[\mathcal{L}(\hat{y}, y) = -\sum^{10,000}_{i=1} y_i\log\hat{y}_i\]

Problem = Computational speed: everytime you want to evaluate the probability, need to sum over all 10,000 words in vocabulary.  


Solutions to speed up softmax classification:

* Hierarchical softmax classifier: 1 classifier which says whether word is within 5,000 words, and then split, until you get to where it is.  scales to log vocab size, rather than linear.  Doesn't generally use equal words in each branch , typically most common words are up the top so that need only a few traversals for those.  Many algorithms exist to construct the tree.


How to sample the context $c$?  

* Sample randomly from training corpus, but there are some words such as the, of, a, and, to, $\ldots$ that occur frequently.  Then embedding will be updated many for those.
* Heuristics exist to balance out sampling from common and less common words


Two versions of Word2Vec algorithm within original paper: Skip-Gram and CBow



## Negative Sampling
* Modified learning problem that allows you to do something similar to skip-gram with much more efficient learning algorithm (cf Softmax.
*  Example sentence: _I want a glass of orange juice to go along with my cereal_
* Sample context word, look around a window of $\pm$10 words
* Choose random words from dictionary to get negative targets to get, e.g..

Source| Context (c) | Word (t) | Target (y) 
-------| -------| ------| --------
$\pm 10$ word | orange | juice | 1
dictionary | orange | king | 0
dictionary | orange | book | 0
dictionary | orange | the | 0
dictionary | orange | of | 0
$\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ 
$k$ |$k$ |$k$ |$k$ 


Choose $k$ = 2-5 for larger data set and $k$ = 5-20 for smaller datasets.  In above example $k$ = 4 (i.e. number of words taken from dictionary)

Recall, softmax: \[p(t|c) = \frac{e^{\theta_t^Te_c}}{\sum^{10,000}_{j=1}e^{\theta_j^T e_c} }\]

Model as logistic model:
\[P(y = 1|c,t) = \sigma (\theta_t^T, e_c)\]

Training data:

* $k$:1 negative:positive examples
* If input word is orange (position 6257 in vocab), input one-hot vector pass through E and do multiplication to get embedding vector, i.e.
  \[o_{6257} \rightarrow E \rightarrow e_{6257}\]
  So have 10,000 possible logistic regression classification regression classifiers but only train $k + 1$  So instead of having one giant 10,000 way Softmax which is very expensive to compute, have turned it into 10,000 binary classification problems.  On every iteration we're only going to train $k + 1$ of them.  
* Called negative sampling, because you have a positive example and generate negative examples for which to train $k$ additional binary classifiers.  
  
Sampling the negative words:
  
* Sample according to the empirical frequency of words in your corpus, $p(w_i)$ but again get high representation of words like the, of, and, etc.
* Other extreme is to use $p(w_i) =  1/vocab size$, sample the negative examples uniformly at random
* Middle ground: Take heuristic value of sampling between empirical and uniform, $p(w_i) = \frac{f(w_i)^{3/4}}{\sum^{10000}_{j=1}f(w_j)^{3/4}}$.  Not theoretically justified but seems to work well

There are pre-trained word vectors and open source code.



## GloVe word vectors

* Previously sampled pairs of words that appeared in close proximity to each other within the corpus
* $c,t$: context, target
* $x_{ij}$: number of times $i$ (the target, $t$) appears in context of $j$ (the context, $c$), i.e. how often two words appear close to each other
* If within $\pm 10$, then $X_{ij} = X_{ji}$

Model:

* minimise 
\[\sum^{10,000}_{i=1}\sum^{10,000}_{j=1} f(X_{ij}) (\theta^T_i e_j + b_i + b_j^\prime -\log X_{ij})^2\]

* Solve $\theta$ and $e$ using gradient descent
* Don't want to sum over terms where $X_{ij} = 0$, so use weighting term  $f(X_{ij}) = 0$ if $x_ij = 0$, assume $0 \log 0 = 0$.  It is also used to weight less common and more frequent words appropriately.  
* Note that $\theta_i$ and $e_j$ are symmetric, so can 
* Can initialise $\theta$ and $e$ 
* For given word $w$ let $e_w^\text{final} = \frac{e_w + \theta_w}{2}$


### Properties

* Started with featurization view of word embeddings, but cannot guarantee that individual components of embedding are interpretable
* Note that $(\theta^T_i e_j$ can be replaced by $(A \theta_i)^T(A^T e_j)$ where A is invertible. i.e. first component might be combination of gender, royal, age, etc. due to this linear transformation

## Sentiment Classification
## Debiasing word embeddings


# Sequence Models & Attention Mechanism {#week3}
## Basic Models
## Picking the most likely sentence
## Beam Search
## Refinements to Beam Search
## Error analysis in beam search
## Bleu Score (optional)
## Attention Model Intuition
## Attention Model
## Speech recognition
## Trigger Word Detection
## Conclusion and thank you
