---
title: 'Book: Cluster Analysis'
author: Marie
date: '2018-07-20'
slug: book-cluster-analysis
banner: "img/banners/everitt.png"
categories:
  - Study-Notes
tags:
- R
- Study-Notes
- Book
- Clustering
---



<pre class="r"><code>require(cluster.datasets)</code></pre>
<pre><code>## Loading required package: cluster.datasets</code></pre>
<div id="measurement-of-proximity" class="section level1">
<h1>Measurement of proximity</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Dissimilarity = distance = simalirty = proximity. Small dissimilarity/distance. Large similarity</p>
<p>Proximity:</p>
<ul>
<li>Direct</li>
<li>Indirect</li>
</ul>
</div>
<div id="similarity-measures-for-cateogrical-data" class="section level2">
<h2>Similarity measures for cateogrical data</h2>
<ul>
<li>Similarity measures most commonly used</li>
<li>Generally scaled in [0,1]</li>
<li>Two individuals have similarity coefficient equal to one if they have identical values for all variables.</li>
</ul>
<div id="similarity-measures-for-binary-data" class="section level3">
<h3>Similarity measures for binary data</h3>
<ul>
<li>Difficulty arises with zero; attributing a large degree of similarity to a pair of individuals because they lack a number of attributes may not be sensible</li>
<li>Similarity measures include
<ul>
<li>Matching coefficient: Use when co-abscence important</li>
<li>Jaccard coefficient</li>
<li>Rogers and Tanimoto: Also treat positive matches and negative matches equally.</li>
<li>Sneath and Sokal</li>
<li>Gower and Leendre</li>
</ul></li>
</ul>
</div>
<div id="similarity-measures-for-categorical-data-with-more-than-two-levels" class="section level3">
<h3>Similarity measures for categorical data with more than two levels</h3>
<ul>
<li>Can consider each level as a single binary vairable, however there will be a large number of negative matches.</li>
<li>Better to allocate a score of zero or one to each variable, depending on whether the two individuals are the same on that variable. These scores are then averaged over all p variables to give similarity coefficient <span class="math display">\[S_{ij} \frac{1}{p}\sum^p_{k = 1} s_{ijk}\]</span></li>
<li>Transformed genetic dissimilarity measure discussed</li>
<li>Linguists have assigned 1 to pairs if the variable value is in the same group (e.g.Â of words with a similar meaning)</li>
</ul>
</div>
</div>
<div id="dissimilarity-and-distance-measures-for-continuous-data" class="section level2">
<h2>Dissimilarity and distance measures for continuous data</h2>
<p>Distance measure types;</p>
<ul>
<li>Distance measures
<ul>
<li>Euclidean distance (l2 norm)<span class="math display">\[d_{ij} = \left[\sum^p_{k = 1}w^2_k(x_{ik} - x_{jk})^2\right]\]</span></li>
<li>City block distance (taxicab, rectilinear, Manhattan) (l1 norm) <span class="math display">\[d_{ij} = \left[\sum^p_{k = 1}w^2_k|x_{ik} - x_{jk}|\right]\]</span></li>
<li>Minkowski distance (<span class="math inline">\(l_r\)</span> norm)</li>
<li>Canberra distance; very sensitive to small changes near <span class="math inline">\(x_{ik} = x_{jk} = 0\)</span></li>
<li>Pearson correlation</li>
<li>Angular separation (cross product index())</li>
</ul></li>
<li>Correlation-type measures</li>
</ul>
</div>
<div id="similarity-measures-for-data-containing-both-continuous-and-categorical-variables" class="section level2">
<h2>Similarity measures for data containing both continuous and categorical variables</h2>
</div>
<div id="proximity-measures-fro-structured-data" class="section level2">
<h2>Proximity measures fro structured data</h2>
</div>
<div id="inter-group-proximity-measures" class="section level2">
<h2>Inter-group proximity measures</h2>
<div id="inter-group-proximity-derived-from-teh-proximity-matrix" class="section level3">
<h3>Inter-group proximity derived from teh proximity matrix</h3>
</div>
<div id="inter-group-proximity-based-on-group-summaries-for-contiuous-data" class="section level3">
<h3>Inter-group proximity based on group summaries for contiuous data</h3>
</div>
<div id="inter-group-proximity-based-on-gorup-summaries-for-categorical-data" class="section level3">
<h3>Inter-group proximity based on gorup summaries for categorical data</h3>
</div>
</div>
<div id="weighting-variables" class="section level2">
<h2>Weighting variables</h2>
</div>
<div id="standardization" class="section level2">
<h2>Standardization</h2>
</div>
<div id="choise-of-proximity-measure" class="section level2">
<h2>Choise of proximity measure</h2>
</div>
</div>
<div id="miscellaneous-clustering-methods" class="section level1">
<h1>Miscellaneous clustering methods</h1>
<div id="density-search-clustering-techniques" class="section level2">
<h2>Density search clustering techniques</h2>
</div>
<div id="density-based-spatial-clustering-of-applications-with-noise" class="section level2">
<h2>Density-based spatial clustering of applications with noise</h2>
</div>
<div id="techniques-which-allow-overlapping-clusters" class="section level2">
<h2>Techniques which allow overlapping clusters</h2>
</div>
<div id="simultaneous-clustering-of-objects-and-variables" class="section level2">
<h2>Simultaneous clustering of objects and variables</h2>
</div>
<div id="clustering-with-constraints" class="section level2">
<h2>Clustering with constraints</h2>
</div>
<div id="fuzzy-clustering" class="section level2">
<h2>Fuzzy clustering</h2>
</div>
<div id="clustering-and-artificial-neural-networks" class="section level2">
<h2>Clustering and artificial neural networks</h2>
<div id="components-of-a-neural-network" class="section level3">
<h3>Components of a neural network</h3>
<p>Features of a neural network:</p>
<ol style="list-style-type: decimal">
<li>Neurons</li>
<li>Connections between units</li>
<li>Training algorithms</li>
</ol>
<ul>
<li>From a set of predictors (input) <span class="math inline">\(x_1, x_2, \ldots, x_p\)</span> and weights <span class="math inline">\(w_1, w_2, \ldots, w_p\)</span>, the neuron provides a response (output) <span class="math inline">\(y\)</span> <span class="math display">\[ y = \text{sign} \left(w_0 + \sum^p_{i = 1} w_i x_i     \right) \]</span></li>
<li>The neuron will only fire if the summation is positive (sign = 1)</li>
</ul>
<p>In the following chart, X is the input layer, Z the hidden layer and Y the output layer. <img src="/graphs/nn.png" alt="Neural Network" /></p>
<p>The model is typically trainined by minimising the RSS (residual sum of squares).</p>
</div>
<div id="the-kohonen-self-organising-map" class="section level3">
<h3>The Kohonen self-organising map</h3>
<ul>
<li>NN usually applied for supervised problems</li>
<li>Kohonen self-organising map is unsupervised</li>
<li><p>Network contains two layers</p></li>
<li>Input layer consisting of <span class="math inline">\(p\)</span>-dimensional observations <span class="math inline">\(\mathbf{x}\)</span>;</li>
<li><p>Output layer (grid) consisting of <span class="math inline">\(k\)</span> nodes for the <span class="math inline">\(k\)</span> clusters, each of which is associated with a <span class="math inline">\(p\)</span>-dimensional weight <span class="math inline">\(\mathbf{w}\)</span>.</p></li>
</ul>
<div class="figure">
<img src="/img/everitt_som.png" alt="Figure 8.17, Everitt" />
<p class="caption">Figure 8.17, Everitt</p>
</div>
<p>Algorithm:</p>
<ul>
<li>Unlike in <span class="math inline">\(k\)</span>-means, the <span class="math inline">\(p\)</span>-dimensional weight vectors associated with the <span class="math inline">\(k\)</span> output nodes are initially assigned a random value between (0, 1)</li>
<li>Each of the <span class="math inline">\(p\)</span>-dimensional observations (<span class="math inline">\(\mathbf{x}\)</span>) are also scaled in (0, 1)</li>
<li>The Euclidean distance (or other distance) is calculated between the observation and each of the <span class="math inline">\(k\)</span> <span class="math inline">\(p\)</span>-dimensional weight vectors (neurons).<br />
</li>
<li>The neuron with the smallest distance (the winner) is updated, as are a small neighbourhood of neurons around the winner; its weight vector <span class="math inline">\(\mathbf{w}_\text{old}\)</span> is brought closer to the input patterns <span class="math inline">\(\mathbf{x}\)</span> as follwos: <span class="math display">\[\mathbf{w}_\text{new} = \mathbf{w}_\text{old} + \alpha(\mathbf{x} - \mathbf{w}_\text{old})\]</span> The value of <span class="math inline">\(\alpha\)</span> is a small fraction, which decreases as learning takes place, as does the size of the neighbourhood. The excited neurons in the neighbourhood ofthe winner are udpated in a similar manner but with a smaller <span class="math inline">\(\alpha\)</span>.</li>
<li>As the network learns, the weights are modified and the input observations are provisionally assigned to clusters.</li>
</ul>
</div>
<div id="application-of-neural-nets-to-brainstorming-sessions" class="section level3">
<h3>Application of neural nets to brainstorming sessions</h3>
</div>
</div>
</div>
