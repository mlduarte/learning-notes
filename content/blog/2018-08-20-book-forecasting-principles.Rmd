---
title: 'Book: Forecasting Principles and Practice'
author: "Marie"
date: '2018-08-20'
categories: Study-Notes
slug: book-forecasting-principles
tags:
- Book
- Study-Notes
banner: img/banners/hyndman.png
---

This post consists of my notes made while reading the book: [Forecasting: Principles and Practice](https://otexts.org/fpp2/) by Rob J Hyndman and George Athanasopoulos.

# Getting Started

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(tidyverse)
require(fpp2)
require(GGally)
require(gridExtra)
require(knitr)
```


## What can be forecast?

Predictability increases with:

* Understanding of contributing factors
* Data availability
* Immunity of things we are trying to forecast on the forecasts (self-fulfilling forecasts)

Good forecasts:

* capture genuine patterns but do not replicate past events that will not occur again.
* will capture the way in which things are changing and so will work in a changing environment (assuming that the way in which the environment is changing will continue into the future)

Forecasting methods complexity:

* Simple, e.g. naive method which uses most recent observation as a forecast
* Complex, e.g. neural nets and econometric systems of simultaneous equations

## Forecasting, planning and goals

* Forecast: Predict future based on historical data and knowledge of future events that might impact the forecasts
* Goals: What you would like to happen, should be linked to forecasts and plans
* Planning: Response to forecast and goals; actions required to make forecasts match goals


Horizon:

* Short-term forecasts: personnel, production and transportation
* Medium-term forecasts: future resource requirements
* Long-term forecasts: Strategic planning

## Determining what to forecast

Step One: 

* What to forecast
* Forecasting horizon
* Frequency required
* How will they be used

Step Two: 

* Find or collect data

## Forecasting data and methods

Method Types:

* Qualitative
    * not purely guesswork
    * There exists well-developed structured approaches
    * Use when no relevant data available
    
* Quantitative
    * When historical data available and reasonable to assume that some aspects of past pattern will continue into the future
    
### Time series forecasting

**Time Series**: anything observed sequentially over time; can be observed at regular or irregular periods of time (this book only considers regularly spaced time series)

* May consider only variable to be forecast (and make no attempt to discover factors that affect its behaviour) and extrapolate trend and seasonal patterns

* Models include decomposition, exponential smoothing and ARIMA

### Predictor variables and time series forecasting

**Explanatory model**: model with predictor variables (but does not consider time)
\[\text{ED} = f(\text{current temperature}, \text{strength of economy}, \text{population}, \text{time of day}, \text{day of week}, \text{error})\]

**Time series model**: model based on time series,
\[\text{ED}_{t+1} = f(\text{ED}_{t}, \text{ED}_{t-1}, \text{ED}_{t-3}, \ldots, \text{error})\]

Reasons for use over other two types of models:

* System may not be understood 
* Necessary to forecast future values of predictors to forecast variable of interest
* May only need to predict what will happen, not why
* May be more accurate than explanatory or mixed models

**Mixed Models** aka **dynamic regression model**, **panel data model**, **longitudinal model**, **transfer function model**, **linear system model**

* Incorporate explanatory variables and time series data
\[\text{ED} = f(\text{ED}_{t}, \text{current temperature}, \text{time of day}, \text{day of week}, \text{error})\]

## The basic steps in a forecasting task

### 1. Problem definition
### 2. Gathering information

* Statistical data
* Accumulated expertise of people who collect the data and use the forecasts

### 3. Preliminary (exploratory) analysis

* Are there consistent patterns
* Is there significant trend?
* Is seasonality important
* Are there business cycles present?
* Are there any outliers?
* How strong are the relationships among the variables available for the analysis

### 4. Choosing and fitting models

Best model = f(availability of historical data, relationship between forecast variable and explanatory variables, intended use of forecasts)

### 5. Using and evaluating a forecasting model
When using a forecasting model, may have to deal with  missing values, outliers or short time series


## The statistical forecasting perspective

* Forecast variable = random variable
* Uncertainty of its value increases as forecast horizon increases
* Generally publish point forecasts with prediction interval (range of values the RV could take with X% probability)

Notation:

* $t$: time
* $y_t$: observation at time $t$
* $I$: Observed information
* $\hat{y}_t$: average value of the forecast distribution, could occasionally refer to the median
* $\hat{y}_{t|t-1}$: forecast of $y_t$ taking into account all previous observations($y_1, \ldots, y_{y-1}$)
* $\hat{y}_{T+h|T}$: forecast of $y_{T+h}$ taking into account $y_1, \ldots, y_{T}$ ($h$-step forecast)

# TIme series graphics

## `ts` objects

* If you have annual data, with one observation per year, you only need to provide the starting or ending year

```{r ts_annual}
y <- ts(c(123,39,78,52,110), start=2012)
y
```
If you have observations that are more frequent than once a year, you add a `frequency` argument.  For example, with monthly data set `frequency = 12`.

### Frequency of a time series

When using the `ts()` function, the following choices should be used:

Data        | `frequency`
------------| -----------
Annual      | 1
Quarterly   | 4
Monthly     | 12
Weekly      | 52

For data collected more frequently, there are more options:

Data | Seasonality     | `frequency`
-----|-----------------| -----------
Daily | Weekly      | 7
Daily | Annual      | 365.25
Minutes | Hourly    | 60
Minutes | Daily     | 1440
Minutes | Weekly    | 10080
Minutes | Annual    | 525960

(Chapter 11 will consider multiple seasonality without having to choose just one of the frequencies)


## Time plots

`autoplot`: produced appropriate plot based on data, e.g. if data is a time series it will produce a time plot


Weekly economy passenger load: 
```{r autoplot_melsyd}
autoplot(melsyd[,"Economy.Class"]) +
  ggtitle("Economy class passengers: Melbourne-Sydney") +
  xlab("Year") +
  ylab("Thousands")
```

Features that will need to be considered:

* Period in 1989 with no passengers (industrial dispute)
* Reduced load in period in 1989 (trial replacing economy seats with business)
* Large increase in second half of 1991
* Dips at start of each year
* Long-term fluctuation (increase - decrease - increase)
* Missing observations


Monthly anti-diabetic drug sale data:  
```{r autoplot_a10}
autoplot(a10) +
  ggtitle("Antidiabetic drug sales") +
  ylab("$ million") +
  xlab("Year")
```

Features:

* Seasonality
* Trend is changing
* Sudden drop at start of each year (more cost effective for patients to stockpile at the end of the calendar year)
* Seasonal pattern increases in size as level of series increases


The time series can be aggregated using the `aggregate` function.
```{r autoplot_a10_annual}
# Introductory time series
a10.annual <- aggregate(a10, FUN = sum)
autoplot(a10.annual)  +
  ggtitle("Annual Antidiabetic drug sales") +
    geom_point() + 
  ylab("$ million") +
  xlab("Year") 
```


## Time series patterns

* **Trend**; refers to a long-term increase or decrease in the data, which can change over time (i.e. trend may be _changing direction_)
* **Seasonal**; a seasonal pattern occurs when a time series is affected by seasonal factors, i.e. time of year.  Seasonality is of a fixed and known frequency.  If frequency of fluctuations is unchanging and associated with some aspect of the calendar then the pattern is seasonal.
* **Cyclic**; when data rises and falls but not of a  fixed frequency and may be related to an economic or business cycle.  If fluctuations are not of a fixed frequency then they are cyclic.  

## Seasonal plots

Similar to a `ts` plot but data is plotted against seasons in which data observed.

* Highlights seasonal patterns

```{r a10_seasonplot}

ggseasonplot(a10, year.labels=TRUE, year.labels.left=TRUE) +
  ylab("$ million") +
  ggtitle("Seasonal plot: antidiabetic drug sales")

```

Polar seasonal plot makes the time series axis circular rather than horizontal

```{r a10_polarseasonalplot}
ggseasonplot(a10, polar=TRUE) +
  ylab("$ million") +
  ggtitle("Polar seasonal plot: antidiabetic drug sales")
```


## Seasonal subseries plots
Instead can show each season as a separate mini-time plot

* Horizontal lines used to indicate the mean 
* May enable underlying seasonal pattern to be seen more clearly

```{r a10_subseriesplot}
ggsubseriesplot(a10) +
  ylab("$ million") +
  ggtitle("Seasonal subseries plot: antidiabetic drug sales")
```


## Scatterplots

Useful for showing relationships between time series

```{r elecdemand_auto}
autoplot(elecdemand[,c("Demand","Temperature")], facets=TRUE) +
  xlab("Year: 2014") + ylab("") +
  ggtitle("Half-hourly electricity demand: Victoria, Australia")
```

The relationship between temperature and electrical demand is as follows:
```{r elecdemand_qplot}
qplot(Temperature, Demand, data=as.data.frame(elecdemand)) +
  ylab("Demand (GW)") + xlab("Temperature (Celsius)")
```

### Correlation


The correlation coefficient, \[r = \frac{\sum(x_t - \bar{x})(y_t - \bar{y})}{\sqrt{\sum(x_t - \bar{x})^2}\sqrt{\sum(y_t - \bar{y})^2}},\] which lies between $\pm$ 1 measures the strength of __linear__ relationship, and so can sometimes be misleading.  See for example Anscombe's Quartet in which each of the four date sets have a correlation coefficient of 0.82.  Anscombe's Quartet is a good illustration of why not to solely rely on correlation values.
```{r anscombes_quartet}

dat <- select(anscombe, x1:x4) %>% gather(variable, X) %>% mutate(variable = str_replace(variable, "x", "")) %>%
    cbind(
select(anscombe, y1:y4) %>% gather(variable, Y) %>% select(-variable))


    ggplot(dat) + 
    geom_point(aes(X, Y)) + 
    facet_wrap(~ variable)


```


### Scatterplot matrices

```{r visnights_ggpairs, message=FALSE, warning=FALSE}
GGally::ggpairs(as.data.frame(visnights[,1:5]))
```


## Lag plots

Scatterplot matrix with:

* X-axis = Lagged values of the time series ($y_{t - k}$)
* Y-axis = Current value of the time series ($y_{t}$)
* One chart per value of $k$
* Lines connect points in chronological order


Quarterly beer data: 
```{r ausbeer_lagplot}
beer2 <- window(ausbeer, start=1992)
gglagplot(beer2)
```
Here:

* Strong positive relationship at lags 4 and 8 (strong quarterly seasonality)
* Negative relationship for lags 2 and 6 because peaks (Q4) plotted against troughs (Q2)

Note use of `window()` function to extract a portion of the time series.


## Autocorrelation

Similar to correlation which measures linear relationship between two variables, autocorrelation measures linear relationship between __lagged values__ of a time series.  For lag $k$, the autocorrelation coefficient is 
\[r_k = \frac{\sum_{t = k + 1}^T(y_t - \bar{y})(y_{t-k} - \bar{y})}{\sum_{t = 1}^T(y_t - \bar{y})^2},\] where $T$ is the length of the time series.

Noting that: 

* $r_1$ measures the relationship between $y_t$ and $y_{t-1}$
* $r_2$ measures the relationship between $y_t$ and $y_{t-2}$
* and so on ... 

A correllogram plot is used to show the autocorrelation function (ACF) by plotting the autocorrelation coefficients.

```{r beer2_acf}
ggAcf(beer2)
```
Features:

* $r_4$ is higher than for other lags; peaks four quarters apart, troughs 2 quarters apart
* $r_2$ more negative than for other lags because troughs tend to be 2 quarters behind beaks
* Blue lines indicate whether correlations are significantly different from zero.

### Trend and seasonality in ACF plots

When data has a trend: Autocorrelations for small lags tend to be large and positive because observations nearby in time are nearby in size.  So ACF of trended time series tend to have positive values that slowly decrease as the lags increase.

When data are seasonal:  Autocorrelations are larger for the seasonal lags (at multiples of the seasonal frequency) than for other lags.  

When data are trended and seasonal: Combination of effects is seen, e.g. monthly Australian electricity demand:

```{r elec_auto}
aelec <- window(elec, start=1980)
autoplot(aelec) + xlab("Year") + ylab("GWh")
```
```{r elect_acf}
ggAcf(aelec, lag=48)
```

Features: 

* Trend - The slow decrease in the ACF as the lags increase
* Seasonality - "Scalloped" shape


## White noise

**White noise**:  Time series that show no autocorrelation.
```{r wn_auto}
set.seed(30)
y <- ts(rnorm(50))
autoplot(y) + ggtitle("White noise")
```
```{r wn_acf}
ggAcf(y)
```
White noise ACF features:

* each autocorrelation ~ 0
* Expect 95% of spikes in ACF to lie within $\pm 2 / \sqrt{T}$.  If $\geq$ one large spikes outside bounds, or if substantially more than 5% of spikes are outside of these bounds then series is probably not white noise.

In this chart, $T$ = `r length(y)` and so the bounds are $\pm 2 / \sqrt{50}$ = $\pm$ `r round(2/sqrt(length(y)),2)`.  As all correlation coefficients lie within these limits, ACF confirms that data are white noise.

# The forecaster's toolbox

## Some simple forecasting methods

### Average method
Set all forecasts to average of historical data,

\[\hat{y}_{T+h|T} = \bar{y} = (y_1 + \ldots + y_T)/T,\]
where $\hat{y}_{T+h|T}$ is short-hand for the estimate of \hat{y}_{T+h} based on $y_1, \ldots, y_t$.

Use function `meanf(y, h)` for time series $y$ and forecast horizon $h$.

### Naive method (a.k.a. random walk forecast)

Set all forecasts to last observed value, 
\[\hat{y}_{T+h|T} = y_T\]

Use function `naive(y,h)` or `rwf(y,h)`.

This method is optimal when data follow a random walk.

### Seasonal naive method

Set forecast to last observed value from the same season of the year.
\[\hat{y}_{T+h|T} = y_{T+h-m(k+1)},\]

where $m$ = seasonal period, $k$ = number of complete periods in the forecast period prior to time $T + h$.

Use function `snaive(y, h)`.

### Drift method

Set forecast to last observation plus drift (average increase or decrease that has been seen over time).  Equivalent to drawing a line between first and last observations and extrapolating it into the future, \[\hat{y}_{T+h|T} = y_{T} + \frac{h}{T-1}\sum^T_{t=2}(t_t - t_{t-1}) = y_T + h\left(\frac{y_T - y_1}{T - 1}\right)\]

Use function `rwf(y, h, drift = TRUE)`.

### Examples

```{r beer_simple_forecasts, message=FALSE, warning=FALSE}
# Set training data from 1992 to 2007
beer2 <- window(ausbeer,start=1992,end=c(2007,4))

# Plot some forecasts
autoplot(beer2) +
    geom_point() + 
    scale_x_continuous(minor_breaks = 1992:2007) + 
    theme(panel.grid.minor = element_line()) + 
  autolayer(meanf(beer2, h=11),
    series="Mean", PI=FALSE) +
  autolayer(naive(beer2, h=11),
    series="Naïve", PI=FALSE) +
  autolayer(snaive(beer2, h=11),
    series="Seasonal naïve", PI=FALSE) +
  ggtitle("Forecasts for quarterly beer production") +
  xlab("Year") + ylab("Megalitres") +
  guides(colour=guide_legend(title="Forecast"))
```

```{r ggog200_simple_forecasts}
autoplot(goog200) +
  autolayer(meanf(goog200, h=40),
    series="Mean", PI=FALSE) +
  autolayer(rwf(goog200, h=40),
    series="Naïve", PI=FALSE) +
  autolayer(rwf(goog200, drift=TRUE, h=40),
    series="Drift", PI=FALSE) +
  ggtitle("Google stock (daily ending 6 Dec 2013)") +
  xlab("Day") + ylab("Closing Price (US$)") +
  guides(colour=guide_legend(title="Forecast"))
```
Simple methods may:

* Be best forecasting method available
* Serve as a benchmark for other methods

## Transformations and adjustments

Purpose: Simplify historical data patterns by removing known sources of variation or by making pattern more consistent across whole data set.  Simpler pattern = more accurate forecast.

### Calendar adjustments

EG: Different numbers of days in months can lead to variation in monthly values.  In this case divide TS by `monthdays()` to get TS of daily average.

```{r cal_adj}
dframe <- cbind(Monthly = milk,
                DailyAverage = milk/monthdays(milk))
  autoplot(dframe, facet=TRUE) +
    xlab("Years") + ylab("Pounds") +
    ggtitle("Milk production per cow")
```


### Population adjustments

Data affected by population changes can be adjusted to give per-capita data, i.e. consider data per person (or per thousand, or per million people) rather than the total.  E.g. user beds per thousand people rather than number of hospital beds in a particular region over time.

### Inflation adjustments
Use price index (e.g. CPI) to adjust data affected by the value of money before model to all be in dollar values from a particular year.  

E.g. adjusted price at year 2000 dollar values is $x_t = y_t / z_t * z_{2000}$ where 

* $z_t$ is price index,
* $y_t$ original house price in year $t$

### Mathematical transformation
Transform data (e.g. with logarithmic transformation) when data variation increases or decreases with level of series.  

#### Log transformation
Interpretation of log base 10: An increase of 1 on the log scale corresponds to a multiplication of 10 on the original scale. Another advantage is that they constrain forecasts to positive.

Redacted extracts from [Gerard E. Dallal](http://www.jerrydallal.com/lhsp/logs.htm):
>if you multiply something by 10 in the original scale, you add 1 unit to its value the log scale. If you divide something by 10 in the original scale, you subtract 1 unit from its value in the log scale. As we move from 0.1 to 1 to 10 on the original scale, we move from -1 to 0 to 1 on the logarithmic scale.

>Just as length can be measured in feet, inches, meters, kilometers, centimeters, or whatever, logarithms can be defined in may ways according to what happens in the original scale when there is a one unit change in the log scale. Common logs are defined so that a 1 unit increase in the log scale is equivalent to multiplying by 10 in the original scale. One could define logarithms so that a one unit increase in the log scale is equivalent to multiplying by 2 in the original scale (logs to base 2). The value by which a number is multiplied in the original scale when its logarithm is increased by 1 is known as the base of the logarithm. Any positive number different from 1 can be used as a base.

>Natural logarithms: A 1 unit increase in this log scale is equivalent to multiplying in the original scale by a factor known as Euler's constant, $e$ (~ 2.71828). ... If you apply any logarithmic transformation to a set of data, the average of the logs is approximately equal to the log of the original mean, whatever type of logarithms you use. However, only for natural logs is standard deviation approximately equal to the coefficient of variation (the ratio of the standard deviation to the mean) in the original scale.

>The different types of logs are like different units for measuring height. For example, a height of 71 inches might be referring to an adult male, but a height of 71 cm would probably be referring to a toddler. Similarly, you can't report a logarithm of 2. Is it the common log corresponding to a value of 100 in the original scale or a natural log corresponding to a value of 7.39?

Extract from [Notes on logarithms - Tony Gardner-Medwin](http://www.ucl.ac.uk/lapt/med/logs.htm) regarding logs to base e: 
> The way to think about natural logs is that they are the same as logs to any other base, but measured in different units.  It is always true that \[ln(x)   =  2.303  log_{10}(x)\]

>Logarithms to base e are considered "natural" because:  As $x$ becomes very small,  $log_e(1+x)$  becomes equal to $x$.  The equivalent statement for  $log_{10}(x)$ would be As $x$ becomes very small,  $log_{10}(1+x)$  becomes equal to 2.303 $x$. 

#### Power tranformations
Of form $w_t = y_t^p$, includes square roots and cube roots.  Not as interpretable as log transformation.

#### Box-Cox transformation
Includes logarithms and power transformation.\[w_t = \begin{cases} log(y_t) & \text{if } \lambda = 0;\\
(y_t^\lambda - 1) / \lambda & \text{otherwise} \end{cases}\]

* If $\lambda = 0$, natural (i.e. not base 10!) logarithm is used
* If $\lambda \neq 0$, power transform used
* If $\lambda = 1$, then $w_t = y_t - 1$, so data is shifted downwards, but no change in shape
* If $\lambda \neq 1$, then shape will change


As per online book, a slider for $\lambda$ is a great way of determining which value makes the size of seasonal variation constant across the series.  Alternative, you can use the `BoxCox.lambda()` function.

```{r elec_boxcox}

p1 <- autoplot(elec)
p2 <- autoplot(BoxCox(elec, BoxCox.lambda(elec)))
grid.arrange(p1, p2)
rm(p1, p2)
```

To back-transform the forecasts on the original scale 
\[y_t = \begin{cases} \exp(w_t) & \text{if } \lambda = 0 \\
            (\lambda w_t + 1)^{1/\lambda} &\text{otherwise}\end{cases}\]



#### Features of power transformations

* If some observations have a value $\leq 0$, will need to adjust all observations by adding a constant to all values; otherwise no power transformation is possible
*  A simple value of $\lambda$ makes explanations easier
* Transformation sometimes make little difference to the forecasts but have a large effect on the prediction intervals
* Often no transformation is needed


#### Bias adjustments

* When using mathematical transformation such as the Box-Cox transformation, often the back-transformed forecast will not be the mean, but rather the median, of the forecast distribution.  When want to add up sales forecasts to get a total, the means will add up but medians will not.  Back-transformed mean of Box-Cox transformed data is:

\[y_t = \begin{cases} \exp(w_t) \left[1 + \frac{\sigma^2_h}{2}\right]& \text{if } \lambda = 0 \\
            (\lambda w_t + 1)^{1/\lambda} \left[1 + \frac{\sigma^2_h(1-\lambda)}{2(\lambda w_t + 1)^2}\right] &\text{otherwise}\end{cases}\]

where $\sigma_h^2$ is the $h$-step variance.  The larger the forecast variance, the bigger the bias (difference between the mean and the median).  When using the mean, rather then median, the point forecast is said to have been **bias-adjsted**.  

Bias-adjustment is not the default in the `forecast` package; to implement use the argument `biasadj = TRUE`.




```{r egg_bias_adj_fo}
# Drift forecast with log transformation
fc <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80)
# ... now with bias adjustd forecasts
fc2 <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80,
  biasadj=TRUE)
autoplot(eggs) +
  autolayer(fc, series="Simple back transformation") +
  autolayer(fc2, series="Bias adjusted", PI=FALSE) +
  guides(colour=guide_legend(title="Forecast"))
rm(fc, fc2)

```


## Residual diagnostics

### Fitted values

Fitted value $\hat{y}_{t|t-1}$

* Observation in TS forecast (e.g. at time $t$) using previous observations $y_1, \ldots, y_{t-1}$
* Always involve one-step forecasts
* Not true forecast because parameters in forecast method are estimated using all observations in TS, including future obs

Fitted values for simple methods:

* Average method: $\hat{y} = \hat{c}$ where $\hat{c}$ = average over all observations including those after $t$.
* Drift method:  $\hat{y} = y_{t-1} + \hat{c}$ where $\hat{c} = (y_T - y_1)/(T-1)$
* Naive: Fitted value = true forecast as no parameters used
* Naive seasonal: Fitted value = true forecast as no parameters used

### Residuals

\[e_t = y_t - \hat{y}_t\]

Good model will have:

* Uncorrelated residuals; if there are correlations, then there is information left in the residuals which should be used in computing forecasts
* Zero mean residuals; if residuals have a mean other than zero, then forecasts are biased

Without these properties, method can be improved, noting though, that methods with these properties may still be __improve-able__, i.e. these properties are useful in determining whether all available information is being used but not for selecting a forecasting method.

To fix bias in residuals with constant mean $m$, then simply add $m$ to all forecasts.  Fixing correlation, will be addressed later...

It is also useful (but not essential) for calculation of prediction intervals if:

* Residuals have constant variance
* Residuals are normally distributed

May be possible to apply Box-Cos transformation, but improvement is not always possible; sometimes little that can be done except to use an alternative approach for obtaining the prediction intervals (to be discussed later).

### Example



TS of residuals; large residual result of unexpected price jump. 
```{r goog200_res_TS}
res <- residuals(naive(goog200))
autoplot(res) + xlab("Day") + ylab("") +
  ggtitle("Residuals from naïve method")
```

Histogram of residuals; right tail a little long for normal distribution
```{r goog200_res_Hist, message=FALSE, warning=FALSE}
gghistogram(res) +
  ggtitle("Histogram of Residuals")
```

ACF of residuals; lack of correlation suggests forecasts are good
```{r goog200_res_acf}
ggAcf(res) + ggtitle("ACF of residuals")
```

Summary: forecasts appear to account for all information.  Forecasts will probably be quite good but prediction intervals assuming a normal distribution may be inaccurate. 

* Mean of residuals close to zero 
* No signification correlation in residuals series
* Variation in residuals appears constant, with exception of one outlier (from time series)
* Residuals may not be normal (tail is long even when outlier is ignored)

### Portmanteau tests for autocorrelation

ACF plot involves checking that each spike is within required limits and therefore implicitly carrying out multiple hypothesis tests, each one with a small probability of giving a false positive.  With enough tests, likely that at least one will give a false positive, leading to the incorrect conclusion that residuals have some remaining autocorrelation

Portmanteau tests performs test for a group of autocorrelations by determining whether the first $h$ autocorrelations are significantly different from expected of a white noise process.

In the following tests, if the autocorrelations come from a white noise series, then the statistic would have a $\chi^2$ distribution with ($h - K$) degrees of freedom, where $K$ is the number of parameters in the model.  If they are calculated from raw data (rather than the residual from a model), then set $K = 0$.

#### Box-Pierce Test

Statistic \[Q = T \sigma_{k=1}^hr_k^2,\]

where 

* $h$ is maximum lag (suggest $h = 10$ for non-seasonal data and $h = 2m$ for seasonal data, where $m$ is the period of seasonality.  But if these values are larger than $T/5$ then use $h = T/5$ as the test is not good for large $h$.)
* $T$ is number of observations

If each $r_k$ is close to zero, $Q$ will be small.


#### Ljung-Box Test

More accurate than Box-Pierce test,
\[Q^* = T(T + 2) \sigma_{k=1}^h (T - k)^{-1}r_k^2,\]

where large $Q^*$ suggests that autocorrelations did not come from a white noise process. 

#### Example

In following example, both $Q$ and $Q^*$ are not significant and so conclude that residuals are not distinguishable from a white noise series.

```{r goog200_portmanteau}
res <- residuals(naive(goog200))

# Box-Pierce test
# lag=h and fitdf=K
Box.test(res, lag=10, fitdf=0)

# Box-Ljung test
Box.test(res,lag=10, fitdf=0, type="Lj")
```

The function `checkresiduals()` can be used to complete all components of residual checking (i.e. to create the time plot, ACF, histogram and perform the Ljung-Box test) 
```{r goog200_checkres}
checkresiduals(naive(goog200))
```


## Evaluating forecast accuracy

### Training and test sets

* Residuals not reliable indication of how large true forecast errors likely to be.
* Need to test forecast on new data.
* Best to separate data into training (in-sample data) and test data (hold-out set, out-of-sample data).  Test data ~20% of sample, but f(sample size, how long ahead you want to forecast)

Notes:
* Model that fits training data well may not forecast well
* Over-fitting is as bad as failing to identify a systematic pattern in the data



### Functions to subset a time series
Can use following functions:

* `window()`

```{r subset_window, eval=TRUE, include=FALSE}
window(ausbeer, start=1995)
```


* `subset()`

```{r subset_subset, eval=TRUE, include=FALSE}
subset(ausbeer, start=length(ausbeer)-4*5)
subset(ausbeer, quarter = 1)
```


### Forecast errors

* Difference between observed value and forecast
* Not a mistake; unpredictable part of observation

\[e_{T + h} = y_{T + h} - \hat{y}_{T + h|T},\]
where training data given by ${y_1, \ldots, y_T}$ and test data given by ${y_{T+1}, y_{T+2} \ldots}$

Different to residuals

* Residuals calculated on training set, forecast errors on test set
* Residuals are one-step forecasts, forecast errors can involve multi-step forecasts

Forecast accuracy can summarise forecast errors in different ways.

### Scale-dependent errors

* Forecast errors are on the same scale as the data
* Accuracy measures that are based only on $e_t$ are therefore scale-dependent and cannot be used to make comparisons between series that involve different units
* Measures include:
    * Mean absolute error: \[\text{MAE} = \text{mean}(|e_t|)\]
    Forecast that minimizes MAE will lead to forecasts of the median.
    * Root mean squared error: 
   \[\text{RMSE} = \sqrt{\text{mean}(e_t^2)}\]  Forecast that minimised RMSE will lead to forecasts of the mean (and therefore more widely used than MAE).
    


### Percentage errors

Unlike MAE and RMSE (and other scale-dependent errors), percentage errors are unit-free and so frequently used to compare forecast performances between data sets.  

Percentage error is $p_t = 100 e_t / y_t$.

#### Mean absolute percentage error (MAPE)
\[\text{MAPE} = \text{mean}(|p_t|)\]

Disadvantages:

* Infinite or undefined if $y_t = 0$ for any $t$ in period of interest
* Extreme values if $y_t$ is close to zero
* Assume unit of measurement has a meaningful zero, i.e., make no sense when measuring temperature.
* Place heavier penalty on negative errors than on positive errors

#### Symmetric MAPE (sMAPE)

This measure is **not recommended** despite being widely used.

\[\text{sMAPE} = \text{mean}(200|y_t - \hat{y}_t| / (y_t + \hat{y}_t))\]

Disadvantages:

* Still unstable as if $y_t$ is close to zero, $\hat{y}_t$ also likely to be close to zero and therefore measure still involves division by number close to zero.
* sMAPE can be negative no not really a measure of absolute percentage error


### Scaled errors

Alternative to percentage errors to compare forecast accuracy across series with different units, in which the denominator is the average error of the naive forecast (on the training data).  As both the numerator and denominator are in the original data scale, the ratio is independent of the scale of the data.  
\[q_j = \frac{e_j}{\frac{1}{T-1}\sum_{t=2}^T|y_t - y_{t-1}|}\]


* A scaled error is less than one if it arises from a better forecast than the average naive forecast computed on the training data
* A scaled error is more than one if the forecast is worse than the average naive forecast computed on the training data

For seasonal time series, the scaled error can be defined using seasonal naive forecasts:
\[q_j = \frac{e_j}{\frac{1}{T-m}\sum_{t=m + 1}^T|y_t - y_{t-m}|}\]

The mean absolute scaled error is \[\text{MASE} = \text{mean}(|q_j|)\]

### Examples

#### Example 1
```{r beer_forecasts}
beer2 <- window(ausbeer,start=1992,end=c(2007,4))
beerfit1 <- meanf(beer2,h=10)
beerfit2 <- rwf(beer2,h=10)
beerfit3 <- snaive(beer2,h=10)
autoplot(window(ausbeer, start=1992)) +
  autolayer(beerfit1, series="Mean", PI=FALSE) +
  autolayer(beerfit2, series="Naïve", PI=FALSE) +
  autolayer(beerfit3, series="Seasonal naïve", PI=FALSE) +
  xlab("Year") + ylab("Megalitres") +
  ggtitle("Forecasts for quarterly beer production") +
  guides(colour=guide_legend(title="Forecast"))
```
```{r beer_forecast_accuracy}
beer3 <- window(ausbeer, start=2008)

cbind(Method = c("Mean", "Naive", "Seasonal Naive"), as.data.frame(rbind(
    accuracy(beerfit1, beer3)[2,], 
    accuracy(beerfit2, beer3)[2,], 
    accuracy(beerfit3, beer3)[2,]) )) %>%
    select(Method, RMSE, MAE, MAPE, MASE) %>% kable(digits = 2)

```
Regardless of accuracy measure used, seasonal naive method performs best (which is also obvious from the TS plot)

#### Example 2 (non-seasonal example)
```{r goog200_forecasts}
googfc1 <- meanf(goog200, h=40)
googfc2 <- rwf(goog200, h=40)
googfc3 <- rwf(goog200, drift=TRUE, h=40)
autoplot(subset(goog, end = 240)) +
  autolayer(googfc1, PI=FALSE, series="Mean") +
  autolayer(googfc2, PI=FALSE, series="Naïve") +
  autolayer(googfc3, PI=FALSE, series="Drift") +
  xlab("Day") + ylab("Closing Price (US$)") +
  ggtitle("Google stock price (daily ending 6 Dec 13)") +
  guides(colour=guide_legend(title="Forecast"))
```
```{r goog200_accuracy}
googtest <- window(goog, start=201, end=240)
cbind(Method = c("Mean", "Naive", "Drift"), as.data.frame(rbind(
    accuracy(googfc1, googtest)[2,], 
    accuracy(googfc2, googtest)[2,], 
    accuracy(googfc3, googtest)[2,]) )) %>%
    select(Method, RMSE, MAE, MAPE, MASE) %>% kable(digits = 2)

```
In this case, drift method performs better (regardless of accuracy measure used)

### Time series cross-validation

* Series of test sets each with only a single observation.  Corresponding training sets consists of all observations prior to the observation that forms the test set.    
* The earliest observations are not treated as test sets.
* Forecast accuracy computed by averaging over test sets (a.k.a evaluation on a rolling forecasting origin as origin at which forecast is based rolls forward in time)
* Can be used for one-step and multi-step forecasts.
* Implemented using `tsCV()` function

Following example computes residual (training data) RSME and RMSE with forecasts (test data) obtained via time series cross-validation:
```{r goog200_tscv}

# forecast accuarcy (test data)
e <- tsCV(goog200, rwf, drift=TRUE, h=1)
sqrt(mean(e^2, na.rm=TRUE))

# residual error (training data)
sqrt(mean(residuals(rwf(goog200, drift=TRUE))^2, na.rm=TRUE))
```



### Pipe operator
```{r goog200_tscv_with_pipe}
# forecast accuarcy (test data)
goog200 %>% tsCV(forecastfunction=rwf, drift=TRUE, h=1) -> e
e^2 %>% mean(na.rm=TRUE) %>% sqrt()

# residual error (training data)
goog200 %>% rwf(drift=TRUE) %>% residuals() -> res
res^2 %>% mean(na.rm=TRUE) %>% sqrt()

```



### Example: using tsCV()

Evaluate forecasting performance of 1- to 8-step-ahead naive forecasts, using MSE as forecast error measure.  As expected, forecast error increases as forecast horizon increases.

```{r goog200_tscv_multi}

# forecast errors for h = 1:8
e <- tsCV(goog200, forecastfunction=naive, h=8)

# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = T)

# Plot the MSE values against the forecast horizon
data.frame(h = 1:8, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()
```



## Prediction intervals

### One-step prediction intervals

### Multi-step prediction intervals

### Benchmark methods

### Prediction intervals from bootstrapped residuals

### Prediction intervals with transformations

 
## The `forecast` package in R

### Functions that output a forecast object

### `forecast()` function

# Judgmental forecasts
## Beware of limitations
## Key principles
## The Delphi method
## Forecasting by analogy
## Scenario forecasting
## New product forecasting
## Judgmental adjustments


# Time series regression models

Aim: forecast time series of interest $y$ assuming it has a linear relationship with another time series $x$

Terminology:

* $y$ = forecast = regressand = dependent = explained variable
* $x$ = predictor = regressor = independent = explanatory variable

## The linear model
### Simple linear regression
### Multiple linear regression
### Assumptions

## Least square estimation
## Evaluating the regression model
## Some useful predictions
## Selecting predictors
## Forecasting with regression
## Matrix formulation
## Nonlionear regression
## Correlation, causation and forecasting


# Time series decomposition

Components:

* Trend-cycle (trend and cycle usually combined and referred to as **trend**)
* Seasonal
* Remainder (anything else in TS)
* Level ($\ell$) is introduced within the exponential smoothing section and is the average value of the series (or the smoothed value).

TS decomposition used to improve understanding of TS and to improve forecast accuracy.

## Time series components

Additive decomposition:
\[y_t = S_t + T_t + R_t,\]
where

* $y_t$ is the data
* $S_t$ is the seasonal component
* $T_t$ is the rend-cycle component
* $R_t$ is the remainder

Use additive decomposition when:

* Magnitude of seasonal fluctuations does not vary with level of TS, or
* Variation around trend-cycle does not vary with level of TS


Multiplicative decomposition:

First transform data until variation in series is stable over time, then use additive decomposition.  When a log transformation has been used this is equivalent to using a multiplicative decomposition

  $$
    \begin{aligned}
    y_t &= S_t \times T_t \times R_t \\
    \equiv \log y_t &= \log S_t + \log T_t + \log R_t
     \end{aligned}       $$


### Electrical equipment manufacturing
In the components plot, the grey bars to the right of each panel show the relative scales.  Each grey bar represents the same length, but because the plots are on different scales, the bars vary in size.  Therefore, a panel with a large grey bar has less variation when compared to panels with smaller grey bars.

### Seasonally adjusted data
Data is said to be seasonally adjusted if seasonal component is removed, .e. for additive decomposition, seasonally adjusted data is given by $y_t - S_t$ and for multiplicative data, $y_t/S_t$.

Useful when variation due to seasonality not of primary interest, e.g. employment data is usually seasonally adjusted.

Note, however, seasonally adjusted data contains remainder + trend-cycle and therefore not smooth; consequently upturns/downturns can be misleading.  If aim is to look for turning points in series, then better to use trend-cycle component.

## Moving averages

Moving average method is first step in classical method of time series decomposition (1920-1950s, but basis of modern methods).

### Moving average smoothing

#### Moving average of order $m$ ($m$-MA)
The estimate of trend-cycle at time $t$ is obtained by averaging the values of the time periods within periods $t \pm k$ and the order is said to be $m = 2k + 1$,\[\hat{T}_k = \frac{1}{m}\sum^k_{j=-k}y_{t+j}\]

```{r elecsales_5ma}
data.frame(
    Year = (start(elecsales)[1]: end(elecsales)[1]),
    Sales = as.numeric(elecsales),
    MA5 = as.numeric(ma(elecsales, 5))
) %>% kable(col.names = c("Year", "Sales", "5-MA"))
```


```{r elecsales_5ma_plot, message=FALSE, warning=FALSE}
# use series to give a legend entry
autoplot(elecsales, series="Data") +
  autolayer(ma(elecsales,5), series="5-MA") +
  xlab("Year") + ylab("GWh") +
  ggtitle("Annual electricity sales: South Australia") +
  scale_colour_manual(values=c("Data"="grey50","5-MA"="red"),
                      breaks=c("Data","5-MA"))
```

The order changes the smoothness of the trend-cycle estimate (larger order = smoother curve). Customer is to use odd order (e.g., 3, 5, 7, ...) so that they are symmetric about time $t$.

### Moving averages of moving averages
This is done to make an even-order moving average symmetric.

E.g. a $2 \times 4$-MA is a 2-MA applied on top of an 4-MA  


**Centred moving average of order $m$**: When a 2-MA follows a moving average of an even order $m$.

Note however, that a $3 \times 3$-MA is also common.  In general, an even order MA should be followed by an even order MA and an odd order MA should be followed by an odd order MA.

### Estimating the trend-cycle with seasonal data

Centred moving averages typically used to estimate trend-cycle from seasonal data.

Eg. if using $2 \times 4$-Ma on quarterly data:

$$\begin{aligned}
\hat{T}_t & = \frac{1}{2}\left[ \frac{1}{4}(y_{t-2} + y_{t-1} y_{t} + y_{t+1}) + \frac{1}{4}(y_{t-1} + y_{t} y_{t+1} + y_{t+2} )\right] \\
& = \frac{1}{8}y_{t-2} + \frac{1}{4}y_{t-1} \frac{1}{4}y_{t} +\frac{1}{4} y_{t+1}\frac{1}{8}y_{t-2}
\end{aligned}$$

So each quarter is given equal weight.  

In general, a $2 \times m$-MA $\equiv$ ($m + 1$)-MA where all observations take the weight $1/m$ except the first and last terms with take weights $1/(2m)$. 

Order $m$ of seasonal period | Estimate trend-cycle using:
-----------------------------| -----------------------------
$m$ = Even | $2 \times m$-MA
E.g. $m$ = 12 (monthly)  | $2 \times 12$-MA
Odd  | $m$-MA
E.g. $m$ = 7 (weekly) | 7-MA

### Example: Electrical equipment manufacturing


Monthly data:
```{r elecequip_12ma, message=FALSE, warning=FALSE}
# Note default of ma is centre = TRUE for even orders
autoplot(elecequip, series = "Data") + 
    autolayer(ma(elecequip, 12), series="2x12-MA") +
    xlab("Year") + 
    ylab("New orders index") +
    ggtitle("Electrical equipment manufacturing (Euro area)") +
    scale_colour_manual(values=c("Data"="grey","2x12-MA"="red"),
                      breaks=c("Data","12-MA"))
```


### Weighted moving averages


Combinations of moving averages $\equiv$ weighted moving average.  E.g. $2 \times 4$-MA $\equiv$ weighted 5-MA with weights $[\frac{1}{8}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{8}]$


Weighted $m$-MA:
\[\hat{T}_t = \sum_{j = -k}^k a_j y_{t + j},\]
where 

* $k = (m -1 )/2$ and weights
* weights are $[a_{-k}, \ldots, a_k]$
* weights sum to one
* weights are symmetric

Advantage:

* Yield smoother estimate of trend-cycle as can slowly increase and then slowly decrease weighs.

Note: $m$-MA is a special case of the weighted moving average with all weights equal to $1/m$.


## Classical decomposition

Two forms:

1. Additive
2. Multiplicative (in which case, the $m$ values that form the seasonal component are sometimes called __seasonal indices__)

Described for time series with seasonal period $m$, e.g. 

* $m = 4$ for quarterly data
* $m = 12$ for monthly data
* $m = 7$ for daily data with a weekly pattern
* $m = 365.25$ for daily data with an annual pattern

Assumption:

* Seasonal component is constant from year to year

### Additive decomposition


Steps:

1. Calculate the trend-cycle component ($\hat{T}_t$)
    * If $m$ is event, use $2 \times m$-MA
    * If $m$ is odd, use $m$-MA

2. Calculate the detrended series (such that left with seasonal and remainder components) 
    * Detrended series is $y_t - \hat{T}_t$

3. Estimate the seasonal component ($\hat{S}_t$) for each season
    * Average detrended values for each season.  e.g. with monthly data, for March, average all detrended March values
    * Adjust components so that they add to zero
    * String components together for each year

4. Calculate the remainder component
    * $\hat{R}_t = y_t - \hat{T}_t - \hat{S}_t$

### Multiplicative decomposition

Subtractions are replaced by divisions.

Steps:

1. Calculate the trend-cycle component ($\hat{T}_t$)
    * If $m$ is event, use $2 \times m$-MA
    * If $m$ is odd, use $m$-MA

2. Calculate the detrended series (such that left with seasonal and remainder components) 
    * Detrended series is $y_t/ \hat{T}_t$

3. Estimate the seasonal component ($\hat{S}_t$) for each season
    * Average detrended values for each season.  e.g. with monthly data, for March, average all detrended March values
    * Adjust components so that they add to zero
    * String components together for each year

4. Calculate the remainder component
    * $\hat{R}_t = y_t /(\hat{T}_t  \hat{S}_t)$


### Example
```{r elecequip_decomp}
elecequip %>% decompose(type="multiplicative") %>%
  autoplot() + 
    xlab("Year") +
  ggtitle("Classical multiplicative decomposition
    of electrical equipment index")
```
Features:

* Evidence of "leakage" of trend-cycle component into remainder component in 2009

### Comments on classical decomposition

* Not recommended; exist much better methods
* Estimate unavailable for head and tail obs, e.g. if $m$ = 12, then no estimate for first and last 6 months.
* Trend-cycle estimate tends to over-smooth rapid rises and falls
* Assumes seasonal component repeats year on year.  For longer series, this may not be so and classical decomposition does not allow seasonal changes over time to be captured.
* Classical methods not robust to observations made during unusual events (e.g. industrial dispute)

## X11 decomposition

From US Census Bureau and Statistics Canada.  Based on classical decomposition but:

* Allows for trend-cycle estimates for endpoints
* Allows seasonal component to vary slowly over time
* Has sophisticated methods for handling trading day variation, holiday effects and effects of known predictors
* Highly robust to outliers and level shifts in the time series

Allows for both additive and multiplicative decomposition.  Available using `seasonal::seas()` function.  Given output has the following functions:

* `seasonal()`
* `trendcycle()}`
* `remainder()`
* `seasadj()`


For details, see Dagum and Bianconcini (2016). __Seasonal adjustment methods and real time trend-cycle estimation__.

Example:
```{r elecequip_x11}
require(seasonal)
elecequip %>% seasonal::seas(x11="") -> fit
autoplot(fit) +
  ggtitle("X11 decomposition of electrical equipment index")
```
Features: 

* Captured sudden fall in data in early 2009 (better than STL and classical decomposition)
* Unusual observation at end of 2009 more clearly seen in the remainder.


Illustration of data with trend-cycle component and seasonally-adjusted (i.e. trend + remainder) data:
```{r elecequip_x11_components}

autoplot(elecequip, series="Data") +
  autolayer(trendcycle(fit), series="Trend") +
  autolayer(seasadj(fit), series="Seasonally Adjusted") +
  xlab("Year") + ylab("New orders index") +
  ggtitle("Electrical equipment manufacturing (Euro area)") +
  scale_colour_manual(values=c("gray","blue","red"),
             breaks=c("Data","Seasonally Adjusted","Trend"))
```

Seasonal sub-series plots to see how seasonal component is changing over time:
```{r elecequip_x11_seasonal_subplots}
fit %>% seasonal() %>% ggsubseriesplot() + ylab("Seasonal")
```


Features: Only small changes over time.


## SEATS decomposition

Seasonal Extraction in ARIMA Time Series (SEATS):

* Developed at Bank of Spain
* Only works with quarterly and monthly data; daily, hourly, weekly, etc data require alternative approach
* Details given in Dagum & Bianconcini (2016)
* Available in `seasonal` package; refer to [package website]([http://www.seasonal.website/seasonal.html)

```{r elecequip_seats}
library(seasonal)
elecequip %>% seas() %>%
autoplot() +
  ggtitle("SEATS decomposition of electrical equipment index")
```


## STL decomposition

Seasonal and Trend decomposition using Loess (local polynomial regression)

* Developed by Cleveland, Cleveland, McRae & Terpenning (1990)
* Available using `stl()` function.

Advantages over classical, SEATS and X11 decomposition methods:

* Handles all seasonality, not just monthly and quarterly (unlike SEATS)
* Seasonal component is allowed to change over time, and rate of change can be controlled by the user
* Smoothness of trend-cycle can be controlled by the user
* Can be robust to outliers (i.e user can specify a robust decomposition) s.t occasional unusual observations affect remainder component and not trend-cycle or seasonal components

Disadvantages:

* Does not automatically handle trading day or calendar variation
* Only provides for additive decomposition (although multiplicative decomposition can be obtained by first taking logs of the data and then back-transforming the components, or using a Box-Cos transformation with $\lambda$ = 0 )

Parameters to control how quickly the trend-cycle and seasonal component, respectively change:

* `t.window`: Trend-cycle window (optional parameter)  
* `s.window`: Seasonal window (required, as no default).  Setting it to be infinite is equivalent to forcing the seasonal component to be periodic (i.e. identical across years)

Smaller values allow for more rapid changes.  Equal to number of consecutive years to be used when estimating the trend-cycle and seasonal components, respectively.


The `mstl()` function 

* sets `s.window = 13` and chooses `t.window` automatically
* Usually gives good balance between overfitting the seasonality and allowing it to slowly change over time
* but Default settings will need adjusting for some time series

Example:
```{r autoelec_stl_adj}
elecequip %>%
  stl(t.window=13, s.window="periodic", robust=TRUE) %>%
  autoplot()
```

## Measuring strength of trend and seasonality

Time series decomposition can be used to measure the strength of the trend and seasonality.  
Recall \[y_t = T_t + S_t + R_t\]


### Strength of Trend
For strongly trended data, seasonally adjusted data $(T_t + R_t)$ will have much more variation than the remainder component ($R_t$) and so $\text{Var}(R_t)/\text{Var}(T_t + R_t)$ will be relatively small.

Strength of trend defined as:
\[F_T = \max\left(0, 1 - \frac{\text{Var}(R_t)}{\text{Var}(T_t + R_t)}\right)\]

Because variance of remainder may occasionally be larger than variance of seasonally adjusted data, the minimal possible value of $F_T$ is set to zero.

### Strength of Seasonality
For data with strong seasonality, detrended $(S_t + R_t)$ will have much more variation than the remainder component ($R_t$) and so $\text{Var}(R_t)/\text{Var}(S_t + R_t)$ will be relatively small.

Strength of seasonality defined as:
\[F_S = \max\left(0, 1 - \frac{\text{Var}(R_t)}{\text{Var}(S_t + R_t)}\right)\]

Similar to strength of trend, because variance of remainder may occasionally be larger than variance of detrended data, the minimal possible value of $F_T$ is set to zero.



## Forecasting with decomposition

Decomposition is primarily used for studying TS data and exploring historical changes but can be used for forecasting

$$\begin{aligned}
y_t &= \hat{S}_t + \hat{R}_t + \hat{T}_T \\
    &= \hat{A}_t + \hat{T}_T
\end{aligned}$$ where $\hat{A}_t$ is the seasonally adjusted component

Forecast as follows:

* Forecast seasonal component, $\hat{S}_t$
    * Assume seasonal component is unchanging, or changing extremely slowly
    * Use seasonal naive method (i.e, take last year of estimated component)
* Forecast seasonally adjusted component, $\hat{A}_t$
    * Use any non-seasonal forecasting method, e.g. random walk with drift, Holt's method, non-seasonal ARIMA
* Add these two forecasts together

Prediction intervals are calculated in a similar method

Can use the following functions (noting that they ignore the uncertainty in forecasts of the seasonal component as these are considered small):

* `stl()` then `forecast(method = "naive")` function
* `stlf(method = "naive")`, this uses `mstl` and so there are default options for `s.window` and `t.window`.

Can use other methods than "naive"; default is ETS applied to the seasonally adjusted series.


```{r elecequip_stl_forecast}
fit <- stl(elecequip, t.window=13, s.window="periodic", robust=TRUE)
fit %>% 
    forecast(method="naive") %>%
    autoplot() + 
    ylab("New orders index")
```



# Exponential smoothing

* Proposed late 1950s
* Weighted averages of past observations, with weights decaying exponentially as observations get older; more weight placed on most recent observations
* Method selection generally based on recognising key components of the time series (trend and seasonal) and the way in which these enter the smoothing method (additive, damped or multiplicative)


## Simple exponential smoothing (SES)

* Naive method = weighted average with all weight given to the last observation
* Average method = weighted average in which all observations have equal weights 
* SES method 
    * = more recent observations have larger weights, where weights decrease exponentially as observations come from further in the past
    
    \[\hat{y}_{T+1|T} = \alpha y_T + \alpha(1-\alpha)y_{T-1} + \alpha(1-\alpha)^2 y_{T-2} + \ldots, \] where $0 \leq \alpha \leq 1$ and controls the rate at which the weights decrease.
    
    * Suitable when no clear trend or seasonal pattern
    
    
### Weighted average form

**Forecast** at time $T + 1$ is set to average of most recent observation ($y_T$) and previous forecast ($\hat{y}_{T|T-1}$)
\[\hat{y}_{T+1|t} = \alpha y_T + (1 - \alpha) \hat{y}_{T|T-1}\]

where $0 \leq \alpha \leq 1$.  

Similarly the **fitted values** (1-step forecasts of the training data) can be written as:
\[\hat{y}_{t+1|t} = \alpha y_t + (1 - \alpha) \hat{y}_{t|t-1}\]
for $t = 1, \ldots, T$.

Initialise process with first fitted value at time 1 set to $\ell_0$, such that

$$\begin{aligned}
\hat{y}_{2|1} &= \alpha y_1 + (1 - \alpha) \ell_0 \\
\hat{y}_{3|2} &= \alpha y_2 + (1 - \alpha) \hat{y}_{2|1} \\
\hat{y}_{4|3} &= \alpha y_3 + (1 - \alpha) \hat{y}_{3|2} \\
\vdots\\
\hat{y}_{T|T-1} &= \alpha y_{T-1} + (1 - \alpha) \hat{y}_{T-1|T-2} \\
\hat{y}_{T+1|T} &= \alpha y_{T} + (1 - \alpha) \hat{y}_{T|T-1} 
\end{aligned}$$

By substitution, 
\[\hat{y}_{T+1|T} = \sum_{j=1}^{T-1}\alpha(1-\alpha)^j y_{T-j} + \alpha(1-\alpha)^T \ell_0\]

For large T, the last term becomes tiny and so the weighted average form leads to the same as the forecast ESS.


### Component form

In simple exponential smoothing, the only component included is the level (which is the average value of the series) $\ell_t$.  Other methods may also include a trend $b_t$ and a seasonal component $s_t$.

Comprise forecast equation and smoothing equation for each of the components included in the method. 

$$\begin{aligned}
\text{Forecast equation}\;\;\;  \hat{y}_{t+h|t} &= \ell_t \\
\text{Smoothing equation}\;\;\;          \ell_t &= \alpha y_t + (1-\alpha)\ell_{t-1}
\end{aligned}$$

Setting $h$ = 1 gives fitted values, while setting $t = T$ gives the true forecasts beyond the training data.

To obtain weighted average form:

* Replace $\ell_t$ with $\hat{y}_{t+1|t}$ 
* Replace $\ell_{t-1}$ with $\hat{y}_{t-1|t}$

Component form is useful when start adding components (not so much for simple exponential smoothing!)

### Flat forecasts
All forecasts take the ame value, equal to the last level component (and so considered to have a "flat" forecast function); they are therefore only suitable if time series has no trend nor seasonal component.
\[\hat{y}_{T+h|T} = \hat{y}_{T+1|T} = \ell_T, \;\;\; h = 2, 3, \ldots\]

### Optimisation

Exponential smoothing methods require choice of:

* Smoothing parameter ($\alpha, 0 \leq \alpha \leq 1$)
* Initial value ($\ell_0$)

Can choose:

* Subjectively; previous experience
* Use observed data to minimise the SSE,
\[\text{SSE} = \sum_{t=1}^T(y_t - \hat{y}_{t|t-1})^2 = \sum_{t=1}^T e_t^2\]

Unlike in regression (in which formulae are used to obtain the regression coefficients), this is a non-linear minimisation problem and so requires optimisation tool (or visual inspection?).

### Example: Oil production

```{r ses_oil}
oildata <- window(oil, start=1996)

# Estimate parameters
fc <- ses(oildata, h=5)

# Accuracy of one-step-ahead training errors
round(accuracy(fc),2)
#>               ME  RMSE   MAE MPE MAPE MASE  ACF1
#> Training set 6.4 28.12 22.26 1.1 4.61 0.93 -0.03
```
If `alpha` not supplied in call to `ses` function it is estimated.  Similarly the default method for selecting the initial state values is `optimal`.  In above case, values used are obtained by calling `fc$model$fit` which returns values of \alpha = `r round(fc$model$fit$par[1],2)` and $\hat{\ell}_0$ =`r round(fc$model$fit$par[2],1)`

Here \alpha quite large and so more weight given to more recent observations.  This means that a large adjustment takes place in the estimated level $\ell_t$ at each time.  With a smaller value of $\alpha$ the fitted values would be smoother.

In the following chart, for the period 1996-2013 the black line shows the observed values of oil production and the red line shows the fitted values.  For the forecast period 2014 = 2018, the blue line shows the point forecast (flat at 542.68) alongside the 80% and 90% prediction intervals
```{r ses_oil_plot}
autoplot(fc) + 
  autolayer(fitted(fc), series="Fitted") +
  ylab("Oil (millions of tonnes)") + xlab("Year")
```




Given the large prediciton interval, it would be misleading to dispaly only the point forecasts. 

## Trend methods
### Holt's linear trend method
Extension of simple exponential smoothing to enable forecasting of data with trends; uses two moothing equations (one for the level and one for the trend);

$$\begin{aligned}
\text{Forecast equation}\;\;\;  \hat{y}_{t+h|t} &= \ell_t + hb_t\\
\text{Level equation}\;\;\;          \ell_t &= \alpha y_t + (1-\alpha)(\ell_{t-1}+b_{t-1})\\
\text{Trend equation}\;\;\;          b_t &= \beta^* (\ell_t - \ell_{t-1}) + (1-\beta^*)b_{t-1}
\end{aligned}$$



where

* $\ell_t$ is estimate of level at time $t$
* $b_t$ is estimated of trend at time $t$
* $\alpha$ is the smoothing parameter for the level, $0 \leq \alpha \leq 1$
* $\beta^*$ is the smoothing parameter for the trend (slope), $0 \leq \beta^* \leq 1$


### Example: Air passengers
```{r holt_air}
# see: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/xts_Cheat_Sheet_R.pdf

air <- window(ausair, start=1990)
fc <- holt(air, h=5)

data.frame(Year = as.numeric(time(fc$x))) %>%
    mutate(Time = row_number(Year),
            Observation = as.numeric(fc$x), 
            Forecast = as.numeric(fc$fitted)) %>%
            right_join(data.frame(
            Year = as.numeric(time(fc$model$states)), 
            Level = as.numeric(fc$model$states[,"l"]),
            Slope = as.numeric(fc$model$states[,"b"]))) %>%
            
            select(Year, Time, Observation, Level, Slope, Forecast) %>%
            bind_rows(data.frame(
            Year = as.numeric(time(fc$mean)),
            Time = 1:5,
            Forecast = as.numeric(fc$mean)))

```


### Damped trend methods

Holt's linear method forecasts have a constant (increasing or decreasing) trend indefinitely into the future which leads to over-forecasting, especially in longer forecast horizons.

Damped trend methods dampens trend to flat line at a point in the future by adding a damping parameter $0 < \phi < 1$

$$\begin{aligned}
\text{Forecast equation}\;\;\;  \hat{y}_{t+h|t} &= \ell_t + (\phi + \phi^2 + \ldots + \phi^h)b_t\\
\text{Level equation}\;\;\;          \ell_t &= \alpha y_t + (1-\alpha)(\ell_{t-1}+\phi b_{t-1})\\
\text{Trend equation}\;\;\;          b_t &= \beta^* (\ell_t - \ell_{t-1}) + (1-\beta^*)\phi b_{t-1}
\end{aligned}$$


If $\phi=1$, method identical to Holt's.  
Short-run forecasts are trended; while long-run forecasts are constant (convergin to $\ell_T + \phi b_t/(1-\phi))$

In practice, $\phi$ is rarely less than 0.8 as damping has a very strong effect for smaller values; usually $0.8 \leq \phi \leq 0.98$


### Example: Air passengers (continued)

Foreasts for air passengers using Holt's linear trend method and damped trend method, noting that example is artificial due to low damping number (which would usually be estimated) and large forecast horizon

```{r damped_air}
fc <- holt(air, h=15)
fc2 <- holt(air, damped=TRUE, phi = 0.9, h=15)
autoplot(air) +
  autolayer(fc, series="Holt's method", PI=FALSE) +
  autolayer(fc2, series="Damped Holt's method", PI=FALSE) +
  ggtitle("Forecasts from Holt's method") + xlab("Year") +
  ylab("Air passengers in Australia (millions)") +
  guides(colour=guide_legend(title="Forecast"))
```

### Example: Sheep in Asia

```{r livestock}
autoplot(livestock) +
  xlab("Year") + ylab("Livestock, sheep in Asia (millions)")
```

Time series cross-validation of one-step forecast accuracy using simple exponential smoothing, Holt's linear trend method and damped trend method shows that damped trend method is best.  
```{r livestock_cv}


e1 <- tsCV(livestock, ses, h=1)
e2 <- tsCV(livestock, holt, h=1)
e3 <- tsCV(livestock, holt, damped=TRUE, h=1)

data.frame(Method = c("SES", "Holt", "Damped"), 
           MSE = c(mean(e1^2, na.rm=TRUE), mean(e2^2, na.rm=TRUE), mean(e3^2, na.rm=TRUE)),
           MAE = c(mean(abs(e1), na.rm=TRUE), mean(abs(e2), na.rm=TRUE), mean(abs(e3), na.rm=TRUE)))

```

Use of damped trend method to get forecasts for future years using whole data set:
```{r livestock_damped_params}
fc <- holt(livestock, damped=TRUE)
fc[["model"]]
```

* The smoothing parameter for the level ($\alpha$) is very close to one, showing that the level reacts strongly to each new observation
* The smoothing parameter for the slope ($\beta$) shows that trend is not changing over time
* The damping parameter ($\phi$) is claose
```{r livestock_damped_plot}
autoplot(fc) +
  xlab("Year") + ylab("Livestock, sheep in Asia (millions)")
```


## Holt-Winter's seasonal method
Two variations; 

* additive; when seasonal variations roughly constatn through seeries
* Multiplicative; when seasonal variations are changing proportional to the level of the series

### Holt-Winters' additive model
$$\begin{aligned}
\text{Forecast equation}\;\;\;  \hat{y}_{t+h|t} &= \ell_t + hb_t + s_{t + h - m(k + 1)}\\
\text{Level equation}\;\;\;          \ell_t &= \alpha (y_t-s_{t-m}) + (1-\alpha)(\ell_{t-1}+ b_{t-1})\\
\text{Trend equation}\;\;\;          b_t &= \beta^* (\ell_t - \ell_{t-1}) + (1-\beta^*) b_{t-1}\\
\text{Seasonal equation}\;\;\;       s_t &= \gamma(y_t - \ell_{t-1}-b_{t-1}) + (1-\gamma) s_{t-m}
\end{aligned}$$

where:
* $k$ is in the integer part of $(h-1)/m$ to ensure estimates of the seasonal indices come from the final year of hte sample
* The seasonal equation shows a weighted average between current seasonal index ($y_t - \ell_{t-1}-b_{t-1}$) and seasonal index of hte same eason last year (i.e. $m$ time periods ago)

Note, can write seasonal component as \[s_t = \gamma^*(y_t-\ell_t) + (1-\gamma^*)s_{t-m}\]
where $\gamma = \gamma^*(1-\alpha)$

Proof: 
$$\begin{aligned}
s_t &= \gamma^*(y_t-\biggl[\ell_t\biggr]) + (1-\gamma^*)s_{t-m}\\
    &= \gamma^* (y_t-\biggl[\alpha (y_t-s_{t-m}) + (1-\alpha)(\ell_{t-1}+ b_{t-1})\biggr]) + (1-\gamma^*)s_{t-m}\\
    &= \gamma^*y_t - \gamma^*\alpha (y_t-s_{t-m}) - \gamma^*(1-\alpha)(\ell_{t-1}+ b_{t-1}) + (1-\gamma^*)s_{t-m}\\
    &=\gamma^*y_t - \biggl[\gamma^*\alpha y_t\biggr]  - \gamma^*(1-\alpha)(\ell_{t-1}+ b_{t-1}) +   (1-\gamma^*)s_{t-m} + \biggl[\gamma^*\alpha s_{t-m}\biggr]\\
    &=\gamma^*(1-\alpha) y_t - \gamma^*(1-\alpha)(\ell_{t-1}+ b_{t-1}) +   (1-\gamma^*)s_{t-m} + \gamma^*\alpha s_{t-m}\\
    &=\gamma^*(1-\alpha)(y_t - \ell_{t-1} - b_{t-1}) +   (1-\gamma^*(1-\alpha))s_{t-m} \\
      &=\gamma(y_t - \ell_{t-1} - b_{t-1}) +   (1-\gamma)s_{t-m} \\
\end{aligned}$$

### Holt-Winters' multiplicative model
$$\begin{aligned}
\text{Forecast equation}\;\;\;  \hat{y}_{t+h|t} &= (\ell_t + hb_t )s_{t + h - m(k + 1)}\\
\text{Level equation}\;\;\;          \ell_t &= \alpha \frac{y_t}{s_{t-m}} + (1-\alpha)(\ell_{t-1}+ b_{t-1})\\
\text{Trend equation}\;\;\;          b_t &= \beta^* (\ell_t - \ell_{t-1}) + (1-\beta^*) b_{t-1}\\
\text{Seasonal equation}\;\;\;       s_t &= \gamma \frac{y_t}{(\ell_{t-1}+b_{t-1})} + (1-\gamma) s_{t-m}
\end{aligned}$$

### Example: International tourist visitor nights in Australia
```{r austourists_hw}
aust <- window(austourists,start=2005)
fit1 <- hw(aust,seasonal="additive")
fit2 <- hw(aust,seasonal="multiplicative")

autoplot(aust) +
  autolayer(fit1, series="HW additive forecasts", PI=FALSE) +
  autolayer(fit2, series="HW multiplicative forecasts",
    PI=FALSE) +
  xlab("Year") +
  ylab("Visitor nights (millions)") +
  ggtitle("International visitors nights in Australia") +
  guides(colour=guide_legend(title="Forecast"))
```

In additive model, note that seasonal components for 4 quarters ~ 1.

```{r austourists_hw_add}
data.frame(Qtr = as.numeric(time(fit1$x))) %>%
    mutate(Time = row_number(Qtr),
            Observation = as.numeric(fit1$x), 
            Forecast = as.numeric(fit1$fitted)) %>%
            right_join(data.frame(
            Qtr = as.numeric(time(fit1$model$states)), 
            Level = as.numeric(fit1$model$states[,"l"]),
            Slope = as.numeric(fit1$model$states[,"b"]),
            Seasonal = as.numeric(fit1$model$states[,"s1"])), by = "Qtr") %>%
            select(Qtr, Time, Observation, Level, Slope, Seasonal, Forecast) %>%
            bind_rows(data.frame(
            Qtr = as.numeric(time(fit1$mean)),
            Time = 1:8,
            Forecast = as.numeric(fit1$mean))) %>%
    filter(Qtr < 2006|Qtr >= 2015)
```


In additive model, note that seasonal components for 4 quarters ~ $m = 4$.

```{r austourists_hw_mult}
data.frame(Qtr = as.numeric(time(fit2$x))) %>%
    mutate(Time = row_number(Qtr),
            Observation = as.numeric(fit2$x), 
            Forecast = as.numeric(fit2$fitted)) %>%
            right_join(data.frame(
            Qtr = as.numeric(time(fit2$model$states)), 
            Level = as.numeric(fit2$model$states[,"l"]),
            Slope = as.numeric(fit2$model$states[,"b"]),
            Seasonal = as.numeric(fit2$model$states[,"s1"])), by = "Qtr") %>%
            select(Qtr, Time, Observation, Level, Slope, Seasonal, Forecast) %>%
            bind_rows(data.frame(
            Qtr = as.numeric(time(fit2$mean)),
            Time = 1:8,
            Forecast = as.numeric(fit2$mean))) %>%
    filter(Qtr < 2006|Qtr >= 2015)
```


Because both methods have the same number of parameters to estimate, can compare training RMSE from both models and see that multiplicative seasonality method fits data best (as expected given that seasonal variation in plotted data increased with level of the series)

```{r austourists_hw_rmse}
data.frame(Method = c("Additive", "Multiplicative"), 
           RMSE = c(sqrt(fit1$model$mse), sqrt(fit2$model$mse)))

```

Estimated components for the Holt-Winters method with additive and multiplicative seasonal components:

```{r austourists_hw_components}

p <- data.frame(Model = "Additive", Qtr = time(fit1$model$states), fit1$model$states) %>%
    select(Model, Qtr, l, b, s1) %>%
    rename(level = l, slope=b, season = s1) %>%
    bind_rows(
data.frame(Model = "Multiplicative", Qtr = time(fit2$model$states), fit2$model$states) %>%
    select(Model, Qtr, l, b, s1) %>%
    rename(level = l, slope=b, season = s1) 
    ) %>%
    gather("Component", "Value", -Model, -Qtr) %>%
    mutate(Component = factor(Component, levels = c("level", "slope", "season"))) 

p1 <- filter(p, Model == "Additive") %>%
    ggplot(aes(x = Qtr, y = Value)) + 
    geom_line() + 
    facet_grid(Component~Model, scales = "free")

p2 <- filter(p, Model == "Multiplicative") %>%
    ggplot(aes(x = Qtr, y = Value)) + 
    geom_line() + 
    facet_grid(Component~Model, scales = "free")

grid.arrange(p1, p2, nrow = 1)

```

The small value of the seasonal component ($\gamma$ = `r round(fit2$model$par["gamma"],3)`) for the multiplicative model means that the seasonal component hardly changes over time. The small value of the trend component ($\beta^*$ = `r round(fit1$model$par["beta"],4)`)
 for the additive model means the slope component hardly changes over time. The increasing size of the seasonal component for the additive model suggests that the model is less appropriate than the multiplicative model.
  
### Holt-Winters' damped method

Damping is possible with both additive and muliplicative Holt-Winters' methods.  Holt-Witers method with damped trend and multiplicative seasonability often provides reliable forecasts for season data
```{r hw_damped_seasonal, eval=FALSE, include=FALSE, echo=TRUE}
hw(y, damped = TRUE, seasonal = "multplicative")
```

### Example: Holt-Winters' method with daily data
Generate daily forecasts for last five weeks.
```{r hyndsight_hw}
fc <- hw(subset(hyndsight,end=length(hyndsight)-35),
         damped = TRUE, seasonal="multiplicative", h=35)
autoplot(hyndsight) +
  autolayer(fc, series="HW multi damped", PI=FALSE)+
  guides(colour=guide_legend(title="Daily forecasts"))
```


## A taxonomy of exponential smoothing methods
Combinations of how trend (N=None, A=Additive, $A_d$ = Additive Damped) and seasonal components (N=None, A=Additive, M=Multiplicative) treated leads to 9 possible exponential smoothing models:

Short-Hand (Trend-Seasonal) | Method | Function
------------| ---------------------------------------------- | -----------
(N,N)       | Simple exponential smoothing | `ses`
(N,A)        | |
(N,M)       | |

(A,N)       | Holt's linear method | `holt`
(A,A)       |Additive Holt-Winters' method | `hw`
(A,M)       |Multiplicative Holt-Winters' method  | `hw`

($A_d$, N)  | Additive damped trend method | `holt`
($A_d$, A)  |  | `hw`
($A_d$, M)  |Holt-Winters' damped method  | `hw`

(M,N)       |  | `holt`
(M,A)        | |
(M,M)       |  | `hw`


($M_d$, N)  |  |`holt`
($M_d$, A)  | |
($M_d$, M)  |  | `hw`


<table>
<tr>
<th>Trend Component</th>
<th colspan="3">Seasonal Component</th>
</tr>

<tr>
<th>&nbsp;</th>
<th>N (None)</th>
<th>A (Additive)</th>
<th>M (Multiplicative)</th>
</tr>

<tr>
<th>N (None) </th>
<td> (N,N) </td>
<td> (N,A) </td>
<td> (N,M) </td>
</tr>

<tr>
<th>A (Additive) </th>
<td> (A,N) </td>
<td> (A,A) </td>
<td> (A,M) </td>
</tr>

<tr>
<th>A$_d$ (Additive damped) </th>
<td> (A$_d$,N) </td>
<td> (A$_d$,A) </td>
<td> (A$_d$,M) </td>
</tr>

<tr>
<th>M (Multiplicative) </th>
<td> (M,N) </td>
<td> (M,A) </td>
<td> (M,M) </td>
</tr>

<tr>
<th>M$_d$ (Multiplicative damped) </th>
<td> (M$_d$,N) </td>
<td> (M$_d$,A) </td>
<td> (M$_d$,M) </td>
</tr>

</table>

## Innovations state space models for exponential smoothing

The statistical methods in this section generate the same point forecast as the exponential smoothing methods but can also generate prediction (or forecast) intervals.  

Each model (referred to as __state space models__) consists of:

* Equation to describe observed data
* State equations to describe how unobserved components (=states) (level, trend, seasonal) change over time
* Error which is either additive or multiplicative

So, for each of the 9 Trend-Seasonal models, (N=None, A=Additive, $A_d$ = Additive Damped) $\times$ (N=None, A=Additive, M=Multiplicative), there are two models; one with additive errors and one with multiplicative errors.  The point forecasts produced by the models are identical, but will generate differnt predeiction intervals.  

To distinguish between a model with additive errors and a model with multiplicative erros, each state-space model is labels as ETS ($\cdot$, $\cdot$, $\cdot$) for (Error, Trend, Seasonal), although it could be argued that ETS stands for ExponenTial Smoothing.  The possibility for each component are: Error = {A,M}, Trend = {N, A, $A_d$} and Seasonal = {N, A, M}

### ETS(A,N,N): simple exponential smoothing with additive errors

Recall: Single exponential smoothing component form:
$$\begin{aligned}
\text{Forecast equation}\;\;\;  \hat{y}_{t+h|t} &= \ell_t \\
\text{Smoothing equation}\;\;\;          \ell_t &= \alpha y_t + (1-\alpha)\ell_{t-1}
\end{aligned}$$

The smoothing equation can be re-arranged to get the error correction form:
$$\begin{aligned}\ell_t &= \ell_{t-1} + \alpha(y_t - \ell_{t-1})\\
&=\ell_{t-1} + \alpha e_t
\end{aligned}$$
where $e_t = y_t - \ell_{t-1} = y_t - \hat{y}_{t|t-1}$ is the residual at time $t$.

Residuals at time $t$ lead to adjustment of level at time $t+1$, e.g.

* If $e_t < 0 $ then $y_t <  \hat{y}_{t|t-1}$ (i.e., level has been over-estimated)
* The next level, $\ell_t$, will adjust $\ell_{t-1}$ downards.
* The closer $\alpha$ is to one, the greater the adjustment; smaller $\alpha$ leads to smaller adjustemnts and smoother levels.

The state-space model with additive errors uses the error correction form of the smoothing equation
$$\begin{aligned}
\text{Measurement (or Observation) Equation} \;\;\;\; y_t &= \ell_{t-1} + \varepsilon_t\\
\text{State (or Transition) Equation} \;\;\;\; \ell_t &= \ell_{t-1} + \alpha e_t
\end{aligned}$$
and assumes $\varepsilon_t = y_t - \hat{y}_{t|t-1}$ is ~$\text{NID}(0, \sigma^2)$

* Measurement equation: Observation is linear function of the level (predictable) and error (unpredictable)
* State equation: Evolution of thestate through time; if $\alpha = 0$, level is constant over time.  If $alpha = 1$, model reduces to a random walk model.

### ETS(M,N,N): simple exponential smoothing with multiplicative errors
Assuming that 
$$\begin{aligned}\varepsilon_t &= \frac{y_t - \hat{y}_{t|t-1}}{\hat{y}_{t|t-1}}\\
                            &\sim \text{NID}(0, \sigma^2),
\end{aligned}$$

get multiplicative form of the state space model:
$$\begin{aligned}
\text{Measurement Equation} \;\;\;\; y_t &= \ell_{t-1}(1 + \varepsilon_t)\\
\text{State Equation} \;\;\;\; \ell_t &= \ell_{t-1}(1 + + \alpha e_t)
\end{aligned}$$


### ETS(A,A,N): Holt’s linear method with additive errors
Assuming that one-step-ahead training errors given by
$$\begin{aligned}\varepsilon_t &= y_t - \ell_{t-1} -  b_{t-1}\\
                            &\sim \text{NID}(0, \sigma^2),
\end{aligned}$$

and substituting into error correction equations for Holt's linear method obtain: 
$$\begin{aligned}
\text{Measurement Equation} \;\;\;\; y_t &= \ell_{t-1} +  b_{t-1} + \varepsilon_t\\
\text{State Equation} \;\;\;\; \ell_t &= \ell_{t-1} +  b_{t-1} + \alpha \varepsilon_t\\
\text{Trend Equation}\;\;\;\;   b_t &=  b_{t-1} + \beta \varepsilon_t
\end{aligned}$$
where $\beta = \alpha \beta^*$.

### ETS(M,A,N): Holt’s linear method with multiplicative errors
See book for details.

### Other ETS models
See book for taxonomy of innovations state space model for each of the ETS models.

Possibilities are:

Additive Error Model:

<table>
<tr>
<th>Trend Component</th>
<th colspan="3">Seasonal Component</th>
</tr>

<tr>
<th>&nbsp;</th>
<th>N (None)</th>
<th>A (Additive)</th>
<th>M (Multiplicative)</th>
</tr>

<tr>
<th>N (None) </th>
<td> (A,N,N) </td>
<td> (A,N,A) </td>
<td style="background-color:grey"> (A,N,M) </td>
</tr>

<tr>
<th>A (Additive) </th>
<td> (A,A,N) </td>
<td> (A,A,A) </td>
<td style="background-color:grey"> (A,A,M) </td>
</tr>

<tr>
<th>A$_d$ (Additive damped) </th>
<td> (A,A$_d$,N) </td>
<td> (A,A$_d$,A) </td>
<td style="background-color:grey"> (A,A$_d$,M) </td>
</tr>

<tr>
<th>M (Multiplicative) </th>
<td> (A,M,N) </td>
<td> (A,M,A) </td>
<td> (A,M,M) </td>
</tr>

<tr>
<th>M$_d$ (Multiplicative damped) </th>
<td> (A,M$_d$,N) </td>
<td> (A,M$_d$,A) </td>
<td> (A,M$_d$,M) </td>
</tr>
</table>


Note those that are greyed out are not usually considered in model selection; the last two trend rows have not been discussed.


## Estimation and model selection

MLE is used to estimate parameters.

### Model selection

Can determine which ETS model most appropriate using either:

* AIC = $-2\log(L) + 2k$
* AIC$_{\text{c}}$ (AIC adjusted for small sample bias) = \frac{k(k+1)}{T-k+1}
* BIC = $\text{AIC} + k[\log{T}-1]

The following models can cause instability due to division by values potential close to zero in the state equations, and so are usually not considered in model selection:

* ETS(A,N,M)
* ETC(A,A,M)
* ETC(A,A$_d$,M)

Models with multiplicative errors are useful when the data are strictly positive, but are not numerically stable when the data contain zeros or negative values in which case only six fully additive models would be applied).  

### The `ets()` function in R

The `ets()` function in the `forecast` package:

* Used to estimate the ets models
* Does not produce forcasts, but instead estimates the model parameters.
* Uses the AICc model criteria by default

Function arguments:
```{r eval=FALSE, include=FALSE, echo = TRUE}
ets(y, model="ZZZ", damped=NULL, alpha=NULL, beta=NULL,
    gamma=NULL, phi=NULL, lambda=NULL, biasadj=FALSE,
    additive.only=FALSE, restrict=TRUE,
    allow.multiplicative.trend=FALSE)
```

Parameter | Description
----------| ------------------------------------
`y` | Time series to be forecast
`model` | Three letter code (Error, Trend, Seasonality), where Error$\in{A,M,Z}$, Trend|Season$\in{N,A,M,Z}$; Z is for automatic selection.
`damped` |  To dampen trend, i.e. to transform $A$ to $A_d$ or $M$ to $M_d$
`alpha` |  Smoothing parameter for the level
`beta` |  Smoothing parameter for the trend (slope)
`gamma` |  Smoothing parameter for the seasonality
`phi` |  Damping parameter
`\lambda`  | Box-Cox transformation parmater; whether time series is transformed before model is estimated
`biasadj` | If `TRUE` and `lambda` is not null, then back-transformed fitted values and forecasts will be bias-adjusted
`additive.only` | Only models with additive components will be considered
`restrict` | If true (default), models that cause numerical difficulaties are not considered
`allow.multiplicative.method` | Multiplicative trend models which were not disucssed here

### Working with `ets` objects

Useful functions that work on `ets` objects:

Function   | Returns:
----------| ------------------------------------
`coef()` | Fitted parameters
`accuracy()` | Accuracty measures computed on training data
`summary()` | summary information about fitted values
`autoplot()` | Time plots of the components
`plot()` | Time plots of the componetns
`residuals()` | Residuals of the estimated model
`fitted()` | One-step forecasts for the training data
`simulate()` |  Future sample paths from the fitted model
`forecast()` | Point forecasts and prediction intervals

### Example: International tourist visitor nights in Australia
Example: Fit ETS model tourist visitor nights in Australia over the period 2005-2015.

```{r austourist_ETS}
aust <- window(austourists, start=2005)
fit <- ets(aust)
summary(fit)
```

Best fit model is a multiplicative Holt-Winters' method with multiplicative error (i.e. incorporates trend and seasonality)


$$\begin{aligned}
\text{Forecast equation}\;\;\;  \hat{y}_t &= (\ell_{t-1} + b_{t-1} )s_{t - m}(1 + \varepsilon_t)\\
\text{Level equation}\;\;\;          \ell_t &=  (\ell_{t-1} + b_{t-1})(1 + \alpha\varepsilon_t)\\
\text{Trend equation}\;\;\;          b_t &= b_{t-1} + \beta(\ell_{t-1} + b_{t-1}) \varepsilon_t \\
\text{Seasonal equation}\;\;\;       s_t &= s_{t-m}(1 + \gamma\varepsilon_t) 
\end{aligned}$$

Components: 
```{r austourists_ets_autoplot}
autoplot(fit)
```

Residuals (note, if type = "response", then the fitted values are computed for the h-step forecasts, otherwise the detault is type = 'innovation' in which the regular residuals are used.)
```{r austourists_ets_resid}
cbind('Residuals' = residuals(fit),
      'Forecast errors' = residuals(fit,type='response')) %>%
  autoplot(facet=TRUE) + xlab("Year") + ylab("")
```

Note: 

* Level: estimate of the local mean or level of the data generating process
* Trend: Change between successive data points; at a particular point of time, if trend is 2, the estimated growth between two time points is 2.
* Seasonality: Deviation from the local mean due to seasonality

## Forecasting with ETS models
Point forecast obtained by iterating the equations for each time point and setting all $\varepsilon_t = 0$.

To predict international visitor nights for 2016-2017:

```{r austourists_ets_forecast}
fit %>% forecast(h = 8) %>%
    autoplot() +
  ylab("International visitor night in Australia (millions)")
```

Forecast variance expressions provided in book.


# ARIMA models

Approaches to time series forecasting:

* Exponential smoothing; based on description of the trend and seasonality in the data
* ARIMA; based on description of autocorrelations in the data. Requires understanding of stationarity and differencing techniques.


## Stationarity and differencing
**Stationary time series**: One whose properties does not depend on time at which series is observed; time series with trends, or with seasonality, are not stationary.  The following will rule out a series form being stationary:

* Seasonality
* Trends and chaning levels
* Increasing variance

Recall:  **Cyclic**; when data rises and falls but not of a  fixed frequency and may be related to an economic or business cycle.  If fluctuations are not of a fixed frequency then they are cyclic.  A time series with cyclic behaviour (but with no trend or seasonality) is stationary.      

Stationary time series can be identified using:

* Time series plot
* ACF plot
    * if time series is stationary, ACF will drop to zero relativly quickly (similarly, $r_1$ will often by large and positive)
    * If time series is non-stationary, ACF will decrease sloly

### Differencing

**Differencing**: Difference between consecutive observations

    * Help stability the mean of the time series by removing changes in the level of a time series (and therefore reduce) trend and seasonality.  In case of daily data, the difference time series gives the daily change in the observed values.
    * Makes non-stationary series stationary 
 
 
Transformations: To stability variance of a time series

### Random walk model
Differenced series 

    * Change between consecutive observations in the original series, \[y_t^\prime = y_t - y_{t-1}\]
    * May be referred to as **first differences** to distinguish from seasonal differences.
    * Will only have $T-1$ values, since not possible to calculate $y_1^\prime$ for first observation
    * If white noise, the model for the original series can be written as a random walk model
    $$\begin{aligned}
        y_t - y_{t-1} &= \varepsilon_t \\
        y_t = y_{t-1} + \varepsilon_t
    \end{aligned}$$
    
Random walk model:

    * Widely used for non-stationary data, esp in finance/economics
    * Have long periods of apparent trends (up or down)
    * Sudden and unpreditable changes in direction
    * Forecasts future values equal to last observation
    * Underpins naive forecasts
    
Closely related model in which differences have a non-zero mean:
\[y_t = y_{t-1} + \varepsilon_t + c\]

    * If $c>0$, $y_t$ will drift upwards
    * If $c < 0$, $y_t$ will drift downwards
    * Underpins drift method
    
### Second-order differencing

It may be necessary to difference data twice, in which case $y_t^{\prime \prime}$ will have $T-2$ values.  Usually not necessary to difference more than twice.
$$\begin{aligned}
y_t^{\prime \prime} &= y_t^{\prime} - y_{t-1}^{\prime \prime}\\
                    &= (y_t - y_{t-1}) - (y_{t-1} - y_{t-2})\\
                    &= y_t - 2y_{t-1} - y_{t-2}
\end{aligned}$$

### Seasonal differencing

**Seasonal difference**

* Difference between observation and previous observation from same season
\[y_t^{\prime} = y_t - y_{t-m}\]
where $m$ = number of seasons.

* Referred to as lag-$m$ differences.
* If seasonally differenced appears to be white noise, then appropriate model is:
\[y_t = y_{t-m} + \varepsilon_t\]
* Forecasts are set to last observation from same season
* Interpreted as the change between one year to the next

```{r a10_differencing}
cbind("Sales ($million)" = a10,
      "Monthly log sales" = log(a10),
      "Annual change in log sales" = diff(log(a10),12)) %>%
  autoplot(facets=TRUE) +
    xlab("Year") + ylab("") +
    ggtitle("Antidiabetic drug sales")
```
It may sometimes be necessary to take first differences of seasonal differences, e.g.;
```{r usmelec_differences}
cbind("Billion kWh" = usmelec,
      "Logs" = log(usmelec),
      "Seasonally\n differenced logs" = diff(log(usmelec),12),
      "Doubly\n differenced logs" =  diff(diff(log(usmelec),12),1)) %>%
  autoplot(facets=TRUE) +
    xlab("Year") + ylab("") +
    ggtitle("Monthly US net electricity generation")
```

Formal tests for differencing exist, but can be subjective as to which differeing to apply.

Whilst it makes no difference whether to apply seasonal or first difference first, if stat has a strong seasonal pattern, recommend to do first as it may be all that is required. 

### Unit root tests

* Used to determine, somewhat more objectively, whether differencing is required.
* Exist a number of tests with different assumptions that may lead to different examples
* Example: Kwiatkowski-Philllips-Schmidt-Shin (KPSS) test, via `ur.kpss()` function:
    H$_0$: data is stationary
    H$_a$: data is not stationary, i.e. if p < 0.05, differencing required.  
    
```{r goog_ur}
require(urca)
goog %>% ur.kpss() %>% summary()
```
Given test statistic > 1pct critical value, p-value < 0.01 and so reject null hypothesis.

```{r goog_diff_ur}
goog %>% diff() %>% ur.kpss() %>% summary()
```
Given test statistic < 10 pct critical value, p-value > 0.1 and so accept null hypothesis and conclude that differenced dat are stationary.

The `ndiffs()` function can be used to determine the appropritate number of first differences
```{r goog_ndiffs}
ndiffs(goog)
```

Similarly, the  `nsdiffs()` function can be used to deternube the appropritate number of seasonal differences
```{r usmelec_nsdiffs}
usmelec %>% log() %>% nsdiffs()
#> [1] 1
usmelec %>% log() %>% diff(lag=12) %>% ndiffs()
#> [1] 1
```

## Backshift notation ($B$)

* Useful notation when working with time series lags
* $B$ = "Back up by one time unit"
* $By_t = y_{t-1}$: $B$ operating on $y_t$ has the effect of shifting the data back one period
* $B(By_t) = B^2y_t = y_{t-2}$: $B$ operating on $y_t$ has the effect of shifting the data back two periods period
* $B^{12}y_t$: For monthly data, the same month last year
* Useful for describing the process of _differencing_
    * First order difference: \[y_t^{\prime} = y_t - y_{t-1} = y_t - By_t = (1-B)y_t \]
    * Second order difference: $$
    \begin{aligned}
    y_t^{\prime\prime} &= y_t - 2y_{t-1} - y_{t-2} \\
                                                    &= y_t - 2By_t - B^2y_t \\
                                                    &= (1-B)^2y_t
                                            \end{aligned}$$
                                            
* In general, a $d$th-order difference can be written as \[(1-B)^dy_t\]
* Useful when combining differences, as operator can be treated using ordinary algebraic rules, example: seasonal difference followed by a first difference
$$\begin{aligned}
(1-B)(1-B^m)y_t &= (1-B-B^m + B^{m+1})y_t  \\
                &= y_t - y_{t-1}  - y_{t-m}  - y_{t-m-1}
                                            \end{aligned}$$

## Autoregressive models, AR(p)

* Multiple regression: Forecast variable of interest using linear combination of predictors
* Autogregression: Forecast variable of interest using linear combination of past values of itself
* Autoregressive model of order $p$ model, **AR(p) model**,:
\[y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots +  \phi_p y_{t-p} + \varepsilon_t\]

The variance of the error term will change the scale of the series, not the patterns.  

For an AR(1) model:

    * when $\phi_1 = 0$, $y_t$ is equivalent ot white noise
    * when $\phi_1 = 1$ and $c = 0$, $y_t$ is equivalent to a random walk
    * when $\phi_1 = 1$ and $c \neq 0$, $y_t$ is equivalent to a random walk with drift
    * when $\phi_1 < 0$, $y_t$ tends to oscillate between positive and negative values
    
Autoregressive models generally restricted to stationary data; following restrictions required:

    * AR(1): $|\phi_1| < 1$
    * AR(2): $\phi_2|< 1$, $\phi_2 + \phi_1 < 1$, $\phi_2 - \phi_1 < 1$
    * AR(3) and onwards: complicated!  Let R deal with it.


## Moving average models, MA(q)

* Uses forecast errors in a regression-like model
* Moving average of order $q$: 
\[y_t = c + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots +  \theta_q \varepsilon_{t-q} \]

* Not a regression in the usual sense as the values of $\epsilon_t$ are not observed.
* Each value of $y_t$ ~ weighted moving average of the past few forecast errors
* MA(q) $\neq$  moving average smoothing (`ma` function); MA(q) to forecast future values, while moving average smoothing used to estimate trend-cycle of past values.


Any AR($p$) model can be written as an MA($\infty$) model
$$\begin{aligned}
y_t &= \phi_1 y_{t-1} + \varepsilon_t \\
    &= \phi_1(\phi_1 y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t\\
    &= \phi_1^2 y_{t-2} + \phi_1\varepsilon_{t-1} + \varepsilon_t\\
    &= \phi_1^3 y_{t-3} + \phi_1^2 \varepsilon_{t-2} + \phi_1\varepsilon_{t-1} + \varepsilon_t\\
    \text{etc.}
\end{aligned}$$

Provided $-1 < \phi_1 < 1$, $\phi_1^k \rightarrow 0$ as $k$ gets largers, such that 
\[y_t = \varepsilon_t + \phi_1 \varepsilon_{t-1} + \phi_1^2 \varepsilon_{t-2} + \phi_1^3 \varepsilon_{t-3} + \ldots\]
which is an MA($\infty$) process

If certain constraints imposed on the MA parameters:

    * MA is also invertible; i.e. can write MA($q$) as AR($\infty$)
    * MA model will be mathematically _desirable_

Invertibility constraints (similar to stationarity constraints of AR models):

    * MA(1): $|\theta_1| < 1$,
    * MA(2): $|\theta_2| < 1$,  $\theta_2 + \theta_1 < 1$, $\theta_2 - \theta_1 < 1$
    * MA(3) and onwards: complicated!  Let R deal with it
    
Reasoning behind constraints:  The invertible MA(1) process can be written as AR($\infty$) 
\[\varepsilon_t = \sum_{j=1}^{\infty}(-\theta)^j y_{t-j}\]

* Nonsensical! $|\theta| > 1$: more distant observations will have greater influence on current error
* Nonsensical! $|\theta| = 1$: distant and recent observations will have the same influence on current error
* Sensible! $|\theta| < 1$: more recent observations will have greater influence on current error



## Non-seasonal ARIMA(p,d,q) models

Non-seasonal ARIMA model:
* AutoRegressive Integrated Moving Average
* AR(p) + Differencing(d) + MA(q)
* Integration = antonym of differencing; a time series which needs to be differenced to be made stationary is said to be an "integrated" version of a stationary series
* Model written as:
\[
y_t^\prime = c + \phi_1 y_{t-1}^\prime + \ldots + \phi_p y_{t-p} +
                + \theta_1 \varepsilon_{t-1} +  \ldots +  \theta_q \varepsilon_{t-q}
                + \varepsilon_t \]
where $y_t^\prime$ is the differenced series and may have been differenced mroe than once and 

* $p$ = order of the autoregressive part (lagged values of $y_t$ as predictors)
* $d$ = degree of first differencing
* $q$ = order of the moving average part (lagged errors as predictors)

Random-walk and random-trend models, autoregressive models, and exponential smoothing models are all special cases of ARIMA models:

-----------------------|---------------------------------
White noise             | ARIMA(0, 0, 0)
Random Walk             | ARIMA(0, 1, 0) with no constant
Random Walk with drift  | ARIMA(0, 1, 0) with a constant
Autoregression          | ARIMA($p$, 0, 0)
Moving average          | ARIMA(0, 0, $q$)

Combining components to form more complicated models easier when working with backshift ntation.

* AR($p$): 



### US consumption expenditure
### Understanding ARIMA models
### ACF and PACF plots

## Estimation and order selection
### Maximum likelihood estimation
### Information Criteria

## ARIMA modelling in R
### How does `auto.arima()` work?
### Choosing your own model
### Modelling procedure
### Example: Seasonally adjusted electrical equipment orders
### Understanding constants in R

## Forecasting
### Point forecasts
### Prediction intervals

## Seasonal ARIMA models
### ACF/PACF
### Example: European quarterly retail trade
### Example: Corticosteroid drug sales in Australia


## ARIMA vs ETS
### Example: Comparing auto.arima() and ets() on non-seasonal data
### Example: Comparing auto.arima() and ets() on seasonal data



# Dynamic regression models
## Estimation
## Regression and ARIMA errors in R
## Forecasting
## Stochastic and deterministic trends
## Lagged predictors

# Forecasting hierarchical or grouped time series
## Hierarchical time sereis
## Grouped time series
## The bottom-up approach
## Top-down approaches
## Middle-out approaches
## Mapping matrices
## The optimal reconciliation approach

# Advanced forecasting methods
## Complex seasonality
## Vector autoregressions
## Neural network models
## Bootstrapping and bagging

# Some practical forecasting issues
## Weekly, daily and sub-daily data
## Time series of counts
## Ensuring forecasts say within limits
## Forecast combinations
## Prediction intervals for aggregates
## Backcasting
## Very long and very short time series
## Forecasting on training and test sets
## Dealing with missing vlues and other outliers






    
    




```{r}
 #plot(x <- stl(log(co2), s.window = "per", t.window = 1000))
```

