---
title: 'Book: Forecasting Principles and Practice'
author: "Marie"
date: '2018-08-20'
categories: Study-Notes
slug: book-forecasting-principles
tags:
- Book
- Study-Notes
banner: img/banners/hyndman.png
---

This post consists of my notes made while reading the book: [Forecasting: Principles and Practice](https://otexts.org/fpp2/) by Rob J Hyndman and George Athanasopoulos.

# Getting Started

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 3, fig.width = 6)

require(tidyverse)
require(fpp2)
require(GGally)
require(gridExtra)
require(knitr)
require(lubridate)
```


## What can be forecast?

Predictability increases with:

* Understanding of contributing factors
* Data availability
* Immunity of things we are trying to forecast on the forecasts (self-fulfilling forecasts)

Good forecasts:

* capture genuine patterns but do not replicate past events that will not occur again.
* will capture the way in which things are changing and so will work in a changing environment (assuming that the way in which the environment is changing will continue into the future)

Forecasting methods complexity:

* Simple, e.g. naive method which uses most recent observation as a forecast
* Complex, e.g. neural nets and econometric systems of simultaneous equations

## Forecasting, planning and goals

* Forecast: Predict future based on historical data and knowledge of future events that might impact the forecasts
* Goals: What you would like to happen, should be linked to forecasts and plans
* Planning: Response to forecast and goals; actions required to make forecasts match goals


Horizon:

* Short-term forecasts: personnel, production and transportation
* Medium-term forecasts: future resource requirements
* Long-term forecasts: Strategic planning

## Determining what to forecast

Step One: 

* What to forecast
* Forecasting horizon
* Frequency required
* How will they be used

Step Two: 

* Find or collect data

## Forecasting data and methods

Method Types:

* Qualitative
    * not purely guesswork
    * There exists well-developed structured approaches
    * Use when no relevant data available
    
* Quantitative
    * When historical data available and reasonable to assume that some aspects of past pattern will continue into the future
    
### Time series forecasting

**Time Series**: anything observed sequentially over time; can be observed at regular or irregular periods of time (this book only considers regularly spaced time series)

* May consider only variable to be forecast (and make no attempt to discover factors that affect its behaviour) and extrapolate trend and seasonal patterns

* Models include decomposition, exponential smoothing and ARIMA

### Predictor variables and time series forecasting

**Explanatory model**: model with predictor variables (but does not consider time)
\[\text{ED} = f(\text{current temperature}, \text{strength of economy}, \text{population}, \text{time of day}, \text{day of week}, \text{error})\]

**Time series model**: model based on time series,
\[\text{ED}_{t+1} = f(\text{ED}_{t}, \text{ED}_{t-1}, \text{ED}_{t-3}, \ldots, \text{error})\]

Reasons for use over other two types of models:

* System may not be understood 
* Necessary to forecast future values of predictors to forecast variable of interest
* May only need to predict what will happen, not why
* May be more accurate than explanatory or mixed models

**Mixed Models** aka **dynamic regression model**, **panel data model**, **longitudinal model**, **transfer function model**, **linear system model**

* Incorporate explanatory variables and time series data
\[\text{ED} = f(\text{ED}_{t}, \text{current temperature}, \text{time of day}, \text{day of week}, \text{error})\]

## The basic steps in a forecasting task

### 1. Problem definition
### 2. Gathering information

* Statistical data
* Accumulated expertise of people who collect the data and use the forecasts

### 3. Preliminary (exploratory) analysis

* Are there consistent patterns
* Is there significant trend?
* Is seasonality important
* Are there business cycles present?
* Are there any outliers?
* How strong are the relationships among the variables available for the analysis

### 4. Choosing and fitting models

Best model = f(availability of historical data, relationship between forecast variable and explanatory variables, intended use of forecasts)

### 5. Using and evaluating a forecasting model
When using a forecasting model, may have to deal with  missing values, outliers or short time series


## The statistical forecasting perspective

* Forecast variable = random variable
* Uncertainty of its value increases as forecast horizon increases
* Generally publish point forecasts with prediction interval (range of values the RV could take with X% probability)

Notation:

* $t$: time
* $y_t$: observation at time $t$
* $I$: Observed information
* $\hat{y}_t$: average value of the forecast distribution, could occasionally refer to the median
* $\hat{y}_{t|t-1}$: forecast of $y_t$ taking into account all previous observations($y_1, \ldots, y_{y-1}$)
* $\hat{y}_{T+h|T}$: forecast of $y_{T+h}$ taking into account $y_1, \ldots, y_{T}$ ($h$-step forecast)

# TIme series graphics

## `ts` objects

* If you have annual data, with one observation per year, you only need to provide the starting or ending year

```{r ts_annual}
y <- ts(c(123,39,78,52,110), start=2012)
y
```
If you have observations that are more frequent than once a year, you add a `frequency` argument.  For example, with monthly data set `frequency = 12`.

### Frequency of a time series

When using the `ts()` function, the following choices should be used:

Data        | `frequency`
------------| -----------
Annual      | 1
Quarterly   | 4
Monthly     | 12
Weekly      | 52

For data collected more frequently, there are more options:

Data | Seasonality     | `frequency`
-----|-----------------| -----------
Daily | Weekly      | 7
Daily | Annual      | 365.25
Minutes | Hourly    | 60
Minutes | Daily     | 1440
Minutes | Weekly    | 10080
Minutes | Annual    | 525960

(Chapter 11 will consider multiple seasonality without having to choose just one of the frequencies)


## Time plots

`autoplot`: produced appropriate plot based on data, e.g. if data is a time series it will produce a time plot


Weekly economy passenger load: 
```{r autoplot_melsyd}
autoplot(melsyd[,"Economy.Class"]) +
  ggtitle("Economy class passengers: Melbourne-Sydney") +
  xlab("Year") +
  ylab("Thousands")
```

Features that will need to be considered:

* Period in 1989 with no passengers (industrial dispute)
* Reduced load in period in 1989 (trial replacing economy seats with business)
* Large increase in second half of 1991
* Dips at start of each year
* Long-term fluctuation (increase - decrease - increase)
* Missing observations


Monthly anti-diabetic drug sale data:  
```{r autoplot_a10}
autoplot(a10) +
  ggtitle("Antidiabetic drug sales") +
  ylab("$ million") +
  xlab("Year")
```

Features:

* Seasonality
* Trend is changing
* Sudden drop at start of each year (more cost effective for patients to stockpile at the end of the calendar year)
* Seasonal pattern increases in size as level of series increases


The time series can be aggregated using the `aggregate` function.
```{r autoplot_a10_annual}
# Introductory time series
a10.annual <- aggregate(a10, FUN = sum)
autoplot(a10.annual)  +
  ggtitle("Annual Antidiabetic drug sales") +
    geom_point() + 
  ylab("$ million") +
  xlab("Year") 
```


## Time series patterns

* **Trend**; refers to a long-term increase or decrease in the data, which can change over time (i.e. trend may be _changing direction_)
* **Seasonal**; a seasonal pattern occurs when a time series is affected by seasonal factors, i.e. time of year.  Seasonality is of a fixed and known frequency.  If frequency of fluctuations is unchanging and associated with some aspect of the calendar then the pattern is seasonal.
* **Cyclic**; when data rises and falls but not of a  fixed frequency and may be related to an economic or business cycle.  If fluctuations are not of a fixed frequency then they are cyclic.  

## Seasonal plots

Similar to a `ts` plot but data is plotted against seasons in which data observed.

* Highlights seasonal patterns

```{r a10_seasonplot}

ggseasonplot(a10, year.labels=TRUE, year.labels.left=TRUE) +
  ylab("$ million") +
  ggtitle("Seasonal plot: antidiabetic drug sales")

```

Polar seasonal plot makes the time series axis circular rather than horizontal

```{r a10_polarseasonalplot}
ggseasonplot(a10, polar=TRUE) +
  ylab("$ million") +
  ggtitle("Polar seasonal plot: antidiabetic drug sales")
```


## Seasonal subseries plots
Instead can show each season as a separate mini-time plot

* Horizontal lines used to indicate the mean 
* May enable underlying seasonal pattern to be seen more clearly

```{r a10_subseriesplot}
ggsubseriesplot(a10) +
  ylab("$ million") +
  ggtitle("Seasonal subseries plot: antidiabetic drug sales")
```


## Scatterplots

Useful for showing relationships between time series

```{r elecdemand_auto}
autoplot(elecdemand[,c("Demand","Temperature")], facets=TRUE) +
  xlab("Year: 2014") + ylab("") +
  ggtitle("Half-hourly electricity demand: Victoria, Australia")
```

The relationship between temperature and electrical demand is as follows:
```{r elecdemand_qplot}
qplot(Temperature, Demand, data=as.data.frame(elecdemand)) +
  ylab("Demand (GW)") + xlab("Temperature (Celsius)")
```

### Correlation


The correlation coefficient, \[r = \frac{\sum(x_t - \bar{x})(y_t - \bar{y})}{\sqrt{\sum(x_t - \bar{x})^2}\sqrt{\sum(y_t - \bar{y})^2}},\] which lies between $\pm$ 1 measures the strength of __linear__ relationship, and so can sometimes be misleading.  See for example Anscombe's Quartet in which each of the four date sets have a correlation coefficient of 0.82.  Anscombe's Quartet is a good illustration of why not to solely rely on correlation values.
```{r anscombes_quartet}

dat <- select(anscombe, x1:x4) %>% gather(variable, X) %>% mutate(variable = str_replace(variable, "x", "")) %>%
    cbind(
select(anscombe, y1:y4) %>% gather(variable, Y) %>% select(-variable))


    ggplot(dat) + 
    geom_point(aes(X, Y)) + 
    facet_wrap(~ variable)


```


### Scatterplot matrices

```{r visnights_ggpairs, message=FALSE, warning=FALSE}
GGally::ggpairs(as.data.frame(visnights[,1:5]))
```


## Lag plots

Scatterplot matrix with:

* X-axis = Lagged values of the time series ($y_{t - k}$)
* Y-axis = Current value of the time series ($y_{t}$)
* One chart per value of $k$
* Lines connect points in chronological order


Quarterly beer data: 
```{r ausbeer_lagplot}
beer2 <- window(ausbeer, start=1992)
gglagplot(beer2)
```
Here:

* Strong positive relationship at lags 4 and 8 (strong quarterly seasonality)
* Negative relationship for lags 2 and 6 because peaks (Q4) plotted against troughs (Q2)

Note use of `window()` function to extract a portion of the time series.


## Autocorrelation

Similar to correlation which measures linear relationship between two variables, autocorrelation measures linear relationship between __lagged values__ of a time series.  For lag $k$, the autocorrelation coefficient is 
\[r_k = \frac{\sum_{t = k + 1}^T(y_t - \bar{y})(y_{t-k} - \bar{y})}{\sum_{t = 1}^T(y_t - \bar{y})^2},\] where $T$ is the length of the time series.

Noting that: 

* $r_1$ measures the relationship between $y_t$ and $y_{t-1}$
* $r_2$ measures the relationship between $y_t$ and $y_{t-2}$
* and so on ... 

A correllogram plot is used to show the autocorrelation function (ACF) by plotting the autocorrelation coefficients.

```{r beer2_acf}
ggAcf(beer2)
```
Features:

* $r_4$ is higher than for other lags; peaks four quarters apart, troughs 2 quarters apart
* $r_2$ more negative than for other lags because troughs tend to be 2 quarters behind beaks
* Blue lines indicate whether correlations are significantly different from zero.

### Trend and seasonality in ACF plots

When data has a trend: Autocorrelations for small lags tend to be large and positive because observations nearby in time are nearby in size.  So ACF of trended time series tend to have positive values that slowly decrease as the lags increase.

When data are seasonal:  Autocorrelations are larger for the seasonal lags (at multiples of the seasonal frequency) than for other lags.  

When data are trended and seasonal: Combination of effects is seen, e.g. monthly Australian electricity demand:

```{r elec_auto}
aelec <- window(elec, start=1980)
autoplot(aelec) + xlab("Year") + ylab("GWh")
```
```{r elect_acf}
ggAcf(aelec, lag=48)
```

Features: 

* Trend - The slow decrease in the ACF as the lags increase
* Seasonality - "Scalloped" shape


## White noise

**White noise**:  Time series that show no autocorrelation.
```{r wn_auto}
set.seed(30)
y <- ts(rnorm(50))
autoplot(y) + ggtitle("White noise")
```
```{r wn_acf}
ggAcf(y)
```
White noise ACF features:

* each autocorrelation ~ 0
* Expect 95% of spikes in ACF to lie within $\pm 2 / \sqrt{T}$.  If $\geq$ one large spikes outside bounds, or if substantially more than 5% of spikes are outside of these bounds then series is probably not white noise.

In this chart, $T$ = `r length(y)` and so the bounds are $\pm 2 / \sqrt{50}$ = $\pm$ `r round(2/sqrt(length(y)),2)`.  As all correlation coefficients lie within these limits, ACF confirms that data are white noise.

# The forecaster's toolbox

## Some simple forecasting methods

### Average method
Set all forecasts to average of historical data,

\[\hat{y}_{T+h|T} = \bar{y} = (y_1 + \ldots + y_T)/T,\]
where $\hat{y}_{T+h|T}$ is short-hand for the estimate of \hat{y}_{T+h} based on $y_1, \ldots, y_t$.

Use function `meanf(y, h)` for time series $y$ and forecast horizon $h$.

### Naive method (a.k.a. random walk forecast)

Set all forecasts to last observed value, 
\[\hat{y}_{T+h|T} = y_T\]

Use function `naive(y,h)` or `rwf(y,h)`.

This method is optimal when data follow a random walk.

### Seasonal naive method

Set forecast to last observed value from the same season of the year.
\[\hat{y}_{T+h|T} = y_{T+h-m(k+1)},\]

where $m$ = seasonal period, $k$ = number of complete periods in the forecast period prior to time $T + h$.

Use function `snaive(y, h)`.

### Drift method

Set forecast to last observation plus drift (average increase or decrease that has been seen over time).  Equivalent to drawing a line between first and last observations and extrapolating it into the future, \[\hat{y}_{T+h|T} = y_{T} + \frac{h}{T-1}\sum^T_{t=2}(t_t - t_{t-1}) = y_T + h\left(\frac{y_T - y_1}{T - 1}\right)\]

Use function `rwf(y, h, drift = TRUE)`.

### Examples

```{r beer_simple_forecasts, message=FALSE, warning=FALSE}
# Set training data from 1992 to 2007
beer2 <- window(ausbeer,start=1992,end=c(2007,4))

# Plot some forecasts
autoplot(beer2) +
    geom_point() + 
    scale_x_continuous(minor_breaks = 1992:2007) + 
    theme(panel.grid.minor = element_line()) + 
  autolayer(meanf(beer2, h=11),
    series="Mean", PI=FALSE) +
  autolayer(naive(beer2, h=11),
    series="Naïve", PI=FALSE) +
  autolayer(snaive(beer2, h=11),
    series="Seasonal naïve", PI=FALSE) +
  ggtitle("Forecasts for quarterly beer production") +
  xlab("Year") + ylab("Megalitres") +
  guides(colour=guide_legend(title="Forecast"))
```

```{r ggog200_simple_forecasts}
autoplot(goog200) +
  autolayer(meanf(goog200, h=40),
    series="Mean", PI=FALSE) +
  autolayer(rwf(goog200, h=40),
    series="Naïve", PI=FALSE) +
  autolayer(rwf(goog200, drift=TRUE, h=40),
    series="Drift", PI=FALSE) +
  ggtitle("Google stock (daily ending 6 Dec 2013)") +
  xlab("Day") + ylab("Closing Price (US$)") +
  guides(colour=guide_legend(title="Forecast"))
```
Simple methods may:

* Be best forecasting method available
* Serve as a benchmark for other methods

## Transformations and adjustments

Purpose: Simplify historical data patterns by removing known sources of variation or by making pattern more consistent across whole data set.  Simpler pattern = more accurate forecast.

### Calendar adjustments

EG: Different numbers of days in months can lead to variation in monthly values.  In this case divide TS by `monthdays()` to get TS of daily average.

```{r cal_adj}
dframe <- cbind(Monthly = milk,
                DailyAverage = milk/monthdays(milk))
  autoplot(dframe, facet=TRUE) +
    xlab("Years") + ylab("Pounds") +
    ggtitle("Milk production per cow")
```


### Population adjustments

Data affected by population changes can be adjusted to give per-capita data, i.e. consider data per person (or per thousand, or per million people) rather than the total.  E.g. user beds per thousand people rather than number of hospital beds in a particular region over time.

### Inflation adjustments
Use price index (e.g. CPI) to adjust data affected by the value of money before model to all be in dollar values from a particular year.  

E.g. adjusted price at year 2000 dollar values is $x_t = y_t / z_t * z_{2000}$ where 

* $z_t$ is price index,
* $y_t$ original house price in year $t$

### Mathematical transformation
Transform data (e.g. with logarithmic transformation) when data variation increases or decreases with level of series.  

#### Log transformation
Interpretation of log base 10: An increase of 1 on the log scale corresponds to a multiplication of 10 on the original scale. Another advantage is that they constrain forecasts to positive.

Redacted extracts from [Gerard E. Dallal](http://www.jerrydallal.com/lhsp/logs.htm):
>if you multiply something by 10 in the original scale, you add 1 unit to its value the log scale. If you divide something by 10 in the original scale, you subtract 1 unit from its value in the log scale. As we move from 0.1 to 1 to 10 on the original scale, we move from -1 to 0 to 1 on the logarithmic scale.

>Just as length can be measured in feet, inches, meters, kilometers, centimeters, or whatever, logarithms can be defined in may ways according to what happens in the original scale when there is a one unit change in the log scale. Common logs are defined so that a 1 unit increase in the log scale is equivalent to multiplying by 10 in the original scale. One could define logarithms so that a one unit increase in the log scale is equivalent to multiplying by 2 in the original scale (logs to base 2). The value by which a number is multiplied in the original scale when its logarithm is increased by 1 is known as the base of the logarithm. Any positive number different from 1 can be used as a base.

>Natural logarithms: A 1 unit increase in this log scale is equivalent to multiplying in the original scale by a factor known as Euler's constant, $e$ (~ 2.71828). ... If you apply any logarithmic transformation to a set of data, the average of the logs is approximately equal to the log of the original mean, whatever type of logarithms you use. However, only for natural logs is standard deviation approximately equal to the coefficient of variation (the ratio of the standard deviation to the mean) in the original scale.

>The different types of logs are like different units for measuring height. For example, a height of 71 inches might be referring to an adult male, but a height of 71 cm would probably be referring to a toddler. Similarly, you can't report a logarithm of 2. Is it the common log corresponding to a value of 100 in the original scale or a natural log corresponding to a value of 7.39?

Extract from [Notes on logarithms - Tony Gardner-Medwin](http://www.ucl.ac.uk/lapt/med/logs.htm) regarding logs to base e: 
> The way to think about natural logs is that they are the same as logs to any other base, but measured in different units.  It is always true that \[ln(x)   =  2.303  log_{10}(x)\]

>Logarithms to base e are considered "natural" because:  As $x$ becomes very small,  $log_e(1+x)$  becomes equal to $x$.  The equivalent statement for  $log_{10}(x)$ would be As $x$ becomes very small,  $log_{10}(1+x)$  becomes equal to 2.303 $x$. 

#### Power tranformations
Of form $w_t = y_t^p$, includes square roots and cube roots.  Not as interpretable as log transformation.

#### Box-Cox transformation
Includes logarithms and power transformation.\[w_t = \begin{cases} log(y_t) & \text{if } \lambda = 0;\\
(y_t^\lambda - 1) / \lambda & \text{otherwise} \end{cases}\]

* If $\lambda = 0$, natural (i.e. not base 10!) logarithm is used
* If $\lambda \neq 0$, power transform used
* If $\lambda = 1$, then $w_t = y_t - 1$, so data is shifted downwards, but no change in shape
* If $\lambda \neq 1$, then shape will change


As per online book, a slider for $\lambda$ is a great way of determining which value makes the size of seasonal variation constant across the series.  Alternative, you can use the `BoxCox.lambda()` function.

```{r elec_boxcox}

p1 <- autoplot(elec)
p2 <- autoplot(BoxCox(elec, BoxCox.lambda(elec)))
grid.arrange(p1, p2)
rm(p1, p2)
```

To back-transform the forecasts on the original scale 
\[y_t = \begin{cases} \exp(w_t) & \text{if } \lambda = 0 \\
            (\lambda w_t + 1)^{1/\lambda} &\text{otherwise}\end{cases}\]



#### Features of power transformations

* If some observations have a value $\leq 0$, will need to adjust all observations by adding a constant to all values; otherwise no power transformation is possible
*  A simple value of $\lambda$ makes explanations easier
* Transformation sometimes make little difference to the forecasts but have a large effect on the prediction intervals
* Often no transformation is needed


#### Bias adjustments

* When using mathematical transformation such as the Box-Cox transformation, often the back-transformed forecast will not be the mean, but rather the median, of the forecast distribution.  When want to add up sales forecasts to get a total, the means will add up but medians will not.  Back-transformed mean of Box-Cox transformed data is:

\[y_t = \begin{cases} \exp(w_t) \left[1 + \frac{\sigma^2_h}{2}\right]& \text{if } \lambda = 0 \\
            (\lambda w_t + 1)^{1/\lambda} \left[1 + \frac{\sigma^2_h(1-\lambda)}{2(\lambda w_t + 1)^2}\right] &\text{otherwise}\end{cases}\]

where $\sigma_h^2$ is the $h$-step variance.  The larger the forecast variance, the bigger the bias (difference between the mean and the median).  When using the mean, rather then median, the point forecast is said to have been **bias-adjsted**.  

Bias-adjustment is not the default in the `forecast` package; to implement use the argument `biasadj = TRUE`.




```{r egg_bias_adj_fo}
# Drift forecast with log transformation
fc <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80)
# ... now with bias adjustd forecasts
fc2 <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80,
  biasadj=TRUE)
autoplot(eggs) +
  autolayer(fc, series="Simple back transformation") +
  autolayer(fc2, series="Bias adjusted", PI=FALSE) +
  guides(colour=guide_legend(title="Forecast"))
rm(fc, fc2)

```


## Residual diagnostics

### Fitted values

Fitted value $\hat{y}_{t|t-1}$

* Observation in TS forecast (e.g. at time $t$) using previous observations $y_1, \ldots, y_{t-1}$
* Always involve one-step forecasts
* Not true forecast because parameters in forecast method are estimated using all observations in TS, including future obs

Fitted values for simple methods:

* Average method: $\hat{y} = \hat{c}$ where $\hat{c}$ = average over all observations including those after $t$.
* Drift method:  $\hat{y} = y_{t-1} + \hat{c}$ where $\hat{c} = (y_T - y_1)/(T-1)$
* Naive: Fitted value = true forecast as no parameters used
* Naive seasonal: Fitted value = true forecast as no parameters used

### Residuals

\[e_t = y_t - \hat{y}_t\]

Good model will have:

* Uncorrelated residuals; if there are correlations, then there is information left in the residuals which should be used in computing forecasts
* Zero mean residuals; if residuals have a mean other than zero, then forecasts are biased

Without these properties, method can be improved, noting though, that methods with these properties may still be __improve-able__, i.e. these properties are useful in determining whether all available information is being used but not for selecting a forecasting method.

To fix bias in residuals with constant mean $m$, then simply add $m$ to all forecasts.  Fixing correlation, will be addressed later...

It is also useful (but not essential) for calculation of prediction intervals if:

* Residuals have constant variance
* Residuals are normally distributed

May be possible to apply Box-Cos transformation, but improvement is not always possible; sometimes little that can be done except to use an alternative approach for obtaining the prediction intervals (to be discussed later).

### Example



TS of residuals; large residual result of unexpected price jump. 
```{r goog200_res_TS}
res <- residuals(naive(goog200))
autoplot(res) + xlab("Day") + ylab("") +
  ggtitle("Residuals from naïve method")
```

Histogram of residuals; right tail a little long for normal distribution
```{r goog200_res_Hist, message=FALSE, warning=FALSE}
gghistogram(res) +
  ggtitle("Histogram of Residuals")
```

ACF of residuals; lack of correlation suggests forecasts are good
```{r goog200_res_acf}
ggAcf(res) + ggtitle("ACF of residuals")
```

Summary: forecasts appear to account for all information.  Forecasts will probably be quite good but prediction intervals assuming a normal distribution may be inaccurate. 

* Mean of residuals close to zero 
* No signification correlation in residuals series
* Variation in residuals appears constant, with exception of one outlier (from time series)
* Residuals may not be normal (tail is long even when outlier is ignored)

### Portmanteau tests for autocorrelation

ACF plot involves checking that each spike is within required limits and therefore implicitly carrying out multiple hypothesis tests, each one with a small probability of giving a false positive.  With enough tests, likely that at least one will give a false positive, leading to the incorrect conclusion that residuals have some remaining autocorrelation

Portmanteau tests performs test for a group of autocorrelations by determining whether the first $h$ autocorrelations are significantly different from expected of a white noise process.

In the following tests, if the autocorrelations come from a white noise series, then the statistic would have a $\chi^2$ distribution with ($h - K$) degrees of freedom, where $K$ is the number of parameters in the model.  If they are calculated from raw data (rather than the residual from a model), then set $K = 0$.

#### Box-Pierce Test

Statistic \[Q = T \sigma_{k=1}^hr_k^2,\]

where 

* $h$ is maximum lag (suggest $h = 10$ for non-seasonal data and $h = 2m$ for seasonal data, where $m$ is the period of seasonality.  But if these values are larger than $T/5$ then use $h = T/5$ as the test is not good for large $h$.)
* $T$ is number of observations

If each $r_k$ is close to zero, $Q$ will be small.


#### Ljung-Box Test

More accurate than Box-Pierce test,
\[Q^* = T(T + 2) \sigma_{k=1}^h (T - k)^{-1}r_k^2,\]

where large $Q^*$ suggests that autocorrelations did not come from a white noise process. 

#### Example

In following example, both $Q$ and $Q^*$ are not significant and so conclude that residuals are not distinguishable from a white noise series.

```{r goog200_portmanteau}
res <- residuals(naive(goog200))

# Box-Pierce test
# lag=h and fitdf=K
Box.test(res, lag=10, fitdf=0)

# Box-Ljung test
Box.test(res,lag=10, fitdf=0, type="Lj")
```

The function `checkresiduals()` can be used to complete all components of residual checking (i.e. to create the time plot, ACF, histogram and perform the Ljung-Box test) 
```{r goog200_checkres}
checkresiduals(naive(goog200))
```


## Evaluating forecast accuracy

### Training and test sets

* Residuals not reliable indication of how large true forecast errors likely to be.
* Need to test forecast on new data.
* Best to separate data into training (in-sample data) and test data (hold-out set, out-of-sample data).  Test data ~20% of sample, but f(sample size, how long ahead you want to forecast)

Notes:
* Model that fits training data well may not forecast well
* Over-fitting is as bad as failing to identify a systematic pattern in the data



### Functions to subset a time series
Can use following functions:

* `window()`

```{r subset_window, eval=TRUE, include=FALSE}
window(ausbeer, start=1995)
```


* `subset()`

```{r subset_subset, eval=TRUE, include=FALSE}
subset(ausbeer, start=length(ausbeer)-4*5)
subset(ausbeer, quarter = 1)
```


### Forecast errors

* Difference between observed value and forecast
* Not a mistake; unpredictable part of observation

\[e_{T + h} = y_{T + h} - \hat{y}_{T + h|T},\]
where training data given by ${y_1, \ldots, y_T}$ and test data given by ${y_{T+1}, y_{T+2} \ldots}$

Different to residuals

* Residuals calculated on training set, forecast errors on test set
* Residuals are one-step forecasts, forecast errors can involve multi-step forecasts

Forecast accuracy can summarise forecast errors in different ways.

### Scale-dependent errors

* Forecast errors are on the same scale as the data
* Accuracy measures that are based only on $e_t$ are therefore scale-dependent and cannot be used to make comparisons between series that involve different units
* Measures include:
    * Mean absolute error: \[\text{MAE} = \text{mean}(|e_t|)\]
    Forecast that minimizes MAE will lead to forecasts of the median.
    * Root mean squared error: 
   \[\text{RMSE} = \sqrt{\text{mean}(e_t^2)}\]  Forecast that minimised RMSE will lead to forecasts of the mean (and therefore more widely used than MAE).
    


### Percentage errors

Unlike MAE and RMSE (and other scale-dependent errors), percentage errors are unit-free and so frequently used to compare forecast performances between data sets.  

Percentage error is $p_t = 100 e_t / y_t$.

#### Mean absolute percentage error (MAPE)
\[\text{MAPE} = \text{mean}(|p_t|)\]

Disadvantages:

* Infinite or undefined if $y_t = 0$ for any $t$ in period of interest
* Extreme values if $y_t$ is close to zero
* Assume unit of measurement has a meaningful zero, i.e., make no sense when measuring temperature.
* Place heavier penalty on negative errors than on positive errors

#### Symmetric MAPE (sMAPE)

This measure is **not recommended** despite being widely used.

\[\text{sMAPE} = \text{mean}(200|y_t - \hat{y}_t| / (y_t + \hat{y}_t))\]

Disadvantages:

* Still unstable as if $y_t$ is close to zero, $\hat{y}_t$ also likely to be close to zero and therefore measure still involves division by number close to zero.
* sMAPE can be negative no not really a measure of absolute percentage error


### Scaled errors

Alternative to percentage errors to compare forecast accuracy across series with different units, in which the denominator is the average error of the naive forecast (on the training data).  As both the numerator and denominator are in the original data scale, the ratio is independent of the scale of the data.  
\[q_j = \frac{e_j}{\frac{1}{T-1}\sum_{t=2}^T|y_t - y_{t-1}|}\]


* A scaled error is less than one if it arises from a better forecast than the average naive forecast computed on the training data
* A scaled error is more than one if the forecast is worse than the average naive forecast computed on the training data

For seasonal time series, the scaled error can be defined using seasonal naive forecasts:
\[q_j = \frac{e_j}{\frac{1}{T-m}\sum_{t=m + 1}^T|y_t - y_{t-m}|}\]

The mean absolute scaled error is \[\text{MASE} = \text{mean}(|q_j|)\]

### Examples

#### Example 1
```{r beer_forecasts}
beer2 <- window(ausbeer,start=1992,end=c(2007,4))
beerfit1 <- meanf(beer2,h=10)
beerfit2 <- rwf(beer2,h=10)
beerfit3 <- snaive(beer2,h=10)
autoplot(window(ausbeer, start=1992)) +
  autolayer(beerfit1, series="Mean", PI=FALSE) +
  autolayer(beerfit2, series="Naïve", PI=FALSE) +
  autolayer(beerfit3, series="Seasonal naïve", PI=FALSE) +
  xlab("Year") + ylab("Megalitres") +
  ggtitle("Forecasts for quarterly beer production") +
  guides(colour=guide_legend(title="Forecast"))
```
```{r beer_forecast_accuracy}
beer3 <- window(ausbeer, start=2008)

cbind(Method = c("Mean", "Naive", "Seasonal Naive"), as.data.frame(rbind(
    accuracy(beerfit1, beer3)[2,], 
    accuracy(beerfit2, beer3)[2,], 
    accuracy(beerfit3, beer3)[2,]) )) %>%
    select(Method, RMSE, MAE, MAPE, MASE) %>% kable(digits = 2)

```
Regardless of accuracy measure used, seasonal naive method performs best (which is also obvious from the TS plot)

#### Example 2 (non-seasonal example)
```{r goog200_forecasts}
googfc1 <- meanf(goog200, h=40)
googfc2 <- rwf(goog200, h=40)
googfc3 <- rwf(goog200, drift=TRUE, h=40)
autoplot(subset(goog, end = 240)) +
  autolayer(googfc1, PI=FALSE, series="Mean") +
  autolayer(googfc2, PI=FALSE, series="Naïve") +
  autolayer(googfc3, PI=FALSE, series="Drift") +
  xlab("Day") + ylab("Closing Price (US$)") +
  ggtitle("Google stock price (daily ending 6 Dec 13)") +
  guides(colour=guide_legend(title="Forecast"))
```
```{r goog200_accuracy}
googtest <- window(goog, start=201, end=240)
cbind(Method = c("Mean", "Naive", "Drift"), as.data.frame(rbind(
    accuracy(googfc1, googtest)[2,], 
    accuracy(googfc2, googtest)[2,], 
    accuracy(googfc3, googtest)[2,]) )) %>%
    select(Method, RMSE, MAE, MAPE, MASE) %>% kable(digits = 2)

```
In this case, drift method performs better (regardless of accuracy measure used)

### Time series cross-validation

* Series of test sets each with only a single observation.  Corresponding training sets consists of all observations prior to the observation that forms the test set.    
* The earliest observations are not treated as test sets.
* Forecast accuracy computed by averaging over test sets (a.k.a evaluation on a rolling forecasting origin as origin at which forecast is based rolls forward in time)
* Can be used for one-step and multi-step forecasts.
* Implemented using `tsCV()` function

Following example computes residual (training data) RSME and RMSE with forecasts (test data) obtained via time series cross-validation:
```{r goog200_tscv}

# forecast accuarcy (test data)
e <- tsCV(goog200, rwf, drift=TRUE, h=1)
sqrt(mean(e^2, na.rm=TRUE))

# residual error (training data)
sqrt(mean(residuals(rwf(goog200, drift=TRUE))^2, na.rm=TRUE))
```



### Pipe operator
```{r goog200_tscv_with_pipe}
# forecast accuarcy (test data)
goog200 %>% tsCV(forecastfunction=rwf, drift=TRUE, h=1) -> e
e^2 %>% mean(na.rm=TRUE) %>% sqrt()

# residual error (training data)
goog200 %>% rwf(drift=TRUE) %>% residuals() -> res
res^2 %>% mean(na.rm=TRUE) %>% sqrt()

```



### Example: using tsCV()

Evaluate forecasting performance of 1- to 8-step-ahead naive forecasts, using MSE as forecast error measure.  As expected, forecast error increases as forecast horizon increases.

```{r goog200_tscv_multi}

# forecast errors for h = 1:8
e <- tsCV(goog200, forecastfunction=naive, h=8)

# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = T)

# Plot the MSE values against the forecast horizon
data.frame(h = 1:8, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()
```



## Prediction intervals

### One-step prediction intervals

### Multi-step prediction intervals

### Benchmark methods

### Prediction intervals from bootstrapped residuals

### Prediction intervals with transformations

 
## The `forecast` package in R

### Functions that output a forecast object

### `forecast()` function

# Judgmental forecasts
## Beware of limitations
## Key principles
## The Delphi method
## Forecasting by analogy
## Scenario forecasting
## New product forecasting
## Judgmental adjustments


# Time series regression models

Aim: forecast time series of interest $y$ assuming it has a linear relationship with another time series $x$

Terminology:

* $y$ = forecast = regressand = dependent = explained variable
* $x$ = predictor = regressor = independent = explanatory variable

## The linear model
### Simple linear regression
### Multiple linear regression
### Assumptions

## Least square estimation
## Evaluating the regression model
## Some useful predictions
## Selecting predictors
## Forecasting with regression
## Matrix formulation
## Nonlionear regression
## Correlation, causation and forecasting


# Time series decomposition

Components:

* Trend-cycle (trend and cycle usually combined and referred to as **trend**)
* Seasonal
* Remainder (anything else in TS)
* Level ($\ell$) is introduced within the exponential smoothing section and is the average value of the series (or the smoothed value).

TS decomposition used to improve understanding of TS and to improve forecast accuracy.

## Time series components

Additive decomposition:
\[y_t = S_t + T_t + R_t,\]
where

* $y_t$ is the data
* $S_t$ is the seasonal component
* $T_t$ is the rend-cycle component
* $R_t$ is the remainder

Use additive decomposition when:

* Magnitude of seasonal fluctuations does not vary with level of TS, or
* Variation around trend-cycle does not vary with level of TS


Multiplicative decomposition:

First transform data until variation in series is stable over time, then use additive decomposition.  When a log transformation has been used this is equivalent to using a multiplicative decomposition

  $$
    \begin{aligned}
    y_t &= S_t \times T_t \times R_t \\
    \equiv \log y_t &= \log S_t + \log T_t + \log R_t
     \end{aligned}       $$


### Electrical equipment manufacturing
In the components plot, the grey bars to the right of each panel show the relative scales.  Each grey bar represents the same length, but because the plots are on different scales, the bars vary in size.  Therefore, a panel with a large grey bar has less variation when compared to panels with smaller grey bars.

### Seasonally adjusted data
Data is said to be seasonally adjusted if seasonal component is removed, .e. for additive decomposition, seasonally adjusted data is given by $y_t - S_t$ and for multiplicative data, $y_t/S_t$.

Useful when variation due to seasonality not of primary interest, e.g. employment data is usually seasonally adjusted.

Note, however, seasonally adjusted data contains remainder + trend-cycle and therefore not smooth; consequently upturns/downturns can be misleading.  If aim is to look for turning points in series, then better to use trend-cycle component.

## Moving averages

Moving average method is first step in classical method of time series decomposition (1920-1950s, but basis of modern methods).

### Moving average smoothing

#### Moving average of order $m$ ($m$-MA)
The estimate of trend-cycle at time $t$ is obtained by averaging the values of the time periods within periods $t \pm k$ and the order is said to be $m = 2k + 1$,\[\hat{T}_k = \frac{1}{m}\sum^k_{j=-k}y_{t+j}\]

```{r elecsales_5ma}
data.frame(
    Year = (start(elecsales)[1]: end(elecsales)[1]),
    Sales = as.numeric(elecsales),
    MA5 = as.numeric(ma(elecsales, 5))
) %>% kable(col.names = c("Year", "Sales", "5-MA"))
```


```{r elecsales_5ma_plot, message=FALSE, warning=FALSE}
# use series to give a legend entry
autoplot(elecsales, series="Data") +
  autolayer(ma(elecsales,5), series="5-MA") +
  xlab("Year") + ylab("GWh") +
  ggtitle("Annual electricity sales: South Australia") +
  scale_colour_manual(values=c("Data"="grey50","5-MA"="red"),
                      breaks=c("Data","5-MA"))
```

The order changes the smoothness of the trend-cycle estimate (larger order = smoother curve). Customer is to use odd order (e.g., 3, 5, 7, ...) so that they are symmetric about time $t$.

### Moving averages of moving averages
This is done to make an even-order moving average symmetric.

E.g. a $2 \times 4$-MA is a 2-MA applied on top of an 4-MA  


**Centred moving average of order $m$**: When a 2-MA follows a moving average of an even order $m$.

Note however, that a $3 \times 3$-MA is also common.  In general, an even order MA should be followed by an even order MA and an odd order MA should be followed by an odd order MA.

### Estimating the trend-cycle with seasonal data

Centred moving averages typically used to estimate trend-cycle from seasonal data.

Eg. if using $2 \times 4$-Ma on quarterly data:

$$\begin{aligned}
\hat{T}_t & = \frac{1}{2}\left[ \frac{1}{4}(y_{t-2} + y_{t-1} y_{t} + y_{t+1}) + \frac{1}{4}(y_{t-1} + y_{t} y_{t+1} + y_{t+2} )\right] \\
& = \frac{1}{8}y_{t-2} + \frac{1}{4}y_{t-1} \frac{1}{4}y_{t} +\frac{1}{4} y_{t+1}\frac{1}{8}y_{t-2}
\end{aligned}$$

So each quarter is given equal weight.  

In general, a $2 \times m$-MA $\equiv$ ($m + 1$)-MA where all observations take the weight $1/m$ except the first and last terms with take weights $1/(2m)$. 

Order $m$ of seasonal period | Estimate trend-cycle using:
-----------------------------| -----------------------------
$m$ = Even | $2 \times m$-MA
E.g. $m$ = 12 (monthly)  | $2 \times 12$-MA
Odd  | $m$-MA
E.g. $m$ = 7 (weekly) | 7-MA

### Example: Electrical equipment manufacturing


Monthly data:
```{r elecequip_12ma, message=FALSE, warning=FALSE}
# Note default of ma is centre = TRUE for even orders
autoplot(elecequip, series = "Data") + 
    autolayer(ma(elecequip, 12), series="2x12-MA") +
    xlab("Year") + 
    ylab("New orders index") +
    ggtitle("Electrical equipment manufacturing (Euro area)") +
    scale_colour_manual(values=c("Data"="grey","2x12-MA"="red"),
                      breaks=c("Data","12-MA"))
```


### Weighted moving averages


Combinations of moving averages $\equiv$ weighted moving average.  E.g. $2 \times 4$-MA $\equiv$ weighted 5-MA with weights $[\frac{1}{8}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{8}]$


Weighted $m$-MA:
\[\hat{T}_t = \sum_{j = -k}^k a_j y_{t + j},\]
where 

* $k = (m -1 )/2$ and weights
* weights are $[a_{-k}, \ldots, a_k]$
* weights sum to one
* weights are symmetric

Advantage:

* Yield smoother estimate of trend-cycle as can slowly increase and then slowly decrease weighs.

Note: $m$-MA is a special case of the weighted moving average with all weights equal to $1/m$.


## Classical decomposition

Two forms:

1. Additive
2. Multiplicative (in which case, the $m$ values that form the seasonal component are sometimes called __seasonal indices__)

Described for time series with seasonal period $m$, e.g. 

* $m = 4$ for quarterly data
* $m = 12$ for monthly data
* $m = 7$ for daily data with a weekly pattern
* $m = 365.25$ for daily data with an annual pattern

Assumption:

* Seasonal component is constant from year to year

### Additive decomposition


Steps:

1. Calculate the trend-cycle component ($\hat{T}_t$)
    * If $m$ is event, use $2 \times m$-MA
    * If $m$ is odd, use $m$-MA

2. Calculate the detrended series (such that left with seasonal and remainder components) 
    * Detrended series is $y_t - \hat{T}_t$

3. Estimate the seasonal component ($\hat{S}_t$) for each season
    * Average detrended values for each season.  e.g. with monthly data, for March, average all detrended March values
    * Adjust components so that they add to zero
    * String components together for each year

4. Calculate the remainder component
    * $\hat{R}_t = y_t - \hat{T}_t - \hat{S}_t$

### Multiplicative decomposition

Subtractions are replaced by divisions.

Steps:

1. Calculate the trend-cycle component ($\hat{T}_t$)
    * If $m$ is event, use $2 \times m$-MA
    * If $m$ is odd, use $m$-MA

2. Calculate the detrended series (such that left with seasonal and remainder components) 
    * Detrended series is $y_t/ \hat{T}_t$

3. Estimate the seasonal component ($\hat{S}_t$) for each season
    * Average detrended values for each season.  e.g. with monthly data, for March, average all detrended March values
    * Adjust components so that they add to zero
    * String components together for each year

4. Calculate the remainder component
    * $\hat{R}_t = y_t /(\hat{T}_t  \hat{S}_t)$


### Example
```{r elecequip_decomp}
elecequip %>% decompose(type="multiplicative") %>%
  autoplot() + 
    xlab("Year") +
  ggtitle("Classical multiplicative decomposition
    of electrical equipment index")
```
Features:

* Evidence of "leakage" of trend-cycle component into remainder component in 2009

### Comments on classical decomposition

* Not recommended; exist much better methods
* Estimate unavailable for head and tail obs, e.g. if $m$ = 12, then no estimate for first and last 6 months.
* Trend-cycle estimate tends to over-smooth rapid rises and falls
* Assumes seasonal component repeats year on year.  For longer series, this may not be so and classical decomposition does not allow seasonal changes over time to be captured.
* Classical methods not robust to observations made during unusual events (e.g. industrial dispute)

## X11 decomposition

From US Census Bureau and Statistics Canada.  Based on classical decomposition but:

* Allows for trend-cycle estimates for endpoints
* Allows seasonal component to vary slowly over time
* Has sophisticated methods for handling trading day variation, holiday effects and effects of known predictors
* Highly robust to outliers and level shifts in the time series

Allows for both additive and multiplicative decomposition.  Available using `seasonal::seas()` function.  Given output has the following functions:

* `seasonal()`
* `trendcycle()}`
* `remainder()`
* `seasadj()`


For details, see Dagum and Bianconcini (2016). __Seasonal adjustment methods and real time trend-cycle estimation__.

Example:
```{r elecequip_x11}
require(seasonal)
elecequip %>% seasonal::seas(x11="") -> fit
autoplot(fit) +
  ggtitle("X11 decomposition of electrical equipment index")
```
Features: 

* Captured sudden fall in data in early 2009 (better than STL and classical decomposition)
* Unusual observation at end of 2009 more clearly seen in the remainder.


Illustration of data with trend-cycle component and seasonally-adjusted (i.e. trend + remainder) data:
```{r elecequip_x11_components}

autoplot(elecequip, series="Data") +
  autolayer(trendcycle(fit), series="Trend") +
  autolayer(seasadj(fit), series="Seasonally Adjusted") +
  xlab("Year") + ylab("New orders index") +
  ggtitle("Electrical equipment manufacturing (Euro area)") +
  scale_colour_manual(values=c("gray","blue","red"),
             breaks=c("Data","Seasonally Adjusted","Trend"))
```

Seasonal sub-series plots to see how seasonal component is changing over time:
```{r elecequip_x11_seasonal_subplots}
fit %>% seasonal() %>% ggsubseriesplot() + ylab("Seasonal")
```


Features: Only small changes over time.


## SEATS decomposition

Seasonal Extraction in ARIMA Time Series (SEATS):

* Developed at Bank of Spain
* Only works with quarterly and monthly data; daily, hourly, weekly, etc data require alternative approach
* Details given in Dagum & Bianconcini (2016)
* Available in `seasonal` package; refer to [package website]([http://www.seasonal.website/seasonal.html)

```{r elecequip_seats}
library(seasonal)
elecequip %>% seas() %>%
autoplot() +
  ggtitle("SEATS decomposition of electrical equipment index")
```


## STL decomposition

Seasonal and Trend decomposition using Loess (local polynomial regression)

* Developed by Cleveland, Cleveland, McRae & Terpenning (1990)
* Available using `stl()` function.

Advantages over classical, SEATS and X11 decomposition methods:

* Handles all seasonality, not just monthly and quarterly (unlike SEATS)
* Seasonal component is allowed to change over time, and rate of change can be controlled by the user
* Smoothness of trend-cycle can be controlled by the user
* Can be robust to outliers (i.e user can specify a robust decomposition) s.t occasional unusual observations affect remainder component and not trend-cycle or seasonal components

Disadvantages:

* Does not automatically handle trading day or calendar variation
* Only provides for additive decomposition (although multiplicative decomposition can be obtained by first taking logs of the data and then back-transforming the components, or using a Box-Cos transformation with $\lambda$ = 0 )

Parameters to control how quickly the trend-cycle and seasonal component, respectively change:

* `t.window`: Trend-cycle window (optional parameter)  
* `s.window`: Seasonal window (required, as no default).  Setting it to be infinite is equivalent to forcing the seasonal component to be periodic (i.e. identical across years)

Smaller values allow for more rapid changes.  Equal to number of consecutive years to be used when estimating the trend-cycle and seasonal components, respectively.


The `mstl()` function 

* sets `s.window = 13` and chooses `t.window` automatically
* Usually gives good balance between overfitting the seasonality and allowing it to slowly change over time
* but Default settings will need adjusting for some time series

Example:
```{r autoelec_stl_adj}
elecequip %>%
  stl(t.window=13, s.window="periodic", robust=TRUE) %>%
  autoplot()
```

## Measuring strength of trend and seasonality

Time series decomposition can be used to measure the strength of the trend and seasonality.  
Recall \[y_t = T_t + S_t + R_t\]


### Strength of Trend
For strongly trended data, seasonally adjusted data $(T_t + R_t)$ will have much more variation than the remainder component ($R_t$) and so $\text{Var}(R_t)/\text{Var}(T_t + R_t)$ will be relatively small.

Strength of trend defined as:
\[F_T = \max\left(0, 1 - \frac{\text{Var}(R_t)}{\text{Var}(T_t + R_t)}\right)\]

Because variance of remainder may occasionally be larger than variance of seasonally adjusted data, the minimal possible value of $F_T$ is set to zero.

### Strength of Seasonality
For data with strong seasonality, detrended $(S_t + R_t)$ will have much more variation than the remainder component ($R_t$) and so $\text{Var}(R_t)/\text{Var}(S_t + R_t)$ will be relatively small.

Strength of seasonality defined as:
\[F_S = \max\left(0, 1 - \frac{\text{Var}(R_t)}{\text{Var}(S_t + R_t)}\right)\]

Similar to strength of trend, because variance of remainder may occasionally be larger than variance of detrended data, the minimal possible value of $F_T$ is set to zero.



## Forecasting with decomposition

Decomposition is primarily used for studying TS data and exploring historical changes but can be used for forecasting

$$\begin{aligned}
y_t &= \hat{S}_t + \hat{R}_t + \hat{T}_T \\
    &= \hat{A}_t + \hat{T}_T
\end{aligned}$$ where $\hat{A}_t$ is the seasonally adjusted component

Forecast as follows:

* Forecast seasonal component, $\hat{S}_t$
    * Assume seasonal component is unchanging, or changing extremely slowly
    * Use seasonal naive method (i.e, take last year of estimated component)
* Forecast seasonally adjusted component, $\hat{A}_t$
    * Use any non-seasonal forecasting method, e.g. random walk with drift, Holt's method, non-seasonal ARIMA
* Add these two forecasts together

Prediction intervals are calculated in a similar method

Can use the following functions (noting that they ignore the uncertainty in forecasts of the seasonal component as these are considered small):

* `stl()` then `forecast(method = "naive")` function
* `stlf(method = "naive")`, this uses `mstl` and so there are default options for `s.window` and `t.window`.

Can use other methods than "naive"; default is ETS applied to the seasonally adjusted series.


```{r elecequip_stl_forecast}
fit <- stl(elecequip, t.window=13, s.window="periodic", robust=TRUE)
fit %>% 
    forecast(method="naive") %>%
    autoplot() + 
    ylab("New orders index")
```



# Exponential smoothing

* Proposed late 1950s
* Weighted averages of past observations, with weights decaying exponentially as observations get older; more weight placed on most recent observations
* Method selection generally based on recognising key components of the time series (trend and seasonal) and the way in which these enter the smoothing method (additive, damped or multiplicative)


## Simple exponential smoothing (SES)

* Naive method = weighted average with all weight given to the last observation
* Average method = weighted average in which all observations have equal weights 
* SES method 
    * = more recent observations have larger weights, where weights decrease exponentially as observations come from further in the past
    
    \[\hat{y}_{T+1|T} = \alpha y_T + \alpha(1-\alpha)y_{T-1} + \alpha(1-\alpha)^2 y_{T-2} + \ldots, \] where $0 \leq \alpha \leq 1$ and controls the rate at which the weights decrease.
    
    * Suitable when no clear trend or seasonal pattern
    
    
### Weighted average form

**Forecast** at time $T + 1$ is set to average of most recent observation ($y_T$) and previous forecast ($\hat{y}_{T|T-1}$)
\[\hat{y}_{T+1|t} = \alpha y_T + (1 - \alpha) \hat{y}_{T|T-1}\]

where $0 \leq \alpha \leq 1$.  

Similarly the **fitted values** (1-step forecasts of the training data) can be written as:
\[\hat{y}_{t+1|t} = \alpha y_t + (1 - \alpha) \hat{y}_{t|t-1}\]
for $t = 1, \ldots, T$.

Initialise process with first fitted value at time 1 set to $\ell_0$, such that

$$\begin{aligned}
\hat{y}_{2|1} &= \alpha y_1 + (1 - \alpha) \ell_0 \\
\hat{y}_{3|2} &= \alpha y_2 + (1 - \alpha) \hat{y}_{2|1} \\
\hat{y}_{4|3} &= \alpha y_3 + (1 - \alpha) \hat{y}_{3|2} \\
\vdots\\
\hat{y}_{T|T-1} &= \alpha y_{T-1} + (1 - \alpha) \hat{y}_{T-1|T-2} \\
\hat{y}_{T+1|T} &= \alpha y_{T} + (1 - \alpha) \hat{y}_{T|T-1} 
\end{aligned}$$

By substitution, 
\[\hat{y}_{T+1|T} = \sum_{j=1}^{T-1}\alpha(1-\alpha)^j y_{T-j} + \alpha(1-\alpha)^T \ell_0\]

For large T, the last term becomes tiny and so the weighted average form leads to the same as the forecast ESS.


### Component form

In simple exponential smoothing, the only component included is the level (which is the average value of the series) $\ell_t$.  Other methods may also include a trend $b_t$ and a seasonal component $s_t$.

Comprise forecast equation and smoothing equation for each of the components included in the method. 

$$\begin{aligned}
\text{Forecast equation}\;\;\;  \hat{y}_{t+h|t} &= \ell_t \\
\text{Smoothing equation}\;\;\;          \ell_t &= \alpha y_t + (1-\alpha)\ell_{t-1}
\end{aligned}$$

Setting $h$ = 1 gives fitted values, while setting $t = T$ gives the true forecasts beyond the training data.

To obtain weighted average form:

* Replace $\ell_t$ with $\hat{y}_{t+1|t}$ 
* Replace $\ell_{t-1}$ with $\hat{y}_{t-1|t}$

Component form is useful when start adding components (not so much for simple exponential smoothing!)

### Flat forecasts
All forecasts take the ame value, equal to the last level component (and so considered to have a "flat" forecast function); they are therefore only suitable if time series has no trend nor seasonal component.
\[\hat{y}_{T+h|T} = \hat{y}_{T+1|T} = \ell_T, \;\;\; h = 2, 3, \ldots\]

### Optimisation

Exponential smoothing methods require choice of:

* Smoothing parameter ($\alpha, 0 \leq \alpha \leq 1$)
* Initial value ($\ell_0$)

Can choose:

* Subjectively; previous experience
* Use observed data to minimise the SSE,
\[\text{SSE} = \sum_{t=1}^T(y_t - \hat{y}_{t|t-1})^2 = \sum_{t=1}^T e_t^2\]

Unlike in regression (in which formulae are used to obtain the regression coefficients), this is a non-linear minimisation problem and so requires optimisation tool (or visual inspection?).

### Example: Oil production

```{r ses_oil}
oildata <- window(oil, start=1996)

# Estimate parameters
fc <- ses(oildata, h=5)

# Accuracy of one-step-ahead training errors
round(accuracy(fc),2)
#>               ME  RMSE   MAE MPE MAPE MASE  ACF1
#> Training set 6.4 28.12 22.26 1.1 4.61 0.93 -0.03
```
If `alpha` not supplied in call to `ses` function it is estimated.  Similarly the default method for selecting the initial state values is `optimal`.  In above case, values used are obtained by calling `fc$model$fit` which returns values of \alpha = `r round(fc$model$fit$par[1],2)` and $\hat{\ell}_0$ =`r round(fc$model$fit$par[2],1)`

Here \alpha quite large and so more weight given to more recent observations.  This means that a large adjustment takes place in the estimated level $\ell_t$ at each time.  With a smaller value of $\alpha$ the fitted values would be smoother.

In the following chart, for the period 1996-2013 the black line shows the observed values of oil production and the red line shows the fitted values.  For the forecast period 2014 = 2018, the blue line shows the point forecast (flat at 542.68) alongside the 80% and 90% prediction intervals
```{r ses_oil_plot}
autoplot(fc) + 
  autolayer(fitted(fc), series="Fitted") +
  ylab("Oil (millions of tonnes)") + xlab("Year")
```




Given the large prediciton interval, it would be misleading to dispaly only the point forecasts. 

## Trend methods
### Holt's linear trend method
Extension of simple exponential smoothing to enable forecasting of data with trends; uses two moothing equations (one for the level and one for the trend);

$$\begin{aligned}
\text{Forecast equation}\;\;\;  \hat{y}_{t+h|t} &= \ell_t + hb_t\\
\text{Level equation}\;\;\;          \ell_t &= \alpha y_t + (1-\alpha)(\ell_{t-1}+b_{t-1})\\
\text{Trend equation}\;\;\;          b_t &= \beta^* (\ell_t - \ell_{t-1}) + (1-\beta^*)b_{t-1}
\end{aligned}$$



where

* $\ell_t$ is estimate of level at time $t$
* $b_t$ is estimated of trend at time $t$
* $\alpha$ is the smoothing parameter for the level, $0 \leq \alpha \leq 1$
* $\beta^*$ is the smoothing parameter for the trend (slope), $0 \leq \beta^* \leq 1$


### Example: Air passengers
```{r holt_air}
# see: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/xts_Cheat_Sheet_R.pdf

air <- window(ausair, start=1990)
fc <- holt(air, h=5)

data.frame(Year = as.numeric(time(fc$x))) %>%
    mutate(Time = row_number(Year),
            Observation = as.numeric(fc$x), 
            Forecast = as.numeric(fc$fitted)) %>%
            right_join(data.frame(
            Year = as.numeric(time(fc$model$states)), 
            Level = as.numeric(fc$model$states[,"l"]),
            Slope = as.numeric(fc$model$states[,"b"]))) %>%
            
            select(Year, Time, Observation, Level, Slope, Forecast) %>%
            bind_rows(data.frame(
            Year = as.numeric(time(fc$mean)),
            Time = 1:5,
            Forecast = as.numeric(fc$mean)))

```


### Damped trend methods

Holt's linear method forecasts have a constant (increasing or decreasing) trend indefinitely into the future which leads to over-forecasting, especially in longer forecast horizons.

Damped trend methods dampens trend to flat line at a point in the future by adding a damping parameter $0 < \phi < 1$

$$\begin{aligned}
\text{Forecast equation}\;\;\;  \hat{y}_{t+h|t} &= \ell_t + (\phi + \phi^2 + \ldots + \phi^h)b_t\\
\text{Level equation}\;\;\;          \ell_t &= \alpha y_t + (1-\alpha)(\ell_{t-1}+\phi b_{t-1})\\
\text{Trend equation}\;\;\;          b_t &= \beta^* (\ell_t - \ell_{t-1}) + (1-\beta^*)\phi b_{t-1}
\end{aligned}$$


If $\phi=1$, method identical to Holt's.  
Short-run forecasts are trended; while long-run forecasts are constant (convergin to $\ell_T + \phi b_t/(1-\phi))$

In practice, $\phi$ is rarely less than 0.8 as damping has a very strong effect for smaller values; usually $0.8 \leq \phi \leq 0.98$


### Example: Air passengers (continued)

Foreasts for air passengers using Holt's linear trend method and damped trend method, noting that example is artificial due to low damping number (which would usually be estimated) and large forecast horizon

```{r damped_air}
fc <- holt(air, h=15)
fc2 <- holt(air, damped=TRUE, phi = 0.9, h=15)
autoplot(air) +
  autolayer(fc, series="Holt's method", PI=FALSE) +
  autolayer(fc2, series="Damped Holt's method", PI=FALSE) +
  ggtitle("Forecasts from Holt's method") + xlab("Year") +
  ylab("Air passengers in Australia (millions)") +
  guides(colour=guide_legend(title="Forecast"))
```

### Example: Sheep in Asia

```{r livestock}
autoplot(livestock) +
  xlab("Year") + ylab("Livestock, sheep in Asia (millions)")
```

Time series cross-validation of one-step forecast accuracy using simple exponential smoothing, Holt's linear trend method and damped trend method shows that damped trend method is best.  
```{r livestock_cv}


e1 <- tsCV(livestock, ses, h=1)
e2 <- tsCV(livestock, holt, h=1)
e3 <- tsCV(livestock, holt, damped=TRUE, h=1)

data.frame(Method = c("SES", "Holt", "Damped"), 
           MSE = c(mean(e1^2, na.rm=TRUE), mean(e2^2, na.rm=TRUE), mean(e3^2, na.rm=TRUE)),
           MAE = c(mean(abs(e1), na.rm=TRUE), mean(abs(e2), na.rm=TRUE), mean(abs(e3), na.rm=TRUE)))

```

Use of damped trend method to get forecasts for future years using whole data set:
```{r livestock_damped_params}
fc <- holt(livestock, damped=TRUE)
fc[["model"]]
```

* The smoothing parameter for the level ($\alpha$) is very close to one, showing that the level reacts strongly to each new observation
* The smoothing parameter for the slope ($\beta$) shows that trend is not changing over time
* The damping parameter ($\phi$) is claose
```{r livestock_damped_plot}
autoplot(fc) +
  xlab("Year") + ylab("Livestock, sheep in Asia (millions)")
```


## Holt-Winter's seasonal method
Two variations; 

* additive; when seasonal variations roughly constatn through seeries
* Multiplicative; when seasonal variations are changing proportional to the level of the series

### Holt-Winters' additive model
$$\begin{aligned}
\text{Forecast equation}\;\;\;  \hat{y}_{t+h|t} &= \ell_t + hb_t + s_{t + h - m(k + 1)}\\
\text{Level equation}\;\;\;          \ell_t &= \alpha (y_t-s_{t-m}) + (1-\alpha)(\ell_{t-1}+ b_{t-1})\\
\text{Trend equation}\;\;\;          b_t &= \beta^* (\ell_t - \ell_{t-1}) + (1-\beta^*) b_{t-1}\\
\text{Seasonal equation}\;\;\;       s_t &= \gamma(y_t - \ell_{t-1}-b_{t-1}) + (1-\gamma) s_{t-m}
\end{aligned}$$

where:
* $k$ is in the integer part of $(h-1)/m$ to ensure estimates of the seasonal indices come from the final year of hte sample
* The seasonal equation shows a weighted average between current seasonal index ($y_t - \ell_{t-1}-b_{t-1}$) and seasonal index of hte same eason last year (i.e. $m$ time periods ago)

Note, can write seasonal component as \[s_t = \gamma^*(y_t-\ell_t) + (1-\gamma^*)s_{t-m}\]
where $\gamma = \gamma^*(1-\alpha)$

Proof: 
$$\begin{aligned}
s_t &= \gamma^*(y_t-\biggl[\ell_t\biggr]) + (1-\gamma^*)s_{t-m}\\
    &= \gamma^* (y_t-\biggl[\alpha (y_t-s_{t-m}) + (1-\alpha)(\ell_{t-1}+ b_{t-1})\biggr]) + (1-\gamma^*)s_{t-m}\\
    &= \gamma^*y_t - \gamma^*\alpha (y_t-s_{t-m}) - \gamma^*(1-\alpha)(\ell_{t-1}+ b_{t-1}) + (1-\gamma^*)s_{t-m}\\
    &=\gamma^*y_t - \biggl[\gamma^*\alpha y_t\biggr]  - \gamma^*(1-\alpha)(\ell_{t-1}+ b_{t-1}) +   (1-\gamma^*)s_{t-m} + \biggl[\gamma^*\alpha s_{t-m}\biggr]\\
    &=\gamma^*(1-\alpha) y_t - \gamma^*(1-\alpha)(\ell_{t-1}+ b_{t-1}) +   (1-\gamma^*)s_{t-m} + \gamma^*\alpha s_{t-m}\\
    &=\gamma^*(1-\alpha)(y_t - \ell_{t-1} - b_{t-1}) +   (1-\gamma^*(1-\alpha))s_{t-m} \\
      &=\gamma(y_t - \ell_{t-1} - b_{t-1}) +   (1-\gamma)s_{t-m} \\
\end{aligned}$$

### Holt-Winters' multiplicative model
$$\begin{aligned}
\text{Forecast equation}\;\;\;  \hat{y}_{t+h|t} &= (\ell_t + hb_t )s_{t + h - m(k + 1)}\\
\text{Level equation}\;\;\;          \ell_t &= \alpha \frac{y_t}{s_{t-m}} + (1-\alpha)(\ell_{t-1}+ b_{t-1})\\
\text{Trend equation}\;\;\;          b_t &= \beta^* (\ell_t - \ell_{t-1}) + (1-\beta^*) b_{t-1}\\
\text{Seasonal equation}\;\;\;       s_t &= \gamma \frac{y_t}{(\ell_{t-1}+b_{t-1})} + (1-\gamma) s_{t-m}
\end{aligned}$$

### Example: International tourist visitor nights in Australia
```{r austourists_hw}
aust <- window(austourists,start=2005)
fit1 <- hw(aust,seasonal="additive")
fit2 <- hw(aust,seasonal="multiplicative")

autoplot(aust) +
  autolayer(fit1, series="HW additive forecasts", PI=FALSE) +
  autolayer(fit2, series="HW multiplicative forecasts",
    PI=FALSE) +
  xlab("Year") +
  ylab("Visitor nights (millions)") +
  ggtitle("International visitors nights in Australia") +
  guides(colour=guide_legend(title="Forecast"))
```

In additive model, note that seasonal components for 4 quarters ~ 1.

```{r austourists_hw_add}
data.frame(Qtr = as.numeric(time(fit1$x))) %>%
    mutate(Time = row_number(Qtr),
            Observation = as.numeric(fit1$x), 
            Forecast = as.numeric(fit1$fitted)) %>%
            right_join(data.frame(
            Qtr = as.numeric(time(fit1$model$states)), 
            Level = as.numeric(fit1$model$states[,"l"]),
            Slope = as.numeric(fit1$model$states[,"b"]),
            Seasonal = as.numeric(fit1$model$states[,"s1"])), by = "Qtr") %>%
            select(Qtr, Time, Observation, Level, Slope, Seasonal, Forecast) %>%
            bind_rows(data.frame(
            Qtr = as.numeric(time(fit1$mean)),
            Time = 1:8,
            Forecast = as.numeric(fit1$mean))) %>%
    filter(Qtr < 2006|Qtr >= 2015)
```


In additive model, note that seasonal components for 4 quarters ~ $m = 4$.

```{r austourists_hw_mult}
data.frame(Qtr = as.numeric(time(fit2$x))) %>%
    mutate(Time = row_number(Qtr),
            Observation = as.numeric(fit2$x), 
            Forecast = as.numeric(fit2$fitted)) %>%
            right_join(data.frame(
            Qtr = as.numeric(time(fit2$model$states)), 
            Level = as.numeric(fit2$model$states[,"l"]),
            Slope = as.numeric(fit2$model$states[,"b"]),
            Seasonal = as.numeric(fit2$model$states[,"s1"])), by = "Qtr") %>%
            select(Qtr, Time, Observation, Level, Slope, Seasonal, Forecast) %>%
            bind_rows(data.frame(
            Qtr = as.numeric(time(fit2$mean)),
            Time = 1:8,
            Forecast = as.numeric(fit2$mean))) %>%
    filter(Qtr < 2006|Qtr >= 2015)
```


Because both methods have the same number of parameters to estimate, can compare training RMSE from both models and see that multiplicative seasonality method fits data best (as expected given that seasonal variation in plotted data increased with level of the series)

```{r austourists_hw_rmse}
data.frame(Method = c("Additive", "Multiplicative"), 
           RMSE = c(sqrt(fit1$model$mse), sqrt(fit2$model$mse)))

```

Estimated components for the Holt-Winters method with additive and multiplicative seasonal components:

```{r austourists_hw_components}

p <- data.frame(Model = "Additive", Qtr = time(fit1$model$states), fit1$model$states) %>%
    select(Model, Qtr, l, b, s1) %>%
    rename(level = l, slope=b, season = s1) %>%
    bind_rows(
data.frame(Model = "Multiplicative", Qtr = time(fit2$model$states), fit2$model$states) %>%
    select(Model, Qtr, l, b, s1) %>%
    rename(level = l, slope=b, season = s1) 
    ) %>%
    gather("Component", "Value", -Model, -Qtr) %>%
    mutate(Component = factor(Component, levels = c("level", "slope", "season"))) 

p1 <- filter(p, Model == "Additive") %>%
    ggplot(aes(x = Qtr, y = Value)) + 
    geom_line() + 
    facet_grid(Component~Model, scales = "free")

p2 <- filter(p, Model == "Multiplicative") %>%
    ggplot(aes(x = Qtr, y = Value)) + 
    geom_line() + 
    facet_grid(Component~Model, scales = "free")

grid.arrange(p1, p2, nrow = 1)

```

The small value of the seasonal component ($\gamma$ = `r round(fit2$model$par["gamma"],3)`) for the multiplicative model means that the seasonal component hardly changes over time. The small value of the trend component ($\beta^*$ = `r round(fit1$model$par["beta"],4)`)
 for the additive model means the slope component hardly changes over time. The increasing size of the seasonal component for the additive model suggests that the model is less appropriate than the multiplicative model.
  
### Holt-Winters' damped method

Damping is possible with both additive and muliplicative Holt-Winters' methods.  Holt-Witers method with damped trend and multiplicative seasonability often provides reliable forecasts for season data
```{r hw_damped_seasonal, eval=FALSE, include=FALSE, echo=TRUE}
hw(y, damped = TRUE, seasonal = "multplicative")
```

### Example: Holt-Winters' method with daily data
Generate daily forecasts for last five weeks.
```{r hyndsight_hw}
fc <- hw(subset(hyndsight,end=length(hyndsight)-35),
         damped = TRUE, seasonal="multiplicative", h=35)
autoplot(hyndsight) +
  autolayer(fc, series="HW multi damped", PI=FALSE)+
  guides(colour=guide_legend(title="Daily forecasts"))
```


## A taxonomy of exponential smoothing methods
Combinations of how trend (N=None, A=Additive, $A_d$ = Additive Damped) and seasonal components (N=None, A=Additive, M=Multiplicative) treated leads to 9 possible exponential smoothing models:

Short-Hand (Trend-Seasonal) | Method | Function
------------| ---------------------------------------------- | -----------
(N,N)       | Simple exponential smoothing | `ses`
(N,A)        | |
(N,M)       | |

(A,N)       | Holt's linear method | `holt`
(A,A)       |Additive Holt-Winters' method | `hw`
(A,M)       |Multiplicative Holt-Winters' method  | `hw`

($A_d$, N)  | Additive damped trend method | `holt`
($A_d$, A)  |  | `hw`
($A_d$, M)  |Holt-Winters' damped method  | `hw`

(M,N)       |  | `holt`
(M,A)        | |
(M,M)       |  | `hw`


($M_d$, N)  |  |`holt`
($M_d$, A)  | |
($M_d$, M)  |  | `hw`


<table>
<tr>
<th>Trend Component</th>
<th colspan="3">Seasonal Component</th>
</tr>

<tr>
<th>&nbsp;</th>
<th>N (None)</th>
<th>A (Additive)</th>
<th>M (Multiplicative)</th>
</tr>

<tr>
<th>N (None) </th>
<td> (N,N) </td>
<td> (N,A) </td>
<td> (N,M) </td>
</tr>

<tr>
<th>A (Additive) </th>
<td> (A,N) </td>
<td> (A,A) </td>
<td> (A,M) </td>
</tr>

<tr>
<th>A$_d$ (Additive damped) </th>
<td> (A$_d$,N) </td>
<td> (A$_d$,A) </td>
<td> (A$_d$,M) </td>
</tr>

<tr>
<th>M (Multiplicative) </th>
<td> (M,N) </td>
<td> (M,A) </td>
<td> (M,M) </td>
</tr>

<tr>
<th>M$_d$ (Multiplicative damped) </th>
<td> (M$_d$,N) </td>
<td> (M$_d$,A) </td>
<td> (M$_d$,M) </td>
</tr>

</table>

## Innovations state space models for exponential smoothing

The statistical methods in this section generate the same point forecast as the exponential smoothing methods but can also generate prediction (or forecast) intervals.  

Each model (referred to as __state space models__) consists of:

* Equation to describe observed data
* State equations to describe how unobserved components (=states) (level, trend, seasonal) change over time
* Error which is either additive or multiplicative

So, for each of the 9 Trend-Seasonal models, (N=None, A=Additive, $A_d$ = Additive Damped) $\times$ (N=None, A=Additive, M=Multiplicative), there are two models; one with additive errors and one with multiplicative errors.  The point forecasts produced by the models are identical, but will generate differnt predeiction intervals.  

To distinguish between a model with additive errors and a model with multiplicative erros, each state-space model is labels as ETS ($\cdot$, $\cdot$, $\cdot$) for (Error, Trend, Seasonal), although it could be argued that ETS stands for ExponenTial Smoothing.  The possibility for each component are: Error = {A,M}, Trend = {N, A, $A_d$} and Seasonal = {N, A, M}

### ETS(A,N,N): simple exponential smoothing with additive errors

Recall: Single exponential smoothing component form:
$$\begin{aligned}
\text{Forecast equation}\;\;\;  \hat{y}_{t+h|t} &= \ell_t \\
\text{Smoothing equation}\;\;\;          \ell_t &= \alpha y_t + (1-\alpha)\ell_{t-1}
\end{aligned}$$

The smoothing equation can be re-arranged to get the error correction form:
$$\begin{aligned}\ell_t &= \ell_{t-1} + \alpha(y_t - \ell_{t-1})\\
&=\ell_{t-1} + \alpha e_t
\end{aligned}$$
where $e_t = y_t - \ell_{t-1} = y_t - \hat{y}_{t|t-1}$ is the residual at time $t$.

Residuals at time $t$ lead to adjustment of level at time $t+1$, e.g.

* If $e_t < 0 $ then $y_t <  \hat{y}_{t|t-1}$ (i.e., level has been over-estimated)
* The next level, $\ell_t$, will adjust $\ell_{t-1}$ downards.
* The closer $\alpha$ is to one, the greater the adjustment; smaller $\alpha$ leads to smaller adjustemnts and smoother levels.

The state-space model with additive errors uses the error correction form of the smoothing equation
$$\begin{aligned}
\text{Measurement (or Observation) Equation} \;\;\;\; y_t &= \ell_{t-1} + \varepsilon_t\\
\text{State (or Transition) Equation} \;\;\;\; \ell_t &= \ell_{t-1} + \alpha e_t
\end{aligned}$$
and assumes $\varepsilon_t = y_t - \hat{y}_{t|t-1}$ is ~$\text{NID}(0, \sigma^2)$

* Measurement equation: Observation is linear function of the level (predictable) and error (unpredictable)
* State equation: Evolution of thestate through time; if $\alpha = 0$, level is constant over time.  If $alpha = 1$, model reduces to a random walk model.

### ETS(M,N,N): simple exponential smoothing with multiplicative errors
Assuming that 
$$\begin{aligned}\varepsilon_t &= \frac{y_t - \hat{y}_{t|t-1}}{\hat{y}_{t|t-1}}\\
                            &\sim \text{NID}(0, \sigma^2),
\end{aligned}$$

get multiplicative form of the state space model:
$$\begin{aligned}
\text{Measurement Equation} \;\;\;\; y_t &= \ell_{t-1}(1 + \varepsilon_t)\\
\text{State Equation} \;\;\;\; \ell_t &= \ell_{t-1}(1 + + \alpha e_t)
\end{aligned}$$


### ETS(A,A,N): Holt’s linear method with additive errors
Assuming that one-step-ahead training errors given by
$$\begin{aligned}\varepsilon_t &= y_t - \ell_{t-1} -  b_{t-1}\\
                            &\sim \text{NID}(0, \sigma^2),
\end{aligned}$$

and substituting into error correction equations for Holt's linear method obtain: 
$$\begin{aligned}
\text{Measurement Equation} \;\;\;\; y_t &= \ell_{t-1} +  b_{t-1} + \varepsilon_t\\
\text{State Equation} \;\;\;\; \ell_t &= \ell_{t-1} +  b_{t-1} + \alpha \varepsilon_t\\
\text{Trend Equation}\;\;\;\;   b_t &=  b_{t-1} + \beta \varepsilon_t
\end{aligned}$$
where $\beta = \alpha \beta^*$.

### ETS(M,A,N): Holt’s linear method with multiplicative errors
See book for details.

### Other ETS models
See book for taxonomy of innovations state space model for each of the ETS models.

Possibilities are:

Additive Error Model:

<table>
<tr>
<th>Trend Component</th>
<th colspan="3">Seasonal Component</th>
</tr>

<tr>
<th>&nbsp;</th>
<th>N (None)</th>
<th>A (Additive)</th>
<th>M (Multiplicative)</th>
</tr>

<tr>
<th>N (None) </th>
<td> (A,N,N) </td>
<td> (A,N,A) </td>
<td style="background-color:grey"> (A,N,M) </td>
</tr>

<tr>
<th>A (Additive) </th>
<td> (A,A,N) </td>
<td> (A,A,A) </td>
<td style="background-color:grey"> (A,A,M) </td>
</tr>

<tr>
<th>A$_d$ (Additive damped) </th>
<td> (A,A$_d$,N) </td>
<td> (A,A$_d$,A) </td>
<td style="background-color:grey"> (A,A$_d$,M) </td>
</tr>

<tr>
<th>M (Multiplicative) </th>
<td> (A,M,N) </td>
<td> (A,M,A) </td>
<td> (A,M,M) </td>
</tr>

<tr>
<th>M$_d$ (Multiplicative damped) </th>
<td> (A,M$_d$,N) </td>
<td> (A,M$_d$,A) </td>
<td> (A,M$_d$,M) </td>
</tr>
</table>


Note those that are greyed out are not usually considered in model selection; the last two trend rows have not been discussed.


## Estimation and model selection

MLE is used to estimate parameters.

### Model selection

Can determine which ETS model most appropriate using either:

* AIC = $-2\log(L) + 2k$
* AIC$_{\text{c}}$ (AIC adjusted for small sample bias) = \frac{k(k+1)}{T-k+1}
* BIC = $\text{AIC} + k[\log{T}-1]

The following models can cause instability due to division by values potential close to zero in the state equations, and so are usually not considered in model selection:

* ETS(A,N,M)
* ETC(A,A,M)
* ETC(A,A$_d$,M)

Models with multiplicative errors are useful when the data are strictly positive, but are not numerically stable when the data contain zeros or negative values in which case only six fully additive models would be applied).  

### The `ets()` function in R

The `ets()` function in the `forecast` package:

* Used to estimate the ets models
* Does not produce forcasts, but instead estimates the model parameters.
* Uses the AICc model criteria by default

Function arguments:
```{r eval=FALSE, include=FALSE, echo = TRUE}
ets(y, model="ZZZ", damped=NULL, alpha=NULL, beta=NULL,
    gamma=NULL, phi=NULL, lambda=NULL, biasadj=FALSE,
    additive.only=FALSE, restrict=TRUE,
    allow.multiplicative.trend=FALSE)
```

Parameter | Description
----------| ------------------------------------
`y` | Time series to be forecast
`model` | Three letter code (Error, Trend, Seasonality), where Error$\in{A,M,Z}$, Trend|Season$\in{N,A,M,Z}$; Z is for automatic selection.
`damped` |  To dampen trend, i.e. to transform $A$ to $A_d$ or $M$ to $M_d$
`alpha` |  Smoothing parameter for the level
`beta` |  Smoothing parameter for the trend (slope)
`gamma` |  Smoothing parameter for the seasonality
`phi` |  Damping parameter
`\lambda`  | Box-Cox transformation parmater; whether time series is transformed before model is estimated
`biasadj` | If `TRUE` and `lambda` is not null, then back-transformed fitted values and forecasts will be bias-adjusted
`additive.only` | Only models with additive components will be considered
`restrict` | If true (default), models that cause numerical difficulaties are not considered
`allow.multiplicative.method` | Multiplicative trend models which were not disucssed here

### Working with `ets` objects

Useful functions that work on `ets` objects:

Function   | Returns:
----------| ------------------------------------
`coef()` | Fitted parameters
`accuracy()` | Accuracty measures computed on training data
`summary()` | summary information about fitted values
`autoplot()` | Time plots of the components
`plot()` | Time plots of the componetns
`residuals()` | Residuals of the estimated model
`fitted()` | One-step forecasts for the training data
`simulate()` |  Future sample paths from the fitted model
`forecast()` | Point forecasts and prediction intervals

### Example: International tourist visitor nights in Australia
Example: Fit ETS model tourist visitor nights in Australia over the period 2005-2015.

```{r austourist_ETS}
aust <- window(austourists, start=2005)
fit <- ets(aust)
summary(fit)
```

Best fit model is a multiplicative Holt-Winters' method with multiplicative error (i.e. incorporates trend and seasonality)


$$\begin{aligned}
\text{Forecast equation}\;\;\;  \hat{y}_t &= (\ell_{t-1} + b_{t-1} )s_{t - m}(1 + \varepsilon_t)\\
\text{Level equation}\;\;\;          \ell_t &=  (\ell_{t-1} + b_{t-1})(1 + \alpha\varepsilon_t)\\
\text{Trend equation}\;\;\;          b_t &= b_{t-1} + \beta(\ell_{t-1} + b_{t-1}) \varepsilon_t \\
\text{Seasonal equation}\;\;\;       s_t &= s_{t-m}(1 + \gamma\varepsilon_t) 
\end{aligned}$$

Components: 
```{r austourists_ets_autoplot}
autoplot(fit)
```

Residuals (note, if type = "response", then the fitted values are computed for the h-step forecasts, otherwise the detault is type = 'innovation' in which the regular residuals are used.)
```{r austourists_ets_resid}
cbind('Residuals' = residuals(fit),
      'Forecast errors' = residuals(fit,type='response')) %>%
  autoplot(facet=TRUE) + xlab("Year") + ylab("")
```

Note: 

* Level: estimate of the local mean or level of the data generating process
* Trend: Change between successive data points; at a particular point of time, if trend is 2, the estimated growth between two time points is 2.
* Seasonality: Deviation from the local mean due to seasonality

## Forecasting with ETS models
Point forecast obtained by iterating the equations for each time point and setting all $\varepsilon_t = 0$.

To predict international visitor nights for 2016-2017:

```{r austourists_ets_forecast}
fit %>% forecast(h = 8) %>%
    autoplot() +
  ylab("International visitor night in Australia (millions)")
```

Forecast variance expressions provided in book.


# ARIMA models

Approaches to time series forecasting:

* Exponential smoothing; based on description of the trend and seasonality in the data
* ARIMA; based on description of autocorrelations in the data. Requires understanding of stationarity and differencing techniques.


## Stationarity and differencing
**Stationary time series**: One whose properties does not depend on time at which series is observed; time series with trends, or with seasonality, are not stationary.  The following will rule out a series form being stationary:

* Seasonality
* Trends and chaning levels
* Increasing variance

Recall:  **Cyclic**; when data rises and falls but not of a  fixed frequency and may be related to an economic or business cycle.  If fluctuations are not of a fixed frequency then they are cyclic.  A time series with cyclic behaviour (but with no trend or seasonality) is stationary.      

Stationary time series can be identified using:

* Time series plot
* ACF plot
    * if time series is stationary, ACF will drop to zero relativly quickly (similarly, $r_1$ will often by large and positive)
    * If time series is non-stationary, ACF will decrease sloly

### Differencing

**Differencing**: Difference between consecutive observations

    * Help stability the mean of the time series by removing changes in the level of a time series (and therefore reduce) trend and seasonality.  In case of daily data, the difference time series gives the daily change in the observed values.
    * Makes non-stationary series stationary 
 
 
Transformations: To stability variance of a time series

### Random walk model
Differenced series 

    * Change between consecutive observations in the original series, \[y_t^\prime = y_t - y_{t-1}\]
    * May be referred to as **first differences** to distinguish from seasonal differences.
    * Will only have $T-1$ values, since not possible to calculate $y_1^\prime$ for first observation
    * If white noise, the model for the original series can be written as a random walk model
    $$\begin{aligned}
        y_t - y_{t-1} &= \varepsilon_t \\
        y_t = y_{t-1} + \varepsilon_t
    \end{aligned}$$
    
Random walk model:

    * Widely used for non-stationary data, esp in finance/economics
    * Have long periods of apparent trends (up or down)
    * Sudden and unpreditable changes in direction
    * Forecasts future values equal to last observation
    * Underpins naive forecasts
    
Closely related model in which differences have a non-zero mean:
\[y_t = y_{t-1} + \varepsilon_t + c\]

    * If $c>0$, $y_t$ will drift upwards
    * If $c < 0$, $y_t$ will drift downwards
    * Underpins drift method
    
### Second-order differencing

It may be necessary to difference data twice, in which case $y_t^{\prime \prime}$ will have $T-2$ values.  Usually not necessary to difference more than twice.
$$\begin{aligned}
y_t^{\prime \prime} &= y_t^{\prime} - y_{t-1}^{\prime \prime}\\
                    &= (y_t - y_{t-1}) - (y_{t-1} - y_{t-2})\\
                    &= y_t - 2y_{t-1} - y_{t-2}
\end{aligned}$$

### Seasonal differencing

**Seasonal difference**

* Difference between observation and previous observation from same season
\[y_t^{\prime} = y_t - y_{t-m}\]
where $m$ = number of seasons.

* Referred to as lag-$m$ differences.
* If seasonally differenced appears to be white noise, then appropriate model is:
\[y_t = y_{t-m} + \varepsilon_t\]
* Forecasts are set to last observation from same season
* Interpreted as the change between one year to the next

```{r a10_differencing}
cbind("Sales ($million)" = a10,
      "Monthly log sales" = log(a10),
      "Annual change in log sales" = diff(log(a10),12)) %>%
  autoplot(facets=TRUE) +
    xlab("Year") + ylab("") +
    ggtitle("Antidiabetic drug sales")
```
It may sometimes be necessary to take first differences of seasonal differences, e.g.;
```{r usmelec_differences}
cbind("Billion kWh" = usmelec,
      "Logs" = log(usmelec),
      "Seasonally\n differenced logs" = diff(log(usmelec),12),
      "Doubly\n differenced logs" =  diff(diff(log(usmelec),12),1)) %>%
  autoplot(facets=TRUE) +
    xlab("Year") + ylab("") +
    ggtitle("Monthly US net electricity generation")
```

Formal tests for differencing exist, but can be subjective as to which differeing to apply.

Whilst it makes no difference whether to apply seasonal or first difference first, if stat has a strong seasonal pattern, recommend to do first as it may be all that is required. 

### Unit root tests

* Used to determine, somewhat more objectively, whether differencing is required.
* Exist a number of tests with different assumptions that may lead to different examples
* Example: Kwiatkowski-Philllips-Schmidt-Shin (KPSS) test, via `ur.kpss()` function:
    H$_0$: data is stationary
    H$_a$: data is not stationary, i.e. if p < 0.05, differencing required.  
    
```{r goog_ur}
require(urca)
goog %>% ur.kpss() %>% summary()
```
Given test statistic > 1pct critical value, p-value < 0.01 and so reject null hypothesis.

```{r goog_diff_ur}
goog %>% diff() %>% ur.kpss() %>% summary()
```
Given test statistic < 10 pct critical value, p-value > 0.1 and so accept null hypothesis and conclude that differenced dat are stationary.

The `ndiffs()` function can be used to determine the appropritate number of first differences
```{r goog_ndiffs}
ndiffs(goog)
```

Similarly, the  `nsdiffs()` function can be used to deternube the appropritate number of seasonal differences
```{r usmelec_nsdiffs}
usmelec %>% log() %>% nsdiffs()
#> [1] 1
usmelec %>% log() %>% diff(lag=12) %>% ndiffs()
#> [1] 1
```

## Backshift notation ($B$)

* Useful notation when working with time series lags
* $B$ = "Back up by one time unit"
* $By_t = y_{t-1}$: $B$ operating on $y_t$ has the effect of shifting the data back one period
* $B(By_t) = B^2y_t = y_{t-2}$: $B$ operating on $y_t$ has the effect of shifting the data back two periods period
* $B^{12}y_t$: For monthly data, the same month last year
* Useful for describing the process of _differencing_
    * First order difference: \[y_t^{\prime} = y_t - y_{t-1} = y_t - By_t = (1-B)y_t \]
    * Second order difference: $$
    \begin{aligned}
    y_t^{\prime\prime} &= y_t - 2y_{t-1} - y_{t-2} \\
                                                    &= y_t - 2By_t - B^2y_t \\
                                                    &= (1-B)^2y_t
                                            \end{aligned}$$
                                            
* In general, a $d$th-order difference can be written as \[(1-B)^dy_t\]
* Useful when combining differences, as operator can be treated using ordinary algebraic rules, example: seasonal difference followed by a first difference
$$\begin{aligned}
(1-B)(1-B^m)y_t &= (1-B-B^m + B^{m+1})y_t  \\
                &= y_t - y_{t-1}  - y_{t-m}  - y_{t-m-1}
                                            \end{aligned}$$

## Autoregressive models, AR(p)

* Multiple regression: Forecast variable of interest using linear combination of predictors
* Autogregression: Forecast variable of interest using linear combination of past values of itself
* Autoregressive model of order $p$ model, **AR(p) model**,:
\[y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots +  \phi_p y_{t-p} + \varepsilon_t\]

The variance of the error term will change the scale of the series, not the patterns.  

For an AR(1) model:

    * when $\phi_1 = 0$, $y_t$ is equivalent ot white noise
    * when $\phi_1 = 1$ and $c = 0$, $y_t$ is equivalent to a random walk
    * when $\phi_1 = 1$ and $c \neq 0$, $y_t$ is equivalent to a random walk with drift
    * when $\phi_1 < 0$, $y_t$ tends to oscillate between positive and negative values
    
Autoregressive models generally restricted to stationary data; following restrictions required:

    * AR(1): $|\phi_1| < 1$
    * AR(2): $\phi_2|< 1$, $\phi_2 + \phi_1 < 1$, $\phi_2 - \phi_1 < 1$
    * AR(3) and onwards: complicated!  Let R deal with it.


## Moving average models, MA(q)

* Uses forecast errors in a regression-like model
* Moving average of order $q$: 
\[y_t = c + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots +  \theta_q \varepsilon_{t-q} \]

* Not a regression in the usual sense as the values of $\epsilon_t$ are not observed.
* Each value of $y_t$ ~ weighted moving average of the past few forecast errors
* MA(q) $\neq$  moving average smoothing (`ma` function); MA(q) to forecast future values, while moving average smoothing used to estimate trend-cycle of past values.


Any AR($p$) model can be written as an MA($\infty$) model
$$\begin{aligned}
y_t &= \phi_1 y_{t-1} + \varepsilon_t \\
    &= \phi_1(\phi_1 y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t\\
    &= \phi_1^2 y_{t-2} + \phi_1\varepsilon_{t-1} + \varepsilon_t\\
    &= \phi_1^3 y_{t-3} + \phi_1^2 \varepsilon_{t-2} + \phi_1\varepsilon_{t-1} + \varepsilon_t\\
    \text{etc.}
\end{aligned}$$

Provided $-1 < \phi_1 < 1$, $\phi_1^k \rightarrow 0$ as $k$ gets largers, such that 
\[y_t = \varepsilon_t + \phi_1 \varepsilon_{t-1} + \phi_1^2 \varepsilon_{t-2} + \phi_1^3 \varepsilon_{t-3} + \ldots\]
which is an MA($\infty$) process

If certain constraints imposed on the MA parameters:

    * MA is also invertible; i.e. can write MA($q$) as AR($\infty$)
    * MA model will be mathematically _desirable_

Invertibility constraints (similar to stationarity constraints of AR models):

    * MA(1): $|\theta_1| < 1$,
    * MA(2): $|\theta_2| < 1$,  $\theta_2 + \theta_1 < 1$, $\theta_2 - \theta_1 < 1$
    * MA(3) and onwards: complicated!  Let R deal with it
    
Reasoning behind constraints:  The invertible MA(1) process can be written as AR($\infty$) 
\[\varepsilon_t = \sum_{j=1}^{\infty}(-\theta)^j y_{t-j}\]

* Nonsensical! $|\theta| > 1$: more distant observations will have greater influence on current error
* Nonsensical! $|\theta| = 1$: distant and recent observations will have the same influence on current error
* Sensible! $|\theta| < 1$: more recent observations will have greater influence on current error



## Non-seasonal ARIMA(p,d,q) models

Non-seasonal ARIMA model:

* AutoRegressive Integrated Moving Average
* AR(p) + Differencing(d) + MA(q)
* Integration = antonym of differencing; a time series which needs to be differenced to be made stationary is said to be an "integrated" version of a stationary series
* Model written as:
\[
y_t^\prime = c + \phi_1 y_{t-1}^\prime + \ldots + \phi_p y_{t-p}^\prime +
                + \theta_1 \varepsilon_{t-1} +  \ldots +  \theta_q \varepsilon_{t-q}
                + \varepsilon_t \]
where $y_t^\prime$ is the differenced series and may have been differenced mroe than once and 

* $p$ = order of the autoregressive part (lagged values of $y_t$ as predictors)
* $d$ = degree of first differencing
* $q$ = order of the moving average part (lagged errors as predictors)

Random-walk and random-trend models, autoregressive models, and exponential smoothing models are all special cases of ARIMA models:

-----------------------|---------------------------------
-----------------------|---------------------------------
White noise             | ARIMA(0, 0, 0)
Random Walk             | ARIMA(0, 1, 0) with no constant
Random Walk with drift  | ARIMA(0, 1, 0) with a constant
Autoregression          | ARIMA($p$, 0, 0)
Moving average          | ARIMA(0, 0, $q$)

Combining components to form more complicated models easier when working with backshift ntation.

* AR($p$): 


<!-- Other references:
https://www.quora.com/Whats-the-difference-between-ARMA-ARIMA-and-ARIMAX-in-laymans-terms-What-exactly-do-P-D-Q-mean-and-how-do-you-know-what-to-put-in-for-them-in-say-R-1-0-2-or-2-1-1

https://onlinecourses.science.psu.edu/stat510/node/72/

Time Series Analysis: Forecasting and Control
https://robjhyndman.com/hyndsight/arimax/

TODO: Read these:

https://www.theguardian.com/science/2014/jul/13/fourier-transform-maths-equations-history

https://content.pivotal.io/blog/forecasting-time-series-data-with-multiple-seasonal-periods

-->

Combining components to form more complicated models easier when working with backshift notation.

* ARIMA($p,d,q$) in backshift notation: 

$$\begin{aligned}
y_t^\prime &= c + \phi_1 y_{t-1}^\prime + \ldots + \phi_p y_{t-p}^\prime  +
                + \theta_1 \varepsilon_{t-1} +  \ldots +  \theta_q \varepsilon_{t-q} + \varepsilon_t \\
y_t^\prime - \phi_1 y_{t-1}^\prime - \ldots - \phi_p y_{t-p}^\prime &= c + \varepsilon_t + \theta_1 \varepsilon_{t-1} +  \ldots +  \theta_q \varepsilon_{t-q} \\
(1-B)^d y_t - \phi_1 B (1-B)^d y_t - \ldots - \phi_p B^p (1-B)^d y_t &= c +\varepsilon_t + \theta_1 B \varepsilon_{t} +  \ldots +  \theta_q B^q \varepsilon_{t} \\
 (1-\phi_1 B  - \ldots - \phi_p B^p)(1-B)^dy_t &= c + (1 + \theta_1 B +  \ldots +  \theta_q B^q) \varepsilon_{t} 
\end{aligned}$$

Note that `R` uses a slightly different parameterisation:
\[(1-\phi_1 B  - \ldots - \phi_p B^p)(y_t^\prime - \mu) =  (1 + \theta_1 B +  \ldots +  \theta_q B^q) \varepsilon_{t} \]
where $y_t^\prime = (1-B)^dy_t$, $c = \mu(1 - \phi_1 - \ldots - \phi_p )$ and $\mu$ is the mean of $y_t^\prime$.


**Example**: US consumption expenditure

Fit non-seasonal ARIMA model to quarterly percentage changes in US consumption expenditure:

```{r uschange_autoplot}
autoplot(uschange[,"Consumption"]) +
  xlab("Year") + 
    ylab("Quarterly percentage change")
```

```{r uschange_autoarima}
(fit <- auto.arima(uschange[,"Consumption"]))
```
The resulting ARIMA(2,0,2) model is:
\[y_t  = c + `r round(fit$model$phi[1],3)` y_{t-1} + `r round(fit$model$phi[2],3)` y_{t-2} +  `r round(fit$model$theta[1],3)` \varepsilon_{t-1}  +  `r round(fit$model$theta[2],3)` \varepsilon_{t-2} + \varepsilon_t\]

where $c = `r round(fit$coef["intercept"],3)` (1 - `r round(fit$model$phi[1],3)` - `r round(fit$model$phi[2],3)` ) = `r round(fit$coef["intercept"]*(1 - fit$model$phi[1] -  fit$model$phi[2]),3)`$ and $\epsilon_t$ is white noise with standard deviation of `r round(sqrt(fit$sigma2),3)` = $\sqrt{`r round(fit$sigma2,3)`}$.

Forecasts for 10 quarters:
```{r uschange_arima_forecast}
fit %>% forecast(h = 10) %>% autoplot(include = 80) +  ylab("Quarterly % change in US Expenditure Consumption")

```
```{r uschange_arima_clean, echo = FALSE}
rm(fit)
```

### Understanding ARIMA models

* `auto.arima()` useful, but anything automated can be dangerous unless understand models
* Long-term forecasts will,:
    * if $c = 0$ and $d = 0$, go to zero
    * if $c = 0$ and $d = 1$, go to non-zero constant
    * if $c = 0$ and $d = 2$, follow straight line
    * if $c \neq 0$ and $d = 0$, go to mean of data
    * if $c \neq 0$ and $d = 1$, follow straight line
    * if $c \neq 0$ and $d = 2$, follow quadratic trend
* Impact of $d$ on prediction interval: 
    * $d = 0$, long-term forecast standard deviation $\rightarrow$ standard deviation of historical data
    * Higher $d$, more rapidly prediction intervals increase in size
* Impact of $p$ important for cycles
    * $p \geq 2$ + additional conditions $\rightarrow$ cyclic forecasts
    * E.g. cyclic behaviour occurs if $p = 2$ and $\phi_1^2 + 4\phi_2 < 0$, in which case the average period of the cycles is:
        \[\frac{2\pi}{\text{arc} \cos (-\phi_1(1-\phi_2) / (4\phi_2)}\]




### ACF and PACF plots

* ACF and PACF plots can be used to determine $p$ and $q$
* ACF: if $y_t$ and $y_{t-1}$  are correlated:
    * $y_{t-1}$ and $y_{t-2}$ will be correlated
    * But we don't know if $y$ and $y_{t-2}$ are correlated because the are both connected to $y_{t-1}$ or because of additional information contained in $y_{t-2}$.... PACF to the rescue.
* PACF: Measure relationship between $y_t$ and $y_{t-k}$ after removing the effects of lags $1, 2, 3, \ldots, k-1$.  Note:
    *  $\alpha_k$, the $k$th partial autocorrelation coefficient = $\phi_k$ in AR($k$) model.
    * partial autocorrelations have same critical values of $\pm/\sqrt{T}$
 
If both $p$ and $q$ positive, then plots not useful for determining values. However, plots can be helpful if data are from ARIMA($p,d,0$) or ARIMA($0,d,q$):

* ARIMA($p,d,0$):
    * ACF exponentially decaying or sinusoidal
    * Significant spike at lag $p$ in PACF but none beyond lag $p$
* ARIMA($0,d,q$):
    * PACF is exponentially decaying or sinusoidal
    * Significant spike at lag $q$ in the ACF, but non beyond lag $q$
  
Example ACF:   
```{r uschange_acf, fig.height = 3, fig.width = 6}
ggAcf(uschange[,"Consumption"],main="")
```


Example PACF:   
```{r uschange_pacf, fig.height = 3, fig.width = 6}
ggPacf(uschange[,"Consumption"],main="")
```


Example ACF and  PACF with one command:   
```{r uschange_acf_pacf, fig.height = 3, fig.width = 6}
ggtsdisplay(uschange[,"Consumption"],main="")
```

In this example, we see:

* Exponentially decaying ACF
* Significant spike at lag 3 in PACF, and none afterwards (ignoring the one significant spike at lag 22 as it is just outside the limits and not in the first few lags, recall multiple test error)
* Suggests ARIMA(3,0,0) process

```{r uschange_arima300}
Arima(uschange[,"Consumption"], order=c(3,0,0))
```

Note:

* This model slightly better than that obtained using `auto.arima()` based on AICc.
* `auto.arima()$ does not consider all possible models; can be improved by setting `stepwise=FALSE` and `approximation = FALSE`.

```{r uschange_autoarima_accurate}
auto.arima(uschange[,"Consumption"], seasonal=FALSE, stepwise=FALSE, approximation=FALSE)
```


## Estimation and order selection



Steps in fitting ARIMA model:

1. Identify model order (i.e. $p$, $d$ and $q$) 
2. Estimate parameters $c, \phi_1, \ldots \phi_p, \theta_1, \ldots, \theta_q$ using **MLE**.  Note that different software will give slightly different answers; _log likelihood_ (logarithm of the probability of the observed data coming from the estimated model) usually reported  
3. Aim: Minimise one of the following criteria (authors preference is AICc) in order to determine $p$ and $q$.  For $d$, need another approach as differencing changes data on which likelihood is computed and so criterion values not comparable for models with different values of $d$.

Criterion | Formula
---------------------------------------------------------|-----------------------------------------------------------------
Akaike's Information Criterion (AIC) | $\text{AIC} = -2\log{L} + 2(p + q + k + 1)$ 
Corrected Akaike's Information Criterion (AICc) | $\text{AIC}_c = \text{AIC} + \frac{2(p + q + k + 1)(p + q + k + 2)}{(T -p - q - k - 2)}$ 
Bayesian Information Criterion (BIC) | $\text{AIC} = \text{AIC} + [log(T)(p + q + k + 1)]$ 


## ARIMA modelling in R

### How does `auto.arima()` work?

Algorithm: 

1. Determine number of differences $0 \leq d \leq 2$ using repeated KPSS tests
2. Determine $p$ and $q$ using stepwise search as follows:
    a. Fit initial models
        * p = q = 0 (with constant if $d \leq 1$)
        * p = q = 0 (with no constant but only if  $d \leq 1$, otherwise equivalent to above!)
        * p = 1, q = 0 (with constant if $d \leq 1$)
        * p = 0, q = 1 (with constant if $d \leq 1$)
        * p = q = 2 (with constant if $d \leq 1$)
    b Choose model with smallest AICc as current model
    c. Vary current model by adding/subtracting 1 from $p$ or $q$, or by including/excluding $c$ from current model.   Repeat until no lower AICc can be found.
    
Set `appromiation = FALSE` to avoid approximations being used to speed up search.  Use `stepwise = FALSE` to search a larger set of models.


### Choosing your own model

* Use `Arima()` function to choose the model yourself
* Although `arima()` exists, not recommended as it does allow:
    * constant $c$ unless $d=0$ 
    * `forecast` package to work
    * application of model to new data
    

### Modelling procedure

Recommended approach:

1 Plot data to look for unusual observations
2 Transform data to stabilise variance (Box-Cox)
3 Take first differences of data until data stationary
4 Examine ACF/PACF: Is ARIMA($p,d,0$) or ARIMA($0,d,q$) appropriate
5 Try chosen model(s) and use AICc to search for better model
6 Plot ACF of residual and do portmanteau test; if residuals do not look like white noise try a modified model
7 Calculate forecasts

Only steps 3-5 taken care of by `auto.arima()`.

**Example**: Seasonally adjusted electrical equipment orders

**Step 1** Plot seasonally adjusted data: shows sudden changes (GFC), but nothing unusual
```{r elecequip_step1, fig.height = 3, fig.width = 6}
elecequip %>% stl(s.window='periodic') %>% seasadj() -> eeadj
autoplot(eeadj)
```

**Step 2**  Based on plot, doesn't appear necessary to do Box-Cox transformation

**Step 3**  Data clearly not stationary, therefore difference.  First differenced data appears stationary.
```{r elecequip_step3}
eeadj %>% diff() %>% ggtsdisplay(main="")
```

**Step 4**  ACF and PACF suggest ARIMA(3,1,0) process as:
    * ACF __sinusoidal__
    * Significant spike at lag 3 in PACF but none beyond lag 3 (with few exceptions at much later lags)
    
**Step 5**  Try chosen ARIMA(3,1,0) model and then variations to compare AICc:
```{r elecequip_step5, message=FALSE, warning=FALSE}
fit1 <- Arima(eeadj, order=c(3,1,0))
fit2 <- Arima(eeadj, order=c(4,1,0))
fit3 <- Arima(eeadj, order=c(2,1,0))
fit4 <- Arima(eeadj, order=c(3,1,1))

data.frame(model = c("orig", "p + 1", "p-1", "q+1"), aicc = c(fit1$aicc, fit2$aicc, fit3$aicc, fit4$aicc)) %>%
    mutate(aicc = round(aicc,1)) %>% kable()

```

```{r elecequip_step5_clean, echo = FALSE}
rm(fit1, fit2, fit3, fit4)
```


Note that ARIMA(3,1,1) is slightly lower (but is it significantly lower?)

**Step 6** ACF plot of residual shows no significant lags; portmanteau test returns large $p$ value and therefore accept null hypothesis that residuals are white noise.
```{r elecequip_step6}
fit <- Arima(eeadj, order=c(3,1,1))
checkresiduals(fit)
```
**Step 7** Forecast from chosen model
```{r elecequip_step7}
autoplot(forecast(fit))

```
```{r elecequip_step7_clean, echo = FALSE}
rm(eeadj, fit)
```

    
### Understanding constants in R
Apparently the following three are equivalent (although I unfortunately couldn't determine the algebra )
$$\begin{aligned}
 (1-\phi_1 B  - \ldots - \phi_p B^p)(1-B)^dy_t &= c + (1 + \theta_1 B +  \ldots +  \theta_q B^q) \varepsilon_{t} \\
 (1-\phi_1 B  - \ldots - \phi_p B^p)(1-B)^d(y_t - \mu t^d/d!) &=  (1 + \theta_1 B +  \ldots +  \theta_q B^q) \varepsilon_{t} \\
 (1-\phi_1 B  - \ldots - \phi_p B^p)(y_t^\prime - \mu) &=  (1 + \theta_1 B +  \ldots +  \theta_q B^q) \varepsilon_{t} \\
\end{aligned}$$


where $y_t^\prime = (1-B)^dy_t$, $c = \mu(1 - \phi_1 - \ldots - \phi_p )$ and $\mu$ is the mean of $y_t^\prime$.
and so:

* When the process is non-stationary (i.e. a $d$ term exists), the constant is equivalent to a polynomial trend of order $d$
* When $d = 0$, then $\mu=\bar{y}_t$ and `Arima()` provides an estimate.  This will be close to the sample mean of the time series but not exact as sample mean is not MLE when $p + q > 0$.  Setting `include.mean = FALSE` will force $\mu = c = 0$
* When $d > 0$, `Arima()` sets $c = \mu = 0$
    * When $d = 1$ then setting `include.drift` or `include.constant` allows $\mu \neq 0$
    * When $d > 1$, no constant allowed as quadratic or higher order trend dangerous when forecasting.
 
## Forecasting


Steps to obtain **point forecasts**: 


* Expand ARIMA equation so that $y_t$ is alone on LHS.  Example: ARIMA(3,1,1) model can be re-written as:
\[y_t = (1 + \hat{\phi}_1)y_{t-1} - (\hat{\phi}_1 - \hat{\phi}_2)y_{t-2} - (\hat{\phi}_2 - \hat{\phi}_3)y_{t-3} + \hat{\phi}_3y_{t-4} + \varepsilon_t + \hat{\theta}_1 \varepsilon_{t-1}\]

* Rewrite equation, replacing $t$ by $T+h$, starting with $h = 1$ then incrementing $h$ as required
\[y_{T+1} = (1 + \hat{\phi}_1)y_{T} - (\hat{\phi}_1 - \hat{\phi}_2)y_{T-1} - (\hat{\phi}_2 - \hat{\phi}_3)y_{T-2} + \hat{\phi}_3y_{T-3} + \varepsilon_{T+1} + \hat{\theta}_1 \varepsilon_{T}\]
 
* Replace:
    * Future observations with their forecasts (none at lag 1)
    * Future errors with zero (in e.g., set $\varepsilon_{T+1} = 0$)
    * Past errors with residuals (in e.g., set $\varepsilon_{T} = e_T$, the last observed residual )


    
**Prediction intervals**

* Calculations provided in book
* Based on assumption that residual are uncorrelated and normally distributed, otherwise prediction intervals incorrect.  Therefore, always plot ACF and histogram of residuals to check assumptions before producing prediction intervals.
* For stationary models ($d=0$), prediction intervals will converge and so for long horizons all the same
* For non-stationary models ($d > 0$), prediction intervals will continue to grow into the future


## Seasonal ARIMA models, ARIMA(p,d,q)(P,D,Q)$_m$

* $m$ = number of observations per year
* E.g. ARIMA(1,1,1)(1,1,1)$_4$ without constant:
    * Quarterly data (m = 4)
    * $(1-\phi_1B)(1-\Phi_1B^4)(1-B)(1-B^4)y_t = (1 + \theta_1B)(1 + \Theta_1B^4)\varepsilon_t$
    

ARIMA(0,0,0)(1,0,0)$_{12}$

* Exponential decay in seasonal lags of __ACF__ 
* Spike at lag 12 in __PACF__

ARIMA(0,0,0)(0,0,1)$_{12}$

* Spike at lag 12 in __ACF__ , no other significant spikes
* Exponential decay in seasonal lags of __PACF__ (at lags 12, 24, 36, ...)



**Example**: European quarterly retail trade

Data:
```{r euretail_data, fig.height = 3, fig.width = 6}
autoplot(euretail) + ylab("Retail index") + xlab("Year")
```

* First take seasonal difference (before considering other non-stationarity)

```{r euretail_seasdiff}
euretail %>% diff(lag=4) %>% ggtsdisplay()
```

* Still non-stationarity exists, therefore take difference
```{r euretail_seasdiff_diff}
euretail %>% diff(lag=4) %>% diff() %>% ggtsdisplay()
```

* Now need to find ARIMA model based on ACF and PACF:
    * Significant spike at lag 4 in PACF and ACF suggest seasonal MA(1) (or seasonal AR(1)) lag
    * Significant spike at lag 1 suggest MA(1) (or AR(1))
    
```{r euretail_ma1}
euretail %>%  Arima(order = c(0, 1, 1), seasonal = c(0, 1, 1)) -> fit1 
euretail %>%  Arima(order = c(1, 1, 0), seasonal = c(1, 1, 0)) -> fit2
fit1$aicc

fit2$aicc

fit1 %>% residuals() %>% ggtsdisplay()

```
Given there exists significant lag at 2 and almost at 3, consider other models:
```{r euretail_ma2}
euretail %>%  Arima(order = c(0, 1, 2), seasonal = c(0, 1, 1)) -> fit3
euretail %>%  Arima(order = c(0, 1, 3), seasonal = c(0, 1, 1)) -> fit4
fit1$aicc
fit3$aicc
fit4$aicc
fit4 %>% checkresiduals()
```
As all spikes are within significance limits and residuals appear to be white noise, can use model for forecasting (next three years):
```{r euretail_forecast}
fit4 %>% forecast(h=12) %>% autoplot()
```

* Forecasts follow recent trend in data
* Large and rapidly increasing prediction intervals show could increase or decrease at any time (i.e. while point forecast trend downward, prediction interval allows data to trend upwards during forecast period.)

```{r euretail_autoarima}
auto.arima(euretail) # shortcuts
auto.arima(euretail, stepwise = FALSE, approximation = FALSE)

```
```{r euretail_auto_clean, echo = FALSE}
rm(list = ls(pattern = "^fit"))
```

    
**Example**: Corticosteroid drug sales in Australia

Data; take logarithm to stabilise variance
```{r h02_data}
lh02 <- log(h02)
cbind("H02 sales (million scripts)" = h02, "Log H02 sales"=lh02) %>%
  autoplot(facets=TRUE) + xlab("Year") + ylab("")
```

Seasonally differenced data:
```{r h02_season_diff}
lh02 %>% diff(lag=12) %>%
  ggtsdisplay(xlab="Year",
    main="Seasonally differenced H02 scripts")
```

Initial model to try: ARIMA(3,0,0)(2,1,0)[12]

* Seasonal lag in PACF at 12, 24, .. -> Seasonal AR(2)
* Exponentially declining ACF and significant lags in PACF at 1, 2, 3 -> AR(3)

Recall that the Box-Cox transformation with $\lambda = 0$ is the logarithmic transformation.
```{r h02_arima}
 res <- data.frame(p=rep(3,7), d = rep(0,7), q = c(1,1,1,1,0,2,1), P = c(0,1,0,2,2,2,1), D = rep(1,7), Q = c(2,1,1,0,0,0,0)) %>%
    mutate(model = paste0("ARIMA(", p, ",", d, ",", q, ")(", P, ",", D, ",", Q, ")[12]")) %>%
    group_by(model) %>%
    nest() %>%
    mutate(fit = map(data, ~Arima(h02, order = c(.x$p, .x$d, .x$q), seasonal = c(.x$P, .x$D, .x$Q), lambda = 0))) %>%
    mutate(aicc = map_dbl(fit, "aicc"))

res %>% select(model, aicc)

```

Of all models, ARIMA(3,0,1)(0,1,2)[12], has the smallest AICc (noting the negative values!)
```{r h02_arima_best}
# note can also use first()
(fit <- res %>% filter(model == "ARIMA(3,0,1)(0,1,2)[12]") %>% select(fit) %>% pull() %>% `[[`(1))
checkresiduals(fit, lag = 36)
```

* There are still significant spikes in the ACF and model fails Ljung-Box test
* Model can be used for forecasting and prediction intervals may not be accurate.
* Sometimes it is not possible to find a model that passes all of the tests

Test set evaluation:

* Fit model using data from July 1991 to June 2006
* Forecast data for July 2006 to June 2008


```{r h02_test_arima}

res <- data.frame(
    p = c(rep(3,7),2,2), 
    d = c(rep(0,7),1,1), 
    q = c(1,1,1,1,0,2,1,4,3), 
    P = c(0,1,0,2,2,2,1,0,0), 
    D = c(rep(1,7),1,1), 
    Q = c(2,1,1,0,0,0,0,1,1)
    ) %>%
    mutate(model = paste0("ARIMA(", p, ",", d, ",", q, ")(", P, ",", D, ",", Q, ")[12]")) %>%
    group_by(model) %>%
    nest() %>%
    mutate(fit = map(data, ~Arima(window(h02, start = c(1991, 7), end = c(2006, 6)), order = c(.x$p, .x$d, .x$q), seasonal = c(.x$P, .x$D, .x$Q), lambda = 0)),
           pred = map(fit, ~forecast(.x, h = 24)),
           accuracy = map(pred, ~accuracy(.x, h02)), 
           rmse = map_dbl(accuracy, function(x) x[2, "RMSE"]))

select(res, model, rmse)


```

* When comparing models using test set, comparisons are always valid (i.e. can include some with no and first differencing), unlike when comparing models based on AICc.
* No model pass all residuals test, in practice use best model could find, even if it could not pass all tests

```{r h02_arima_forecast_test}
res %>% filter(model == "ARIMA(3,0,1)(0,1,2)[12]") %>% select(pred) %>% pull() %>% first() %>% autoplot(PI = FALSE) +  ylab("H02 sales (million scripts)") + xlab("Year")


```

```{r h02_arima_clean, echo = FALSE}
rm(fit, res, lh02)
```

## ARIMA vs ETS

* Linear exponential smoothing models are special cases of ARIMA models
* Non-linear exponential models have no ARIMA equivalent; equally there exist ARIMA models with no equivalent ETS model.
* All ETS models are non-stationary; there exist stationary ARIMA models.

By this, it is meant, as per [Stack Exchange](https://stats.stackexchange.com/questions/76696/non-stationary-time-series-forecasting) 

> Exponential smoothing methods are only really appropriate if the data are non-stationary. Using an exponential smoothing method on stationary data is not wrong but is sub-optimal.

Recall: ETS(Error,Trend,Seasonality) where:

* Error = {Additive, Multiplicative}
* Trend = {None, Additive, Additive Damped}
* Seasonal = {None, Additive, Multiplicative}

ETS Model | ARIMA Model 
-----------------------| -------------------------
ETS(A,N,N) | ARIMA(0,1,1) 
ETS(A,A,N) | ARIMA(0,2,1) 
ETS(A,A$_D$,N) | ARIMA(1,1,2) 
ETS(A,N,A) | ARIMA(0,1,m)(0,1,0)$_m$ 
ETS(A,A,A) | ARIMA(0,1,m+1)(0,1,0)$_m$ 
ETS(A,A$_D$,A) | ARIMA(1,1,m+1) (0,1,0)$_m$

* Equivalence of parameters also provided.  
* AICc cannot be used to compare models of different classes (i.e. to compare ARIMA and ETS model), because likelihood computed in different ways. Only train/test or cross-validation can be used.


**Example**: Comparing auto.arima() and ets() on non-seasonal data
```{r air_multimodel}
air <- window(ausair, start=1990)

fets <- function(x, h) {
  forecast(ets(x), h = h)
}

farima <- function(x, h) {
  forecast(auto.arima(x), h=h)
}

tribble(~model, ~func,
             'fets', fets,
             'farima', farima) %>%
    mutate(e = map(func, ~tsCV(y=air, forecastfunction = .x, h = 1)), 
           mse = map_dbl(e, ~mean(.^2, na.rm = TRUE)))

air %>% ets() %>% forecast() %>% autoplot()


```
```{r air_multimodel_clean, echo = FALSE}
rm(air, farima, fets)
```


**Example**: Comparing auto.arima() and ets() on seasonal data
```{r qcement_multimodel}

require(fpp2); require(forecast); require(tidyverse)
# Consider the qcement data beginning in 1988
cement <- window(qcement, start=1988)

# Use 20 years of the data as the training set
train <- window(cement, end=c(2007,4))

res <- tribble(~func,
             'auto.arima',
             'ets') %>%
    mutate(fit = invoke_map(func, list(train)),
           pred = map(fit, ~forecast(., h = 4*(2013-2007)+1)),
           accuracy = map(pred, ~accuracy(., qcement)),
           train = map(accuracy, function(x) x[1, ]),
           test = map(accuracy, function(x) x[2, ]))

res %>% unnest(test, train, .preserve = func) %>% 
    
    # How do I get the name of the list of lists instead of having to do the following line?
    mutate(metric = rep(names(res$train[1][[1]]), 2)) %>%
    filter(metric %in% c("RMSE", "MAE", "MAPE", "MASE")) %>% select(func, metric, train, test) %>%
    gather("data", "value", -func, -metric) %>%
    mutate(value = round(value, 3)) %>%
    spread(func, value) %>%
    arrange(desc(data), metric) 


```
See that ARIMA fits training data slightly better, but ETS model providers more accurate forecasts on test set.  

```{r qcement_ets}
# Generate forecasts from an ETS model
cement %>% ets() %>% forecast(h=12) %>% autoplot()

```
```{r qcement_ets_clean, echo = FALSE}
rm(res, cement, train)
```


# Dynamic regression models

Allow error in regression model to contain autocorrelation following an ARIMA model (to emphasise, $\epsilon_t$ replaced with $\eta_t$), e.g. if error follows ARIMA(1,1,1):
$$\begin{aligned}
y_t = \beta_0 + \beta_1 x_{1,t} + \ldots + \beta_{k,t} + \eta_t,\\
(1 - \phi_1B)(1-B)\eta_t = (1 + \theta_1B)\epsilon_t
\end{aligned}$$

Model has two error terms:
1. Regression model error ($\eta_t$)
2. ARIMA model error ($\epsilon_t$), assumed to be white noise

## Estimation

* Need to minimise SSE($\epsilon_t$), not SSE($\eta_t$)
* All variables need to be stationary (i.e. response variable and predictors); otherwise estimates will not be consistent / meaningful.  Exception: if non-stationary variables are co-integrated (i.e. if there is a linear combination of the non-stationary $y_t# and the predictors that is stationary)
* Usually best to difference all variables if at least one needs differencing.  
* New definitions:
    * **Model in levels**: Model based on original data (with no differencing)
    * **Model in differences**: Model with all variables differenced
* A regression model with ARIMA models is equivalent to a regression model in differences with ARMA errors (i.e. it is missing the "I" for integrated/differencing)


$$\begin{aligned}
y_t = \beta_0 + \beta_1 x_{1,t} + \ldots + \beta_{k,t} + \eta_t,\\
(1 - \phi_1B)(1-B)\eta_t = (1 + \theta_1B)\epsilon_t
\end{aligned}$$

$$\begin{aligned}
y_t = \beta_0 + \beta_1 x_{1,t}^\prime + \ldots + \beta_{k,t}^\prime + \eta_t^\prime,\\
(1 - \phi_1B)\eta_t^\prime = (1 + \theta_1B)\epsilon_t
\end{aligned}$$



## Regression and ARIMA errors in R

* `Arima()` with argument `xreg`

    * Fits regression model with ARIMA errors
    * `order` specifies order of ARIMA error model
    * Differencing applied to all variables, if specified.
    * Set `include.drift = TRUE` to include constant term in differenced model

* `auto.arima()` with argument `xreg` 

    * Requires specification of the predictors
    * Will determine best ARIMA model for the errors
    * Best predictors can be determined via AICc; repeat for all subsets of predictors to be considered and select model with lowest AICc
    
**Example**: US Personal Consumption and Income

Aim: Forecast changes in expenditure based on changes in income, noting that a change in income may not lead to an instant change in consumption; but ignored here.

```{r uschange_regarima_data}

autoplot(uschange[,1:2], facets=TRUE) +
  xlab("Year") + ylab("") +
  ggtitle("Quarterly changes in US consumption  and personal income")

```
No need for any differencing as already stationary.  


```{r uschange_regarima}

(fit <- auto.arima(uschange[,"Consumption"], xreg=uschange[,"Income"]))

```

Fitted model is a regression model with ARMA(1,2) error:
$$\begin{aligned}
y_t  = `r round(fit$coef["intercept"],3)` + `r round(fit$coef["xreg"],3)` x_{t} + \eta_t \\
\eta_t = `r round(fit$coef["ar1"],3)`\eta_{t-1} + \varepsilon_t + 
`r round(fit$coef["ma1"],3)`\varepsilon_{t-1} + `r round(fit$coef["ma2"],3)`\varepsilon_{t-2}\\
\epsilon_t \sim \text{NID}(0,`r round(fit$sigma2,3)`)
\end{aligned}$$


Regression and ARIMA errors:

```{r uschange_regarima_residuals}
cbind("Regression Errors" = residuals(fit, type="regression"),
      "ARIMA errors" = residuals(fit, type="innovation")) %>%
  autoplot(facets=TRUE)
```

Check ARIMA errors for white noise series:

```{r uschange_regarima_arima_residuals}
checkresiduals(fit)

```
```{r uschange_regarima_arima_residuals_clean, echo = FALSE}
rm(fit)
```


## Forecasting

To forecast:

* Forecast regression part: First forecast predictors using separate model or assumed future values
* Forecast ARIMA part
* Combine results

**Example**: US Personal Consumption and Income

Continuing on with example to: Forecast changes in expenditure based on changes in income, noting that a change in income may not lead to an instant change in consumption; but ignored here.

Assumption: Future percentage change in personal disposable income = mean percentage change from the last 40 years

```{r us_regarima_forecast}
fit <- auto.arima(uschange[,"Consumption"], xreg=uschange[,"Income"])

fcast <- forecast(fit, xreg=rep(mean(uschange[,2]),8))

autoplot(fcast) + xlab("Year") +  ylab("Percentage change")
```

```{r us_regarima_clean, include=FALSE, echo = FALSE}
rm(fit, fcast)
```


Prediction interval now smaller than with ARIMA model only; i.e. income change explains some of the variation

**Note** Prediction intervals of regression models (with/without ARIMA errors) do not take into account uncertainty in the forecasts of the predictors; prediction interval therefore condition on estimated future values of predictor variables


**Example**: Forecasting electricity demand
```{r elec_temp_demand}

ggplot(data.frame(elecdaily)) + 
    geom_point(aes(Temperature, Demand))

autoplot(elecdaily[, c("Demand", "Temperature")], facets = TRUE) + ylab("")


```
U-Shape suggests quadratic regression model.

```{r elecdaily_xreg}
xreg <- cbind(MaxTemp = elecdaily[, "Temperature"],
              MaxTempSq = elecdaily[, "Temperature"]^2,
              Workday = elecdaily[, "WorkDay"])

fit <- auto.arima(elecdaily[, "Demand"], xreg = xreg)

checkresiduals(fit)
```

This model has significant autocorrelation in residuals, therefore prediction intervals are not reliable (and should not be shown).  Can provide point forecast though:

* For 14 days ahead
* For illustration, temperature set to constant 26 degrees (scenario based forecasting)


```{r elecdaily_xreg_forecast}
fcast <- forecast(fit,
  xreg = cbind(rep(26,14), rep(26^2,14),
    c(0,1,0,0,1,1,1,1,1,0,0,1,1,1)))

#?autoplot.forecast
autoplot(fcast, PI = FALSE) + ylab("Electricity demand (GW)")



```

```{r elecdaily_xreg_clean, include=FALSE, echo = FALSE}
rm(fcast, fit, xreg)
```


## Stochastic and deterministic trends

Linear trend can be modelled as:

* **Deterministic trend**: Regression model where $\eta_t$ is an ARMA process

$$\begin{aligned}
y_t = \beta_0 + \beta_1 t + \eta_t,\\
(1 - \phi_1B)\eta_t = (1 + \theta_1B)\epsilon_t
\end{aligned}$$

* Obtained in `auto.arima()` by setting `d=0`.
* Assumes slope of trend is not going to change over time
    
* **Stochastic trend**:  Regression model where $\eta_t$ is an ARIMA process with $d$ = 1.  This is equivalent to $\eta_t^\prime$ being an ARMA process

$$\begin{aligned}
y_t - y_{t-1} = \beta_1 + \eta_t^\prime,\\
(1 - \phi_1B)\eta_t^\prime = (1 + \theta_1B)\epsilon_t
\end{aligned}$$

The regression model can be rewritten as $y_t = $y_{t-1} + \beta_1 + \eta_t^\prime$ is is similar to a random walk model with drift, but different in that the error term is an ARMA process rather than white noise.

* Obtained in `auto.arima()` by setting `d=1`.
* Doesn't assume that rate of growth will continue
* Safer to use for forecasting, specially for longer forecast horizons as prediction itnervals allows for greater uncertainty in future growth.
    
**Example**: International visitors to Australia

Data:
```{r austa_preview, fig.width=6, fig.height=3}
autoplot(austa) + xlab("Year") +
  ylab("millions of people") +
  ggtitle("Total annual international visitors to Australia")
```

Deterministic trend model:
```{r austa_deterministic} 
trend <- seq_along(austa)
(fit1 <- auto.arima(austa, d=0, xreg=trend))
```

$$\begin{aligned}
y_t  &= `r round(fit1$coef["intercept"],3)` + `r round(fit1$coef["xreg"],3)` x_{t} + \eta_t \\
\eta_t &= `r round(fit1$coef["ar1"],3)`\eta_{t-1} + `r round(fit1$coef["ar2"],3)`\eta_{t-2}  + \varepsilon_t\\
\epsilon_t &\sim \text{NID}(0,`r round(fit1$sigma2,3)`)
\end{aligned}$$

As per regression equation, the estimated growth in visitor numbers is 0.17 million people per year.


Stochastic trend model:
```{r austa_stochastic} 
trend <- seq_along(austa)
(fit2 <- auto.arima(austa, d=1))
```

The model is as follows:
$$\begin{aligned}
y_t - y_{t-1}  &= `r round(fit2$coef["drift"],3)`  + \eta_t^\prime \\
\eta_t^\prime  &=   
`r round(fit2$coef["ma1"],3)`\varepsilon_{t-1} + \varepsilon_t\\
\epsilon_t &\sim \text{NID}(0,`r round(fit2$sigma2,3)`)
\end{aligned}$$

Equivalently, written as:
$$\begin{aligned}
y_t   &= y_0 + `r round(fit2$coef["drift"],3)`t  + \eta_t^\prime \\
\eta_t &=  \eta_{t-1}  + 
`r round(fit2$coef["ma1"],3)`\varepsilon_{t-1} + \varepsilon_t \\
\epsilon_t &\sim \text{NID}(0,`r round(fit2$sigma2,3)`)
\end{aligned}$$

Similar to the deterministic trend model, estimated growth in visitor numbers is 0.17.

Difference between models is in prediction intervals; stochastic trend model has much wider prediction interval due to non-stationary error

```{r austa_regtrend_forecast} 
fc1 <- forecast(fit1, xreg = cbind(trend = length(austa) + 1:10))
fc2 <- forecast(fit2, h=10)

autoplot(austa) +
  autolayer(fc2, series="Stochastic trend") +
  autolayer(fc1, series="Deterministic trend") +
  ggtitle("Forecasts from trend models") +
  xlab("Year") + ylab("Visitors to Australia (millions)") +
  guides(colour=guide_legend(title="Forecast"))


```

```{r austa_regtrend_clean, echo=FALSE, message=FALSE, warning=FALSE}
rm(fc1, fc2, fit1, fit2, trend)
```


## Dynamic harmonic regression

Data Frequency | Seasonality (example only)
---------------|-----------------------------
Daily   | 365
Weekly  | 52
Half/Hourly | 48 (shortest)

* ARIMA, ETS designed for shorter periods, e.g. for monthly data (12) or quarterly (4)
* `ets()` allows maximum frequency of $m=24$, equivalent to 23 parameters to be estimated
* `Arima()` and `auto.arima()` allow maximum $m = 350$, but will usually run out of memory at $m > 200$.
* Seasonal differencing of high order does not make much sense, e.g. for daily data: What happened today vs what happened exactly 1 year ago with no constraint on a smooth seasonal pattern


Recommended approach: **Harmonic regression**.  

* Seasonal pattern modelled using Fourier terms and short-term series dynamics handled by ARMA error.
* Advantages:
    * Any length seasonality allowed
    * Can have different seasonal periods (each with their own Fourier terms)
    * THe number of fourier terms ($K$ is the number of sine and cosine terms to return)
    * For small $K$, seasonal pattern is smooth
    * Short-term dynamics handles with simple ARMA error
* Disadvantage (compared to seasonal ARIMA model)
    * Seasonality assumed to be fixed (i.e. pattern not allowed to change over time) (however usually constant except for long time series)
    
**Example**: Australian eating out expenditure

* Monthly data
* Seasonality - Fourier
* Other dynamics - ARIMA errors
* `forecast::fourier()`: 
    * `x`: Seasonal time series.
    * `K`: Maximum order(s) of Fourier terms
    * `h`: Number of periods ahead to forecast 
    * Period of Fourier terms is determined from the ts characteristics of $x$.

In following example, AICc minimised for $K = 5$.
```{r auscafe_harmonic}

cafe04 <- window(auscafe, start=2004)
plots <- list()
for (i in seq(6)) {
  fit <- auto.arima(cafe04, xreg = fourier(cafe04, K = i), seasonal = FALSE, lambda = 0)
  plots[[i]] <- autoplot(forecast(fit, xreg=fourier(cafe04, K=i, h=24))) +
    xlab(paste("K=",i,"   AICC=",round(fit[["aicc"]],2))) +
    ylab("") + ylim(1.5,4.7)
}
gridExtra::grid.arrange( plots[[1]],plots[[2]],plots[[3]], plots[[4]],plots[[5]],plots[[6]], nrow=3)



```

```{r auscafe_harmonic_clean, message=FALSE, warning=FALSE, include=FALSE, echo=FALSE}
rm(fit, plots, i, cafe04)
```


## Lagged predictors

* Predictor impact may not occur for some time
* Predictor impact may diminish over time
* With one predictor, model allowing for lagged effects is:
\[y_t = \beta_0 + \gamma_0 x_t + \gamma_1 x_{t-1} + \ldots + \gamma_k x_{t-k} + \eta_t\]
where $\eta_t$ is an ARIMA process.  Value of $k$ (regression) and $p$ and $q$ (ARIMA) can be determined using AICc.

**Example**: TV advertising and insurance quotations

Aim: Use ad expenditure to predict insurance quotations (and therefore new policies)

* Consider that ad expenditure can impact sales for current and following 3 months (i.e. test with $k$ = 0, 1, 2 and 3.)
* In order to compare models using same data, exclude first three months 

```{r insurance_data}
autoplot(insurance, facets=TRUE) +
  xlab("Year") + ylab("") +
  ggtitle("Insurance advertising and quotations")
```

* Data (lag 0) is for Jan 2002 until April 2005.
* Lag 1 will be from Dec 2002 
* Lag 2 will be from Nov 2002 
* Lag 3 will be from Oct 2002

Note: In the following, if you do not include `head(nrow(insurance))`, the lagged values will have greater number of rows than the response variable.

```{r insurance_lags}
# Lagged predictors. Test 0, 1, 2 or 3 lags.
Advert <- cbind(
    AdLag0 = insurance[,"TV.advert"],
    AdLag1 = stats::lag(insurance[,"TV.advert"],-1),
    AdLag2 = stats::lag(insurance[,"TV.advert"],-2),
    AdLag3 = stats::lag(insurance[,"TV.advert"],-3))  %>%
  head(nrow(insurance))

max.lag <- 4
N <- nrow(insurance)
# Restrict data so models use same fitting period
fit1 <- auto.arima(insurance[max.lag:N,1], xreg=Advert[max.lag:N,1], stationary=TRUE)
fit2 <- auto.arima(insurance[max.lag:N,1], xreg=Advert[max.lag:N,1:2], stationary=TRUE)
fit3 <- auto.arima(insurance[max.lag:N,1], xreg=Advert[max.lag:N,1:3], stationary=TRUE)
fit4 <- auto.arima(insurance[max.lag:N,1], xreg=Advert[max.lag:N,1:4], stationary=TRUE)

# AICc
c(fit1[["aicc"]],fit2[["aicc"]],fit3[["aicc"]],fit4[["aicc"]])


```
```{r insurance_lags_clean, include=FALSE, echo = FALSE}
rm(max.lag, N, fit1, fit2, fit3, fit4)
```


Best model (based on AICc) has 2 lagged predictors; advertising have effect on quotes for current and following month.  Can now re-estimate model with all data.

```{r insurance_chosen_lag}
(fit <- auto.arima(insurance[,1], xreg=Advert[,1:2],  stationary=TRUE))
```

Resulting model: 2-period lagged regression model with AR(3) errors;

$$\begin{aligned}
y_t  &= `r round(fit$coef["intercept"],3)` + `r round(fit$coef["AdLag0"],3)` x_{t} + `r round(fit$coef["AdLag1"],3)` x_{t-1} + \eta_t \\
\eta_t &= `r round(fit$coef["ar1"],3)`\eta_{t-1} + `r round(fit$coef["ar2"],3)`\eta_{t-2}  + `r round(fit$coef["ar3"],3)`\eta_{t-3}  + \varepsilon_t\\
\epsilon_t &\sim \text{NID}(0,`r round(fit$sigma2,3)`)
\end{aligned}$$


Forecasts can be calculating by assuming future values for advertising variable.  Here set to 8 units; forecasts made for next 20 months.
```{r insurance_lag_forecast}
fc8 <- forecast(fit, 
                h=20,  
                xreg=cbind(
                    AdLag0 = rep(8,20), 
                    AdLag1 = c(Advert[40,1], rep(8,19))))

autoplot(fc8) + ylab("Quotes") +  ggtitle("Forecast quotes with future advertising set to 8")

rm(Advert, fit, fc8)
```


# Forecasting hierarchical or grouped time series

When want, e.g. forecasts for Australia wide and each state, with state forecasts adding up to Australia wide forecast. 
<!--
## Hierarchical time series
## Grouped time series
## The bottom-up approach
## Top-down approaches
## Middle-out approaches
## Mapping matrices
## The optimal reconciliation approach
-->

# Advanced forecasting methods

## Complex seasonality

* High frequency data can have multiple seasonal patterns:

Data frequency | Possible patterns
-----------------| ----------------------------
Daily Data | Weekly, Anual
Hourly Data | Daily, Weekly, Annual
Weekly | Annual (period = ~52.179)

* `ts` can handle only one type of seasonality, which is usally assumed to take __integer__ values
* Methods: 

    * `msts` for multiple seasonality time series and non-integer frequencies.  Retruns mulitple seasonal components, trend and remainder components.  Ensure to consider vertical scales when interpretign charts.  To forecast, use `stlf()` to:
        * Forecast each seasonal foreast using seasonal naive method
        * Use ETS (or othe rmethod) for seasonally adjusted data
    * Dynamic harmonic regression with multiple seasonal periods; add Fourier terms for each seasonal period
    * TBATS model (Fourier terms with an exponential smoothing state space model and Box Cox trasnformation in automated model); unlike dynamic harmonic regression allows seasonality to change slowly over time.  Disadvantage is that is can be slow to estimate and does not allow covariates (unlike harmonic regression)
    
**Example**: Call arrivals per 5 minute interval each weekday between 7:00am and 9:05pm for 33 weeks

* Daily seasaonality has frequency = 169
* Weekly seasonality has frequency = 845


```{r call_freq_calcs, eval=FALSE, include=FALSE}
md <- as.duration(ymd_hm("2018-09-07 07:00") %--% ymd_hm("2018-09-07 21:05"))/dminutes(1)/5
mw <- md * 5
```
Data: 
```{r call_view, message=FALSE, warning=FALSE}
# All data
p1 <- autoplot(calls) +
  ylab("Call volume") + xlab("Weeks") +
  scale_x_continuous(breaks=seq(1,33,by=2))

# 4 weeks
p2 <- autoplot(window(calls, end=4)) +
  ylab("Call volume") + xlab("Weeks") +
  scale_x_continuous(minor_breaks = seq(1,4,by=0.2))

gridExtra::grid.arrange(p1,p2)

rm(p1, p2)
```
Decomposition using STL with multiple seasonal periods model; note that trend and weekly seasonality explain much less of the variation than daily seasonality.

```{r calls_decompose_mstl}
calls %>% mstl() %>% autoplot() + xlab("Week")
```

Forecast using decomposition:
```{r calls_decompose_forecast}
calls %>% stlf() %>% autoplot() + xlab("Week")
```

Dynamic harmonic regression with multiple seasonal periods, using log transformation to ensure forecasts and prediction intervals remain positive:

TODO: Understand why 10,10 chosen for fourier.

This model is EXTREMELY slow....
```{r calls_harmonic_reg, eval=FALSE, include=FALSE, echo = TRUE}
fit <- auto.arima(calls, seasonal=FALSE, lambda=0, xreg=fourier(calls, K=c(10,10)))

fit %>%
  forecast(xreg=fourier(calls, K=c(10,10), h=2*169)) %>%
  autoplot(include=5*169) +
    ylab("Call volume") + xlab("Weeks")
```


TBATS example provided.


**Example**: Electricity demand modelling

Aim: Forecast electricity demand based on:
    * Historical demand over 1/2 hourly intervals  (1year)
    * Temperatures

Data:
```{r elecdemand_view2}
p1 <- autoplot(elecdemand[,c("Demand","Temperature")],
    facet=TRUE) +
  scale_x_continuous(minor_breaks=NULL,
    breaks=2014+
      cumsum(c(0,31,28,31,30,31,30,31,31,30,31,30))/365,
    labels=month.abb) +
  xlab("Time") + ylab("")

p2 <- elecdemand %>%
  as.data.frame() %>%
  ggplot(aes(x=Temperature, y=Demand)) + geom_point() +
    xlab("Temperature (degrees Celsius)") +
    ylab("Demand (GW)")

gridExtra::grid.arrange(p1, p2, nrow = 1)
rm(p1, p2)
```

Model: Regression with piecewise linear function of termperature (with knot at 18 degrees) and harmonic regression terms to allow for daily seasonal pattern

TODO: Understand this after reviewing regression chapter.
```{r elecdemand_plf, eval=FALSE, include=FALSE}
# Get temperatures of 18 and above; set to 18 is temperature below 18 

cooling <- pmax(elecdemand[,"Temperature"], 18)

fit <- auto.arima(elecdemand[,"Demand"],
         xreg = 
             cbind(fourier(elecdemand, c(10,10,0)),
               heating=elecdemand[,"Temperature"],
               cooling=cooling))
```

Forecasts require  meteorological model forecasts or scenario forecasts.  Here use repeat of last 2 days

TODO: Review and understand: 
```{r elecdemand_plf_forecast, eval=FALSE, include=FALSE}
temps <- subset(elecdemand[,"Temperature"],
          start=NROW(elecdemand)-2*48+1)
fc <- forecast(fit,
        xreg=cbind(fourier(temps, c(10,10,0)),
          heating=temps, cooling=pmax(temps,18)))
autoplot(fc, include=14*48)
```



TODO: Read following sections in more detail:

## Vector autoregressions

Allow for bidirectional relationship between forecast and predictor variables.  i.e. that predictor infleunces forecast variable which then adjusts the predictor variable.  


## Neural network models

* Allow complex nonlinear relationships between response variable and predictors.

* Lagged values of the time series can be used as inputs to a neural network (similar to linear autorgression model); called neural network autoregression or NNAR model.

## Bootstrapping and bagging

Can simulate time series (generate new series similar to observed one) by:

* Box-Cox transform series
* Decompose into trend, seasonal and remainder coponents using STL
* Bootstrap remainder series (but because there may be autocorrelation within STL remainder, use "blocked bloostrap", where contiguous sections of TS selected at random and joined together)
* Add bootstrapped remainder to trend and seasonal compoennts
* Reverse Box-Cox transformation

Advantage:

* Better measure of forecast uncertainty; most prediction intervals too narrow
* Improve point forecast using "bagging"

Sources of uncertainty:

* Random error term (only term accounted for in prediction intervals)
* Parameter estimates (bootstrapping incorporates this as welll)
* Choice of model
* Continuaction of process into future


```{r bootseries, message=FALSE, warning=FALSE}
bootseries <- bld.mbb.bootstrap(debitcards, 10) %>%
  as.data.frame() %>% ts(start=2000, frequency=12)

autoplot(debitcards) +
  autolayer(bootseries, colour=TRUE) +
  autolayer(debitcards, colour=FALSE) +
  ylab("Bootstrapped series") + guides(colour="none")
```

Method of using bootstrapped samples to obtain better idea of forecast accuracy and prediction interval:

* Fit model to each series; model will likely be the same but parameters will likely differ
* Use mean (point forecast) and quantiles (prediction intervals) of each simulated sample path to form point forecasts and prediction intervals



# Some practical forecasting issues
## Weekly, daily and sub-daily data
## Time series of counts

* Methods to date assume continuous sample space, which is fine if counts > 100.
* Small counts need to use alternative model (outside scope of book)
* Croston's method: Transform data into:
    * Demand ($q$)
    * Time between demand (inter-arrival time) ($a$)

## Ensuring forecasts say within limits

* To impose positivity, specify Box-Cox parameter $\lambda = 0$.  Set `biasadj = TRUE` to ensure forecasts are the means fo the fore distirbutions
* To impose that data is within interval $(a, b)$, 
    * Map data to whole real line using scaled logit transform \[y = \log \left(\frac{x-a}{b-x}\right)\]
    * Fit model
    * Transform back using:
    \[x = \frac{(b-a)e^y}{1 + e^y} + a\]

## Forecast combinations

By averaging the forecast values of many models get best results

## Prediction intervals for aggregates

Eg: To forecast next month's sales based on weekly data

* If point forecasts are means, adding up will give good point estimate
* Due to correlatinos between forecast errors, prediction intervals tricky>
* Approach:
    * Forecast many (1000) future sample paths for next 4 weeks
    * Take mean of total forecasts for point estimate
    * Take quantil for prediction intervals
    
## Backcasting

Backcasting = forecasting time series in reverse

## Very long and very short time series

No rule of thumb for what is short (some say < 30) or long (? > 200)


## Forecasting on training and test sets

is linear interpolated. More sophisticated approaches in `imputeTs` package.

## Dealing with missing values and other outliers

### Missing values
* If missingness is not random, need to account. E.g. store closed on public holiday, but next day may have more sales.  Use indicator variable to state whether day is holiday or not. 
* Some methos allow for missing values, for those that don't (`ets()`, `stlf()` and `tbats()`):
    * Take the seciton of data after hte last mising vlue
    * Replace missing value with estimates, e.g. using `na.interp()`.  For non-seasonal data, simple linear interpolation is used to fill in the missing sections; for seasonal data, an STL decomposition is used to estimate the seassonal component and then the seasonally adjsuted series 
    
### Outliers

* May be error or simply unusual
* Impact how well methods work
* Best to replace with missing values or with a more consistent estimate
* HOWEVER, best not to replace without thinking why they have occurred as may provide useful information about process that produced them (which should be taken into account when forecasting)
* If genuine error, or won't occur in forecasting period, then replacing them is the best option
* `tsoutliers()` function designed to identify ouliers and suggest potential replacement values.
* `tsclean()` identifies and replaces outliers and replaces missing values.  Should be used with caution.  
